{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/favicon.ico","path":"favicon.ico","modified":1,"renderable":0},{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"themes/maupassant/source/js/fancybox.js","path":"js/fancybox.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/css/jquery.fancybox.css","path":"css/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/maupassant/source/css/style.scss","path":"css/style.scss","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/codeblock-resizer.js","path":"js/codeblock-resizer.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/share.js","path":"js/share.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/totop.js","path":"js/totop.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/smartresize.js","path":"js/smartresize.js","modified":1,"renderable":1}],"Cache":[{"_id":"themes/maupassant/.travis.yml","hash":"f8da426b97088e4caa5226cff219a5d95087961f","modified":1473044478261},{"_id":"themes/maupassant/README.md","hash":"3a0a67b3afc676d7080c3c68e6accecfeab05303","modified":1498804494569},{"_id":"themes/maupassant/.gitignore","hash":"2d5c5d52dc40e1c75c1c563baa923694c421e599","modified":1498805070880},{"_id":"themes/maupassant/package.json","hash":"04ca87ea475f37841d3610fe5806feb8022416c3","modified":1473044478393},{"_id":"source/favicon.ico","hash":"67b8e2158a7df3267399bf9b0ad94aab8fa7888b","modified":1473078132303},{"_id":"source/CNAME","hash":"0a3210ffb33e2dbc4eaf2a4efcdd736ad203462f","modified":1473080914447},{"_id":"themes/maupassant/_config.yml","hash":"a93cdd51e6304f4ec72b7b0eeb39a71330a1852d","modified":1500448093157},{"_id":"themes/maupassant/LICENSE","hash":"0663fd3a7ea9fc4f4c634b4d73e2da426b536f86","modified":1498804494557},{"_id":"source/_drafts/2016-10-18-景区位置服务项目说明文档.md","hash":"8037920dda77801e76a3dd7f2a80ea9449cca8eb","modified":1509451014260},{"_id":"source/_drafts/位置服务说明文档.md","hash":"5689ef09dff0340a95b8364d42ca8eca8d50c931","modified":1482137072831},{"_id":"source/about/index.md","hash":"7e1521acb503856814e9cb6c3742375ba472881d","modified":1473080492991},{"_id":"source/_posts/2016-05-20-Scala学习资料总结.md","hash":"f07a496102f242a9db7903fc4044e47c695c76c9","modified":1475314907041},{"_id":"source/_posts/2016-06-13-Scala中-的用法.md","hash":"2398d0f1a5252c96ef9eb77396c43a4c87eb399e","modified":1509450972864},{"_id":"source/_posts/2016-06-13-使用scaladiagrams工具构建scala项目的UML图.md","hash":"be9cefdb21070f9ce24c1109cd2f5e7487006821","modified":1473817932848},{"_id":"source/_posts/2016-08-15-spark支持snappy压缩踩坑总结.md","hash":"eb5184d1812e8cd6d239a31b4330ea2f39c99a3d","modified":1474194748138},{"_id":"source/_posts/2016-09-05-在Kerberos环境下配置hue通过spark-thrift-server访问SparkSql.md","hash":"484584e4cda032a3c9c8c57a27f8d0f355e45893","modified":1473751382559},{"_id":"source/_posts/2016-09-06-kerberos下spark客户端的配置.md","hash":"2804f3b556692444cfbaf27459694faa574a7b0b","modified":1474189240812},{"_id":"source/_posts/2016-09-07-Spark-Web与history测试.md","hash":"d4c379f8c9e4f0144d64ad35def2b68a3e9744de","modified":1473215598620},{"_id":"source/_posts/2016-09-08-Spark在Kerberos环境下指定任意用户在yarn上提交任务.md","hash":"fe2033b1b173faca9f9ef2b661e15989c1d4f8f6","modified":1473315297473},{"_id":"source/_posts/2016-09-13-livy-server初探1——简介与提交脚本以及LivyServer类.md","hash":"af772290ebf09040c5d315558d770c3f647cc0ed","modified":1474872282956},{"_id":"source/_posts/2016-09-18-Spark在Kerberos下连接使用Hbase的配置.md","hash":"53be40f3f4d00cc9ea1e8556eb183a31b5cc437c","modified":1474192704636},{"_id":"source/_posts/2016-09-18-Spark在yarn中的资源申请与分配.md","hash":"ce0a31777c5174f1be51f97170c95c3a2272d229","modified":1474526524832},{"_id":"source/_posts/2016-09-23-scala中-的用法.md","hash":"5ddbcee017a417aa5e05d58804c82de15c0dd643","modified":1474870579924},{"_id":"source/_posts/2016-10-18-Java中的“钩子”.md","hash":"474f210ccf94b31d1dfd20f384069ac4c73e5712","modified":1476790443743},{"_id":"source/_posts/2016-10-18-flume的http监控参数说明.md","hash":"dcd395c5cf14df260c6daca7d8e1eebf1cba57aa","modified":1476790229383},{"_id":"source/_posts/2016-10-27-记HashMap遇到的java-util-ConcurrentModificationException的bug.md","hash":"59d09452190541a9c8c1e2738f81f82ccf273990","modified":1479975740615},{"_id":"source/_posts/2016-10-18-景区位置服务项目说明文档.md","hash":"8037920dda77801e76a3dd7f2a80ea9449cca8eb","modified":1498656553096},{"_id":"source/_posts/2016-10-31-scala通过slick连接数据库.md","hash":"36650fb9bb269495eb59ff08fe09e6219c8fe838","modified":1479978321561},{"_id":"source/_posts/2016-11-24-Spark踩坑之Streaming程序实时读写哨兵模式的Redis.md","hash":"618182ca12cba543035230c91a5b09a1407d08c1","modified":1495437486184},{"_id":"source/_posts/2016-11-24-Spark踩坑之Streaming程序实时读取数据库.md","hash":"e2d247fb0c8599eb1f5acceb9a9d367f7b37ff7f","modified":1479969091966},{"_id":"source/_posts/2016-11-24-Spark踩坑之Streaming在Kerberos的hadoop中renew失败.md","hash":"e859e8b788029e43333cec259c2bb64b15f7db82","modified":1513305791144},{"_id":"source/_posts/2016-11-24-通过经纬度计算距离算法的scala实现.md","hash":"aea54f9fc2ff47952c7eead8db4897508a9f2075","modified":1498654373157},{"_id":"source/_posts/2016-12-19-spark中读取hdfs文件简记.md","hash":"7659dc0d89530c68f8770622f313b762d177a497","modified":1482116851009},{"_id":"source/_posts/2016-11-29-桂花真香.md","hash":"812ac697465f91ff6cbc3c835b4556b463fd1c39","modified":1484134077685},{"_id":"source/_posts/2017-01-03-Spark源码阅读之——StreamingContext详解.md","hash":"9bb76f41cf22a7340425fd455513685550b6dc0f","modified":1483693658608},{"_id":"source/_posts/2017-01-11-给未来的自己.md","hash":"f20e1a18a6651d9c9967c025398cebae0592d130","modified":1484136000467},{"_id":"source/_posts/2017-01-09-大数据团队scala代码规范.md","hash":"a6017640ff50e0588d97d49fd2a699bf8257b08a","modified":1483956635793},{"_id":"source/_posts/2017-01-11-天鹅湖之暮.md","hash":"eb44b669bbbfb639a807c68b78ef302686120c78","modified":1484134023166},{"_id":"source/_posts/2017-01-12-E20914023王小刚学期总结.md","hash":"dcd99a08a81cfcc52106acd88a0060b5f679ad28","modified":1484221212258},{"_id":"source/_posts/2017-01-12-羁绊.md","hash":"a6ab47f238794260f50bcbaaadda6f1cd1cba355","modified":1484221078622},{"_id":"source/_posts/2017-02-24-Livy-server的搭建与简单测试.md","hash":"e914cbb3130f4835ea76aeb18de51b997471d087","modified":1487933277357},{"_id":"source/_posts/2017-02-28-spark-on-yarn-作业提交源码分析.md","hash":"99c663d0f7d482fd553a810cee69c6572d3bb95c","modified":1492497076487},{"_id":"source/_posts/2017-03-10-Apache-Eagle定义一个Application.md","hash":"250574cd0498fc0c3c3fa3eb7a0398326ddc4e20","modified":1490320445847},{"_id":"source/_posts/2017-03-23-flume1-7使用KafkaSource采集大量数据.md","hash":"032f4f41874206ab09d89b910af34ab92da65e2c","modified":1490239093222},{"_id":"source/_posts/2017-03-13-阿里巴巴Java开发手册学习笔记.md","hash":"7f5c8e90ab11b113cd0715b8af8af8f62344799b","modified":1490346240718},{"_id":"source/_posts/2017-03-29-OpenApi之我浅薄见解.md","hash":"74b52ca08b3c258edc0a33ada2185f488db0b6f1","modified":1490968960954},{"_id":"source/_posts/2017-03-24-阿里巴巴Java开发手册学习笔记2.md","hash":"323769e4e9228fdaa525eee5d67df226d3c2b64a","modified":1490346435878},{"_id":"source/_posts/2017-05-22-OpenAPI微服务接入规范.md","hash":"25e84c43acb1ce138031e4617291cf45b6b31ab4","modified":1495524032549},{"_id":"source/_posts/2017-06-22-spark奇技淫巧总结之flatMap.md","hash":"ceb67240a8f4b1c2b886dfbdc5d5ee0d8d483213","modified":1498636207299},{"_id":"source/_posts/2017-06-28-位置服务开发上线总结.md","hash":"9868b545a372e860cd624339100f2eb629a0b716","modified":1498736302421},{"_id":"source/_posts/2017-06-29-返回区域实时人数的思路与总结.md","hash":"3ef7974c11af365409f5876a5bceb11e79ebc877","modified":1498736249146},{"_id":"source/_posts/2017-07-17-spark往ES中写入数据的方法.md","hash":"5a7c0e2fea1d69d2b2c3d16f17a9e63b0f10bbe4","modified":1500365489391},{"_id":"source/_posts/2017-07-17-spark指定java版本向yarn提交程序.md","hash":"a3552744fe8113ca0f5c0ab70c655e3f9af79260","modified":1500356940494},{"_id":"source/_posts/2018-08-17-菜鸟如何单排上王者.md","hash":"e711ade27d0fa94efb89cf11c55e9063f5c47b94","modified":1535452580222},{"_id":"source/timeline/index.md","hash":"38fe9d86286e179d74ba3ae75765c46bc7eff36b","modified":1500366208683},{"_id":"themes/maupassant/.idea/encodings.xml","hash":"adbda6ae60cd3b219acbd2aa8dfa3de9548b737c","modified":1498803755271},{"_id":"source/_posts/2017-10-31-使用Ansable安装管理Spark客户端.md","hash":"9e275640d54860677ff5d7934053a53798c57e2f","modified":1515570869807},{"_id":"themes/maupassant/.idea/compiler.xml","hash":"bff5196ea91a033d64bb5c4d6647a2e3b71bb548","modified":1498803755254},{"_id":"themes/maupassant/.idea/maupassant.iml","hash":"980957b57c4f1eae5e85d664d8375f83d47d3e5a","modified":1498803755181},{"_id":"themes/maupassant/.idea/misc.xml","hash":"0e7b5250b90e318b2ac16f64934128d05c0ac8f3","modified":1498803755216},{"_id":"themes/maupassant/.git/FETCH_HEAD","hash":"ab0b9d73554cafe066d1b5084b1e03fe8df29f2d","modified":1498804494466},{"_id":"themes/maupassant/.idea/vcs.xml","hash":"e692344d8c981d4806a673b29b1a6dc3b6b6d345","modified":1498804514100},{"_id":"source/_posts/2018-01-08-spark2-2新版本编译打包.md","hash":"597b71979cb4089ca724d60f89132048fa37ff0c","modified":1515486339799},{"_id":"themes/maupassant/.idea/modules.xml","hash":"0b1c44c5cdf031f86132e25fd7c387003ee1080e","modified":1498803755287},{"_id":"themes/maupassant/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1473044472694},{"_id":"themes/maupassant/.git/config","hash":"fb9cab07ffe3866880dd7f679478429b9de1d5c3","modified":1473044478237},{"_id":"themes/maupassant/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1473044478209},{"_id":"themes/maupassant/.git/ORIG_HEAD","hash":"9138f76c9461186fe7e14add27270f388fdabe2f","modified":1498804494541},{"_id":"themes/maupassant/.git/index","hash":"71b259c76693b5a5afc76ff84d7f42109da07f86","modified":1498807819890},{"_id":"themes/maupassant/.git/packed-refs","hash":"53e4275068b4dbaa003c4f6e8f4eeaa4ed2360d0","modified":1473044478193},{"_id":"themes/maupassant/languages/de-DE.yml","hash":"25d1d8cd8113045a7603c14af1ea1539fc6456ed","modified":1473044478289},{"_id":"themes/maupassant/languages/fr-FR.yml","hash":"3a50568f200b9c1258415b53727e42c6b6c7ea0b","modified":1473044478301},{"_id":"themes/maupassant/languages/es-ES.yml","hash":"3cc9312fbdba4a8f8e8254804121e4724c719bcc","modified":1473044478297},{"_id":"themes/maupassant/.idea/workspace.xml","hash":"613d0985cc82d1127f5eb01a3af5fc1c50bdfe28","modified":1498812137513},{"_id":"themes/maupassant/languages/ko.yml","hash":"a454bcec60113507bc1d593a699849822386c196","modified":1473044478301},{"_id":"themes/maupassant/languages/ru.yml","hash":"36edc014c6aaef367d58929089bf7915375e71a6","modified":1498804494586},{"_id":"themes/maupassant/languages/en.yml","hash":"8574e8c36f4c72118f27f7ff7353b2a9ceda2a9b","modified":1498804494581},{"_id":"themes/maupassant/languages/zh-CN.yml","hash":"fc32c57220ecc58cb1920623105ed8901bc4cbcf","modified":1498804494592},{"_id":"themes/maupassant/layout/index.jade","hash":"a266b098acb8ac4bf1ff6fdf9d14686b38e128af","modified":1498807818909},{"_id":"themes/maupassant/languages/zh-TW.yml","hash":"34dba7ac67aeb316f629ca73e546fa143cc362d5","modified":1498804494598},{"_id":"themes/maupassant/layout/archive.jade","hash":"d70d1ed241b1a431a2434768d02bb2de03070f70","modified":1473044478365},{"_id":"themes/maupassant/layout/base-without-sidebar.jade","hash":"5901de9f23358158079a02c6470a491cf36ffbf2","modified":1498804494656},{"_id":"themes/maupassant/layout/base.jade","hash":"513c2b66a327fc281917f4a5c567f93a39ecb96e","modified":1498804494662},{"_id":"themes/maupassant/layout/post.jade","hash":"52fd7c4cd2518500a4ddc00cfcc109abe0dad4d2","modified":1498804494673},{"_id":"themes/maupassant/layout/page.jade","hash":"1e98aa785073dc1bbfe70f2863bc19fe287483b9","modified":1473080879338},{"_id":"themes/maupassant/layout/timeline.jade","hash":"31f2826d86201c9a86e00d42690f26473ab14363","modified":1498804494677},{"_id":"themes/maupassant/layout/single-column.jade","hash":"4be14ac93c154216e174fe0c4c05608c26f59c42","modified":1473044478385},{"_id":"source/_posts/2016-09-07-Spark-Web与history测试/spark-yarn-web1.png","hash":"77c0b873948474107c51309f35c9dd8052ff63a6","modified":1473214990825},{"_id":"themes/maupassant/layout/poetry.jade","hash":"04c81e87d924933e3ffc5860872afcb7834cc159","modified":1498803981954},{"_id":"source/_posts/2016-09-18-Spark在yarn中的资源申请与分配/spark-yarn-allocation.png","hash":"4be9f85c2ad8ba6649e8d8f28f9ea4697e96e236","modified":1474199181606},{"_id":"source/_posts/2016-09-07-Spark-Web与history测试/spark-yarn-web2.png","hash":"8982e8f028573d635be83cf54fe24102824e1795","modified":1473215049379},{"_id":"source/_posts/2017-05-22-OpenAPI微服务接入规范/open-api1.png","hash":"10ca65c84eb74cecb6e7d96097a92f00e01770e8","modified":1495438510526},{"_id":"source/_posts/2016-10-18-Java中的“钩子”/hook.jpg","hash":"44ef4ca173f0349aa0813297711859395d51795e","modified":1464342268104},{"_id":"themes/maupassant/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1473044472714},{"_id":"themes/maupassant/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1473044472706},{"_id":"themes/maupassant/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1473044472738},{"_id":"themes/maupassant/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1473044472730},{"_id":"themes/maupassant/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1473044472758},{"_id":"themes/maupassant/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1473044472750},{"_id":"themes/maupassant/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1473044472786},{"_id":"themes/maupassant/.git/logs/HEAD","hash":"932e9d1cad089b7d3121c21854e83db2fcdfcc5a","modified":1498804494726},{"_id":"themes/maupassant/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1473044472806},{"_id":"themes/maupassant/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1473044472798},{"_id":"themes/maupassant/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1473044472774},{"_id":"themes/maupassant/.git/refs/stash","hash":"1f6777f126b30fadbc6276f83c4dc0ff0e463f39","modified":1498804486351},{"_id":"themes/maupassant/source/js/fancybox.js","hash":"8a993c1c4ad40789d2960b682cb2130382a0f26a","modified":1473044478417},{"_id":"themes/maupassant/source/css/jquery.fancybox.css","hash":"0d142e11e27e9de1a325c32369c42254101ddd34","modified":1473044478397},{"_id":"themes/maupassant/source/css/style.scss","hash":"bd85a22ca5c78e64721998efca3cffa84f7ffe35","modified":1498808335320},{"_id":"themes/maupassant/source/js/codeblock-resizer.js","hash":"c77270e684a60babc1abb7353e700ecdc5a66d30","modified":1473044478413},{"_id":"themes/maupassant/source/js/share.js","hash":"60fa9e1da0fcf47381ea31275b9c010006225d7a","modified":1498804494710},{"_id":"themes/maupassant/source/js/search.js","hash":"dbda07a03e6edc73f1dc28a068c24a6037b97b56","modified":1498804494699},{"_id":"themes/maupassant/layout/_partial/head.jade","hash":"855ab59e813d0e8eab2387d587a58b0ebbc330b1","modified":1498804494651},{"_id":"themes/maupassant/source/js/totop.js","hash":"15de186b089c245fe60766d509b587919f05ff23","modified":1473044478441},{"_id":"themes/maupassant/layout/_partial/footer.jade","hash":"92aa15e813bfb411803cc54218feb5410469a9c2","modified":1498804494646},{"_id":"themes/maupassant/layout/_partial/helpers.jade","hash":"9e44f6d32f2449b4109c33118f8285fa2fc7b023","modified":1473044478325},{"_id":"themes/maupassant/layout/_partial/paginator.jade","hash":"03ad0c49ae6f8a999ae35b38d08e25775f51f52a","modified":1473044478337},{"_id":"themes/maupassant/layout/_partial/mathjax.jade","hash":"ac6e3a92bf18ab6bbd0e041b6796b295bae963ee","modified":1473044478329},{"_id":"themes/maupassant/layout/_partial/mathjax2.jade","hash":"234a792e64ba208fa97d2f99772ece23056a53ec","modified":1473044478333},{"_id":"themes/maupassant/layout/_partial/post_nav.jade","hash":"b11d9e6000449838b17f508429f29ffb60f53096","modified":1473044478341},{"_id":"themes/maupassant/layout/_partial/after_footer.jade","hash":"65c496ac99ca6f0779454568b7c03ae5834c6160","modified":1498804494614},{"_id":"themes/maupassant/layout/_partial/tag.jade","hash":"6145b483b271bba05ad1db7c039fe352a768215b","modified":1473044478345},{"_id":"themes/maupassant/layout/_partial/totop.jade","hash":"eb91a3baf9411188c7c8130f63a674f541ca9c81","modified":1473044478345},{"_id":"themes/maupassant/layout/_widget/links.jade","hash":"2a1dd6a0e2befd073e3347347994d8dcd2047879","modified":1473044478353},{"_id":"themes/maupassant/layout/_widget/category.jade","hash":"7707b4c718a935882ee986d0bb0078e50cdbea64","modified":1473044478349},{"_id":"themes/maupassant/layout/_widget/recent_comments.jade","hash":"0b9e78e7f20bc69690e05f2e0c956bded9058a84","modified":1473044478353},{"_id":"themes/maupassant/layout/_widget/recent_posts.jade","hash":"770b6c41cbf7969ed33adf87eec3be6f50a0911b","modified":1473044478357},{"_id":"themes/maupassant/source/js/smartresize.js","hash":"150ab1cad40d7ae081b0896b13f7d7cbac4e6338","modified":1473044478437},{"_id":"source/_drafts/位置服务说明文档/1.png","hash":"c9389b9d6f07951b9f4f51cd432148f4db59dbd4","modified":1476772118927},{"_id":"source/_drafts/2016-10-18-景区位置服务项目说明文档/1.png","hash":"c9389b9d6f07951b9f4f51cd432148f4db59dbd4","modified":1476772118927},{"_id":"source/_posts/2016-06-13-使用scaladiagrams工具构建scala项目的UML图/spark_storage_part.png","hash":"79f54054e3d8717759b328dbf4c2d385352a7c48","modified":1465970298637},{"_id":"source/_posts/2016-09-07-Spark-Web与history测试/spark-yarn-web3.png","hash":"960b5197d706cca538bf7fec7d3dcd5d524ab1f9","modified":1473215364629},{"_id":"themes/maupassant/layout/_widget/search.jade","hash":"c5301ca7f2ec24d790fd7f35f69f8fd8acbdb725","modified":1473044478361},{"_id":"themes/maupassant/layout/_widget/tag.jade","hash":"37f236365b153fc40324391e5a602d6d50014e18","modified":1473044478365},{"_id":"source/_posts/2017-01-03-Spark源码阅读之——StreamingContext详解/040.png","hash":"4c9e1b6b257fa50bcfe835d13a1c3af0f3a6383e","modified":1483517308491},{"_id":"source/_posts/2017-03-10-Apache-Eagle定义一个Application/eagle_1.png","hash":"eb9bf94e71d858b5a606874210496b3eda5b47b9","modified":1489545095162},{"_id":"source/_posts/2017-06-28-位置服务开发上线总结/wzfw2_1.png","hash":"93b52c89bf4ff62d7cd6e807cfdec9d21d8cb3f2","modified":1498639304065},{"_id":"source/_posts/2017-06-29-返回区域实时人数的思路与总结/wzfw2_1.png","hash":"93b52c89bf4ff62d7cd6e807cfdec9d21d8cb3f2","modified":1498639304065},{"_id":"themes/maupassant/layout/_partial/comments.jade","hash":"974c3448b5e32d91c745a5c3c66260ab32683da3","modified":1498804494640},{"_id":"source/_posts/2016-05-20-Scala学习资料总结/idea_scala.png","hash":"8db1512f431b5239d79664ce642e2a3bda188463","modified":1463731938943},{"_id":"source/_posts/2016-06-13-使用scaladiagrams工具构建scala项目的UML图/spark_storage_part2.png","hash":"76762272c641e5b6b45008d0073441a88f62740e","modified":1465972098476},{"_id":"source/_posts/2017-06-28-位置服务开发上线总结/wzfw2_2.png","hash":"09720cf99c1d1385d5939315141e8b214e667fba","modified":1498726882971},{"_id":"source/_posts/2017-06-28-位置服务开发上线总结/wzfw2_3.png","hash":"af8948c24c5e4094c931aa02741207c11bc2abf3","modified":1498728163495},{"_id":"source/_posts/2017-06-29-返回区域实时人数的思路与总结/wzfw2_2.png","hash":"09720cf99c1d1385d5939315141e8b214e667fba","modified":1498726882971},{"_id":"source/_posts/2017-06-29-返回区域实时人数的思路与总结/wzfw2_3.png","hash":"af8948c24c5e4094c931aa02741207c11bc2abf3","modified":1498728163495},{"_id":"themes/maupassant/.git/logs/refs/stash","hash":"2e43c72f5a5ac901611eeb4b8a4e7f66e84ad891","modified":1498804486359},{"_id":"themes/maupassant/.git/objects/15/05a7de46ebb0337c7e383dc72cd7b6d33200f5","hash":"fd58feecc8c576c6e755cccfffba10090fa92b86","modified":1498804514114},{"_id":"themes/maupassant/.git/objects/24/5916c7db75d732b5af2477353676f397358c5e","hash":"60d8b9f5c5769dde529de853df1fa8c8d8ef171d","modified":1498805092411},{"_id":"themes/maupassant/.git/objects/24/db651195da33c7ef5a2fdc6702e2119650acd5","hash":"50786c07ad587e8d838507a526b0e2644ac13d5a","modified":1498804485692},{"_id":"themes/maupassant/.git/objects/23/b548e9168d588d55e02d951e8295b9c586a779","hash":"90126d5972f998f36a59904b876b275a6a8a1321","modified":1498805092398},{"_id":"themes/maupassant/.git/objects/2f/a061ac667c5e0eb6901551c9fb0d803b7d5baf","hash":"e0af9ce2b4c00676f94902474690b664b045749d","modified":1498805092405},{"_id":"themes/maupassant/.git/objects/35/eb1ddfbbc029bcab630581847471d7f238ec53","hash":"3d55863c44c84e5bd27d6a8f12fd230399173996","modified":1498803908887},{"_id":"themes/maupassant/.git/objects/42/399049e7b61cf4665f9a802515f71942f6bdd7","hash":"eed10f698b86259d5584102fcc6743d93558678a","modified":1498804486023},{"_id":"themes/maupassant/.git/refs/heads/master","hash":"fced5a8b032d8f23541d7da5779d9c0f369610c5","modified":1498804494719},{"_id":"themes/maupassant/.git/objects/43/b299a5c1e222e60d7d4eeb5cf60c92878f9c20","hash":"2d9e9d40ed5692cda9084dd393d480fc333b490c","modified":1498804514131},{"_id":"themes/maupassant/.git/objects/54/c290f8ec1b973a436070d2f45510edd1438d05","hash":"849be2295f930bd037479af2f1283bf163b24f60","modified":1498805187892},{"_id":"themes/maupassant/.git/objects/87/f60b798d5d37eab70d7b705f279953dafd8fb0","hash":"8751552a49cf26805041510bb21a8d46fa872874","modified":1498804485930},{"_id":"themes/maupassant/.git/objects/b8/db94299aeac634a1a611154adb904b7dbad37c","hash":"1b312c7884cbb296c7673d2d4081b4ca20bd3d7d","modified":1498804485570},{"_id":"themes/maupassant/.git/objects/93/af4d07138cd69ea3b183c57fa39c26b9a008c8","hash":"de0f082b464f2a735c3e93faacd166b311ff5683","modified":1498804485941},{"_id":"themes/maupassant/.git/objects/6f/5b6e0498af62cc02e52630c227d816ede11869","hash":"ebb49f77aeb0e6d89346ae3bf24c873729e4d2fc","modified":1498804485564},{"_id":"themes/maupassant/.git/objects/cf/55c458a34096a0a0b640a8aa8c6effcd63321a","hash":"88699feba646ffe3f2c20fcccb88fb6ad4225e4f","modified":1498804486017},{"_id":"themes/maupassant/.git/objects/c5/676598ae2cd8900d383fdcc73e6752edb249c7","hash":"52170f9e068869649b89fb0df28fb9ae466fe3e6","modified":1498804486029},{"_id":"themes/maupassant/.git/objects/bf/a6a22a524852a8df1d6462f0323d20449c4e95","hash":"e0f859a9395ed5b8a7b8512f90c0120ecdb204de","modified":1498803783061},{"_id":"themes/maupassant/.git/objects/d0/1438c8a111fdc7382a58a8c7d32d1036f24bf1","hash":"c72402baf1d1015e048ee6c0c86275b65169518e","modified":1498804486289},{"_id":"themes/maupassant/.git/objects/pack/pack-c2907028ceaaa77d8d138c3d41cf61adecbb5128.idx","hash":"058353916ea72049bc2d9252b872450cbafde7a0","modified":1498804494393},{"_id":"themes/maupassant/.git/objects/pack/pack-c2907028ceaaa77d8d138c3d41cf61adecbb5128.pack","hash":"fb4caa6d51a05937bfe7dd6a021c10a2bdb74fd2","modified":1498804494392},{"_id":"themes/maupassant/.git/objects/pack/pack-c6dba05953390f50b2ba089c0eaf8ffc5aeb7fb8.idx","hash":"a78229e7589715a8522ce9c0ac1d8d3410a955dd","modified":1473044478025},{"_id":"themes/maupassant/.git/refs/remotes/origin/master","hash":"fced5a8b032d8f23541d7da5779d9c0f369610c5","modified":1498804494456},{"_id":"themes/maupassant/.git/logs/refs/heads/master","hash":"932e9d1cad089b7d3121c21854e83db2fcdfcc5a","modified":1498804494724},{"_id":"themes/maupassant/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1473044478205},{"_id":"themes/maupassant/.git/logs/refs/remotes/origin/master","hash":"93d88b1928452b706d37edd9853306115b48f180","modified":1498804494462},{"_id":"themes/maupassant/.git/logs/refs/remotes/origin/HEAD","hash":"eb3fdb343960f4aacf5762593608ed178cfc117f","modified":1473044478201},{"_id":"source/_posts/2018-08-17-菜鸟如何单排上王者/wangzhe_1.png","hash":"35ab41f26e9c3cf7367f903f3d747cbc15e9371c","modified":1535452434151},{"_id":"themes/maupassant/.git/objects/pack/pack-c6dba05953390f50b2ba089c0eaf8ffc5aeb7fb8.pack","hash":"b1a85b5345201d828e89d954050b07908d070d06","modified":1473044478037},{"_id":"source/_posts/2018-08-17-菜鸟如何单排上王者/wangzhe_2.png","hash":"e7a6a64efa9b60dce76c1187438424f1726ea1a0","modified":1535452469926}],"Category":[{"name":"spark开发","_id":"cjldkwft70005psguwctee2f2"},{"name":"scala","_id":"cjldkwftz000apsgu9pzyy6eu"},{"name":"spark","_id":"cjldkwfvb000ppsgu564yv1op"},{"name":"flume","_id":"cjldkwfwn0024psgukclid3vp"},{"name":"java","_id":"cjldkwfws002dpsgubghi19zc"},{"name":"单车岁月","_id":"cjldkwfx2002vpsguck8p3rdk"},{"name":"eagle","_id":"cjldkwfxu003xpsguiu18xzrk"},{"name":"spring cloud","_id":"cjldkwfy00044psgunqa7mfuh"}],"Data":[],"Page":[{"comments":0,"_content":"hi，你好","source":"about/index.md","raw":"---\ncomments: false\n---\nhi，你好","date":"2016-09-05T13:01:32.991Z","updated":"2016-09-05T13:01:32.991Z","path":"about/index.html","title":"","layout":"page","_id":"cjldkwfst0001psgumfdieig1","content":"<p>hi，你好</p>\n","excerpt":"","more":"<p>hi，你好</p>\n"},{"title":"timeline","date":"2016-09-05T05:47:02.000Z","layout":"timeline","_content":"","source":"timeline/index.md","raw":"---\ntitle: timeline\ndate: 2016-09-05 13:47:02\nlayout: timeline\n---","updated":"2017-07-18T08:23:28.683Z","path":"timeline/index.html","comments":1,"_id":"cjldkwfsx0003psguf7bulq4z","content":"","excerpt":"","more":""}],"Post":[{"title":"位置服务说明文档","_content":"\n## 总体架构\n总体架构图如下：\n{% asset_img 1.png %}\n\n如上图：主要分为三大部分：\n#### 上游数据\n由东方国信提供各个省份的Oidd数据，发送至kafka集群\n#### 处理逻辑\n1. Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；\n2. 景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；\n\n#### 下游系统\n处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。\n\n## 处理逻辑\n### 景区用户识别端详解\n\n景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑\n分别实现如下功能：\n\n#### 配置信息读取\nkafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取\n\n#### 接收Kakfa消息流\n接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；\n\n#### 对所有的号码根据mdn生成key\n\n#### 对mdn重复数据进行去重\n注意这里是在每台不同的机器下进行的去重，而不是整体的去重\n\n#### 建立Redis连接\n在每台机器上建立Redis连接池，\n\n#### 动态读取Mysql订阅信息\n对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -> PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]\n\n#### 对源数据进行过滤，不在景区所属城市的数据丢掉\n\n#### 读取Redis中已经保存的景区用户信息列表\n从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除\n\n#### 判断数据是否需要推送\n数据是否需要推送需要满足：\n\n1. 不在Redis中：与Redis中数据进行对比，在Redis中的数据删除\n2. 在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回\"\";\n3. 更新Redis \n\n#### 数据统一发送\n将处理好的数据按照一定的规则统一批量发送至flume","source":"_drafts/位置服务说明文档.md","raw":"---\ntitle: 位置服务说明文档\ntags:\n---\n\n## 总体架构\n总体架构图如下：\n{% asset_img 1.png %}\n\n如上图：主要分为三大部分：\n#### 上游数据\n由东方国信提供各个省份的Oidd数据，发送至kafka集群\n#### 处理逻辑\n1. Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；\n2. 景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；\n\n#### 下游系统\n处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。\n\n## 处理逻辑\n### 景区用户识别端详解\n\n景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑\n分别实现如下功能：\n\n#### 配置信息读取\nkafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取\n\n#### 接收Kakfa消息流\n接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；\n\n#### 对所有的号码根据mdn生成key\n\n#### 对mdn重复数据进行去重\n注意这里是在每台不同的机器下进行的去重，而不是整体的去重\n\n#### 建立Redis连接\n在每台机器上建立Redis连接池，\n\n#### 动态读取Mysql订阅信息\n对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -> PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]\n\n#### 对源数据进行过滤，不在景区所属城市的数据丢掉\n\n#### 读取Redis中已经保存的景区用户信息列表\n从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除\n\n#### 判断数据是否需要推送\n数据是否需要推送需要满足：\n\n1. 不在Redis中：与Redis中数据进行对比，在Redis中的数据删除\n2. 在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回\"\";\n3. 更新Redis \n\n#### 数据统一发送\n将处理好的数据按照一定的规则统一批量发送至flume","slug":"位置服务说明文档","published":0,"date":"2016-12-19T08:43:00.474Z","updated":"2016-12-19T08:44:32.831Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfsp0000psguigmu2nd9","content":"<h2 id=\"总体架构\"><a href=\"#总体架构\" class=\"headerlink\" title=\"总体架构\"></a>总体架构</h2><p>总体架构图如下：<br><img src=\"/2016/12/19/位置服务说明文档/1.png\" alt=\"1.png\" title=\"\"></p>\n<p>如上图：主要分为三大部分：</p>\n<h4 id=\"上游数据\"><a href=\"#上游数据\" class=\"headerlink\" title=\"上游数据\"></a>上游数据</h4><p>由东方国信提供各个省份的Oidd数据，发送至kafka集群</p>\n<h4 id=\"处理逻辑\"><a href=\"#处理逻辑\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h4><ol>\n<li>Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；</li>\n<li>景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；</li>\n</ol>\n<h4 id=\"下游系统\"><a href=\"#下游系统\" class=\"headerlink\" title=\"下游系统\"></a>下游系统</h4><p>处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。</p>\n<h2 id=\"处理逻辑-1\"><a href=\"#处理逻辑-1\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h2><h3 id=\"景区用户识别端详解\"><a href=\"#景区用户识别端详解\" class=\"headerlink\" title=\"景区用户识别端详解\"></a>景区用户识别端详解</h3><p>景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑<br>分别实现如下功能：</p>\n<h4 id=\"配置信息读取\"><a href=\"#配置信息读取\" class=\"headerlink\" title=\"配置信息读取\"></a>配置信息读取</h4><p>kafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取</p>\n<h4 id=\"接收Kakfa消息流\"><a href=\"#接收Kakfa消息流\" class=\"headerlink\" title=\"接收Kakfa消息流\"></a>接收Kakfa消息流</h4><p>接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；</p>\n<h4 id=\"对所有的号码根据mdn生成key\"><a href=\"#对所有的号码根据mdn生成key\" class=\"headerlink\" title=\"对所有的号码根据mdn生成key\"></a>对所有的号码根据mdn生成key</h4><h4 id=\"对mdn重复数据进行去重\"><a href=\"#对mdn重复数据进行去重\" class=\"headerlink\" title=\"对mdn重复数据进行去重\"></a>对mdn重复数据进行去重</h4><p>注意这里是在每台不同的机器下进行的去重，而不是整体的去重</p>\n<h4 id=\"建立Redis连接\"><a href=\"#建立Redis连接\" class=\"headerlink\" title=\"建立Redis连接\"></a>建立Redis连接</h4><p>在每台机器上建立Redis连接池，</p>\n<h4 id=\"动态读取Mysql订阅信息\"><a href=\"#动态读取Mysql订阅信息\" class=\"headerlink\" title=\"动态读取Mysql订阅信息\"></a>动态读取Mysql订阅信息</h4><p>对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -&gt; PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]</p>\n<h4 id=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"><a href=\"#对源数据进行过滤，不在景区所属城市的数据丢掉\" class=\"headerlink\" title=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"></a>对源数据进行过滤，不在景区所属城市的数据丢掉</h4><h4 id=\"读取Redis中已经保存的景区用户信息列表\"><a href=\"#读取Redis中已经保存的景区用户信息列表\" class=\"headerlink\" title=\"读取Redis中已经保存的景区用户信息列表\"></a>读取Redis中已经保存的景区用户信息列表</h4><p>从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除</p>\n<h4 id=\"判断数据是否需要推送\"><a href=\"#判断数据是否需要推送\" class=\"headerlink\" title=\"判断数据是否需要推送\"></a>判断数据是否需要推送</h4><p>数据是否需要推送需要满足：</p>\n<ol>\n<li>不在Redis中：与Redis中数据进行对比，在Redis中的数据删除</li>\n<li>在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回””;</li>\n<li>更新Redis </li>\n</ol>\n<h4 id=\"数据统一发送\"><a href=\"#数据统一发送\" class=\"headerlink\" title=\"数据统一发送\"></a>数据统一发送</h4><p>将处理好的数据按照一定的规则统一批量发送至flume</p>\n","excerpt":"","more":"<h2 id=\"总体架构\"><a href=\"#总体架构\" class=\"headerlink\" title=\"总体架构\"></a>总体架构</h2><p>总体架构图如下：<br><img src=\"/2016/12/19/位置服务说明文档/1.png\" alt=\"1.png\" title=\"\"></p>\n<p>如上图：主要分为三大部分：</p>\n<h4 id=\"上游数据\"><a href=\"#上游数据\" class=\"headerlink\" title=\"上游数据\"></a>上游数据</h4><p>由东方国信提供各个省份的Oidd数据，发送至kafka集群</p>\n<h4 id=\"处理逻辑\"><a href=\"#处理逻辑\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h4><ol>\n<li>Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；</li>\n<li>景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；</li>\n</ol>\n<h4 id=\"下游系统\"><a href=\"#下游系统\" class=\"headerlink\" title=\"下游系统\"></a>下游系统</h4><p>处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。</p>\n<h2 id=\"处理逻辑-1\"><a href=\"#处理逻辑-1\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h2><h3 id=\"景区用户识别端详解\"><a href=\"#景区用户识别端详解\" class=\"headerlink\" title=\"景区用户识别端详解\"></a>景区用户识别端详解</h3><p>景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑<br>分别实现如下功能：</p>\n<h4 id=\"配置信息读取\"><a href=\"#配置信息读取\" class=\"headerlink\" title=\"配置信息读取\"></a>配置信息读取</h4><p>kafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取</p>\n<h4 id=\"接收Kakfa消息流\"><a href=\"#接收Kakfa消息流\" class=\"headerlink\" title=\"接收Kakfa消息流\"></a>接收Kakfa消息流</h4><p>接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；</p>\n<h4 id=\"对所有的号码根据mdn生成key\"><a href=\"#对所有的号码根据mdn生成key\" class=\"headerlink\" title=\"对所有的号码根据mdn生成key\"></a>对所有的号码根据mdn生成key</h4><h4 id=\"对mdn重复数据进行去重\"><a href=\"#对mdn重复数据进行去重\" class=\"headerlink\" title=\"对mdn重复数据进行去重\"></a>对mdn重复数据进行去重</h4><p>注意这里是在每台不同的机器下进行的去重，而不是整体的去重</p>\n<h4 id=\"建立Redis连接\"><a href=\"#建立Redis连接\" class=\"headerlink\" title=\"建立Redis连接\"></a>建立Redis连接</h4><p>在每台机器上建立Redis连接池，</p>\n<h4 id=\"动态读取Mysql订阅信息\"><a href=\"#动态读取Mysql订阅信息\" class=\"headerlink\" title=\"动态读取Mysql订阅信息\"></a>动态读取Mysql订阅信息</h4><p>对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -&gt; PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]</p>\n<h4 id=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"><a href=\"#对源数据进行过滤，不在景区所属城市的数据丢掉\" class=\"headerlink\" title=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"></a>对源数据进行过滤，不在景区所属城市的数据丢掉</h4><h4 id=\"读取Redis中已经保存的景区用户信息列表\"><a href=\"#读取Redis中已经保存的景区用户信息列表\" class=\"headerlink\" title=\"读取Redis中已经保存的景区用户信息列表\"></a>读取Redis中已经保存的景区用户信息列表</h4><p>从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除</p>\n<h4 id=\"判断数据是否需要推送\"><a href=\"#判断数据是否需要推送\" class=\"headerlink\" title=\"判断数据是否需要推送\"></a>判断数据是否需要推送</h4><p>数据是否需要推送需要满足：</p>\n<ol>\n<li>不在Redis中：与Redis中数据进行对比，在Redis中的数据删除</li>\n<li>在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回””;</li>\n<li>更新Redis </li>\n</ol>\n<h4 id=\"数据统一发送\"><a href=\"#数据统一发送\" class=\"headerlink\" title=\"数据统一发送\"></a>数据统一发送</h4><p>将处理好的数据按照一定的规则统一批量发送至flume</p>\n"},{"layout":"false","title":"景区位置服务项目说明文档","toc":true,"date":"2016-10-18T06:20:54.000Z","_content":"\n## 总体架构\n总体架构图如下：\n{% asset_img 1.png %}\n\n如上图：主要分为三大部分：\n#### 上游数据\n由东方国信提供各个省份的Oidd数据，发送至kafka集群\n#### 处理逻辑\n1. Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；\n2. 景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；\n\n#### 下游系统\n处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。\n\n## 处理逻辑\n### 景区用户识别端详解\n\n景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑\n分别实现如下功能：\n\n#### 配置信息读取\nkafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取\n\n#### 接收Kakfa消息流\n接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；\n\n#### 对所有的号码根据mdn生成key\n\n#### 对mdn重复数据进行去重\n注意这里是在每台不同的机器下进行的去重，而不是整体的去重\n\n#### 建立Redis连接\n在每台机器上建立Redis连接池，\n\n#### 动态读取Mysql订阅信息\n对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -> PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]\n\n#### 对源数据进行过滤，不在景区所属城市的数据丢掉\n\n#### 读取Redis中已经保存的景区用户信息列表\n从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除\n\n#### 判断数据是否需要推送\n数据是否需要推送需要满足：\n\n1. 不在Redis中：与Redis中数据进行对比，在Redis中的数据删除\n2. 在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回\"\";\n3. 更新Redis \n\n#### 数据统一发送\n将处理好的数据按照一定的规则统一批量发送至flume","source":"_drafts/2016-10-18-景区位置服务项目说明文档.md","raw":"---\nlayout: false\ntitle: 景区位置服务项目说明文档\ntoc: true\ndate: 2016-10-18 14:20:54\ntags: \n- spark streaming\n- flume\n- kafka\n- 大数据开发\n- redis\ncategories: spark开发\n---\n\n## 总体架构\n总体架构图如下：\n{% asset_img 1.png %}\n\n如上图：主要分为三大部分：\n#### 上游数据\n由东方国信提供各个省份的Oidd数据，发送至kafka集群\n#### 处理逻辑\n1. Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；\n2. 景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；\n\n#### 下游系统\n处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。\n\n## 处理逻辑\n### 景区用户识别端详解\n\n景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑\n分别实现如下功能：\n\n#### 配置信息读取\nkafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取\n\n#### 接收Kakfa消息流\n接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；\n\n#### 对所有的号码根据mdn生成key\n\n#### 对mdn重复数据进行去重\n注意这里是在每台不同的机器下进行的去重，而不是整体的去重\n\n#### 建立Redis连接\n在每台机器上建立Redis连接池，\n\n#### 动态读取Mysql订阅信息\n对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -> PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]\n\n#### 对源数据进行过滤，不在景区所属城市的数据丢掉\n\n#### 读取Redis中已经保存的景区用户信息列表\n从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除\n\n#### 判断数据是否需要推送\n数据是否需要推送需要满足：\n\n1. 不在Redis中：与Redis中数据进行对比，在Redis中的数据删除\n2. 在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回\"\";\n3. 更新Redis \n\n#### 数据统一发送\n将处理好的数据按照一定的规则统一批量发送至flume","slug":"景区位置服务项目说明文档","published":0,"updated":"2017-10-31T11:56:54.260Z","comments":1,"photos":[],"link":"","_id":"cjldkwfsv0002psgu3wsy26qj","content":"<h2 id=\"总体架构\"><a href=\"#总体架构\" class=\"headerlink\" title=\"总体架构\"></a>总体架构</h2><p>总体架构图如下：<br><img src=\"/2016/10/18/景区位置服务项目说明文档/1.png\" alt=\"1.png\" title=\"\"></p>\n<p>如上图：主要分为三大部分：</p>\n<h4 id=\"上游数据\"><a href=\"#上游数据\" class=\"headerlink\" title=\"上游数据\"></a>上游数据</h4><p>由东方国信提供各个省份的Oidd数据，发送至kafka集群</p>\n<h4 id=\"处理逻辑\"><a href=\"#处理逻辑\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h4><ol>\n<li>Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；</li>\n<li>景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；</li>\n</ol>\n<h4 id=\"下游系统\"><a href=\"#下游系统\" class=\"headerlink\" title=\"下游系统\"></a>下游系统</h4><p>处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。</p>\n<h2 id=\"处理逻辑-1\"><a href=\"#处理逻辑-1\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h2><h3 id=\"景区用户识别端详解\"><a href=\"#景区用户识别端详解\" class=\"headerlink\" title=\"景区用户识别端详解\"></a>景区用户识别端详解</h3><p>景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑<br>分别实现如下功能：</p>\n<h4 id=\"配置信息读取\"><a href=\"#配置信息读取\" class=\"headerlink\" title=\"配置信息读取\"></a>配置信息读取</h4><p>kafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取</p>\n<h4 id=\"接收Kakfa消息流\"><a href=\"#接收Kakfa消息流\" class=\"headerlink\" title=\"接收Kakfa消息流\"></a>接收Kakfa消息流</h4><p>接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；</p>\n<h4 id=\"对所有的号码根据mdn生成key\"><a href=\"#对所有的号码根据mdn生成key\" class=\"headerlink\" title=\"对所有的号码根据mdn生成key\"></a>对所有的号码根据mdn生成key</h4><h4 id=\"对mdn重复数据进行去重\"><a href=\"#对mdn重复数据进行去重\" class=\"headerlink\" title=\"对mdn重复数据进行去重\"></a>对mdn重复数据进行去重</h4><p>注意这里是在每台不同的机器下进行的去重，而不是整体的去重</p>\n<h4 id=\"建立Redis连接\"><a href=\"#建立Redis连接\" class=\"headerlink\" title=\"建立Redis连接\"></a>建立Redis连接</h4><p>在每台机器上建立Redis连接池，</p>\n<h4 id=\"动态读取Mysql订阅信息\"><a href=\"#动态读取Mysql订阅信息\" class=\"headerlink\" title=\"动态读取Mysql订阅信息\"></a>动态读取Mysql订阅信息</h4><p>对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -&gt; PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]</p>\n<h4 id=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"><a href=\"#对源数据进行过滤，不在景区所属城市的数据丢掉\" class=\"headerlink\" title=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"></a>对源数据进行过滤，不在景区所属城市的数据丢掉</h4><h4 id=\"读取Redis中已经保存的景区用户信息列表\"><a href=\"#读取Redis中已经保存的景区用户信息列表\" class=\"headerlink\" title=\"读取Redis中已经保存的景区用户信息列表\"></a>读取Redis中已经保存的景区用户信息列表</h4><p>从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除</p>\n<h4 id=\"判断数据是否需要推送\"><a href=\"#判断数据是否需要推送\" class=\"headerlink\" title=\"判断数据是否需要推送\"></a>判断数据是否需要推送</h4><p>数据是否需要推送需要满足：</p>\n<ol>\n<li>不在Redis中：与Redis中数据进行对比，在Redis中的数据删除</li>\n<li>在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回””;</li>\n<li>更新Redis </li>\n</ol>\n<h4 id=\"数据统一发送\"><a href=\"#数据统一发送\" class=\"headerlink\" title=\"数据统一发送\"></a>数据统一发送</h4><p>将处理好的数据按照一定的规则统一批量发送至flume</p>\n","excerpt":"","more":"<h2 id=\"总体架构\"><a href=\"#总体架构\" class=\"headerlink\" title=\"总体架构\"></a>总体架构</h2><p>总体架构图如下：<br><img src=\"/2016/10/18/景区位置服务项目说明文档/1.png\" alt=\"1.png\" title=\"\"></p>\n<p>如上图：主要分为三大部分：</p>\n<h4 id=\"上游数据\"><a href=\"#上游数据\" class=\"headerlink\" title=\"上游数据\"></a>上游数据</h4><p>由东方国信提供各个省份的Oidd数据，发送至kafka集群</p>\n<h4 id=\"处理逻辑\"><a href=\"#处理逻辑\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h4><ol>\n<li>Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；</li>\n<li>景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；</li>\n</ol>\n<h4 id=\"下游系统\"><a href=\"#下游系统\" class=\"headerlink\" title=\"下游系统\"></a>下游系统</h4><p>处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。</p>\n<h2 id=\"处理逻辑-1\"><a href=\"#处理逻辑-1\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h2><h3 id=\"景区用户识别端详解\"><a href=\"#景区用户识别端详解\" class=\"headerlink\" title=\"景区用户识别端详解\"></a>景区用户识别端详解</h3><p>景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑<br>分别实现如下功能：</p>\n<h4 id=\"配置信息读取\"><a href=\"#配置信息读取\" class=\"headerlink\" title=\"配置信息读取\"></a>配置信息读取</h4><p>kafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取</p>\n<h4 id=\"接收Kakfa消息流\"><a href=\"#接收Kakfa消息流\" class=\"headerlink\" title=\"接收Kakfa消息流\"></a>接收Kakfa消息流</h4><p>接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；</p>\n<h4 id=\"对所有的号码根据mdn生成key\"><a href=\"#对所有的号码根据mdn生成key\" class=\"headerlink\" title=\"对所有的号码根据mdn生成key\"></a>对所有的号码根据mdn生成key</h4><h4 id=\"对mdn重复数据进行去重\"><a href=\"#对mdn重复数据进行去重\" class=\"headerlink\" title=\"对mdn重复数据进行去重\"></a>对mdn重复数据进行去重</h4><p>注意这里是在每台不同的机器下进行的去重，而不是整体的去重</p>\n<h4 id=\"建立Redis连接\"><a href=\"#建立Redis连接\" class=\"headerlink\" title=\"建立Redis连接\"></a>建立Redis连接</h4><p>在每台机器上建立Redis连接池，</p>\n<h4 id=\"动态读取Mysql订阅信息\"><a href=\"#动态读取Mysql订阅信息\" class=\"headerlink\" title=\"动态读取Mysql订阅信息\"></a>动态读取Mysql订阅信息</h4><p>对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -&gt; PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]</p>\n<h4 id=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"><a href=\"#对源数据进行过滤，不在景区所属城市的数据丢掉\" class=\"headerlink\" title=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"></a>对源数据进行过滤，不在景区所属城市的数据丢掉</h4><h4 id=\"读取Redis中已经保存的景区用户信息列表\"><a href=\"#读取Redis中已经保存的景区用户信息列表\" class=\"headerlink\" title=\"读取Redis中已经保存的景区用户信息列表\"></a>读取Redis中已经保存的景区用户信息列表</h4><p>从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除</p>\n<h4 id=\"判断数据是否需要推送\"><a href=\"#判断数据是否需要推送\" class=\"headerlink\" title=\"判断数据是否需要推送\"></a>判断数据是否需要推送</h4><p>数据是否需要推送需要满足：</p>\n<ol>\n<li>不在Redis中：与Redis中数据进行对比，在Redis中的数据删除</li>\n<li>在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回””;</li>\n<li>更新Redis </li>\n</ol>\n<h4 id=\"数据统一发送\"><a href=\"#数据统一发送\" class=\"headerlink\" title=\"数据统一发送\"></a>数据统一发送</h4><p>将处理好的数据按照一定的规则统一批量发送至flume</p>\n"},{"title":"Scala学习资料总结","date":"2016-05-20T13:01:26.000Z","toc":true,"_content":"\n还在路上，持续更新\n\n以前对函数式编程很陌生，初学scala，感觉scala很难学，学习途中走了许多弯路，虽然目前本人只是连scala入门都算不上，希望能够分享一些资料和读后感给和我一样初学scala的同学，少一些搜集资料的时间，也希望能够抛砖引玉，也请更多高手们多多指教，随便修改本帖，有好资料随便贴上。\n\n-----------------------\n\n#### 书籍推荐\n\n- 1.1 快学Scala(Scala for the Impatient)\n\n只推荐这一本书，因为比较薄，看得时候也能看懂，但只是大概懂一点点 \n这本纸质书是李总买的公共书籍，也是本人学习scala的第一本书籍，因为读了一遍后并没有直接使用，故刚读完就把里面的知识给忘光了。直到最近需要模块地方知识翻第二遍的时候，才发现这本书写得蛮好的，从简单到困难都有提及，对我来说可以当成工具书。推荐给喜欢纸质书的同学。\n\n本人这里有我从网上收集的每一章的课后答案，供大家下载使用：密码：7ry4\n\n[快学Scala 课后习题答案集合.rar](http://pan.baidu.com/s/1cynlgU)\n\n- 1.2 Programming Scala 2nd edition (2015年出版）\n\n很有名，没看过，对我来说看起来太厚太慢了\n\n#### 网站推荐\n\n个人觉得scala的好的学习资料都在网上，只分享个人觉得好的，将我所知分享给各位：\n\n- 2.1 [Scala School](http://twitter.github.io/scala_school/)\n\n吐血推荐，twitter的scala教学网站，言简意赅，真的非常非常好，我几经周转看到这个资料，有种相见恨晚的感觉。重要的是有中文版。\n\n- 2.2 [scala-tour](http://www.scala-tour.com/#/welcome)\n\n网友做的scala学习网站，之前中文版有bug，现在中文版也很好用。是一个交互式的学习网站，右边是概念，左边直接是代码示例，可以直接修改并运行，很清晰，适合假期在家里学习。\n\n- 2.3 [scala-api](http://www.scala-lang.org/api/current/)\n\nscala 官方API不解释，作为浏览器书签\n\n- 2.4 [scala快查](http://docs.scala-lang.org/cheatsheets/)\n\nscala官方出的让你快速查阅的网页，就一页涵盖了基本所有的操作\n\n- 2.5 [effective scala](http://twitter.github.io/effectivescala/index-cn.html)\n\ntwitter的scala资深玩家讲述了一些scala编程需要注意的问题，在纠结用哪一种编程方式或者数据结构好的时候，这是一个很好的参考，有中文版\n\n- 2.6 [scala 说点什么](http://hongjiang.info/scala/)\n\nhongjiang大神对scala的一些理解，写得很深入，之前很多scala的英文文档都是hongjiang大神翻译的\n\n- 2.7 [scala编程规范](http://www.tuicool.com/articles/2QFRZfE)\n\n国人总结的scala编程规范，写得很全很详细，希望大家能够从中受到启发，写出优质的代码\n\n- 2.8 [面向 Java 开发人员的 Scala 指南](http://www.ibm.com/developerworks/cn/java/j-scala01228.html)\n面向 Java 开发人员的 Scala 指南\n\n#### 工具推荐\n\n- 3.1 Intellij Idea\n\n强烈推荐Intellij Idea（缺点是比较占内存），其对scala的支持是极好的，可以自动检测出你的代码中的一些问题，帮你看想看的源码，极好的单元测试的支持，遵循scala规范的自动格式化等功能\n\n    IntelliJ IDEA https://www.jetbrains.com/idea/  \n    (请支持正版，下载后24小时内删除) http://idea.lanyus.com\n\nIdea有一个scala学习的插件，左边是代码，右边是命令结果，学习效果比在shell中好很多。","source":"_posts/2016-05-20-Scala学习资料总结.md","raw":"---\ntitle: Scala学习资料总结\ndate: 2016-05-20 21:01:26\ntoc: true\ntags: scala\ncategories: scala\n---\n\n还在路上，持续更新\n\n以前对函数式编程很陌生，初学scala，感觉scala很难学，学习途中走了许多弯路，虽然目前本人只是连scala入门都算不上，希望能够分享一些资料和读后感给和我一样初学scala的同学，少一些搜集资料的时间，也希望能够抛砖引玉，也请更多高手们多多指教，随便修改本帖，有好资料随便贴上。\n\n-----------------------\n\n#### 书籍推荐\n\n- 1.1 快学Scala(Scala for the Impatient)\n\n只推荐这一本书，因为比较薄，看得时候也能看懂，但只是大概懂一点点 \n这本纸质书是李总买的公共书籍，也是本人学习scala的第一本书籍，因为读了一遍后并没有直接使用，故刚读完就把里面的知识给忘光了。直到最近需要模块地方知识翻第二遍的时候，才发现这本书写得蛮好的，从简单到困难都有提及，对我来说可以当成工具书。推荐给喜欢纸质书的同学。\n\n本人这里有我从网上收集的每一章的课后答案，供大家下载使用：密码：7ry4\n\n[快学Scala 课后习题答案集合.rar](http://pan.baidu.com/s/1cynlgU)\n\n- 1.2 Programming Scala 2nd edition (2015年出版）\n\n很有名，没看过，对我来说看起来太厚太慢了\n\n#### 网站推荐\n\n个人觉得scala的好的学习资料都在网上，只分享个人觉得好的，将我所知分享给各位：\n\n- 2.1 [Scala School](http://twitter.github.io/scala_school/)\n\n吐血推荐，twitter的scala教学网站，言简意赅，真的非常非常好，我几经周转看到这个资料，有种相见恨晚的感觉。重要的是有中文版。\n\n- 2.2 [scala-tour](http://www.scala-tour.com/#/welcome)\n\n网友做的scala学习网站，之前中文版有bug，现在中文版也很好用。是一个交互式的学习网站，右边是概念，左边直接是代码示例，可以直接修改并运行，很清晰，适合假期在家里学习。\n\n- 2.3 [scala-api](http://www.scala-lang.org/api/current/)\n\nscala 官方API不解释，作为浏览器书签\n\n- 2.4 [scala快查](http://docs.scala-lang.org/cheatsheets/)\n\nscala官方出的让你快速查阅的网页，就一页涵盖了基本所有的操作\n\n- 2.5 [effective scala](http://twitter.github.io/effectivescala/index-cn.html)\n\ntwitter的scala资深玩家讲述了一些scala编程需要注意的问题，在纠结用哪一种编程方式或者数据结构好的时候，这是一个很好的参考，有中文版\n\n- 2.6 [scala 说点什么](http://hongjiang.info/scala/)\n\nhongjiang大神对scala的一些理解，写得很深入，之前很多scala的英文文档都是hongjiang大神翻译的\n\n- 2.7 [scala编程规范](http://www.tuicool.com/articles/2QFRZfE)\n\n国人总结的scala编程规范，写得很全很详细，希望大家能够从中受到启发，写出优质的代码\n\n- 2.8 [面向 Java 开发人员的 Scala 指南](http://www.ibm.com/developerworks/cn/java/j-scala01228.html)\n面向 Java 开发人员的 Scala 指南\n\n#### 工具推荐\n\n- 3.1 Intellij Idea\n\n强烈推荐Intellij Idea（缺点是比较占内存），其对scala的支持是极好的，可以自动检测出你的代码中的一些问题，帮你看想看的源码，极好的单元测试的支持，遵循scala规范的自动格式化等功能\n\n    IntelliJ IDEA https://www.jetbrains.com/idea/  \n    (请支持正版，下载后24小时内删除) http://idea.lanyus.com\n\nIdea有一个scala学习的插件，左边是代码，右边是命令结果，学习效果比在shell中好很多。","slug":"Scala学习资料总结","published":1,"updated":"2016-10-01T09:41:47.041Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfsz0004psgu2lhev5rz","content":"<p>还在路上，持续更新</p>\n<p>以前对函数式编程很陌生，初学scala，感觉scala很难学，学习途中走了许多弯路，虽然目前本人只是连scala入门都算不上，希望能够分享一些资料和读后感给和我一样初学scala的同学，少一些搜集资料的时间，也希望能够抛砖引玉，也请更多高手们多多指教，随便修改本帖，有好资料随便贴上。</p>\n<hr>\n<h4 id=\"书籍推荐\"><a href=\"#书籍推荐\" class=\"headerlink\" title=\"书籍推荐\"></a>书籍推荐</h4><ul>\n<li>1.1 快学Scala(Scala for the Impatient)</li>\n</ul>\n<p>只推荐这一本书，因为比较薄，看得时候也能看懂，但只是大概懂一点点<br>这本纸质书是李总买的公共书籍，也是本人学习scala的第一本书籍，因为读了一遍后并没有直接使用，故刚读完就把里面的知识给忘光了。直到最近需要模块地方知识翻第二遍的时候，才发现这本书写得蛮好的，从简单到困难都有提及，对我来说可以当成工具书。推荐给喜欢纸质书的同学。</p>\n<p>本人这里有我从网上收集的每一章的课后答案，供大家下载使用：密码：7ry4</p>\n<p><a href=\"http://pan.baidu.com/s/1cynlgU\" target=\"_blank\" rel=\"external\">快学Scala 课后习题答案集合.rar</a></p>\n<ul>\n<li>1.2 Programming Scala 2nd edition (2015年出版）</li>\n</ul>\n<p>很有名，没看过，对我来说看起来太厚太慢了</p>\n<h4 id=\"网站推荐\"><a href=\"#网站推荐\" class=\"headerlink\" title=\"网站推荐\"></a>网站推荐</h4><p>个人觉得scala的好的学习资料都在网上，只分享个人觉得好的，将我所知分享给各位：</p>\n<ul>\n<li>2.1 <a href=\"http://twitter.github.io/scala_school/\" target=\"_blank\" rel=\"external\">Scala School</a></li>\n</ul>\n<p>吐血推荐，twitter的scala教学网站，言简意赅，真的非常非常好，我几经周转看到这个资料，有种相见恨晚的感觉。重要的是有中文版。</p>\n<ul>\n<li>2.2 <a href=\"http://www.scala-tour.com/#/welcome\" target=\"_blank\" rel=\"external\">scala-tour</a></li>\n</ul>\n<p>网友做的scala学习网站，之前中文版有bug，现在中文版也很好用。是一个交互式的学习网站，右边是概念，左边直接是代码示例，可以直接修改并运行，很清晰，适合假期在家里学习。</p>\n<ul>\n<li>2.3 <a href=\"http://www.scala-lang.org/api/current/\" target=\"_blank\" rel=\"external\">scala-api</a></li>\n</ul>\n<p>scala 官方API不解释，作为浏览器书签</p>\n<ul>\n<li>2.4 <a href=\"http://docs.scala-lang.org/cheatsheets/\" target=\"_blank\" rel=\"external\">scala快查</a></li>\n</ul>\n<p>scala官方出的让你快速查阅的网页，就一页涵盖了基本所有的操作</p>\n<ul>\n<li>2.5 <a href=\"http://twitter.github.io/effectivescala/index-cn.html\" target=\"_blank\" rel=\"external\">effective scala</a></li>\n</ul>\n<p>twitter的scala资深玩家讲述了一些scala编程需要注意的问题，在纠结用哪一种编程方式或者数据结构好的时候，这是一个很好的参考，有中文版</p>\n<ul>\n<li>2.6 <a href=\"http://hongjiang.info/scala/\" target=\"_blank\" rel=\"external\">scala 说点什么</a></li>\n</ul>\n<p>hongjiang大神对scala的一些理解，写得很深入，之前很多scala的英文文档都是hongjiang大神翻译的</p>\n<ul>\n<li>2.7 <a href=\"http://www.tuicool.com/articles/2QFRZfE\" target=\"_blank\" rel=\"external\">scala编程规范</a></li>\n</ul>\n<p>国人总结的scala编程规范，写得很全很详细，希望大家能够从中受到启发，写出优质的代码</p>\n<ul>\n<li>2.8 <a href=\"http://www.ibm.com/developerworks/cn/java/j-scala01228.html\" target=\"_blank\" rel=\"external\">面向 Java 开发人员的 Scala 指南</a><br>面向 Java 开发人员的 Scala 指南</li>\n</ul>\n<h4 id=\"工具推荐\"><a href=\"#工具推荐\" class=\"headerlink\" title=\"工具推荐\"></a>工具推荐</h4><ul>\n<li>3.1 Intellij Idea</li>\n</ul>\n<p>强烈推荐Intellij Idea（缺点是比较占内存），其对scala的支持是极好的，可以自动检测出你的代码中的一些问题，帮你看想看的源码，极好的单元测试的支持，遵循scala规范的自动格式化等功能</p>\n<pre><code>IntelliJ IDEA https://www.jetbrains.com/idea/  \n(请支持正版，下载后24小时内删除) http://idea.lanyus.com\n</code></pre><p>Idea有一个scala学习的插件，左边是代码，右边是命令结果，学习效果比在shell中好很多。</p>\n","excerpt":"","more":"<p>还在路上，持续更新</p>\n<p>以前对函数式编程很陌生，初学scala，感觉scala很难学，学习途中走了许多弯路，虽然目前本人只是连scala入门都算不上，希望能够分享一些资料和读后感给和我一样初学scala的同学，少一些搜集资料的时间，也希望能够抛砖引玉，也请更多高手们多多指教，随便修改本帖，有好资料随便贴上。</p>\n<hr>\n<h4 id=\"书籍推荐\"><a href=\"#书籍推荐\" class=\"headerlink\" title=\"书籍推荐\"></a>书籍推荐</h4><ul>\n<li>1.1 快学Scala(Scala for the Impatient)</li>\n</ul>\n<p>只推荐这一本书，因为比较薄，看得时候也能看懂，但只是大概懂一点点<br>这本纸质书是李总买的公共书籍，也是本人学习scala的第一本书籍，因为读了一遍后并没有直接使用，故刚读完就把里面的知识给忘光了。直到最近需要模块地方知识翻第二遍的时候，才发现这本书写得蛮好的，从简单到困难都有提及，对我来说可以当成工具书。推荐给喜欢纸质书的同学。</p>\n<p>本人这里有我从网上收集的每一章的课后答案，供大家下载使用：密码：7ry4</p>\n<p><a href=\"http://pan.baidu.com/s/1cynlgU\">快学Scala 课后习题答案集合.rar</a></p>\n<ul>\n<li>1.2 Programming Scala 2nd edition (2015年出版）</li>\n</ul>\n<p>很有名，没看过，对我来说看起来太厚太慢了</p>\n<h4 id=\"网站推荐\"><a href=\"#网站推荐\" class=\"headerlink\" title=\"网站推荐\"></a>网站推荐</h4><p>个人觉得scala的好的学习资料都在网上，只分享个人觉得好的，将我所知分享给各位：</p>\n<ul>\n<li>2.1 <a href=\"http://twitter.github.io/scala_school/\">Scala School</a></li>\n</ul>\n<p>吐血推荐，twitter的scala教学网站，言简意赅，真的非常非常好，我几经周转看到这个资料，有种相见恨晚的感觉。重要的是有中文版。</p>\n<ul>\n<li>2.2 <a href=\"http://www.scala-tour.com/#/welcome\">scala-tour</a></li>\n</ul>\n<p>网友做的scala学习网站，之前中文版有bug，现在中文版也很好用。是一个交互式的学习网站，右边是概念，左边直接是代码示例，可以直接修改并运行，很清晰，适合假期在家里学习。</p>\n<ul>\n<li>2.3 <a href=\"http://www.scala-lang.org/api/current/\">scala-api</a></li>\n</ul>\n<p>scala 官方API不解释，作为浏览器书签</p>\n<ul>\n<li>2.4 <a href=\"http://docs.scala-lang.org/cheatsheets/\">scala快查</a></li>\n</ul>\n<p>scala官方出的让你快速查阅的网页，就一页涵盖了基本所有的操作</p>\n<ul>\n<li>2.5 <a href=\"http://twitter.github.io/effectivescala/index-cn.html\">effective scala</a></li>\n</ul>\n<p>twitter的scala资深玩家讲述了一些scala编程需要注意的问题，在纠结用哪一种编程方式或者数据结构好的时候，这是一个很好的参考，有中文版</p>\n<ul>\n<li>2.6 <a href=\"http://hongjiang.info/scala/\">scala 说点什么</a></li>\n</ul>\n<p>hongjiang大神对scala的一些理解，写得很深入，之前很多scala的英文文档都是hongjiang大神翻译的</p>\n<ul>\n<li>2.7 <a href=\"http://www.tuicool.com/articles/2QFRZfE\">scala编程规范</a></li>\n</ul>\n<p>国人总结的scala编程规范，写得很全很详细，希望大家能够从中受到启发，写出优质的代码</p>\n<ul>\n<li>2.8 <a href=\"http://www.ibm.com/developerworks/cn/java/j-scala01228.html\">面向 Java 开发人员的 Scala 指南</a><br>面向 Java 开发人员的 Scala 指南</li>\n</ul>\n<h4 id=\"工具推荐\"><a href=\"#工具推荐\" class=\"headerlink\" title=\"工具推荐\"></a>工具推荐</h4><ul>\n<li>3.1 Intellij Idea</li>\n</ul>\n<p>强烈推荐Intellij Idea（缺点是比较占内存），其对scala的支持是极好的，可以自动检测出你的代码中的一些问题，帮你看想看的源码，极好的单元测试的支持，遵循scala规范的自动格式化等功能</p>\n<pre><code>IntelliJ IDEA https://www.jetbrains.com/idea/  \n(请支持正版，下载后24小时内删除) http://idea.lanyus.com\n</code></pre><p>Idea有一个scala学习的插件，左边是代码，右边是命令结果，学习效果比在shell中好很多。</p>\n"},{"title":"Scala中_的用法","date":"2016-06-13T13:08:37.000Z","_content":"\n在看Spark源码的过程中，遇到了很多对 下划线_的运用，后来经过查阅资料总结如下（感谢万能的知乎,StackOverFlow）：\n\n- 作为“通配符”，类似Java中的*。如import scala.math._\n\n- :_*作为一个整体，告诉编译器你希望将某个参数当作参数序列处理！例如val s = sum(1 to 5:_*)就是将1 to 5当作参数序列处理。向函数或方法传入可变参数时不能直接传入Range或集合或数组对象，需要使用:_*转换才可传入\n举个例子:\n\n``` scala\ndef sum(nums: Int*) = {\n\tvar res = 0\n\tfor (num <- nums) {\n\t    res += num\n\t}\n\tres\n}\n```\n上述函数的参数是变长参数，如果直接调用 sum(1 to 5) 是不行的，这时候就需要使用 `sum(1 to 5:_*)`，这一招在spark源码中使用很多。\n\n- 指代一个集合中的每个元素。例如我们要在一个Array a中筛出偶数，并乘以2，可以用以下办法：\na.filter(_%2==0).map(2*_)。\n又如要对缓冲数组ArrayBuffer b排序，可以这样：\nval bSorted = b.sorted(_\n- 在元组中，可以用方法_1, _2, _3访问组员。如a._2。其中句点可以用空格替代。\n\n- 使用模式匹配可以用来获取元组的组员，例如]\n\nval (first, second, third) = t\n但如果不是所有的部件都需要，那么可以在不需要的部件位置上使用_。比如上一例中val (first, second, _) = t\n\n- 还有一点，下划线_代表的是某一类型的默认值。\n对于Int来说，它是0。\n对于Double来说，它是0.0\n对于引用类型，它是null。\n\n- 访问tuple变量的某个元素时通过索引_n来取得第n个元素\n\n- 类的setter方法，比如类A中定义了var f，则相当于定义了setter方法f_=，当然你可以自己定义f_=方法来完成更多的事情，比如设置前作一些判断或预处理之类的操作\n\n- 用于将方法转换成函数，比如val f=sqrt _，以后直接调用f(250)就能求平方根了\n\n- Spark源码中，私有变量约定俗成以 _开头，比如： \n\n``` scala\nprivate var _conf: SparkConf = _\nprivate var _eventLogDir: Option[URI] = None\nprivate var _eventLogCodec: Option[String] = None\nprivate var _env: SparkEnv = _\n```\n\n引用：\n\nhttps://www.zhihu.com/question/21622725/\nhttp://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala","source":"_posts/2016-06-13-Scala中-的用法.md","raw":"---\ntitle: Scala中_的用法\ndate: 2016-06-13 21:08:37\ntags: scala\ncategories: scala\n---\n\n在看Spark源码的过程中，遇到了很多对 下划线_的运用，后来经过查阅资料总结如下（感谢万能的知乎,StackOverFlow）：\n\n- 作为“通配符”，类似Java中的*。如import scala.math._\n\n- :_*作为一个整体，告诉编译器你希望将某个参数当作参数序列处理！例如val s = sum(1 to 5:_*)就是将1 to 5当作参数序列处理。向函数或方法传入可变参数时不能直接传入Range或集合或数组对象，需要使用:_*转换才可传入\n举个例子:\n\n``` scala\ndef sum(nums: Int*) = {\n\tvar res = 0\n\tfor (num <- nums) {\n\t    res += num\n\t}\n\tres\n}\n```\n上述函数的参数是变长参数，如果直接调用 sum(1 to 5) 是不行的，这时候就需要使用 `sum(1 to 5:_*)`，这一招在spark源码中使用很多。\n\n- 指代一个集合中的每个元素。例如我们要在一个Array a中筛出偶数，并乘以2，可以用以下办法：\na.filter(_%2==0).map(2*_)。\n又如要对缓冲数组ArrayBuffer b排序，可以这样：\nval bSorted = b.sorted(_\n- 在元组中，可以用方法_1, _2, _3访问组员。如a._2。其中句点可以用空格替代。\n\n- 使用模式匹配可以用来获取元组的组员，例如]\n\nval (first, second, third) = t\n但如果不是所有的部件都需要，那么可以在不需要的部件位置上使用_。比如上一例中val (first, second, _) = t\n\n- 还有一点，下划线_代表的是某一类型的默认值。\n对于Int来说，它是0。\n对于Double来说，它是0.0\n对于引用类型，它是null。\n\n- 访问tuple变量的某个元素时通过索引_n来取得第n个元素\n\n- 类的setter方法，比如类A中定义了var f，则相当于定义了setter方法f_=，当然你可以自己定义f_=方法来完成更多的事情，比如设置前作一些判断或预处理之类的操作\n\n- 用于将方法转换成函数，比如val f=sqrt _，以后直接调用f(250)就能求平方根了\n\n- Spark源码中，私有变量约定俗成以 _开头，比如： \n\n``` scala\nprivate var _conf: SparkConf = _\nprivate var _eventLogDir: Option[URI] = None\nprivate var _eventLogCodec: Option[String] = None\nprivate var _env: SparkEnv = _\n```\n\n引用：\n\nhttps://www.zhihu.com/question/21622725/\nhttp://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala","slug":"Scala中-的用法","published":1,"updated":"2017-10-31T11:56:12.864Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwftf0007psguuzhzj24i","content":"<p>在看Spark源码的过程中，遇到了很多对 下划线_的运用，后来经过查阅资料总结如下（感谢万能的知乎,StackOverFlow）：</p>\n<ul>\n<li><p>作为“通配符”，类似Java中的*。如import scala.math._</p>\n</li>\n<li><p>:<em>*作为一个整体，告诉编译器你希望将某个参数当作参数序列处理！例如val s = sum(1 to 5:</em><em>)就是将1 to 5当作参数序列处理。向函数或方法传入可变参数时不能直接传入Range或集合或数组对象，需要使用:_</em>转换才可传入<br>举个例子:</p>\n</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(nums: <span class=\"type\">Int</span>*) = &#123;</div><div class=\"line\">\t<span class=\"keyword\">var</span> res = <span class=\"number\">0</span></div><div class=\"line\">\t<span class=\"keyword\">for</span> (num &lt;- nums) &#123;</div><div class=\"line\">\t    res += num</div><div class=\"line\">\t&#125;</div><div class=\"line\">\tres</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>上述函数的参数是变长参数，如果直接调用 sum(1 to 5) 是不行的，这时候就需要使用 <code>sum(1 to 5:_*)</code>，这一招在spark源码中使用很多。</p>\n<ul>\n<li>指代一个集合中的每个元素。例如我们要在一个Array a中筛出偶数，并乘以2，可以用以下办法：<br>a.filter(<em>%2==0).map(2*</em>)。<br>又如要对缓冲数组ArrayBuffer b排序，可以这样：<br>val bSorted = b.sorted(_</li>\n<li><p>在元组中，可以用方法_1, _2, _3访问组员。如a._2。其中句点可以用空格替代。</p>\n</li>\n<li><p>使用模式匹配可以用来获取元组的组员，例如]</p>\n</li>\n</ul>\n<p>val (first, second, third) = t<br>但如果不是所有的部件都需要，那么可以在不需要的部件位置上使用<em>。比如上一例中val (first, second, </em>) = t</p>\n<ul>\n<li><p>还有一点，下划线_代表的是某一类型的默认值。<br>对于Int来说，它是0。<br>对于Double来说，它是0.0<br>对于引用类型，它是null。</p>\n</li>\n<li><p>访问tuple变量的某个元素时通过索引_n来取得第n个元素</p>\n</li>\n<li><p>类的setter方法，比如类A中定义了var f，则相当于定义了setter方法f<em>=，当然你可以自己定义f</em>=方法来完成更多的事情，比如设置前作一些判断或预处理之类的操作</p>\n</li>\n<li><p>用于将方法转换成函数，比如val f=sqrt _，以后直接调用f(250)就能求平方根了</p>\n</li>\n<li><p>Spark源码中，私有变量约定俗成以 _开头，比如： </p>\n</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">var</span> _conf: <span class=\"type\">SparkConf</span> = _</div><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">var</span> _eventLogDir: <span class=\"type\">Option</span>[<span class=\"type\">URI</span>] = <span class=\"type\">None</span></div><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">var</span> _eventLogCodec: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">None</span></div><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">var</span> _env: <span class=\"type\">SparkEnv</span> = _</div></pre></td></tr></table></figure>\n<p>引用：</p>\n<p><a href=\"https://www.zhihu.com/question/21622725/\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/21622725/</a><br><a href=\"http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala</a></p>\n","excerpt":"","more":"<p>在看Spark源码的过程中，遇到了很多对 下划线_的运用，后来经过查阅资料总结如下（感谢万能的知乎,StackOverFlow）：</p>\n<ul>\n<li><p>作为“通配符”，类似Java中的*。如import scala.math._</p>\n</li>\n<li><p>:<em>*作为一个整体，告诉编译器你希望将某个参数当作参数序列处理！例如val s = sum(1 to 5:</em><em>)就是将1 to 5当作参数序列处理。向函数或方法传入可变参数时不能直接传入Range或集合或数组对象，需要使用:_</em>转换才可传入<br>举个例子:</p>\n</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(nums: <span class=\"type\">Int</span>*) = &#123;</div><div class=\"line\">\t<span class=\"keyword\">var</span> res = <span class=\"number\">0</span></div><div class=\"line\">\t<span class=\"keyword\">for</span> (num &lt;- nums) &#123;</div><div class=\"line\">\t    res += num</div><div class=\"line\">\t&#125;</div><div class=\"line\">\tres</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>上述函数的参数是变长参数，如果直接调用 sum(1 to 5) 是不行的，这时候就需要使用 <code>sum(1 to 5:_*)</code>，这一招在spark源码中使用很多。</p>\n<ul>\n<li>指代一个集合中的每个元素。例如我们要在一个Array a中筛出偶数，并乘以2，可以用以下办法：<br>a.filter(<em>%2==0).map(2*</em>)。<br>又如要对缓冲数组ArrayBuffer b排序，可以这样：<br>val bSorted = b.sorted(_</li>\n<li><p>在元组中，可以用方法_1, _2, _3访问组员。如a._2。其中句点可以用空格替代。</p>\n</li>\n<li><p>使用模式匹配可以用来获取元组的组员，例如]</p>\n</li>\n</ul>\n<p>val (first, second, third) = t<br>但如果不是所有的部件都需要，那么可以在不需要的部件位置上使用<em>。比如上一例中val (first, second, </em>) = t</p>\n<ul>\n<li><p>还有一点，下划线_代表的是某一类型的默认值。<br>对于Int来说，它是0。<br>对于Double来说，它是0.0<br>对于引用类型，它是null。</p>\n</li>\n<li><p>访问tuple变量的某个元素时通过索引_n来取得第n个元素</p>\n</li>\n<li><p>类的setter方法，比如类A中定义了var f，则相当于定义了setter方法f<em>=，当然你可以自己定义f</em>=方法来完成更多的事情，比如设置前作一些判断或预处理之类的操作</p>\n</li>\n<li><p>用于将方法转换成函数，比如val f=sqrt _，以后直接调用f(250)就能求平方根了</p>\n</li>\n<li><p>Spark源码中，私有变量约定俗成以 _开头，比如： </p>\n</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">var</span> _conf: <span class=\"type\">SparkConf</span> = _</div><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">var</span> _eventLogDir: <span class=\"type\">Option</span>[<span class=\"type\">URI</span>] = <span class=\"type\">None</span></div><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">var</span> _eventLogCodec: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">None</span></div><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">var</span> _env: <span class=\"type\">SparkEnv</span> = _</div></pre></td></tr></table></figure>\n<p>引用：</p>\n<p><a href=\"https://www.zhihu.com/question/21622725/\">https://www.zhihu.com/question/21622725/</a><br><a href=\"http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala\">http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala</a></p>\n"},{"title":"使用scaladiagrams工具构建scala项目的UML图","date":"2016-06-13T13:12:22.000Z","_content":"\n### 背景\n\n阅读spark源码到storage这一块的时候，由于类的继承，调用之间的关系比较复杂，想要画一下UML图，idea自带的diagrams方法对java支持很好，但对scala的一些继承关系支持不佳，因此google了一下有没有可以画scala UML类图的工具，还真找到了：\n\n我是在x64 windows10下面，使用gitbash工具作为shell命令行，亲测可用\n\n### clone开源项目scaladiagrams并安装\n``` bash\ngit clone https://github.com/mikeyhu/scaladiagrams.git\ncd scaladiagrams\n./build\n```\n### 安装graphviz工具\ngraphviz是一个开源的图形可视化软件，矢量图生成工具，与其他图形软件所不同，它的理念是“所想即所得”，通过dot语言来描述并绘制图形。\nhttp://www.graphviz.org/Download_windows.php \n如上链接下载，然后安装即可，将安装路径加入path中，该工具的目的是通过scaladiagrams工具生成的依赖关系画图；\n\n### 使用\n\n#### 生成依赖关系文件dotFile\n``` bash\n./scaladiagrams --source \"D:\\spark-1.6.0\\core\\src\\main\\scala\\org\\apache\\spark\\storage\" > dotFile\n```\ndotFile文件就是依赖关系的文件：\n官方命名为 dot语言，是一个表示图的语言，挺好玩的：\n\n``` scala dot语言\ndigraph diagram {\n\"BlockException\" [style=filled, fillcolor=burlywood]\n  \"BlockException\" -> \"Exception\";\n\n\"BlockFetchException\" [style=filled, fillcolor=burlywood]\n  \"BlockFetchException\" -> \"SparkException\";\n\n\"BlockId\" [style=filled, fillcolor=darkorange]\n  \n\n\"RDDBlockId\" [style=filled, fillcolor=burlywood]\n  \"RDDBlockId\" -> \"BlockId\";\n\n  。。。\n```\n\n#### 使用graphviz工具画图\n\n生成svg文件，文件比较大的话建议用这个\n``` bash\ncat dotFile | dot -Tsvg > spark_storage.svg\n```\n生成png文件\n``` bash\ncat dotFile | dot -Tpng > spark_storage.png\n```\n画的效果部分截图如下（就是图有点扁平）：\n![spark_storage_part.png](spark_storage_part.png)\n\n因为好玩，又画了一个spark_core的类图，太大了，不好看，为了部分解决这个问题，只要在dot文件的第一行加入\n``` bash\nrankdir=RL; \n```\n即可使得图片稍微好看一点\n![spark_storage_part2.png](spark_storage_part2.png)\n\n\n引用：\nhttp://stackoverflow.com/questions/7227952/generating-uml-diagram-from-scala-sources\nhttps://github.com/mikeyhu/scaladiagrams\nhttp://www.graphviz.org/About.php\nhttp://www.graphviz.org/pdf/dotguide.pdf\nhttp://www.tonyballantyne.com/graphs.html","source":"_posts/2016-06-13-使用scaladiagrams工具构建scala项目的UML图.md","raw":"---\ntitle: 使用scaladiagrams工具构建scala项目的UML图\ndate: 2016-06-13 21:12:22\ntags: scala\ncategories: scala\n---\n\n### 背景\n\n阅读spark源码到storage这一块的时候，由于类的继承，调用之间的关系比较复杂，想要画一下UML图，idea自带的diagrams方法对java支持很好，但对scala的一些继承关系支持不佳，因此google了一下有没有可以画scala UML类图的工具，还真找到了：\n\n我是在x64 windows10下面，使用gitbash工具作为shell命令行，亲测可用\n\n### clone开源项目scaladiagrams并安装\n``` bash\ngit clone https://github.com/mikeyhu/scaladiagrams.git\ncd scaladiagrams\n./build\n```\n### 安装graphviz工具\ngraphviz是一个开源的图形可视化软件，矢量图生成工具，与其他图形软件所不同，它的理念是“所想即所得”，通过dot语言来描述并绘制图形。\nhttp://www.graphviz.org/Download_windows.php \n如上链接下载，然后安装即可，将安装路径加入path中，该工具的目的是通过scaladiagrams工具生成的依赖关系画图；\n\n### 使用\n\n#### 生成依赖关系文件dotFile\n``` bash\n./scaladiagrams --source \"D:\\spark-1.6.0\\core\\src\\main\\scala\\org\\apache\\spark\\storage\" > dotFile\n```\ndotFile文件就是依赖关系的文件：\n官方命名为 dot语言，是一个表示图的语言，挺好玩的：\n\n``` scala dot语言\ndigraph diagram {\n\"BlockException\" [style=filled, fillcolor=burlywood]\n  \"BlockException\" -> \"Exception\";\n\n\"BlockFetchException\" [style=filled, fillcolor=burlywood]\n  \"BlockFetchException\" -> \"SparkException\";\n\n\"BlockId\" [style=filled, fillcolor=darkorange]\n  \n\n\"RDDBlockId\" [style=filled, fillcolor=burlywood]\n  \"RDDBlockId\" -> \"BlockId\";\n\n  。。。\n```\n\n#### 使用graphviz工具画图\n\n生成svg文件，文件比较大的话建议用这个\n``` bash\ncat dotFile | dot -Tsvg > spark_storage.svg\n```\n生成png文件\n``` bash\ncat dotFile | dot -Tpng > spark_storage.png\n```\n画的效果部分截图如下（就是图有点扁平）：\n![spark_storage_part.png](spark_storage_part.png)\n\n因为好玩，又画了一个spark_core的类图，太大了，不好看，为了部分解决这个问题，只要在dot文件的第一行加入\n``` bash\nrankdir=RL; \n```\n即可使得图片稍微好看一点\n![spark_storage_part2.png](spark_storage_part2.png)\n\n\n引用：\nhttp://stackoverflow.com/questions/7227952/generating-uml-diagram-from-scala-sources\nhttps://github.com/mikeyhu/scaladiagrams\nhttp://www.graphviz.org/About.php\nhttp://www.graphviz.org/pdf/dotguide.pdf\nhttp://www.tonyballantyne.com/graphs.html","slug":"使用scaladiagrams工具构建scala项目的UML图","published":1,"updated":"2016-09-14T01:52:12.848Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwftk0008psguwqj7yo49","content":"<h3 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h3><p>阅读spark源码到storage这一块的时候，由于类的继承，调用之间的关系比较复杂，想要画一下UML图，idea自带的diagrams方法对java支持很好，但对scala的一些继承关系支持不佳，因此google了一下有没有可以画scala UML类图的工具，还真找到了：</p>\n<p>我是在x64 windows10下面，使用gitbash工具作为shell命令行，亲测可用</p>\n<h3 id=\"clone开源项目scaladiagrams并安装\"><a href=\"#clone开源项目scaladiagrams并安装\" class=\"headerlink\" title=\"clone开源项目scaladiagrams并安装\"></a>clone开源项目scaladiagrams并安装</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/mikeyhu/scaladiagrams.git</div><div class=\"line\"><span class=\"built_in\">cd</span> scaladiagrams</div><div class=\"line\">./build</div></pre></td></tr></table></figure>\n<h3 id=\"安装graphviz工具\"><a href=\"#安装graphviz工具\" class=\"headerlink\" title=\"安装graphviz工具\"></a>安装graphviz工具</h3><p>graphviz是一个开源的图形可视化软件，矢量图生成工具，与其他图形软件所不同，它的理念是“所想即所得”，通过dot语言来描述并绘制图形。<br><a href=\"http://www.graphviz.org/Download_windows.php\" target=\"_blank\" rel=\"external\">http://www.graphviz.org/Download_windows.php</a><br>如上链接下载，然后安装即可，将安装路径加入path中，该工具的目的是通过scaladiagrams工具生成的依赖关系画图；</p>\n<h3 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h3><h4 id=\"生成依赖关系文件dotFile\"><a href=\"#生成依赖关系文件dotFile\" class=\"headerlink\" title=\"生成依赖关系文件dotFile\"></a>生成依赖关系文件dotFile</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./scaladiagrams --source <span class=\"string\">\"D:\\spark-1.6.0\\core\\src\\main\\scala\\org\\apache\\spark\\storage\"</span> &gt; dotFile</div></pre></td></tr></table></figure>\n<p>dotFile文件就是依赖关系的文件：<br>官方命名为 dot语言，是一个表示图的语言，挺好玩的：</p>\n<figure class=\"highlight scala\"><figcaption><span>dot语言</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">digraph diagram &#123;</div><div class=\"line\"><span class=\"string\">\"BlockException\"</span> [style=filled, fillcolor=burlywood]</div><div class=\"line\">  <span class=\"string\">\"BlockException\"</span> -&gt; <span class=\"string\">\"Exception\"</span>;</div><div class=\"line\"></div><div class=\"line\"><span class=\"string\">\"BlockFetchException\"</span> [style=filled, fillcolor=burlywood]</div><div class=\"line\">  <span class=\"string\">\"BlockFetchException\"</span> -&gt; <span class=\"string\">\"SparkException\"</span>;</div><div class=\"line\"></div><div class=\"line\"><span class=\"string\">\"BlockId\"</span> [style=filled, fillcolor=darkorange]</div><div class=\"line\">  </div><div class=\"line\"></div><div class=\"line\"><span class=\"string\">\"RDDBlockId\"</span> [style=filled, fillcolor=burlywood]</div><div class=\"line\">  <span class=\"string\">\"RDDBlockId\"</span> -&gt; <span class=\"string\">\"BlockId\"</span>;</div><div class=\"line\"></div><div class=\"line\">  。。。</div></pre></td></tr></table></figure>\n<h4 id=\"使用graphviz工具画图\"><a href=\"#使用graphviz工具画图\" class=\"headerlink\" title=\"使用graphviz工具画图\"></a>使用graphviz工具画图</h4><p>生成svg文件，文件比较大的话建议用这个<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">cat dotFile | dot -Tsvg &gt; spark_storage.svg</div></pre></td></tr></table></figure></p>\n<p>生成png文件<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">cat dotFile | dot -Tpng &gt; spark_storage.png</div></pre></td></tr></table></figure></p>\n<p>画的效果部分截图如下（就是图有点扁平）：<br><img src=\"spark_storage_part.png\" alt=\"spark_storage_part.png\"></p>\n<p>因为好玩，又画了一个spark_core的类图，太大了，不好看，为了部分解决这个问题，只要在dot文件的第一行加入<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">rankdir=RL;</div></pre></td></tr></table></figure></p>\n<p>即可使得图片稍微好看一点<br><img src=\"spark_storage_part2.png\" alt=\"spark_storage_part2.png\"></p>\n<p>引用：<br><a href=\"http://stackoverflow.com/questions/7227952/generating-uml-diagram-from-scala-sources\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/7227952/generating-uml-diagram-from-scala-sources</a><br><a href=\"https://github.com/mikeyhu/scaladiagrams\" target=\"_blank\" rel=\"external\">https://github.com/mikeyhu/scaladiagrams</a><br><a href=\"http://www.graphviz.org/About.php\" target=\"_blank\" rel=\"external\">http://www.graphviz.org/About.php</a><br><a href=\"http://www.graphviz.org/pdf/dotguide.pdf\" target=\"_blank\" rel=\"external\">http://www.graphviz.org/pdf/dotguide.pdf</a><br><a href=\"http://www.tonyballantyne.com/graphs.html\" target=\"_blank\" rel=\"external\">http://www.tonyballantyne.com/graphs.html</a></p>\n","excerpt":"","more":"<h3 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h3><p>阅读spark源码到storage这一块的时候，由于类的继承，调用之间的关系比较复杂，想要画一下UML图，idea自带的diagrams方法对java支持很好，但对scala的一些继承关系支持不佳，因此google了一下有没有可以画scala UML类图的工具，还真找到了：</p>\n<p>我是在x64 windows10下面，使用gitbash工具作为shell命令行，亲测可用</p>\n<h3 id=\"clone开源项目scaladiagrams并安装\"><a href=\"#clone开源项目scaladiagrams并安装\" class=\"headerlink\" title=\"clone开源项目scaladiagrams并安装\"></a>clone开源项目scaladiagrams并安装</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/mikeyhu/scaladiagrams.git</div><div class=\"line\"><span class=\"built_in\">cd</span> scaladiagrams</div><div class=\"line\">./build</div></pre></td></tr></table></figure>\n<h3 id=\"安装graphviz工具\"><a href=\"#安装graphviz工具\" class=\"headerlink\" title=\"安装graphviz工具\"></a>安装graphviz工具</h3><p>graphviz是一个开源的图形可视化软件，矢量图生成工具，与其他图形软件所不同，它的理念是“所想即所得”，通过dot语言来描述并绘制图形。<br><a href=\"http://www.graphviz.org/Download_windows.php\">http://www.graphviz.org/Download_windows.php</a><br>如上链接下载，然后安装即可，将安装路径加入path中，该工具的目的是通过scaladiagrams工具生成的依赖关系画图；</p>\n<h3 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h3><h4 id=\"生成依赖关系文件dotFile\"><a href=\"#生成依赖关系文件dotFile\" class=\"headerlink\" title=\"生成依赖关系文件dotFile\"></a>生成依赖关系文件dotFile</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./scaladiagrams --source <span class=\"string\">\"D:\\spark-1.6.0\\core\\src\\main\\scala\\org\\apache\\spark\\storage\"</span> &gt; dotFile</div></pre></td></tr></table></figure>\n<p>dotFile文件就是依赖关系的文件：<br>官方命名为 dot语言，是一个表示图的语言，挺好玩的：</p>\n<figure class=\"highlight scala\"><figcaption><span>dot语言</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">digraph diagram &#123;</div><div class=\"line\"><span class=\"string\">\"BlockException\"</span> [style=filled, fillcolor=burlywood]</div><div class=\"line\">  <span class=\"string\">\"BlockException\"</span> -&gt; <span class=\"string\">\"Exception\"</span>;</div><div class=\"line\"></div><div class=\"line\"><span class=\"string\">\"BlockFetchException\"</span> [style=filled, fillcolor=burlywood]</div><div class=\"line\">  <span class=\"string\">\"BlockFetchException\"</span> -&gt; <span class=\"string\">\"SparkException\"</span>;</div><div class=\"line\"></div><div class=\"line\"><span class=\"string\">\"BlockId\"</span> [style=filled, fillcolor=darkorange]</div><div class=\"line\">  </div><div class=\"line\"></div><div class=\"line\"><span class=\"string\">\"RDDBlockId\"</span> [style=filled, fillcolor=burlywood]</div><div class=\"line\">  <span class=\"string\">\"RDDBlockId\"</span> -&gt; <span class=\"string\">\"BlockId\"</span>;</div><div class=\"line\"></div><div class=\"line\">  。。。</div></pre></td></tr></table></figure>\n<h4 id=\"使用graphviz工具画图\"><a href=\"#使用graphviz工具画图\" class=\"headerlink\" title=\"使用graphviz工具画图\"></a>使用graphviz工具画图</h4><p>生成svg文件，文件比较大的话建议用这个<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">cat dotFile | dot -Tsvg &gt; spark_storage.svg</div></pre></td></tr></table></figure></p>\n<p>生成png文件<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">cat dotFile | dot -Tpng &gt; spark_storage.png</div></pre></td></tr></table></figure></p>\n<p>画的效果部分截图如下（就是图有点扁平）：<br><img src=\"spark_storage_part.png\" alt=\"spark_storage_part.png\"></p>\n<p>因为好玩，又画了一个spark_core的类图，太大了，不好看，为了部分解决这个问题，只要在dot文件的第一行加入<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">rankdir=RL;</div></pre></td></tr></table></figure></p>\n<p>即可使得图片稍微好看一点<br><img src=\"spark_storage_part2.png\" alt=\"spark_storage_part2.png\"></p>\n<p>引用：<br><a href=\"http://stackoverflow.com/questions/7227952/generating-uml-diagram-from-scala-sources\">http://stackoverflow.com/questions/7227952/generating-uml-diagram-from-scala-sources</a><br><a href=\"https://github.com/mikeyhu/scaladiagrams\">https://github.com/mikeyhu/scaladiagrams</a><br><a href=\"http://www.graphviz.org/About.php\">http://www.graphviz.org/About.php</a><br><a href=\"http://www.graphviz.org/pdf/dotguide.pdf\">http://www.graphviz.org/pdf/dotguide.pdf</a><br><a href=\"http://www.tonyballantyne.com/graphs.html\">http://www.tonyballantyne.com/graphs.html</a></p>\n"},{"title":"spark支持snappy压缩踩坑总结","date":"2016-08-15T02:16:54.000Z","_content":"\n### 配置snappy压缩\n\n首先在/usr/lib/hadoop/lib/目录下配置lzo相关的包，\n然后在spark客户端配置如下：\n\n``` bash spark-default.conf\nspark.driver.extraClassPath /usr/lib/hadoop/lib/*\nspark.driver.extraLibraryPath /usr/lib/hadoop/lib/native\nspark.executor.extraClassPath /usr/lib/hadoop/lib/*\nspark.executor.extraLibraryPath /usr/lib/hadoop/lib/native\n```\n如上配置，即可，但是为了得到这么小小的一点配置，浪费了三天的时间啊，网上的资料都是转载，无法解决问题。最新的官网的配置文件中并没有关于spark.executor.extraClassPath的配置，查了源码才得知，作为教训。以后出现问题要冷静思考，不要简单的去网上搜索，先判断问题出现的原因，知其所以然，必要时要去源码中查询，否则会浪费很多时间，走很多弯路。\n\n### 踩坑集锦\n\n首先，会遇到这个错误：\n\n``` bash\nCompression codec com.hadoop.compression.lzo.LzoCodec not found\n```\n\n原因是spark-env.sh的配置文件缺少关联hadoop的配置语句\n\n``` bash\nexport SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/usr/lib/hadoop/lib/native/:/usr/lib/hadoop/lib/*\n```\n\n然后yarn-cluster模式下snappy压缩总会报错：\n\n``` java\n16/08/08 19:05:03 DEBUG util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n16/08/08 19:05:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n1）\n16/08/08 19:05:03 DEBUG util.PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.\n16/08/08 19:05:03 DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection\n16/08/08 19:05:03 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library\njava.lang.UnsatisfiedLinkError: no gplcompression in java.library.path\n at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1886)\n at java.lang.Runtime.loadLibrary0(Runtime.java:849)\n at java.lang.System.loadLibrary(System.java:1088)\n at com.hadoop.compression.lzo.GPLNativeCodeLoader.<clinit>(GPLNativeCodeLoader.java:32)\n at com.hadoop.compression.lzo.LzoCodec.<clinit>(LzoCodec.java:71)\n at java.lang.Class.forName0(Native Method)\n at java.lang.Class.forName(Class.java:274)\n at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2013)\n at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1978)\n at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)\n at org.apache.hadoop.io.compress.CompressionCodecFactory.<init>(CompressionCodecFactory.java:175)\n at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n```\n\n这个究其原因就是程序运行的那个节点找不到lzo解压的包导致的，官网中只说明了 spark.driver.extraClassPath，但并没有说明配置spark.executor.extraClassPath 与 spark.executor.extraLibraryPath，导致不管怎么根据网上博客或者官网配置配，executor还是找不到lzo压缩相关的包，后来聪哥通过源码查看才发现有这么一个参数配置，只是各类文档中都没有，加上就ok了~\n","source":"_posts/2016-08-15-spark支持snappy压缩踩坑总结.md","raw":"---\ntitle: spark支持snappy压缩踩坑总结\ndate: 2016-08-15 10:16:54\ntags: spark\ncategories: spark\n---\n\n### 配置snappy压缩\n\n首先在/usr/lib/hadoop/lib/目录下配置lzo相关的包，\n然后在spark客户端配置如下：\n\n``` bash spark-default.conf\nspark.driver.extraClassPath /usr/lib/hadoop/lib/*\nspark.driver.extraLibraryPath /usr/lib/hadoop/lib/native\nspark.executor.extraClassPath /usr/lib/hadoop/lib/*\nspark.executor.extraLibraryPath /usr/lib/hadoop/lib/native\n```\n如上配置，即可，但是为了得到这么小小的一点配置，浪费了三天的时间啊，网上的资料都是转载，无法解决问题。最新的官网的配置文件中并没有关于spark.executor.extraClassPath的配置，查了源码才得知，作为教训。以后出现问题要冷静思考，不要简单的去网上搜索，先判断问题出现的原因，知其所以然，必要时要去源码中查询，否则会浪费很多时间，走很多弯路。\n\n### 踩坑集锦\n\n首先，会遇到这个错误：\n\n``` bash\nCompression codec com.hadoop.compression.lzo.LzoCodec not found\n```\n\n原因是spark-env.sh的配置文件缺少关联hadoop的配置语句\n\n``` bash\nexport SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:/usr/lib/hadoop/lib/native/:/usr/lib/hadoop/lib/*\n```\n\n然后yarn-cluster模式下snappy压缩总会报错：\n\n``` java\n16/08/08 19:05:03 DEBUG util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n16/08/08 19:05:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n1）\n16/08/08 19:05:03 DEBUG util.PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.\n16/08/08 19:05:03 DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection\n16/08/08 19:05:03 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library\njava.lang.UnsatisfiedLinkError: no gplcompression in java.library.path\n at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1886)\n at java.lang.Runtime.loadLibrary0(Runtime.java:849)\n at java.lang.System.loadLibrary(System.java:1088)\n at com.hadoop.compression.lzo.GPLNativeCodeLoader.<clinit>(GPLNativeCodeLoader.java:32)\n at com.hadoop.compression.lzo.LzoCodec.<clinit>(LzoCodec.java:71)\n at java.lang.Class.forName0(Native Method)\n at java.lang.Class.forName(Class.java:274)\n at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2013)\n at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1978)\n at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)\n at org.apache.hadoop.io.compress.CompressionCodecFactory.<init>(CompressionCodecFactory.java:175)\n at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n```\n\n这个究其原因就是程序运行的那个节点找不到lzo解压的包导致的，官网中只说明了 spark.driver.extraClassPath，但并没有说明配置spark.executor.extraClassPath 与 spark.executor.extraLibraryPath，导致不管怎么根据网上博客或者官网配置配，executor还是找不到lzo压缩相关的包，后来聪哥通过源码查看才发现有这么一个参数配置，只是各类文档中都没有，加上就ok了~\n","slug":"spark支持snappy压缩踩坑总结","published":1,"updated":"2016-09-18T10:32:28.138Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwftx0009psgur5e43tl9","content":"<h3 id=\"配置snappy压缩\"><a href=\"#配置snappy压缩\" class=\"headerlink\" title=\"配置snappy压缩\"></a>配置snappy压缩</h3><p>首先在/usr/lib/hadoop/lib/目录下配置lzo相关的包，<br>然后在spark客户端配置如下：</p>\n<figure class=\"highlight bash\"><figcaption><span>spark-default.conf</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">spark.driver.extraClassPath /usr/lib/hadoop/lib/*</div><div class=\"line\">spark.driver.extraLibraryPath /usr/lib/hadoop/lib/native</div><div class=\"line\">spark.executor.extraClassPath /usr/lib/hadoop/lib/*</div><div class=\"line\">spark.executor.extraLibraryPath /usr/lib/hadoop/lib/native</div></pre></td></tr></table></figure>\n<p>如上配置，即可，但是为了得到这么小小的一点配置，浪费了三天的时间啊，网上的资料都是转载，无法解决问题。最新的官网的配置文件中并没有关于spark.executor.extraClassPath的配置，查了源码才得知，作为教训。以后出现问题要冷静思考，不要简单的去网上搜索，先判断问题出现的原因，知其所以然，必要时要去源码中查询，否则会浪费很多时间，走很多弯路。</p>\n<h3 id=\"踩坑集锦\"><a href=\"#踩坑集锦\" class=\"headerlink\" title=\"踩坑集锦\"></a>踩坑集锦</h3><p>首先，会遇到这个错误：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">Compression codec com.hadoop.compression.lzo.LzoCodec not found</div></pre></td></tr></table></figure>\n<p>原因是spark-env.sh的配置文件缺少关联hadoop的配置语句</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">export</span> SPARK_LIBRARY_PATH=<span class=\"variable\">$SPARK_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/:/usr/lib/hadoop/lib/*</div></pre></td></tr></table></figure>\n<p>然后yarn-cluster模式下snappy压缩总会报错：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"number\">16</span>/<span class=\"number\">08</span>/<span class=\"number\">08</span> <span class=\"number\">19</span>:<span class=\"number\">05</span>:<span class=\"number\">03</span> DEBUG util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class=\"line\"><span class=\"number\">16</span>/<span class=\"number\">08</span>/<span class=\"number\">08</span> <span class=\"number\">19</span>:<span class=\"number\">05</span>:<span class=\"number\">03</span> WARN util.NativeCodeLoader: Unable to load <span class=\"keyword\">native</span>-hadoop library <span class=\"keyword\">for</span> your platform... using builtin-java classes where applicable</div><div class=\"line\"><span class=\"number\">1</span>）</div><div class=\"line\"><span class=\"number\">16</span>/<span class=\"number\">08</span>/<span class=\"number\">08</span> <span class=\"number\">19</span>:<span class=\"number\">05</span>:<span class=\"number\">03</span> DEBUG util.PerformanceAdvisory: Both <span class=\"keyword\">short</span>-circuit local reads and UNIX domain socket are disabled.</div><div class=\"line\"><span class=\"number\">16</span>/<span class=\"number\">08</span>/<span class=\"number\">08</span> <span class=\"number\">19</span>:<span class=\"number\">05</span>:<span class=\"number\">03</span> DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration <span class=\"keyword\">for</span> dfs.data.transfer.protection</div><div class=\"line\"><span class=\"number\">16</span>/<span class=\"number\">08</span>/<span class=\"number\">08</span> <span class=\"number\">19</span>:<span class=\"number\">05</span>:<span class=\"number\">03</span> ERROR lzo.GPLNativeCodeLoader: Could not load <span class=\"keyword\">native</span> gpl library</div><div class=\"line\">java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path</div><div class=\"line\"> at java.lang.ClassLoader.loadLibrary(ClassLoader.java:<span class=\"number\">1886</span>)</div><div class=\"line\"> at java.lang.Runtime.loadLibrary0(Runtime.java:<span class=\"number\">849</span>)</div><div class=\"line\"> at java.lang.System.loadLibrary(System.java:<span class=\"number\">1088</span>)</div><div class=\"line\"> at com.hadoop.compression.lzo.GPLNativeCodeLoader.&lt;clinit&gt;(GPLNativeCodeLoader.java:<span class=\"number\">32</span>)</div><div class=\"line\"> at com.hadoop.compression.lzo.LzoCodec.&lt;clinit&gt;(LzoCodec.java:<span class=\"number\">71</span>)</div><div class=\"line\"> at java.lang.Class.forName0(Native Method)</div><div class=\"line\"> at java.lang.Class.forName(Class.java:<span class=\"number\">274</span>)</div><div class=\"line\"> at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:<span class=\"number\">2013</span>)</div><div class=\"line\"> at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:<span class=\"number\">1978</span>)</div><div class=\"line\"> at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:<span class=\"number\">128</span>)</div><div class=\"line\"> at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:<span class=\"number\">175</span>)</div><div class=\"line\"> at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:<span class=\"number\">45</span>)</div><div class=\"line\"> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div></pre></td></tr></table></figure>\n<p>这个究其原因就是程序运行的那个节点找不到lzo解压的包导致的，官网中只说明了 spark.driver.extraClassPath，但并没有说明配置spark.executor.extraClassPath 与 spark.executor.extraLibraryPath，导致不管怎么根据网上博客或者官网配置配，executor还是找不到lzo压缩相关的包，后来聪哥通过源码查看才发现有这么一个参数配置，只是各类文档中都没有，加上就ok了~</p>\n","excerpt":"","more":"<h3 id=\"配置snappy压缩\"><a href=\"#配置snappy压缩\" class=\"headerlink\" title=\"配置snappy压缩\"></a>配置snappy压缩</h3><p>首先在/usr/lib/hadoop/lib/目录下配置lzo相关的包，<br>然后在spark客户端配置如下：</p>\n<figure class=\"highlight bash\"><figcaption><span>spark-default.conf</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">spark.driver.extraClassPath /usr/lib/hadoop/lib/*</div><div class=\"line\">spark.driver.extraLibraryPath /usr/lib/hadoop/lib/native</div><div class=\"line\">spark.executor.extraClassPath /usr/lib/hadoop/lib/*</div><div class=\"line\">spark.executor.extraLibraryPath /usr/lib/hadoop/lib/native</div></pre></td></tr></table></figure>\n<p>如上配置，即可，但是为了得到这么小小的一点配置，浪费了三天的时间啊，网上的资料都是转载，无法解决问题。最新的官网的配置文件中并没有关于spark.executor.extraClassPath的配置，查了源码才得知，作为教训。以后出现问题要冷静思考，不要简单的去网上搜索，先判断问题出现的原因，知其所以然，必要时要去源码中查询，否则会浪费很多时间，走很多弯路。</p>\n<h3 id=\"踩坑集锦\"><a href=\"#踩坑集锦\" class=\"headerlink\" title=\"踩坑集锦\"></a>踩坑集锦</h3><p>首先，会遇到这个错误：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">Compression codec com.hadoop.compression.lzo.LzoCodec not found</div></pre></td></tr></table></figure>\n<p>原因是spark-env.sh的配置文件缺少关联hadoop的配置语句</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">export</span> SPARK_LIBRARY_PATH=<span class=\"variable\">$SPARK_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/:/usr/lib/hadoop/lib/*</div></pre></td></tr></table></figure>\n<p>然后yarn-cluster模式下snappy压缩总会报错：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"number\">16</span>/<span class=\"number\">08</span>/<span class=\"number\">08</span> <span class=\"number\">19</span>:<span class=\"number\">05</span>:<span class=\"number\">03</span> DEBUG util.NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class=\"line\"><span class=\"number\">16</span>/<span class=\"number\">08</span>/<span class=\"number\">08</span> <span class=\"number\">19</span>:<span class=\"number\">05</span>:<span class=\"number\">03</span> WARN util.NativeCodeLoader: Unable to load <span class=\"keyword\">native</span>-hadoop library <span class=\"keyword\">for</span> your platform... using builtin-java classes where applicable</div><div class=\"line\"><span class=\"number\">1</span>）</div><div class=\"line\"><span class=\"number\">16</span>/<span class=\"number\">08</span>/<span class=\"number\">08</span> <span class=\"number\">19</span>:<span class=\"number\">05</span>:<span class=\"number\">03</span> DEBUG util.PerformanceAdvisory: Both <span class=\"keyword\">short</span>-circuit local reads and UNIX domain socket are disabled.</div><div class=\"line\"><span class=\"number\">16</span>/<span class=\"number\">08</span>/<span class=\"number\">08</span> <span class=\"number\">19</span>:<span class=\"number\">05</span>:<span class=\"number\">03</span> DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration <span class=\"keyword\">for</span> dfs.data.transfer.protection</div><div class=\"line\"><span class=\"number\">16</span>/<span class=\"number\">08</span>/<span class=\"number\">08</span> <span class=\"number\">19</span>:<span class=\"number\">05</span>:<span class=\"number\">03</span> ERROR lzo.GPLNativeCodeLoader: Could not load <span class=\"keyword\">native</span> gpl library</div><div class=\"line\">java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path</div><div class=\"line\"> at java.lang.ClassLoader.loadLibrary(ClassLoader.java:<span class=\"number\">1886</span>)</div><div class=\"line\"> at java.lang.Runtime.loadLibrary0(Runtime.java:<span class=\"number\">849</span>)</div><div class=\"line\"> at java.lang.System.loadLibrary(System.java:<span class=\"number\">1088</span>)</div><div class=\"line\"> at com.hadoop.compression.lzo.GPLNativeCodeLoader.&lt;clinit&gt;(GPLNativeCodeLoader.java:<span class=\"number\">32</span>)</div><div class=\"line\"> at com.hadoop.compression.lzo.LzoCodec.&lt;clinit&gt;(LzoCodec.java:<span class=\"number\">71</span>)</div><div class=\"line\"> at java.lang.Class.forName0(Native Method)</div><div class=\"line\"> at java.lang.Class.forName(Class.java:<span class=\"number\">274</span>)</div><div class=\"line\"> at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:<span class=\"number\">2013</span>)</div><div class=\"line\"> at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:<span class=\"number\">1978</span>)</div><div class=\"line\"> at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:<span class=\"number\">128</span>)</div><div class=\"line\"> at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:<span class=\"number\">175</span>)</div><div class=\"line\"> at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:<span class=\"number\">45</span>)</div><div class=\"line\"> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div></pre></td></tr></table></figure>\n<p>这个究其原因就是程序运行的那个节点找不到lzo解压的包导致的，官网中只说明了 spark.driver.extraClassPath，但并没有说明配置spark.executor.extraClassPath 与 spark.executor.extraLibraryPath，导致不管怎么根据网上博客或者官网配置配，executor还是找不到lzo压缩相关的包，后来聪哥通过源码查看才发现有这么一个参数配置，只是各类文档中都没有，加上就ok了~</p>\n"},{"title":"在Kerberos环境下配置hue通过spark-thrift-server访问SparkSql","date":"2016-09-05T05:47:02.000Z","toc":true,"comments":1,"_content":"hue-spark-thriftserver-kerberos\n### 背景说明\nKerberos项目最后要对基于Hue的TODP平台进行安全测试，在搭建配置的过程中踩了一些坑，现在把其中的配置与步骤进行总结，以免以后忘记。\n\n其中用到以下代号：\n40机器：hue平台所在的机器\n76机器：spark thrift服务端口10010，hive-thrift-server服务端口10000\n74机器：spark thrift服务端口10010，hive-thrift-server服务端口10000\nTEST-BDD-HIVESERVER机器：负载均衡所在的机器，负载均衡机器需要配合开启10000和10010端口\n\n在kerberos认证下, sparksql的thriftserver连接hiveserver2变得相对复杂，主要是因为各种kerberos认证出现各种问题。后来由于hive使用了负载均衡，所以spark-sql也需加入负载均衡，否则不能使用，就是这个负载均衡服务器的加入使得kerberos认证变得更加复杂，使得不明原理的新手在配置kerberos的keytab与principal时各种不匹配。这里是通过Hue可视化界面调用后台的sparksql,然后sparksql通过JDBC连接Hive的hiveServer2服务。\n\n### 40机器hue端配置\n\n进入40机器hue所在的目录\n``` bash hue.ini\n$ cd /usr/lib/hue/ \n$ vim desktop/conf/hue.ini\n```\n修改hue的配置文件如下\n``` bash\n1119 [spark]\n...\n1134   # spark-sql config\n1135   spark_sql_server_host=TEST-BDD-HIVESERVER\n1136   ## spark_sql_server_port=10010\n```\n由于此处使用了负载均衡，所以上述TEST-BDD-HIVESERVER指向的是负载均衡所在的ip，最终会转发给两个spark-thrift-server\n\n### Kerberos服务器端配置\n生成类似 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN 的keytab，配置了负载均衡后，使用test-bdd-hiveserver\n\n### 76机器上的配置\n76机器与74机器配置步骤一样，只是hive-site.xml需要改一处，将下面的 076改成 074即可\n``` bash hive-site.xml\n<property>\n   <name>hive.server2.thrift.bind.host</name>\n   <value>test-bdd-076</value>\n   <description>TCP port number to listen on, default 10000</description>\n</property>\n```\n\n其它都一样，所以在这里只写076的配置步骤\n\n#### 从hive.keytab创建spark的keytab\n然后在/etc/hive/conf/下创建spark需要的keytab，在这里使用hiveserver的keytab，将已有的hive.keytab_hiveserver 拷贝成 hive.keytab_sparkthrift，然后修改权限如下：\n\n``` bash\n-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver\n-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift\n```\n修改好后用如下命令检查：\n\n``` bash\n$ sudo klist -k hive.keytab_sparkthrift \nKeytab name: FILE:hive.keytab_sparkthrift\nKVNO Principal\n---- --------------------------------------------------------------------------\n   1 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN\n   1 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN\n   1 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN\n   1 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN\n   1 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN\n```\n如果klist是如上结果，就对了\n\n#### 配置spark需要的hive-site.xml\n\n由于需要修改hive的一些配置，进入76机器spark所在的目录，将`/etc/hive/conf/`下的`hive-site.xml`拷贝到spark的conf下，赋予权限并修改\n``` bash\n$ sudo cp /etc/hive/conf/hive-site.xml $SPARK_HOME/conf/\n$ cd $SPARK_HOME\n$ sudo chmod op conf/hive-site.xml\n$ vim conf/hive-site.xml\n```\n修改hive-site.xml,增加hive.server2.thrift.bind.host\n\n``` bash hive-site.xml\n<!-- ZooKeeper conf-->\n<property>\n  <name>hive.server2.enable.doAs</name>\n  <value>false</value>\n  <description> Impersonate the connected user </description>\n</property>\n<property>\n  <name>hive.server2.thrift.port</name>\n  <value>10010</value>\n  <description>TCP port number to listen on, default 10000</description>\n</property>\n\n<property>\n   <name>hive.server2.thrift.bind.host</name>\n   <value>test-bdd-076</value>\n   <description>TCP port number to listen on, default 10000</description>\n </property>\n\n<property>\n  <name>hive.metastore.execute.setugi</name>\n  <value>true</value>\n</property>\n<property>\n   <name>hive.server2.authentication.kerberos.principal</name>\n   <value>hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN</value>\n </property>\n<property>\n  <name>hive.server2.authentication.kerberos.keytab</name>\n  <value>/etc/hive/conf/hive.keytab_sparkthrift</value>\n</property>\n\n#### 启动Spark-thrift-server\n``` bash\n$ cd $SPARK_HOME\n$ ./sbin/start-thriftserver.sh\n```\n可以通过如下日志查看是否启动成功：\n``` bash\n$ vim logs/spark-op-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-TEST-BDD-076.out \n```\n启动成功会看到如下日志:\n``` bash\n  96 16/09/05 13:41:25 INFO AbstractService: Service:HiveServer2 is started.\n  97 16/09/05 13:41:25 INFO HiveThriftServer2: HiveThriftServer2 started\n  98 16/09/05 13:41:25 INFO UserGroupInformation: Login successful for user hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN using keytab file /etc/hive/conf/hive.keytab_sparkthrift\n  99 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n 100 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=0\n 101 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\n 102 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n 103 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=1\n 104 16/09/05 13:41:25 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10010 with 5...500 worker threads\n ```\n\n### 负载均衡机器的查看\n进入 67.121机器\n输入 命令 `sudo ipvsadm -ln`\n\n``` bash\n$ sudo ipvsadm -ln\nIP Virtual Server version 1.2.1 (size=4194304)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.142.67.123:10000 wlc persistent 7200 synproxy\n  -> 10.142.78.74:10000           FullNat 50     3          0         \n  -> 10.142.78.76:10000           FullNat 50     0          0         \nTCP  10.142.67.123:10010 wlc persistent 7200 synproxy\n  -> 10.142.78.74:10010           FullNat 50     0          0         \n  -> 10.142.78.76:10010           FullNat 50     2          0    \n```\n就可以看到负载均衡的情况了：\n\n### 踩坑说明以及解决方案\n\n#### 缺少配置kerberos认证错误\n需要在hive-site.xml文件中添加kerberos认证相关配置\n#### kerberos认证失败\n1)  在hive-site.xml中配置好kerberos认证，但是op用户下无法读取hive.keytab的问题，出现unable to login ...given keytab/principal 以及Unable to obtain password from user。因为hive.keytab 是hive用户创建的，op用户无法读取，导致看似kerberos已经配置好，\n但是程序没有读取权限，依旧认为没有配置好，这是会有在日志文件中会有NULLPOINT类似的错误提示，说明是没有读取权限。解决方案是复制hive.keytab到op用户下。\n2）在hue界面连接spark时可能会出现10010端口不能连接的问题，这是sparkthrift没有启动导致的；\n3）spark thriftserver明明已经启动，但是hue界面仍旧不能连接，出现TTransportException的错误，原因是kerberos配置没有配置正确，即没有配置kerberos认证的keytab与principal。hive/test-bdd-hiveserver必须与hive.keytab_hiveserver配套使用，同理，test-bdd-074或者 test-bdd-076必须与hive/test-bdd-74或者hive/test-bdd-76配套使用，否则出现认证失败的问题。\n#### hue的配置问题。\n在hue的desktop/conf目录下hue.ini文件中，主要配置spark_sql_server_host，也就是spark thriftserver所在主机，这里可以是负载均衡服务器TEST-BDD-HIVESERVER,spark_sql_server_port 是spark thriftserver的服务端口。\n需要注意的是，加上kerberos认证后，主机名不能是ip地址的形式，需要FQDN的形式。hive的配置需要注意的是hive_server_host，这里绝对不能是hiveserver2的服务器的地址，一定是负载均衡服务器的地址，不然在hue界面连接HIVE时出现\nUnable to access databases, Query Server or Metastore may be down.的错误以及GSS initial failed的错误，无法访问hive数据库。\n#### metastore的问题\n连接metastore也需要principal的认证。\n``` bash\n20 <property>\n221   <name>hive.metastore.sasl.enabled</name>\n222   <value>true</value>\n223   <description>If true, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.</description>\n224 </property>\n225 <property>\n226   <name>hive.metastore.kerberos.principal</name>\n227   <value>hive/_HOST@HADOOP.CHINATELECOM.CN</value>\n228   <description>The service principal for the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.</description>\n229 </property>\n```\n\n之所以问题多多，主要原因是对kerberos+Hive+lvs整体原理没有搞清楚，以至于在配置过程中出现各种错误。我们搭建的hive集群有74,76两台主机，spark thriftserver也有74,76两台主机，负载均衡服务器在test-bdd-hiveserver上。在配置时，需要将spark-sql-server-host配置成test-bdd-hiveserver,因为对spark而言，74与76上的hiveserver是一个整体，不能配置成单一的主机，不然lvs可能会将服务分到另外一台主机上，造成主机配置失败。","source":"_posts/2016-09-05-在Kerberos环境下配置hue通过spark-thrift-server访问SparkSql.md","raw":"---\ntitle: 在Kerberos环境下配置hue通过spark-thrift-server访问SparkSql\ndate: 2016-09-05 13:47:02\ntoc: true\ncomments: true\ntags: \n- spark\n- hue\n- kerberos\ncategories:\n- spark\n---\nhue-spark-thriftserver-kerberos\n### 背景说明\nKerberos项目最后要对基于Hue的TODP平台进行安全测试，在搭建配置的过程中踩了一些坑，现在把其中的配置与步骤进行总结，以免以后忘记。\n\n其中用到以下代号：\n40机器：hue平台所在的机器\n76机器：spark thrift服务端口10010，hive-thrift-server服务端口10000\n74机器：spark thrift服务端口10010，hive-thrift-server服务端口10000\nTEST-BDD-HIVESERVER机器：负载均衡所在的机器，负载均衡机器需要配合开启10000和10010端口\n\n在kerberos认证下, sparksql的thriftserver连接hiveserver2变得相对复杂，主要是因为各种kerberos认证出现各种问题。后来由于hive使用了负载均衡，所以spark-sql也需加入负载均衡，否则不能使用，就是这个负载均衡服务器的加入使得kerberos认证变得更加复杂，使得不明原理的新手在配置kerberos的keytab与principal时各种不匹配。这里是通过Hue可视化界面调用后台的sparksql,然后sparksql通过JDBC连接Hive的hiveServer2服务。\n\n### 40机器hue端配置\n\n进入40机器hue所在的目录\n``` bash hue.ini\n$ cd /usr/lib/hue/ \n$ vim desktop/conf/hue.ini\n```\n修改hue的配置文件如下\n``` bash\n1119 [spark]\n...\n1134   # spark-sql config\n1135   spark_sql_server_host=TEST-BDD-HIVESERVER\n1136   ## spark_sql_server_port=10010\n```\n由于此处使用了负载均衡，所以上述TEST-BDD-HIVESERVER指向的是负载均衡所在的ip，最终会转发给两个spark-thrift-server\n\n### Kerberos服务器端配置\n生成类似 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN 的keytab，配置了负载均衡后，使用test-bdd-hiveserver\n\n### 76机器上的配置\n76机器与74机器配置步骤一样，只是hive-site.xml需要改一处，将下面的 076改成 074即可\n``` bash hive-site.xml\n<property>\n   <name>hive.server2.thrift.bind.host</name>\n   <value>test-bdd-076</value>\n   <description>TCP port number to listen on, default 10000</description>\n</property>\n```\n\n其它都一样，所以在这里只写076的配置步骤\n\n#### 从hive.keytab创建spark的keytab\n然后在/etc/hive/conf/下创建spark需要的keytab，在这里使用hiveserver的keytab，将已有的hive.keytab_hiveserver 拷贝成 hive.keytab_sparkthrift，然后修改权限如下：\n\n``` bash\n-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver\n-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift\n```\n修改好后用如下命令检查：\n\n``` bash\n$ sudo klist -k hive.keytab_sparkthrift \nKeytab name: FILE:hive.keytab_sparkthrift\nKVNO Principal\n---- --------------------------------------------------------------------------\n   1 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN\n   1 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN\n   1 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN\n   1 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN\n   1 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN\n```\n如果klist是如上结果，就对了\n\n#### 配置spark需要的hive-site.xml\n\n由于需要修改hive的一些配置，进入76机器spark所在的目录，将`/etc/hive/conf/`下的`hive-site.xml`拷贝到spark的conf下，赋予权限并修改\n``` bash\n$ sudo cp /etc/hive/conf/hive-site.xml $SPARK_HOME/conf/\n$ cd $SPARK_HOME\n$ sudo chmod op conf/hive-site.xml\n$ vim conf/hive-site.xml\n```\n修改hive-site.xml,增加hive.server2.thrift.bind.host\n\n``` bash hive-site.xml\n<!-- ZooKeeper conf-->\n<property>\n  <name>hive.server2.enable.doAs</name>\n  <value>false</value>\n  <description> Impersonate the connected user </description>\n</property>\n<property>\n  <name>hive.server2.thrift.port</name>\n  <value>10010</value>\n  <description>TCP port number to listen on, default 10000</description>\n</property>\n\n<property>\n   <name>hive.server2.thrift.bind.host</name>\n   <value>test-bdd-076</value>\n   <description>TCP port number to listen on, default 10000</description>\n </property>\n\n<property>\n  <name>hive.metastore.execute.setugi</name>\n  <value>true</value>\n</property>\n<property>\n   <name>hive.server2.authentication.kerberos.principal</name>\n   <value>hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN</value>\n </property>\n<property>\n  <name>hive.server2.authentication.kerberos.keytab</name>\n  <value>/etc/hive/conf/hive.keytab_sparkthrift</value>\n</property>\n\n#### 启动Spark-thrift-server\n``` bash\n$ cd $SPARK_HOME\n$ ./sbin/start-thriftserver.sh\n```\n可以通过如下日志查看是否启动成功：\n``` bash\n$ vim logs/spark-op-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-TEST-BDD-076.out \n```\n启动成功会看到如下日志:\n``` bash\n  96 16/09/05 13:41:25 INFO AbstractService: Service:HiveServer2 is started.\n  97 16/09/05 13:41:25 INFO HiveThriftServer2: HiveThriftServer2 started\n  98 16/09/05 13:41:25 INFO UserGroupInformation: Login successful for user hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN using keytab file /etc/hive/conf/hive.keytab_sparkthrift\n  99 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n 100 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=0\n 101 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\n 102 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n 103 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=1\n 104 16/09/05 13:41:25 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10010 with 5...500 worker threads\n ```\n\n### 负载均衡机器的查看\n进入 67.121机器\n输入 命令 `sudo ipvsadm -ln`\n\n``` bash\n$ sudo ipvsadm -ln\nIP Virtual Server version 1.2.1 (size=4194304)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.142.67.123:10000 wlc persistent 7200 synproxy\n  -> 10.142.78.74:10000           FullNat 50     3          0         \n  -> 10.142.78.76:10000           FullNat 50     0          0         \nTCP  10.142.67.123:10010 wlc persistent 7200 synproxy\n  -> 10.142.78.74:10010           FullNat 50     0          0         \n  -> 10.142.78.76:10010           FullNat 50     2          0    \n```\n就可以看到负载均衡的情况了：\n\n### 踩坑说明以及解决方案\n\n#### 缺少配置kerberos认证错误\n需要在hive-site.xml文件中添加kerberos认证相关配置\n#### kerberos认证失败\n1)  在hive-site.xml中配置好kerberos认证，但是op用户下无法读取hive.keytab的问题，出现unable to login ...given keytab/principal 以及Unable to obtain password from user。因为hive.keytab 是hive用户创建的，op用户无法读取，导致看似kerberos已经配置好，\n但是程序没有读取权限，依旧认为没有配置好，这是会有在日志文件中会有NULLPOINT类似的错误提示，说明是没有读取权限。解决方案是复制hive.keytab到op用户下。\n2）在hue界面连接spark时可能会出现10010端口不能连接的问题，这是sparkthrift没有启动导致的；\n3）spark thriftserver明明已经启动，但是hue界面仍旧不能连接，出现TTransportException的错误，原因是kerberos配置没有配置正确，即没有配置kerberos认证的keytab与principal。hive/test-bdd-hiveserver必须与hive.keytab_hiveserver配套使用，同理，test-bdd-074或者 test-bdd-076必须与hive/test-bdd-74或者hive/test-bdd-76配套使用，否则出现认证失败的问题。\n#### hue的配置问题。\n在hue的desktop/conf目录下hue.ini文件中，主要配置spark_sql_server_host，也就是spark thriftserver所在主机，这里可以是负载均衡服务器TEST-BDD-HIVESERVER,spark_sql_server_port 是spark thriftserver的服务端口。\n需要注意的是，加上kerberos认证后，主机名不能是ip地址的形式，需要FQDN的形式。hive的配置需要注意的是hive_server_host，这里绝对不能是hiveserver2的服务器的地址，一定是负载均衡服务器的地址，不然在hue界面连接HIVE时出现\nUnable to access databases, Query Server or Metastore may be down.的错误以及GSS initial failed的错误，无法访问hive数据库。\n#### metastore的问题\n连接metastore也需要principal的认证。\n``` bash\n20 <property>\n221   <name>hive.metastore.sasl.enabled</name>\n222   <value>true</value>\n223   <description>If true, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.</description>\n224 </property>\n225 <property>\n226   <name>hive.metastore.kerberos.principal</name>\n227   <value>hive/_HOST@HADOOP.CHINATELECOM.CN</value>\n228   <description>The service principal for the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.</description>\n229 </property>\n```\n\n之所以问题多多，主要原因是对kerberos+Hive+lvs整体原理没有搞清楚，以至于在配置过程中出现各种错误。我们搭建的hive集群有74,76两台主机，spark thriftserver也有74,76两台主机，负载均衡服务器在test-bdd-hiveserver上。在配置时，需要将spark-sql-server-host配置成test-bdd-hiveserver,因为对spark而言，74与76上的hiveserver是一个整体，不能配置成单一的主机，不然lvs可能会将服务分到另外一台主机上，造成主机配置失败。","slug":"在Kerberos环境下配置hue通过spark-thrift-server访问SparkSql","published":1,"updated":"2016-09-13T07:23:02.559Z","layout":"post","photos":[],"link":"","_id":"cjldkwfu7000cpsguapukpi87","content":"<p>hue-spark-thriftserver-kerberos</p>\n<h3 id=\"背景说明\"><a href=\"#背景说明\" class=\"headerlink\" title=\"背景说明\"></a>背景说明</h3><p>Kerberos项目最后要对基于Hue的TODP平台进行安全测试，在搭建配置的过程中踩了一些坑，现在把其中的配置与步骤进行总结，以免以后忘记。</p>\n<p>其中用到以下代号：<br>40机器：hue平台所在的机器<br>76机器：spark thrift服务端口10010，hive-thrift-server服务端口10000<br>74机器：spark thrift服务端口10010，hive-thrift-server服务端口10000<br>TEST-BDD-HIVESERVER机器：负载均衡所在的机器，负载均衡机器需要配合开启10000和10010端口</p>\n<p>在kerberos认证下, sparksql的thriftserver连接hiveserver2变得相对复杂，主要是因为各种kerberos认证出现各种问题。后来由于hive使用了负载均衡，所以spark-sql也需加入负载均衡，否则不能使用，就是这个负载均衡服务器的加入使得kerberos认证变得更加复杂，使得不明原理的新手在配置kerberos的keytab与principal时各种不匹配。这里是通过Hue可视化界面调用后台的sparksql,然后sparksql通过JDBC连接Hive的hiveServer2服务。</p>\n<h3 id=\"40机器hue端配置\"><a href=\"#40机器hue端配置\" class=\"headerlink\" title=\"40机器hue端配置\"></a>40机器hue端配置</h3><p>进入40机器hue所在的目录<br><figure class=\"highlight bash\"><figcaption><span>hue.ini</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ <span class=\"built_in\">cd</span> /usr/lib/hue/ </div><div class=\"line\">$ vim desktop/conf/hue.ini</div></pre></td></tr></table></figure></p>\n<p>修改hue的配置文件如下<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">1119 [spark]</div><div class=\"line\">...</div><div class=\"line\">1134   <span class=\"comment\"># spark-sql config</span></div><div class=\"line\">1135   spark_sql_server_host=TEST-BDD-HIVESERVER</div><div class=\"line\">1136   <span class=\"comment\">## spark_sql_server_port=10010</span></div></pre></td></tr></table></figure></p>\n<p>由于此处使用了负载均衡，所以上述TEST-BDD-HIVESERVER指向的是负载均衡所在的ip，最终会转发给两个spark-thrift-server</p>\n<h3 id=\"Kerberos服务器端配置\"><a href=\"#Kerberos服务器端配置\" class=\"headerlink\" title=\"Kerberos服务器端配置\"></a>Kerberos服务器端配置</h3><p>生成类似 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN 的keytab，配置了负载均衡后，使用test-bdd-hiveserver</p>\n<h3 id=\"76机器上的配置\"><a href=\"#76机器上的配置\" class=\"headerlink\" title=\"76机器上的配置\"></a>76机器上的配置</h3><p>76机器与74机器配置步骤一样，只是hive-site.xml需要改一处，将下面的 076改成 074即可<br><figure class=\"highlight bash\"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;property&gt;</div><div class=\"line\">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class=\"line\">   &lt;value&gt;<span class=\"built_in\">test</span>-bdd-076&lt;/value&gt;</div><div class=\"line\">   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class=\"line\">&lt;/property&gt;</div></pre></td></tr></table></figure></p>\n<p>其它都一样，所以在这里只写076的配置步骤</p>\n<h4 id=\"从hive-keytab创建spark的keytab\"><a href=\"#从hive-keytab创建spark的keytab\" class=\"headerlink\" title=\"从hive.keytab创建spark的keytab\"></a>从hive.keytab创建spark的keytab</h4><p>然后在/etc/hive/conf/下创建spark需要的keytab，在这里使用hiveserver的keytab，将已有的hive.keytab_hiveserver 拷贝成 hive.keytab_sparkthrift，然后修改权限如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver</div><div class=\"line\">-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift</div></pre></td></tr></table></figure>\n<p>修改好后用如下命令检查：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo klist -k hive.keytab_sparkthrift </div><div class=\"line\">Keytab name: FILE:hive.keytab_sparkthrift</div><div class=\"line\">KVNO Principal</div><div class=\"line\">---- --------------------------------------------------------------------------</div><div class=\"line\">   1 hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class=\"line\">   1 hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class=\"line\">   1 hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class=\"line\">   1 hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class=\"line\">   1 hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div></pre></td></tr></table></figure>\n<p>如果klist是如上结果，就对了</p>\n<h4 id=\"配置spark需要的hive-site-xml\"><a href=\"#配置spark需要的hive-site-xml\" class=\"headerlink\" title=\"配置spark需要的hive-site.xml\"></a>配置spark需要的hive-site.xml</h4><p>由于需要修改hive的一些配置，进入76机器spark所在的目录，将<code>/etc/hive/conf/</code>下的<code>hive-site.xml</code>拷贝到spark的conf下，赋予权限并修改<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo cp /etc/hive/conf/hive-site.xml <span class=\"variable\">$SPARK_HOME</span>/conf/</div><div class=\"line\">$ <span class=\"built_in\">cd</span> <span class=\"variable\">$SPARK_HOME</span></div><div class=\"line\">$ sudo chmod op conf/hive-site.xml</div><div class=\"line\">$ vim conf/hive-site.xml</div></pre></td></tr></table></figure></p>\n<p>修改hive-site.xml,增加hive.server2.thrift.bind.host</p>\n<figure class=\"highlight bash\"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;!-- ZooKeeper conf--&gt;</div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">  &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;</div><div class=\"line\">  &lt;value&gt;<span class=\"literal\">false</span>&lt;/value&gt;</div><div class=\"line\">  &lt;description&gt; Impersonate the connected user &lt;/description&gt;</div><div class=\"line\">&lt;/property&gt;</div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">  &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</div><div class=\"line\">  &lt;value&gt;10010&lt;/value&gt;</div><div class=\"line\">  &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class=\"line\">&lt;/property&gt;</div><div class=\"line\"></div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class=\"line\">   &lt;value&gt;<span class=\"built_in\">test</span>-bdd-076&lt;/value&gt;</div><div class=\"line\">   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class=\"line\"> &lt;/property&gt;</div><div class=\"line\"></div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">  &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;</div><div class=\"line\">  &lt;value&gt;<span class=\"literal\">true</span>&lt;/value&gt;</div><div class=\"line\">&lt;/property&gt;</div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">   &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</div><div class=\"line\">   &lt;value&gt;hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class=\"line\"> &lt;/property&gt;</div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">  &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</div><div class=\"line\">  &lt;value&gt;/etc/hive/conf/hive.keytab_sparkthrift&lt;/value&gt;</div><div class=\"line\">&lt;/property&gt;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">#### 启动Spark-thrift-server</span></div><div class=\"line\">``` bash</div><div class=\"line\">$ <span class=\"built_in\">cd</span> <span class=\"variable\">$SPARK_HOME</span></div><div class=\"line\">$ ./sbin/start-thriftserver.sh</div></pre></td></tr></table></figure>\n<p>可以通过如下日志查看是否启动成功：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ vim logs/spark-op-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-TEST-BDD-076.out</div></pre></td></tr></table></figure></p>\n<p>启动成功会看到如下日志:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"> 96 16/09/05 13:41:25 INFO AbstractService: Service:HiveServer2 is started.</div><div class=\"line\"> 97 16/09/05 13:41:25 INFO HiveThriftServer2: HiveThriftServer2 started</div><div class=\"line\"> 98 16/09/05 13:41:25 INFO UserGroupInformation: Login successful <span class=\"keyword\">for</span> user hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN using keytab file /etc/hive/conf/hive.keytab_sparkthrift</div><div class=\"line\"> 99 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key <span class=\"keyword\">for</span> generating delegation tokens</div><div class=\"line\">100 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=0</div><div class=\"line\">101 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)</div><div class=\"line\">102 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key <span class=\"keyword\">for</span> generating delegation tokens</div><div class=\"line\">103 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=1</div><div class=\"line\">104 16/09/05 13:41:25 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10010 with 5...500 worker threads</div></pre></td></tr></table></figure></p>\n<h3 id=\"负载均衡机器的查看\"><a href=\"#负载均衡机器的查看\" class=\"headerlink\" title=\"负载均衡机器的查看\"></a>负载均衡机器的查看</h3><p>进入 67.121机器<br>输入 命令 <code>sudo ipvsadm -ln</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo ipvsadm -ln</div><div class=\"line\">IP Virtual Server version 1.2.1 (size=4194304)</div><div class=\"line\">Prot LocalAddress:Port Scheduler Flags</div><div class=\"line\">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</div><div class=\"line\">TCP  10.142.67.123:10000 wlc persistent 7200 synproxy</div><div class=\"line\">  -&gt; 10.142.78.74:10000           FullNat 50     3          0         </div><div class=\"line\">  -&gt; 10.142.78.76:10000           FullNat 50     0          0         </div><div class=\"line\">TCP  10.142.67.123:10010 wlc persistent 7200 synproxy</div><div class=\"line\">  -&gt; 10.142.78.74:10010           FullNat 50     0          0         </div><div class=\"line\">  -&gt; 10.142.78.76:10010           FullNat 50     2          0</div></pre></td></tr></table></figure>\n<p>就可以看到负载均衡的情况了：</p>\n<h3 id=\"踩坑说明以及解决方案\"><a href=\"#踩坑说明以及解决方案\" class=\"headerlink\" title=\"踩坑说明以及解决方案\"></a>踩坑说明以及解决方案</h3><h4 id=\"缺少配置kerberos认证错误\"><a href=\"#缺少配置kerberos认证错误\" class=\"headerlink\" title=\"缺少配置kerberos认证错误\"></a>缺少配置kerberos认证错误</h4><p>需要在hive-site.xml文件中添加kerberos认证相关配置</p>\n<h4 id=\"kerberos认证失败\"><a href=\"#kerberos认证失败\" class=\"headerlink\" title=\"kerberos认证失败\"></a>kerberos认证失败</h4><p>1)  在hive-site.xml中配置好kerberos认证，但是op用户下无法读取hive.keytab的问题，出现unable to login …given keytab/principal 以及Unable to obtain password from user。因为hive.keytab 是hive用户创建的，op用户无法读取，导致看似kerberos已经配置好，<br>但是程序没有读取权限，依旧认为没有配置好，这是会有在日志文件中会有NULLPOINT类似的错误提示，说明是没有读取权限。解决方案是复制hive.keytab到op用户下。<br>2）在hue界面连接spark时可能会出现10010端口不能连接的问题，这是sparkthrift没有启动导致的；<br>3）spark thriftserver明明已经启动，但是hue界面仍旧不能连接，出现TTransportException的错误，原因是kerberos配置没有配置正确，即没有配置kerberos认证的keytab与principal。hive/test-bdd-hiveserver必须与hive.keytab_hiveserver配套使用，同理，test-bdd-074或者 test-bdd-076必须与hive/test-bdd-74或者hive/test-bdd-76配套使用，否则出现认证失败的问题。</p>\n<h4 id=\"hue的配置问题。\"><a href=\"#hue的配置问题。\" class=\"headerlink\" title=\"hue的配置问题。\"></a>hue的配置问题。</h4><p>在hue的desktop/conf目录下hue.ini文件中，主要配置spark_sql_server_host，也就是spark thriftserver所在主机，这里可以是负载均衡服务器TEST-BDD-HIVESERVER,spark_sql_server_port 是spark thriftserver的服务端口。<br>需要注意的是，加上kerberos认证后，主机名不能是ip地址的形式，需要FQDN的形式。hive的配置需要注意的是hive_server_host，这里绝对不能是hiveserver2的服务器的地址，一定是负载均衡服务器的地址，不然在hue界面连接HIVE时出现<br>Unable to access databases, Query Server or Metastore may be down.的错误以及GSS initial failed的错误，无法访问hive数据库。</p>\n<h4 id=\"metastore的问题\"><a href=\"#metastore的问题\" class=\"headerlink\" title=\"metastore的问题\"></a>metastore的问题</h4><p>连接metastore也需要principal的认证。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">20 &lt;property&gt;</div><div class=\"line\">221   &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt;</div><div class=\"line\">222   &lt;value&gt;<span class=\"literal\">true</span>&lt;/value&gt;</div><div class=\"line\">223   &lt;description&gt;If <span class=\"literal\">true</span>, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.&lt;/description&gt;</div><div class=\"line\">224 &lt;/property&gt;</div><div class=\"line\">225 &lt;property&gt;</div><div class=\"line\">226   &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt;</div><div class=\"line\">227   &lt;value&gt;hive/_HOST@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class=\"line\">228   &lt;description&gt;The service principal <span class=\"keyword\">for</span> the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.&lt;/description&gt;</div><div class=\"line\">229 &lt;/property&gt;</div></pre></td></tr></table></figure></p>\n<p>之所以问题多多，主要原因是对kerberos+Hive+lvs整体原理没有搞清楚，以至于在配置过程中出现各种错误。我们搭建的hive集群有74,76两台主机，spark thriftserver也有74,76两台主机，负载均衡服务器在test-bdd-hiveserver上。在配置时，需要将spark-sql-server-host配置成test-bdd-hiveserver,因为对spark而言，74与76上的hiveserver是一个整体，不能配置成单一的主机，不然lvs可能会将服务分到另外一台主机上，造成主机配置失败。</p>\n","excerpt":"","more":"<p>hue-spark-thriftserver-kerberos</p>\n<h3 id=\"背景说明\"><a href=\"#背景说明\" class=\"headerlink\" title=\"背景说明\"></a>背景说明</h3><p>Kerberos项目最后要对基于Hue的TODP平台进行安全测试，在搭建配置的过程中踩了一些坑，现在把其中的配置与步骤进行总结，以免以后忘记。</p>\n<p>其中用到以下代号：<br>40机器：hue平台所在的机器<br>76机器：spark thrift服务端口10010，hive-thrift-server服务端口10000<br>74机器：spark thrift服务端口10010，hive-thrift-server服务端口10000<br>TEST-BDD-HIVESERVER机器：负载均衡所在的机器，负载均衡机器需要配合开启10000和10010端口</p>\n<p>在kerberos认证下, sparksql的thriftserver连接hiveserver2变得相对复杂，主要是因为各种kerberos认证出现各种问题。后来由于hive使用了负载均衡，所以spark-sql也需加入负载均衡，否则不能使用，就是这个负载均衡服务器的加入使得kerberos认证变得更加复杂，使得不明原理的新手在配置kerberos的keytab与principal时各种不匹配。这里是通过Hue可视化界面调用后台的sparksql,然后sparksql通过JDBC连接Hive的hiveServer2服务。</p>\n<h3 id=\"40机器hue端配置\"><a href=\"#40机器hue端配置\" class=\"headerlink\" title=\"40机器hue端配置\"></a>40机器hue端配置</h3><p>进入40机器hue所在的目录<br><figure class=\"highlight bash\"><figcaption><span>hue.ini</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ <span class=\"built_in\">cd</span> /usr/lib/hue/ </div><div class=\"line\">$ vim desktop/conf/hue.ini</div></pre></td></tr></table></figure></p>\n<p>修改hue的配置文件如下<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">1119 [spark]</div><div class=\"line\">...</div><div class=\"line\">1134   <span class=\"comment\"># spark-sql config</span></div><div class=\"line\">1135   spark_sql_server_host=TEST-BDD-HIVESERVER</div><div class=\"line\">1136   <span class=\"comment\">## spark_sql_server_port=10010</span></div></pre></td></tr></table></figure></p>\n<p>由于此处使用了负载均衡，所以上述TEST-BDD-HIVESERVER指向的是负载均衡所在的ip，最终会转发给两个spark-thrift-server</p>\n<h3 id=\"Kerberos服务器端配置\"><a href=\"#Kerberos服务器端配置\" class=\"headerlink\" title=\"Kerberos服务器端配置\"></a>Kerberos服务器端配置</h3><p>生成类似 hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN 的keytab，配置了负载均衡后，使用test-bdd-hiveserver</p>\n<h3 id=\"76机器上的配置\"><a href=\"#76机器上的配置\" class=\"headerlink\" title=\"76机器上的配置\"></a>76机器上的配置</h3><p>76机器与74机器配置步骤一样，只是hive-site.xml需要改一处，将下面的 076改成 074即可<br><figure class=\"highlight bash\"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;property&gt;</div><div class=\"line\">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class=\"line\">   &lt;value&gt;<span class=\"built_in\">test</span>-bdd-076&lt;/value&gt;</div><div class=\"line\">   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class=\"line\">&lt;/property&gt;</div></pre></td></tr></table></figure></p>\n<p>其它都一样，所以在这里只写076的配置步骤</p>\n<h4 id=\"从hive-keytab创建spark的keytab\"><a href=\"#从hive-keytab创建spark的keytab\" class=\"headerlink\" title=\"从hive.keytab创建spark的keytab\"></a>从hive.keytab创建spark的keytab</h4><p>然后在/etc/hive/conf/下创建spark需要的keytab，在这里使用hiveserver的keytab，将已有的hive.keytab_hiveserver 拷贝成 hive.keytab_sparkthrift，然后修改权限如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver</div><div class=\"line\">-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift</div></pre></td></tr></table></figure>\n<p>修改好后用如下命令检查：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo klist -k hive.keytab_sparkthrift </div><div class=\"line\">Keytab name: FILE:hive.keytab_sparkthrift</div><div class=\"line\">KVNO Principal</div><div class=\"line\">---- --------------------------------------------------------------------------</div><div class=\"line\">   1 hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class=\"line\">   1 hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class=\"line\">   1 hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class=\"line\">   1 hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div><div class=\"line\">   1 hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN</div></pre></td></tr></table></figure>\n<p>如果klist是如上结果，就对了</p>\n<h4 id=\"配置spark需要的hive-site-xml\"><a href=\"#配置spark需要的hive-site-xml\" class=\"headerlink\" title=\"配置spark需要的hive-site.xml\"></a>配置spark需要的hive-site.xml</h4><p>由于需要修改hive的一些配置，进入76机器spark所在的目录，将<code>/etc/hive/conf/</code>下的<code>hive-site.xml</code>拷贝到spark的conf下，赋予权限并修改<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo cp /etc/hive/conf/hive-site.xml <span class=\"variable\">$SPARK_HOME</span>/conf/</div><div class=\"line\">$ <span class=\"built_in\">cd</span> <span class=\"variable\">$SPARK_HOME</span></div><div class=\"line\">$ sudo chmod op conf/hive-site.xml</div><div class=\"line\">$ vim conf/hive-site.xml</div></pre></td></tr></table></figure></p>\n<p>修改hive-site.xml,增加hive.server2.thrift.bind.host</p>\n<figure class=\"highlight bash\"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;!-- ZooKeeper conf--&gt;</div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">  &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;</div><div class=\"line\">  &lt;value&gt;<span class=\"literal\">false</span>&lt;/value&gt;</div><div class=\"line\">  &lt;description&gt; Impersonate the connected user &lt;/description&gt;</div><div class=\"line\">&lt;/property&gt;</div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">  &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</div><div class=\"line\">  &lt;value&gt;10010&lt;/value&gt;</div><div class=\"line\">  &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class=\"line\">&lt;/property&gt;</div><div class=\"line\"></div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">   &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class=\"line\">   &lt;value&gt;<span class=\"built_in\">test</span>-bdd-076&lt;/value&gt;</div><div class=\"line\">   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class=\"line\"> &lt;/property&gt;</div><div class=\"line\"></div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">  &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;</div><div class=\"line\">  &lt;value&gt;<span class=\"literal\">true</span>&lt;/value&gt;</div><div class=\"line\">&lt;/property&gt;</div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">   &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</div><div class=\"line\">   &lt;value&gt;hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class=\"line\"> &lt;/property&gt;</div><div class=\"line\">&lt;property&gt;</div><div class=\"line\">  &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</div><div class=\"line\">  &lt;value&gt;/etc/hive/conf/hive.keytab_sparkthrift&lt;/value&gt;</div><div class=\"line\">&lt;/property&gt;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">#### 启动Spark-thrift-server</span></div><div class=\"line\">``` bash</div><div class=\"line\">$ <span class=\"built_in\">cd</span> <span class=\"variable\">$SPARK_HOME</span></div><div class=\"line\">$ ./sbin/start-thriftserver.sh</div></pre></td></tr></table></figure>\n<p>可以通过如下日志查看是否启动成功：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ vim logs/spark-op-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-TEST-BDD-076.out</div></pre></td></tr></table></figure></p>\n<p>启动成功会看到如下日志:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"> 96 16/09/05 13:41:25 INFO AbstractService: Service:HiveServer2 is started.</div><div class=\"line\"> 97 16/09/05 13:41:25 INFO HiveThriftServer2: HiveThriftServer2 started</div><div class=\"line\"> 98 16/09/05 13:41:25 INFO UserGroupInformation: Login successful <span class=\"keyword\">for</span> user hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN using keytab file /etc/hive/conf/hive.keytab_sparkthrift</div><div class=\"line\"> 99 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key <span class=\"keyword\">for</span> generating delegation tokens</div><div class=\"line\">100 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=0</div><div class=\"line\">101 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)</div><div class=\"line\">102 16/09/05 13:41:25 INFO AbstractDelegationTokenSecretManager: Updating the current master key <span class=\"keyword\">for</span> generating delegation tokens</div><div class=\"line\">103 16/09/05 13:41:25 INFO TokenStoreDelegationTokenSecretManager: New master key with key id=1</div><div class=\"line\">104 16/09/05 13:41:25 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10010 with 5...500 worker threads</div></pre></td></tr></table></figure></p>\n<h3 id=\"负载均衡机器的查看\"><a href=\"#负载均衡机器的查看\" class=\"headerlink\" title=\"负载均衡机器的查看\"></a>负载均衡机器的查看</h3><p>进入 67.121机器<br>输入 命令 <code>sudo ipvsadm -ln</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo ipvsadm -ln</div><div class=\"line\">IP Virtual Server version 1.2.1 (size=4194304)</div><div class=\"line\">Prot LocalAddress:Port Scheduler Flags</div><div class=\"line\">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</div><div class=\"line\">TCP  10.142.67.123:10000 wlc persistent 7200 synproxy</div><div class=\"line\">  -&gt; 10.142.78.74:10000           FullNat 50     3          0         </div><div class=\"line\">  -&gt; 10.142.78.76:10000           FullNat 50     0          0         </div><div class=\"line\">TCP  10.142.67.123:10010 wlc persistent 7200 synproxy</div><div class=\"line\">  -&gt; 10.142.78.74:10010           FullNat 50     0          0         </div><div class=\"line\">  -&gt; 10.142.78.76:10010           FullNat 50     2          0</div></pre></td></tr></table></figure>\n<p>就可以看到负载均衡的情况了：</p>\n<h3 id=\"踩坑说明以及解决方案\"><a href=\"#踩坑说明以及解决方案\" class=\"headerlink\" title=\"踩坑说明以及解决方案\"></a>踩坑说明以及解决方案</h3><h4 id=\"缺少配置kerberos认证错误\"><a href=\"#缺少配置kerberos认证错误\" class=\"headerlink\" title=\"缺少配置kerberos认证错误\"></a>缺少配置kerberos认证错误</h4><p>需要在hive-site.xml文件中添加kerberos认证相关配置</p>\n<h4 id=\"kerberos认证失败\"><a href=\"#kerberos认证失败\" class=\"headerlink\" title=\"kerberos认证失败\"></a>kerberos认证失败</h4><p>1)  在hive-site.xml中配置好kerberos认证，但是op用户下无法读取hive.keytab的问题，出现unable to login …given keytab/principal 以及Unable to obtain password from user。因为hive.keytab 是hive用户创建的，op用户无法读取，导致看似kerberos已经配置好，<br>但是程序没有读取权限，依旧认为没有配置好，这是会有在日志文件中会有NULLPOINT类似的错误提示，说明是没有读取权限。解决方案是复制hive.keytab到op用户下。<br>2）在hue界面连接spark时可能会出现10010端口不能连接的问题，这是sparkthrift没有启动导致的；<br>3）spark thriftserver明明已经启动，但是hue界面仍旧不能连接，出现TTransportException的错误，原因是kerberos配置没有配置正确，即没有配置kerberos认证的keytab与principal。hive/test-bdd-hiveserver必须与hive.keytab_hiveserver配套使用，同理，test-bdd-074或者 test-bdd-076必须与hive/test-bdd-74或者hive/test-bdd-76配套使用，否则出现认证失败的问题。</p>\n<h4 id=\"hue的配置问题。\"><a href=\"#hue的配置问题。\" class=\"headerlink\" title=\"hue的配置问题。\"></a>hue的配置问题。</h4><p>在hue的desktop/conf目录下hue.ini文件中，主要配置spark_sql_server_host，也就是spark thriftserver所在主机，这里可以是负载均衡服务器TEST-BDD-HIVESERVER,spark_sql_server_port 是spark thriftserver的服务端口。<br>需要注意的是，加上kerberos认证后，主机名不能是ip地址的形式，需要FQDN的形式。hive的配置需要注意的是hive_server_host，这里绝对不能是hiveserver2的服务器的地址，一定是负载均衡服务器的地址，不然在hue界面连接HIVE时出现<br>Unable to access databases, Query Server or Metastore may be down.的错误以及GSS initial failed的错误，无法访问hive数据库。</p>\n<h4 id=\"metastore的问题\"><a href=\"#metastore的问题\" class=\"headerlink\" title=\"metastore的问题\"></a>metastore的问题</h4><p>连接metastore也需要principal的认证。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">20 &lt;property&gt;</div><div class=\"line\">221   &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt;</div><div class=\"line\">222   &lt;value&gt;<span class=\"literal\">true</span>&lt;/value&gt;</div><div class=\"line\">223   &lt;description&gt;If <span class=\"literal\">true</span>, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.&lt;/description&gt;</div><div class=\"line\">224 &lt;/property&gt;</div><div class=\"line\">225 &lt;property&gt;</div><div class=\"line\">226   &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt;</div><div class=\"line\">227   &lt;value&gt;hive/_HOST@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class=\"line\">228   &lt;description&gt;The service principal <span class=\"keyword\">for</span> the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.&lt;/description&gt;</div><div class=\"line\">229 &lt;/property&gt;</div></pre></td></tr></table></figure></p>\n<p>之所以问题多多，主要原因是对kerberos+Hive+lvs整体原理没有搞清楚，以至于在配置过程中出现各种错误。我们搭建的hive集群有74,76两台主机，spark thriftserver也有74,76两台主机，负载均衡服务器在test-bdd-hiveserver上。在配置时，需要将spark-sql-server-host配置成test-bdd-hiveserver,因为对spark而言，74与76上的hiveserver是一个整体，不能配置成单一的主机，不然lvs可能会将服务分到另外一台主机上，造成主机配置失败。</p>\n"},{"title":"kerberos下spark客户端的配置","date":"2016-09-06T01:43:27.000Z","toc":true,"_content":"\nKerberos环境下spark的客户端配置并不是很多，主要需要配置的是spark-history与spark-sql\n\n软件版本：spark-1.6.2\n\n注：正式环境中，需要将spark客户端的路径放入其它短路经，比如 /etc/local/spark 等\n``` bash spark-env.sh\n# 由于此处的 hive-site.xml 需要做一定修改，所以需要将hive-site.xml core-site.xml hdfs-site.xml yarn-site.xml等导入conf文件夹下\nexport HADOOP_CONF_DIR=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf\n\nexport JAVA_HOME=/usr/java/jdk1.7.0_75\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/hadoop/lib/native/\nexport SPARK_LIBRARY_PATH=/usr/lib/hadoop/lib/native/:$SPARK_LIBRARY_PATH\n\n# history需要的Kerberos配置\nSPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op    @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab\"\n```\n\n#### 从hive.keytab_hiveserver创建spark-thrift-server的keytab\n``` bash\n-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver\n-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift\n```\n#### hive-site的配置\n\n修改hive-site.xml：\n- 增加hive.server2.thrift.bind.host\n- 修改hive.server2.thrift.port为10010\n- 修改hive.server2.authentication.kerberos.keytab为如下\n\n``` bash hive-site.xml\n145 <!-- ZooKeeper conf-->\n146 <property>\n147   <name>hive.server2.enable.doAs</name>\n148   <value>false</value>\n149   <description> Impersonate the connected user </description>\n150 </property>\n151 <property>\n152   <name>hive.server2.thrift.port</name>\n153   <value>10010</value>\n154   <description>TCP port number to listen on, default 10000</description>\n155 </property>\n156 \n157 <property>\n158    <name>hive.server2.thrift.bind.host</name>\n159    <value>test-bdd-076</value>\n160    <description>TCP port number to listen on, default 10000</description>\n161  </property>\n162 \n163 <property>\n164   <name>hive.metastore.execute.setugi</name>\n165   <value>true</value>\n166 </property>\n...\n209 <property>\n210    <name>hive.server2.authentication.kerberos.principal</name>\n211    <value>hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN</value>\n212  </property>\n213 <property>\n214   <name>hive.server2.authentication.kerberos.keytab</name>\n215   <value>/etc/hive/conf/hive.keytab_sparkthrift</value>\n216 </property>\n```\n","source":"_posts/2016-09-06-kerberos下spark客户端的配置.md","raw":"---\ntitle: kerberos下spark客户端的配置\ndate: 2016-09-06 09:43:27\ntoc: true\ntags: \n- spark\n- kerberos\ncategories:\n- spark\n---\n\nKerberos环境下spark的客户端配置并不是很多，主要需要配置的是spark-history与spark-sql\n\n软件版本：spark-1.6.2\n\n注：正式环境中，需要将spark客户端的路径放入其它短路经，比如 /etc/local/spark 等\n``` bash spark-env.sh\n# 由于此处的 hive-site.xml 需要做一定修改，所以需要将hive-site.xml core-site.xml hdfs-site.xml yarn-site.xml等导入conf文件夹下\nexport HADOOP_CONF_DIR=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf\n\nexport JAVA_HOME=/usr/java/jdk1.7.0_75\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/hadoop/lib/native/\nexport SPARK_LIBRARY_PATH=/usr/lib/hadoop/lib/native/:$SPARK_LIBRARY_PATH\n\n# history需要的Kerberos配置\nSPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op    @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab\"\n```\n\n#### 从hive.keytab_hiveserver创建spark-thrift-server的keytab\n``` bash\n-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver\n-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift\n```\n#### hive-site的配置\n\n修改hive-site.xml：\n- 增加hive.server2.thrift.bind.host\n- 修改hive.server2.thrift.port为10010\n- 修改hive.server2.authentication.kerberos.keytab为如下\n\n``` bash hive-site.xml\n145 <!-- ZooKeeper conf-->\n146 <property>\n147   <name>hive.server2.enable.doAs</name>\n148   <value>false</value>\n149   <description> Impersonate the connected user </description>\n150 </property>\n151 <property>\n152   <name>hive.server2.thrift.port</name>\n153   <value>10010</value>\n154   <description>TCP port number to listen on, default 10000</description>\n155 </property>\n156 \n157 <property>\n158    <name>hive.server2.thrift.bind.host</name>\n159    <value>test-bdd-076</value>\n160    <description>TCP port number to listen on, default 10000</description>\n161  </property>\n162 \n163 <property>\n164   <name>hive.metastore.execute.setugi</name>\n165   <value>true</value>\n166 </property>\n...\n209 <property>\n210    <name>hive.server2.authentication.kerberos.principal</name>\n211    <value>hive/test-bdd-hiveserver@HADOOP.CHINATELECOM.CN</value>\n212  </property>\n213 <property>\n214   <name>hive.server2.authentication.kerberos.keytab</name>\n215   <value>/etc/hive/conf/hive.keytab_sparkthrift</value>\n216 </property>\n```\n","slug":"kerberos下spark客户端的配置","published":1,"updated":"2016-09-18T09:00:40.812Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfuf000epsguapd05z1h","content":"<p>Kerberos环境下spark的客户端配置并不是很多，主要需要配置的是spark-history与spark-sql</p>\n<p>软件版本：spark-1.6.2</p>\n<p>注：正式环境中，需要将spark客户端的路径放入其它短路经，比如 /etc/local/spark 等<br><figure class=\"highlight bash\"><figcaption><span>spark-env.sh</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 由于此处的 hive-site.xml 需要做一定修改，所以需要将hive-site.xml core-site.xml hdfs-site.xml yarn-site.xml等导入conf文件夹下</span></div><div class=\"line\"><span class=\"built_in\">export</span> HADOOP_CONF_DIR=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf</div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">export</span> JAVA_HOME=/usr/java/jdk1.7.0_75</div><div class=\"line\"><span class=\"built_in\">export</span> LD_LIBRARY_PATH=<span class=\"variable\">$LD_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/</div><div class=\"line\"><span class=\"built_in\">export</span> SPARK_LIBRARY_PATH=/usr/lib/hadoop/lib/native/:<span class=\"variable\">$SPARK_LIBRARY_PATH</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># history需要的Kerberos配置</span></div><div class=\"line\">SPARK_HISTORY_OPTS=<span class=\"string\">\"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op    @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab\"</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"从hive-keytab-hiveserver创建spark-thrift-server的keytab\"><a href=\"#从hive-keytab-hiveserver创建spark-thrift-server的keytab\" class=\"headerlink\" title=\"从hive.keytab_hiveserver创建spark-thrift-server的keytab\"></a>从hive.keytab_hiveserver创建spark-thrift-server的keytab</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver</div><div class=\"line\">-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift</div></pre></td></tr></table></figure>\n<h4 id=\"hive-site的配置\"><a href=\"#hive-site的配置\" class=\"headerlink\" title=\"hive-site的配置\"></a>hive-site的配置</h4><p>修改hive-site.xml：</p>\n<ul>\n<li>增加hive.server2.thrift.bind.host</li>\n<li>修改hive.server2.thrift.port为10010</li>\n<li>修改hive.server2.authentication.kerberos.keytab为如下</li>\n</ul>\n<figure class=\"highlight bash\"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div></pre></td><td class=\"code\"><pre><div class=\"line\">145 &lt;!-- ZooKeeper conf--&gt;</div><div class=\"line\">146 &lt;property&gt;</div><div class=\"line\">147   &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;</div><div class=\"line\">148   &lt;value&gt;<span class=\"literal\">false</span>&lt;/value&gt;</div><div class=\"line\">149   &lt;description&gt; Impersonate the connected user &lt;/description&gt;</div><div class=\"line\">150 &lt;/property&gt;</div><div class=\"line\">151 &lt;property&gt;</div><div class=\"line\">152   &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</div><div class=\"line\">153   &lt;value&gt;10010&lt;/value&gt;</div><div class=\"line\">154   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class=\"line\">155 &lt;/property&gt;</div><div class=\"line\">156 </div><div class=\"line\">157 &lt;property&gt;</div><div class=\"line\">158    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class=\"line\">159    &lt;value&gt;<span class=\"built_in\">test</span>-bdd-076&lt;/value&gt;</div><div class=\"line\">160    &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class=\"line\">161  &lt;/property&gt;</div><div class=\"line\">162 </div><div class=\"line\">163 &lt;property&gt;</div><div class=\"line\">164   &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;</div><div class=\"line\">165   &lt;value&gt;<span class=\"literal\">true</span>&lt;/value&gt;</div><div class=\"line\">166 &lt;/property&gt;</div><div class=\"line\">...</div><div class=\"line\">209 &lt;property&gt;</div><div class=\"line\">210    &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</div><div class=\"line\">211    &lt;value&gt;hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class=\"line\">212  &lt;/property&gt;</div><div class=\"line\">213 &lt;property&gt;</div><div class=\"line\">214   &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</div><div class=\"line\">215   &lt;value&gt;/etc/hive/conf/hive.keytab_sparkthrift&lt;/value&gt;</div><div class=\"line\">216 &lt;/property&gt;</div></pre></td></tr></table></figure>\n","excerpt":"","more":"<p>Kerberos环境下spark的客户端配置并不是很多，主要需要配置的是spark-history与spark-sql</p>\n<p>软件版本：spark-1.6.2</p>\n<p>注：正式环境中，需要将spark客户端的路径放入其它短路经，比如 /etc/local/spark 等<br><figure class=\"highlight bash\"><figcaption><span>spark-env.sh</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 由于此处的 hive-site.xml 需要做一定修改，所以需要将hive-site.xml core-site.xml hdfs-site.xml yarn-site.xml等导入conf文件夹下</span></div><div class=\"line\"><span class=\"built_in\">export</span> HADOOP_CONF_DIR=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf</div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">export</span> JAVA_HOME=/usr/java/jdk1.7.0_75</div><div class=\"line\"><span class=\"built_in\">export</span> LD_LIBRARY_PATH=<span class=\"variable\">$LD_LIBRARY_PATH</span>:/usr/lib/hadoop/lib/native/</div><div class=\"line\"><span class=\"built_in\">export</span> SPARK_LIBRARY_PATH=/usr/lib/hadoop/lib/native/:<span class=\"variable\">$SPARK_LIBRARY_PATH</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># history需要的Kerberos配置</span></div><div class=\"line\">SPARK_HISTORY_OPTS=<span class=\"string\">\"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op    @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab\"</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"从hive-keytab-hiveserver创建spark-thrift-server的keytab\"><a href=\"#从hive-keytab-hiveserver创建spark-thrift-server的keytab\" class=\"headerlink\" title=\"从hive.keytab_hiveserver创建spark-thrift-server的keytab\"></a>从hive.keytab_hiveserver创建spark-thrift-server的keytab</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">-rw------- 1 hive hive     424 8月  23 09:55 hive.keytab_hiveserver</div><div class=\"line\">-rw------- 1 op   bigdata  424 9月   3 12:25 hive.keytab_sparkthrift</div></pre></td></tr></table></figure>\n<h4 id=\"hive-site的配置\"><a href=\"#hive-site的配置\" class=\"headerlink\" title=\"hive-site的配置\"></a>hive-site的配置</h4><p>修改hive-site.xml：</p>\n<ul>\n<li>增加hive.server2.thrift.bind.host</li>\n<li>修改hive.server2.thrift.port为10010</li>\n<li>修改hive.server2.authentication.kerberos.keytab为如下</li>\n</ul>\n<figure class=\"highlight bash\"><figcaption><span>hive-site.xml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div></pre></td><td class=\"code\"><pre><div class=\"line\">145 &lt;!-- ZooKeeper conf--&gt;</div><div class=\"line\">146 &lt;property&gt;</div><div class=\"line\">147   &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;</div><div class=\"line\">148   &lt;value&gt;<span class=\"literal\">false</span>&lt;/value&gt;</div><div class=\"line\">149   &lt;description&gt; Impersonate the connected user &lt;/description&gt;</div><div class=\"line\">150 &lt;/property&gt;</div><div class=\"line\">151 &lt;property&gt;</div><div class=\"line\">152   &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</div><div class=\"line\">153   &lt;value&gt;10010&lt;/value&gt;</div><div class=\"line\">154   &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class=\"line\">155 &lt;/property&gt;</div><div class=\"line\">156 </div><div class=\"line\">157 &lt;property&gt;</div><div class=\"line\">158    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</div><div class=\"line\">159    &lt;value&gt;<span class=\"built_in\">test</span>-bdd-076&lt;/value&gt;</div><div class=\"line\">160    &lt;description&gt;TCP port number to listen on, default 10000&lt;/description&gt;</div><div class=\"line\">161  &lt;/property&gt;</div><div class=\"line\">162 </div><div class=\"line\">163 &lt;property&gt;</div><div class=\"line\">164   &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;</div><div class=\"line\">165   &lt;value&gt;<span class=\"literal\">true</span>&lt;/value&gt;</div><div class=\"line\">166 &lt;/property&gt;</div><div class=\"line\">...</div><div class=\"line\">209 &lt;property&gt;</div><div class=\"line\">210    &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;</div><div class=\"line\">211    &lt;value&gt;hive/<span class=\"built_in\">test</span>-bdd-hiveserver@HADOOP.CHINATELECOM.CN&lt;/value&gt;</div><div class=\"line\">212  &lt;/property&gt;</div><div class=\"line\">213 &lt;property&gt;</div><div class=\"line\">214   &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt;</div><div class=\"line\">215   &lt;value&gt;/etc/hive/conf/hive.keytab_sparkthrift&lt;/value&gt;</div><div class=\"line\">216 &lt;/property&gt;</div></pre></td></tr></table></figure>\n"},{"title":"Spark在Kerberos环境下指定任意用户在yarn上提交任务","date":"2016-09-08T03:10:59.000Z","_content":"\n众所周知，Spark在Kerberos环境下提交任务有两种方式，分别是先kinit的方式和通过 --keytab的方式：\n\n``` bash\n[op]$ spark-submit --keytab testJars/op.keytab --principal op --master local --class SparkPi ./testJars/my.jar 4\n```\n\n\nSpark在Kerberos环境下可以在提交任务时通过指定用户的keytab和principal来提交任务，比如：\n\n``` bash\n# 事先进行kinit的方式\n[op]$ kinit -kt op.keytab op\n[op]$ spark-submit --master local --class SparkPi ./testJars/my.jar 4\n\n# 提交keytab的方式\n[op]$ spark-submit --keytab testJars/op.keytab --principal op --master local --class SparkPi ./testJars/my.jar 4\n```\n\n其实还可以模拟其它用户的方式提交任务，比如使用ts账户提交：\n\n``` bash\n[op]$ spark-submit --keytab testJars/ts.keytab --principal ts@HADOOP.CHINATELECOM.CN --master local --class SparkPi ./testJars/my.jar 4\n```\n\n当然没有那么简单，如果想要使用ts账户执行程序，需要进行如下设置：\n\n#### 模拟其它用户需要的条件\n\n1. ts要在KDC下生成对应的keytab和principal；\n2. 要在hadoop集群的所有机器上创建ts账户：\n\n``` bash\nsudo groupadd ts\nsudo useradd -g ts ts\n```\n\n值得注意的是，如果要在yarn中模拟其它用户执行，需要在集群中所有机器上增加该用户。\n\n后期有时间了详细说明原因。","source":"_posts/2016-09-08-Spark在Kerberos环境下指定任意用户在yarn上提交任务.md","raw":"---\ntitle: Spark在Kerberos环境下指定任意用户在yarn上提交任务\ndate: 2016-09-08 11:10:59\ntags: \n- spark\n- kerberos\ncategories:\n- spark\n---\n\n众所周知，Spark在Kerberos环境下提交任务有两种方式，分别是先kinit的方式和通过 --keytab的方式：\n\n``` bash\n[op]$ spark-submit --keytab testJars/op.keytab --principal op --master local --class SparkPi ./testJars/my.jar 4\n```\n\n\nSpark在Kerberos环境下可以在提交任务时通过指定用户的keytab和principal来提交任务，比如：\n\n``` bash\n# 事先进行kinit的方式\n[op]$ kinit -kt op.keytab op\n[op]$ spark-submit --master local --class SparkPi ./testJars/my.jar 4\n\n# 提交keytab的方式\n[op]$ spark-submit --keytab testJars/op.keytab --principal op --master local --class SparkPi ./testJars/my.jar 4\n```\n\n其实还可以模拟其它用户的方式提交任务，比如使用ts账户提交：\n\n``` bash\n[op]$ spark-submit --keytab testJars/ts.keytab --principal ts@HADOOP.CHINATELECOM.CN --master local --class SparkPi ./testJars/my.jar 4\n```\n\n当然没有那么简单，如果想要使用ts账户执行程序，需要进行如下设置：\n\n#### 模拟其它用户需要的条件\n\n1. ts要在KDC下生成对应的keytab和principal；\n2. 要在hadoop集群的所有机器上创建ts账户：\n\n``` bash\nsudo groupadd ts\nsudo useradd -g ts ts\n```\n\n值得注意的是，如果要在yarn中模拟其它用户执行，需要在集群中所有机器上增加该用户。\n\n后期有时间了详细说明原因。","slug":"Spark在Kerberos环境下指定任意用户在yarn上提交任务","published":1,"updated":"2016-09-08T06:14:57.473Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfur000hpsgu99774yfg","content":"<p>众所周知，Spark在Kerberos环境下提交任务有两种方式，分别是先kinit的方式和通过 –keytab的方式：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[op]$ spark-submit --keytab <span class=\"built_in\">test</span>Jars/op.keytab --principal op --master <span class=\"built_in\">local</span> --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>\n<p>Spark在Kerberos环境下可以在提交任务时通过指定用户的keytab和principal来提交任务，比如：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 事先进行kinit的方式</span></div><div class=\"line\">[op]$ kinit -kt op.keytab op</div><div class=\"line\">[op]$ spark-submit --master <span class=\"built_in\">local</span> --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar 4</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 提交keytab的方式</span></div><div class=\"line\">[op]$ spark-submit --keytab <span class=\"built_in\">test</span>Jars/op.keytab --principal op --master <span class=\"built_in\">local</span> --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>\n<p>其实还可以模拟其它用户的方式提交任务，比如使用ts账户提交：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[op]$ spark-submit --keytab <span class=\"built_in\">test</span>Jars/ts.keytab --principal ts@HADOOP.CHINATELECOM.CN --master <span class=\"built_in\">local</span> --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>\n<p>当然没有那么简单，如果想要使用ts账户执行程序，需要进行如下设置：</p>\n<h4 id=\"模拟其它用户需要的条件\"><a href=\"#模拟其它用户需要的条件\" class=\"headerlink\" title=\"模拟其它用户需要的条件\"></a>模拟其它用户需要的条件</h4><ol>\n<li>ts要在KDC下生成对应的keytab和principal；</li>\n<li>要在hadoop集群的所有机器上创建ts账户：</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo groupadd ts</div><div class=\"line\">sudo useradd -g ts ts</div></pre></td></tr></table></figure>\n<p>值得注意的是，如果要在yarn中模拟其它用户执行，需要在集群中所有机器上增加该用户。</p>\n<p>后期有时间了详细说明原因。</p>\n","excerpt":"","more":"<p>众所周知，Spark在Kerberos环境下提交任务有两种方式，分别是先kinit的方式和通过 –keytab的方式：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[op]$ spark-submit --keytab <span class=\"built_in\">test</span>Jars/op.keytab --principal op --master <span class=\"built_in\">local</span> --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>\n<p>Spark在Kerberos环境下可以在提交任务时通过指定用户的keytab和principal来提交任务，比如：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 事先进行kinit的方式</span></div><div class=\"line\">[op]$ kinit -kt op.keytab op</div><div class=\"line\">[op]$ spark-submit --master <span class=\"built_in\">local</span> --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar 4</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 提交keytab的方式</span></div><div class=\"line\">[op]$ spark-submit --keytab <span class=\"built_in\">test</span>Jars/op.keytab --principal op --master <span class=\"built_in\">local</span> --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>\n<p>其实还可以模拟其它用户的方式提交任务，比如使用ts账户提交：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[op]$ spark-submit --keytab <span class=\"built_in\">test</span>Jars/ts.keytab --principal ts@HADOOP.CHINATELECOM.CN --master <span class=\"built_in\">local</span> --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>\n<p>当然没有那么简单，如果想要使用ts账户执行程序，需要进行如下设置：</p>\n<h4 id=\"模拟其它用户需要的条件\"><a href=\"#模拟其它用户需要的条件\" class=\"headerlink\" title=\"模拟其它用户需要的条件\"></a>模拟其它用户需要的条件</h4><ol>\n<li>ts要在KDC下生成对应的keytab和principal；</li>\n<li>要在hadoop集群的所有机器上创建ts账户：</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo groupadd ts</div><div class=\"line\">sudo useradd -g ts ts</div></pre></td></tr></table></figure>\n<p>值得注意的是，如果要在yarn中模拟其它用户执行，需要在集群中所有机器上增加该用户。</p>\n<p>后期有时间了详细说明原因。</p>\n"},{"title":"Spark Web与history web配置与测试","date":"2016-09-07T02:13:30.000Z","_content":"\n## Spark Web的查看\n\n1. 运行任意一个yarn-client或者yarn-cluster模式的spark测试用例\n\n``` bash\n$ cd $SPARK_HOME\n$ spark-submit --keytab testJars/op.keytab --principal op --master yarn-client --class SparkPi ./testJars/my.jar 4\n```\n\n2. 打开http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例\n\n{% asset_img spark-yarn-web1.png %}\n\n点击上图所示的AM，就进入了Spark的Web界面：下图就是Spark程序的web界面，值得注意的是，这个web界面会随着spark程序的运行结束而消失\n{% asset_img spark-yarn-web2.png %}\n\n## Spark history Web查看测试\n\n在Kerberos环境下要启动spark history配置，需要在 spark -env下面开启如下配置 SPARK_HISTORY_OPTS：\n\n``` bash spark-env.sh\n# history需要的Kerberos配置\nSPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab\"\n```\n然后通过 ./sbin/start-history-server.sh 命令启动history-server\n然后登录 http://spark-client-ip:8777/ 即可查看 spark-history-web\n\n{% asset_img spark-yarn-web3.png %}","source":"_posts/2016-09-07-Spark-Web与history测试.md","raw":"---\ntitle: Spark Web与history web配置与测试\ndate: 2016-09-07 10:13:30\ntags: spark\ncategories: spark\n---\n\n## Spark Web的查看\n\n1. 运行任意一个yarn-client或者yarn-cluster模式的spark测试用例\n\n``` bash\n$ cd $SPARK_HOME\n$ spark-submit --keytab testJars/op.keytab --principal op --master yarn-client --class SparkPi ./testJars/my.jar 4\n```\n\n2. 打开http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例\n\n{% asset_img spark-yarn-web1.png %}\n\n点击上图所示的AM，就进入了Spark的Web界面：下图就是Spark程序的web界面，值得注意的是，这个web界面会随着spark程序的运行结束而消失\n{% asset_img spark-yarn-web2.png %}\n\n## Spark history Web查看测试\n\n在Kerberos环境下要启动spark history配置，需要在 spark -env下面开启如下配置 SPARK_HISTORY_OPTS：\n\n``` bash spark-env.sh\n# history需要的Kerberos配置\nSPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab\"\n```\n然后通过 ./sbin/start-history-server.sh 命令启动history-server\n然后登录 http://spark-client-ip:8777/ 即可查看 spark-history-web\n\n{% asset_img spark-yarn-web3.png %}","slug":"Spark-Web与history测试","published":1,"updated":"2016-09-07T02:33:18.620Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfv1000ipsgucy0ghz93","content":"<h2 id=\"Spark-Web的查看\"><a href=\"#Spark-Web的查看\" class=\"headerlink\" title=\"Spark Web的查看\"></a>Spark Web的查看</h2><ol>\n<li>运行任意一个yarn-client或者yarn-cluster模式的spark测试用例</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ <span class=\"built_in\">cd</span> <span class=\"variable\">$SPARK_HOME</span></div><div class=\"line\">$ spark-submit --keytab <span class=\"built_in\">test</span>Jars/op.keytab --principal op --master yarn-client --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>\n<ol>\n<li>打开<a href=\"http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例\" target=\"_blank\" rel=\"external\">http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例</a></li>\n</ol>\n<img src=\"/2016/09/07/Spark-Web与history测试/spark-yarn-web1.png\" alt=\"spark-yarn-web1.png\" title=\"\">\n<p>点击上图所示的AM，就进入了Spark的Web界面：下图就是Spark程序的web界面，值得注意的是，这个web界面会随着spark程序的运行结束而消失<br><img src=\"/2016/09/07/Spark-Web与history测试/spark-yarn-web2.png\" alt=\"spark-yarn-web2.png\" title=\"\"></p>\n<h2 id=\"Spark-history-Web查看测试\"><a href=\"#Spark-history-Web查看测试\" class=\"headerlink\" title=\"Spark history Web查看测试\"></a>Spark history Web查看测试</h2><p>在Kerberos环境下要启动spark history配置，需要在 spark -env下面开启如下配置 SPARK_HISTORY_OPTS：</p>\n<figure class=\"highlight bash\"><figcaption><span>spark-env.sh</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># history需要的Kerberos配置</span></div><div class=\"line\">SPARK_HISTORY_OPTS=<span class=\"string\">\"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab\"</span></div></pre></td></tr></table></figure>\n<p>然后通过 ./sbin/start-history-server.sh 命令启动history-server<br>然后登录 <a href=\"http://spark-client-ip:8777/\" target=\"_blank\" rel=\"external\">http://spark-client-ip:8777/</a> 即可查看 spark-history-web</p>\n<img src=\"/2016/09/07/Spark-Web与history测试/spark-yarn-web3.png\" alt=\"spark-yarn-web3.png\" title=\"\">","excerpt":"","more":"<h2 id=\"Spark-Web的查看\"><a href=\"#Spark-Web的查看\" class=\"headerlink\" title=\"Spark Web的查看\"></a>Spark Web的查看</h2><ol>\n<li>运行任意一个yarn-client或者yarn-cluster模式的spark测试用例</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ <span class=\"built_in\">cd</span> <span class=\"variable\">$SPARK_HOME</span></div><div class=\"line\">$ spark-submit --keytab <span class=\"built_in\">test</span>Jars/op.keytab --principal op --master yarn-client --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar 4</div></pre></td></tr></table></figure>\n<ol>\n<li>打开<a href=\"http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例\">http://yarn-host:8088/cluster页面，找到正在运行的Spark测试用例</a></li>\n</ol>\n<img src=\"/2016/09/07/Spark-Web与history测试/spark-yarn-web1.png\" alt=\"spark-yarn-web1.png\" title=\"\">\n<p>点击上图所示的AM，就进入了Spark的Web界面：下图就是Spark程序的web界面，值得注意的是，这个web界面会随着spark程序的运行结束而消失<br><img src=\"/2016/09/07/Spark-Web与history测试/spark-yarn-web2.png\" alt=\"spark-yarn-web2.png\" title=\"\"></p>\n<h2 id=\"Spark-history-Web查看测试\"><a href=\"#Spark-history-Web查看测试\" class=\"headerlink\" title=\"Spark history Web查看测试\"></a>Spark history Web查看测试</h2><p>在Kerberos环境下要启动spark history配置，需要在 spark -env下面开启如下配置 SPARK_HISTORY_OPTS：</p>\n<figure class=\"highlight bash\"><figcaption><span>spark-env.sh</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># history需要的Kerberos配置</span></div><div class=\"line\">SPARK_HISTORY_OPTS=<span class=\"string\">\"-Dspark.history.ui.port=8777 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://ns/user/op/sparkHistoryServer -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=op @HADOOP.CHINATELECOM.CN -Dspark.history.kerberos.keytab=/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/conf/op.keytab\"</span></div></pre></td></tr></table></figure>\n<p>然后通过 ./sbin/start-history-server.sh 命令启动history-server<br>然后登录 <a href=\"http://spark-client-ip:8777/\">http://spark-client-ip:8777/</a> 即可查看 spark-history-web</p>\n<img src=\"/2016/09/07/Spark-Web与history测试/spark-yarn-web3.png\" alt=\"spark-yarn-web3.png\" title=\"\">"},{"title":"Spark在yarn中的资源申请与分配调研","date":"2016-09-18T03:09:21.000Z","toc":true,"description":"spark作业提交到yarn的时候，如果用户(wzfw)所在队列本来有500个executor的权限，但是他跑一个简单的程序根本不需要这么多的资源，只需要200个核就足够了，那他如果申请了500个核的话，是否需要全部分配给他？","_content":"\n本文解决遇到的以下问题：\n*spark作业提交到yarn的时候，如果用户(wzfw)所在队列本来有500个executor的权限，但是他跑一个简单的程序根本不需要这么多的资源，只需要200个核就足够了，那他如果申请了400个核的话，是否需要全部分配给他？*\n\n## 前言\n目前我们所有的spark程序的分配都是靠参数设置固定的Executor数量进行资源预分配的，如果用户op在yarn的资源队列里可以申请到200个资源，那它就算跑占用资源很少的程序也能申请到200个核，这是不合理的\n\n比如简单跑如下SparkPi程序，申请20个核：\n``` bash\n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 20 lib/spark-examples-1.6.2-hadoop2.6.0.jar \n```\nyarn中资源占用情况如下：\n{% asset_img spark-yarn-allocation.png %}\n可以看到，我就跑了一个SparkPi啊，竟然用了43G的内存，这样很不合理！\n\n## 总述\nSpark在yarn集群上运行的时候，一方面默认通过num-executors参数设置固定的Executor数量，每个application会独占所有预分配的资源直到整个生命周期的结束。Spark1.2后开始引入动态资源分配（Dynamic Resource Allocation）机制，支持资源弹性分配。\n\n对于已知的业务负载，使用固定的集群资源配置是相对容易的；对于未知的业务负载，使用动态的集群资源分配方式可以满足负载的动态变化，这样集群的资源利用和业务负载的处理效率都会更加灵活。\n\n动态资源分配测试在Spark1.2仅支持Yarn模式，从Spark1.6开始，支持standalone、Yarn、Mesos.这个特性默认是禁用的。\n<!--more-->\n## 动态资源分配的思想\n简单来说，就是基于负载来动态调节Spark应用的资源占用，你的应用会在资源空闲的时候将其释放给集群，而后续用到的时候再重新申请。\n\n### 动态资源分配策略\n其实没有一个固定的方法可以预测一个executor后续是否马上会被分配去执行任务，或者一个新分配的执行器实际上是空闲的，所以我们需要一些试探性的方法，来决定是否申请或移除一个执行器。策略分为**请求策略**与**移除策略**：\n\n#### 请求策略\n\n开启动态分配策略后，application会在task因没有足够资源被挂起的时候去动态申请资源，这种情况意味着该application现有的executor无法满足所有task并行运行。spark一轮一轮的申请资源，当有task挂起或等待spark.dynamicAllocation.schedulerBacklogTimeout(默认1s)时间的时候，会开始动态资源分配；之后会每隔spark.dynamicAllocation.sustainedSchedulerBacklogTimeout(默认1s)时间申请一次，直到申请到足够的资源。**每次申请的资源量是指数增长的，即1,2,4,8等**。\n之所以采用指数增长，出于两方面考虑：其一，开始申请的少是考虑到可能application会马上得到满足；其次要成倍增加，是为了如果application需要很多资源，而该方式可以在很少次数的申请之后得到满足。\n（这段指数增长的策略可以根据实际情况通过修改源码来修改）\n\n#### 资源回收策略\n当application的executor空闲时间超过spark.dynamicAllocation.executorIdleTimeout（默认60s）后，就会被回收。\n\n## 配置思路\n### 启动 external shuffle service\n要使用这一特性有两个前提条件。首先，你的应用必须设置 spark.dynamicAllocation.enabled 为 true。其次，你必须在每个节点上启动一个外部混洗服务（external shuffle service），并在你的应用中将 spark.shuffle.service.enabled 设为true。外部混洗服务的目的就是为了在删除执行器的时候，能够保留其输出的混洗文件（本文后续有更详细的描述）。启用外部混洗的方式在各个集群管理器上各不相同：\n\n在Spark独立部署的集群中，你只需要在worker启动前设置 spark.shuffle.server.enabled 为true即可。\n\n在YARN模式下，混洗服务需要按以下步骤在各个NodeManager上启动：\n\n1. 首先按照YARN profile 构建Spark。如果你已经有打好包的Spark，可以忽略这一步。\n2. 找到 spark-<version>-yarn-shuffle.jar。如果你是自定义编译，其位置应该在 ${SPARK_HOME}/network/yarn/target/scala-<version>，否则应该可以在 lib 目录下找到这个jar包。\n3. 将该jar包添加到NodeManager的classpath路径中。\n4. 配置各个节点上的yarn-site.xml，将 spark_shuffle 添加到 yarn.nodemanager.aux-services 中，然后将 yarn.nodemanager.aux-services.spark_shuffle.class 设为 org.apache.spark.network.yarn.YarnShuffleService，并将 spark.shuffle.service.enabled 设为 true。\n5. 最后重启各节点上的NodeManager。\n\n所有相关的配置都是可选的，并且都在 spark.dynamicAllocation.* 和 spark.shuffle.service.* 命名空间下。更详细请参考：[configurations page](http://spark.apache.org/docs/latest/configuration.html#dynamic-allocation)。\n\n### 外部混洗服务external shuffle service\n非动态分配模式下，执行器可能的退出原因有执行失败或者相关Spark应用已经退出。不管是哪种原因，执行器的所有状态都已经不再需要，可以丢弃掉。但在动态分配的情形下，执行器有可能在Spark应用运行期间被移除。这时候，如果Spark应用尝试去访问该执行器存储的状态，就必须重算这一部分数据。因此，Spark需要一种机制，能够优雅地关闭执行器，同时还保留其状态数据。\n\n这种需求对于混洗操作尤其重要。混洗过程中，Spark执行器首先将map输出写到本地磁盘，同时执行器本身又是一个文件服务器，这样其他执行器就能够通过该执行器获得对应的map结果数据。一旦有某些任务执行时间过长，动态分配有可能在混洗结束前移除任务异常的执行器，而这些被移除的执行器对应的数据将会被重新计算，但这些重算其实是不必要的。\n\n要解决这一问题，就需要用到一个外部混洗服务（external shuffle service），该服务在Spark 1.2引入。该服务在每个节点上都会启动一个不依赖于任何Spark应用或执行器的独立进程。一旦该服务启用，Spark执行器不再从各个执行器上获取shuffle文件，转而从这个service获取。这意味着，任何执行器输出的混洗状态数据都可能存留时间比对应的执行器进程还长。\n\n除了混洗文件之外，执行器也会在磁盘或者内存中缓存数。一旦执行器被移除，其缓存数据将无法访问。这个问题目前还没有解决。或许在未来的版本中，可能会采用外部混洗服务类似的方法，将缓存数据保存在堆外存储中以解决这一问题。\n\n\n## 配置说明\n配置文件：\n$SPARK_HOME/conf/spark-defaults.conf\n$HADOOP_HOME/conf/yarn-site.xml\n\n### Spark配置说明\n在spark-defaults.conf 中添加\n``` bash spark-defaults.conf\nspark.shuffle.service.enabled true   # 开启外部shuffle服务，开启这个服务可以保护executor的shuffle文件，安全移除executor，在Yarn模式下这个shuffle服务以org.apache.spark.yarn.network.YarnShuffleService实现\nspark.shuffle.service.port 7337 # Shuffle Service服务端口，必须和yarn-site中的一致\nspark.dynamicAllocation.enabled true  # 开启动态资源分配\nspark.dynamicAllocation.minExecutors 1  # 每个Application最小分配的executor数\nspark.dynamicAllocation.maxExecutors 30  # 每个Application最大并发分配的executor数\nspark.dynamicAllocation.schedulerBacklogTimeout 1s # 任务待时间（超时便申请新资源)默认60秒\nspark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s #  再次请求等待时间，默认60秒\nspark.dynamicAllocation.executorIdleTimeout # executor闲置时间（超过释放资源）默认600秒\n```\n### yarn的配置\n#### 添加相应的jar包spark-<version>-yarn-shuffle.jar\n如果是自己编译的spark，可以在$SPARK_HOME/network/yarn/target/scala-<version>下面找到\n是预编译的，直接在$SPARK_HOME/lib/下面找到\n找到jar包后，将其添加到每个nodemanager的classpath下面(或者直接放到yarn的lib目录中,${HADOOP_HOME}/share/hadoop/yarn/lib/)\n\n#### 配置yarn-site.xml文件\n在所有节点的yarn-site.xml中，为yarn.nodemanager.aux-services配置项新增spark_shuffle这个值（注意是新增，在原有value的基础上逗号分隔新增即可）\n``` xml yarn-site.xml\n<property>\n  <name>spark.shuffle.service.enabled</name>\n  <value>true</value>\n</property>\n<property>\n  <name>yarn.nodemanager.aux-services</name>\n  <value>mapreduce_shuffle,spark_shuffle</value>\n</property>\n<property>\n  <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\n  <value>org.apache.spark.network.yarn.YarnShuffleService</value>\n</property>\n```\n#### 重启所有的节点\n\n### 注意\n当开启了动态资源分配（spark.dynamicAllocation.enabled），num-executor选项将不再兼容，如果设置了num-executor，那么动态资源分配将被关闭\n\n\n## 引用\n[spark1.6.2作业调度官网](http://spark.apache.org/docs/1.6.2/job-scheduling.html#dynamic-resource-allocation)\n[spark1.6.2作业调度翻译版](http://ifeve.com/spark-schedule/)\n[Apache Spark Resource Management and YARN App Models](http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/)\n[jira/browse/YARN-1197--Support changing resources of an allocated container](https://issues.apache.org/jira/browse/YARN-1197)\n[Spark集群资源动态分配](http://hejunhao.me/archives/675)\n[spark动态资源分配在yarn（hadoop）的配置](http://blog.sina.com.cn/s/blog_a29dec8d0102vfwx.html)\n[Spark Executors在YARN上的动态分配](https://www.linkedin.com/pulse/spark-executors%E5%9C%A8yarn%E4%B8%8A%E7%9A%84%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D-victor-wang)","source":"_posts/2016-09-18-Spark在yarn中的资源申请与分配.md","raw":"---\ntitle: Spark在yarn中的资源申请与分配调研\ndate: 2016-09-18 11:09:21\ntoc: true\ntags: [spark, yarn]\ncategories: spark\ndescription: spark作业提交到yarn的时候，如果用户(wzfw)所在队列本来有500个executor的权限，但是他跑一个简单的程序根本不需要这么多的资源，只需要200个核就足够了，那他如果申请了500个核的话，是否需要全部分配给他？\n---\n\n本文解决遇到的以下问题：\n*spark作业提交到yarn的时候，如果用户(wzfw)所在队列本来有500个executor的权限，但是他跑一个简单的程序根本不需要这么多的资源，只需要200个核就足够了，那他如果申请了400个核的话，是否需要全部分配给他？*\n\n## 前言\n目前我们所有的spark程序的分配都是靠参数设置固定的Executor数量进行资源预分配的，如果用户op在yarn的资源队列里可以申请到200个资源，那它就算跑占用资源很少的程序也能申请到200个核，这是不合理的\n\n比如简单跑如下SparkPi程序，申请20个核：\n``` bash\n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 20 lib/spark-examples-1.6.2-hadoop2.6.0.jar \n```\nyarn中资源占用情况如下：\n{% asset_img spark-yarn-allocation.png %}\n可以看到，我就跑了一个SparkPi啊，竟然用了43G的内存，这样很不合理！\n\n## 总述\nSpark在yarn集群上运行的时候，一方面默认通过num-executors参数设置固定的Executor数量，每个application会独占所有预分配的资源直到整个生命周期的结束。Spark1.2后开始引入动态资源分配（Dynamic Resource Allocation）机制，支持资源弹性分配。\n\n对于已知的业务负载，使用固定的集群资源配置是相对容易的；对于未知的业务负载，使用动态的集群资源分配方式可以满足负载的动态变化，这样集群的资源利用和业务负载的处理效率都会更加灵活。\n\n动态资源分配测试在Spark1.2仅支持Yarn模式，从Spark1.6开始，支持standalone、Yarn、Mesos.这个特性默认是禁用的。\n<!--more-->\n## 动态资源分配的思想\n简单来说，就是基于负载来动态调节Spark应用的资源占用，你的应用会在资源空闲的时候将其释放给集群，而后续用到的时候再重新申请。\n\n### 动态资源分配策略\n其实没有一个固定的方法可以预测一个executor后续是否马上会被分配去执行任务，或者一个新分配的执行器实际上是空闲的，所以我们需要一些试探性的方法，来决定是否申请或移除一个执行器。策略分为**请求策略**与**移除策略**：\n\n#### 请求策略\n\n开启动态分配策略后，application会在task因没有足够资源被挂起的时候去动态申请资源，这种情况意味着该application现有的executor无法满足所有task并行运行。spark一轮一轮的申请资源，当有task挂起或等待spark.dynamicAllocation.schedulerBacklogTimeout(默认1s)时间的时候，会开始动态资源分配；之后会每隔spark.dynamicAllocation.sustainedSchedulerBacklogTimeout(默认1s)时间申请一次，直到申请到足够的资源。**每次申请的资源量是指数增长的，即1,2,4,8等**。\n之所以采用指数增长，出于两方面考虑：其一，开始申请的少是考虑到可能application会马上得到满足；其次要成倍增加，是为了如果application需要很多资源，而该方式可以在很少次数的申请之后得到满足。\n（这段指数增长的策略可以根据实际情况通过修改源码来修改）\n\n#### 资源回收策略\n当application的executor空闲时间超过spark.dynamicAllocation.executorIdleTimeout（默认60s）后，就会被回收。\n\n## 配置思路\n### 启动 external shuffle service\n要使用这一特性有两个前提条件。首先，你的应用必须设置 spark.dynamicAllocation.enabled 为 true。其次，你必须在每个节点上启动一个外部混洗服务（external shuffle service），并在你的应用中将 spark.shuffle.service.enabled 设为true。外部混洗服务的目的就是为了在删除执行器的时候，能够保留其输出的混洗文件（本文后续有更详细的描述）。启用外部混洗的方式在各个集群管理器上各不相同：\n\n在Spark独立部署的集群中，你只需要在worker启动前设置 spark.shuffle.server.enabled 为true即可。\n\n在YARN模式下，混洗服务需要按以下步骤在各个NodeManager上启动：\n\n1. 首先按照YARN profile 构建Spark。如果你已经有打好包的Spark，可以忽略这一步。\n2. 找到 spark-<version>-yarn-shuffle.jar。如果你是自定义编译，其位置应该在 ${SPARK_HOME}/network/yarn/target/scala-<version>，否则应该可以在 lib 目录下找到这个jar包。\n3. 将该jar包添加到NodeManager的classpath路径中。\n4. 配置各个节点上的yarn-site.xml，将 spark_shuffle 添加到 yarn.nodemanager.aux-services 中，然后将 yarn.nodemanager.aux-services.spark_shuffle.class 设为 org.apache.spark.network.yarn.YarnShuffleService，并将 spark.shuffle.service.enabled 设为 true。\n5. 最后重启各节点上的NodeManager。\n\n所有相关的配置都是可选的，并且都在 spark.dynamicAllocation.* 和 spark.shuffle.service.* 命名空间下。更详细请参考：[configurations page](http://spark.apache.org/docs/latest/configuration.html#dynamic-allocation)。\n\n### 外部混洗服务external shuffle service\n非动态分配模式下，执行器可能的退出原因有执行失败或者相关Spark应用已经退出。不管是哪种原因，执行器的所有状态都已经不再需要，可以丢弃掉。但在动态分配的情形下，执行器有可能在Spark应用运行期间被移除。这时候，如果Spark应用尝试去访问该执行器存储的状态，就必须重算这一部分数据。因此，Spark需要一种机制，能够优雅地关闭执行器，同时还保留其状态数据。\n\n这种需求对于混洗操作尤其重要。混洗过程中，Spark执行器首先将map输出写到本地磁盘，同时执行器本身又是一个文件服务器，这样其他执行器就能够通过该执行器获得对应的map结果数据。一旦有某些任务执行时间过长，动态分配有可能在混洗结束前移除任务异常的执行器，而这些被移除的执行器对应的数据将会被重新计算，但这些重算其实是不必要的。\n\n要解决这一问题，就需要用到一个外部混洗服务（external shuffle service），该服务在Spark 1.2引入。该服务在每个节点上都会启动一个不依赖于任何Spark应用或执行器的独立进程。一旦该服务启用，Spark执行器不再从各个执行器上获取shuffle文件，转而从这个service获取。这意味着，任何执行器输出的混洗状态数据都可能存留时间比对应的执行器进程还长。\n\n除了混洗文件之外，执行器也会在磁盘或者内存中缓存数。一旦执行器被移除，其缓存数据将无法访问。这个问题目前还没有解决。或许在未来的版本中，可能会采用外部混洗服务类似的方法，将缓存数据保存在堆外存储中以解决这一问题。\n\n\n## 配置说明\n配置文件：\n$SPARK_HOME/conf/spark-defaults.conf\n$HADOOP_HOME/conf/yarn-site.xml\n\n### Spark配置说明\n在spark-defaults.conf 中添加\n``` bash spark-defaults.conf\nspark.shuffle.service.enabled true   # 开启外部shuffle服务，开启这个服务可以保护executor的shuffle文件，安全移除executor，在Yarn模式下这个shuffle服务以org.apache.spark.yarn.network.YarnShuffleService实现\nspark.shuffle.service.port 7337 # Shuffle Service服务端口，必须和yarn-site中的一致\nspark.dynamicAllocation.enabled true  # 开启动态资源分配\nspark.dynamicAllocation.minExecutors 1  # 每个Application最小分配的executor数\nspark.dynamicAllocation.maxExecutors 30  # 每个Application最大并发分配的executor数\nspark.dynamicAllocation.schedulerBacklogTimeout 1s # 任务待时间（超时便申请新资源)默认60秒\nspark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s #  再次请求等待时间，默认60秒\nspark.dynamicAllocation.executorIdleTimeout # executor闲置时间（超过释放资源）默认600秒\n```\n### yarn的配置\n#### 添加相应的jar包spark-<version>-yarn-shuffle.jar\n如果是自己编译的spark，可以在$SPARK_HOME/network/yarn/target/scala-<version>下面找到\n是预编译的，直接在$SPARK_HOME/lib/下面找到\n找到jar包后，将其添加到每个nodemanager的classpath下面(或者直接放到yarn的lib目录中,${HADOOP_HOME}/share/hadoop/yarn/lib/)\n\n#### 配置yarn-site.xml文件\n在所有节点的yarn-site.xml中，为yarn.nodemanager.aux-services配置项新增spark_shuffle这个值（注意是新增，在原有value的基础上逗号分隔新增即可）\n``` xml yarn-site.xml\n<property>\n  <name>spark.shuffle.service.enabled</name>\n  <value>true</value>\n</property>\n<property>\n  <name>yarn.nodemanager.aux-services</name>\n  <value>mapreduce_shuffle,spark_shuffle</value>\n</property>\n<property>\n  <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\n  <value>org.apache.spark.network.yarn.YarnShuffleService</value>\n</property>\n```\n#### 重启所有的节点\n\n### 注意\n当开启了动态资源分配（spark.dynamicAllocation.enabled），num-executor选项将不再兼容，如果设置了num-executor，那么动态资源分配将被关闭\n\n\n## 引用\n[spark1.6.2作业调度官网](http://spark.apache.org/docs/1.6.2/job-scheduling.html#dynamic-resource-allocation)\n[spark1.6.2作业调度翻译版](http://ifeve.com/spark-schedule/)\n[Apache Spark Resource Management and YARN App Models](http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/)\n[jira/browse/YARN-1197--Support changing resources of an allocated container](https://issues.apache.org/jira/browse/YARN-1197)\n[Spark集群资源动态分配](http://hejunhao.me/archives/675)\n[spark动态资源分配在yarn（hadoop）的配置](http://blog.sina.com.cn/s/blog_a29dec8d0102vfwx.html)\n[Spark Executors在YARN上的动态分配](https://www.linkedin.com/pulse/spark-executors%E5%9C%A8yarn%E4%B8%8A%E7%9A%84%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D-victor-wang)","slug":"Spark在yarn中的资源申请与分配","published":1,"updated":"2016-09-22T06:42:04.832Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfv8000mpsguwun81s5x","content":"<p>本文解决遇到的以下问题：<br><em>spark作业提交到yarn的时候，如果用户(wzfw)所在队列本来有500个executor的权限，但是他跑一个简单的程序根本不需要这么多的资源，只需要200个核就足够了，那他如果申请了400个核的话，是否需要全部分配给他？</em></p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>目前我们所有的spark程序的分配都是靠参数设置固定的Executor数量进行资源预分配的，如果用户op在yarn的资源队列里可以申请到200个资源，那它就算跑占用资源很少的程序也能申请到200个核，这是不合理的</p>\n<p>比如简单跑如下SparkPi程序，申请20个核：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 20 lib/spark-examples-1.6.2-hadoop2.6.0.jar</div></pre></td></tr></table></figure></p>\n<p>yarn中资源占用情况如下：<br><img src=\"/2016/09/18/Spark在yarn中的资源申请与分配/spark-yarn-allocation.png\" alt=\"spark-yarn-allocation.png\" title=\"\"><br>可以看到，我就跑了一个SparkPi啊，竟然用了43G的内存，这样很不合理！</p>\n<h2 id=\"总述\"><a href=\"#总述\" class=\"headerlink\" title=\"总述\"></a>总述</h2><p>Spark在yarn集群上运行的时候，一方面默认通过num-executors参数设置固定的Executor数量，每个application会独占所有预分配的资源直到整个生命周期的结束。Spark1.2后开始引入动态资源分配（Dynamic Resource Allocation）机制，支持资源弹性分配。</p>\n<p>对于已知的业务负载，使用固定的集群资源配置是相对容易的；对于未知的业务负载，使用动态的集群资源分配方式可以满足负载的动态变化，这样集群的资源利用和业务负载的处理效率都会更加灵活。</p>\n<p>动态资源分配测试在Spark1.2仅支持Yarn模式，从Spark1.6开始，支持standalone、Yarn、Mesos.这个特性默认是禁用的。<br><a id=\"more\"></a></p>\n<h2 id=\"动态资源分配的思想\"><a href=\"#动态资源分配的思想\" class=\"headerlink\" title=\"动态资源分配的思想\"></a>动态资源分配的思想</h2><p>简单来说，就是基于负载来动态调节Spark应用的资源占用，你的应用会在资源空闲的时候将其释放给集群，而后续用到的时候再重新申请。</p>\n<h3 id=\"动态资源分配策略\"><a href=\"#动态资源分配策略\" class=\"headerlink\" title=\"动态资源分配策略\"></a>动态资源分配策略</h3><p>其实没有一个固定的方法可以预测一个executor后续是否马上会被分配去执行任务，或者一个新分配的执行器实际上是空闲的，所以我们需要一些试探性的方法，来决定是否申请或移除一个执行器。策略分为<strong>请求策略</strong>与<strong>移除策略</strong>：</p>\n<h4 id=\"请求策略\"><a href=\"#请求策略\" class=\"headerlink\" title=\"请求策略\"></a>请求策略</h4><p>开启动态分配策略后，application会在task因没有足够资源被挂起的时候去动态申请资源，这种情况意味着该application现有的executor无法满足所有task并行运行。spark一轮一轮的申请资源，当有task挂起或等待spark.dynamicAllocation.schedulerBacklogTimeout(默认1s)时间的时候，会开始动态资源分配；之后会每隔spark.dynamicAllocation.sustainedSchedulerBacklogTimeout(默认1s)时间申请一次，直到申请到足够的资源。<strong>每次申请的资源量是指数增长的，即1,2,4,8等</strong>。<br>之所以采用指数增长，出于两方面考虑：其一，开始申请的少是考虑到可能application会马上得到满足；其次要成倍增加，是为了如果application需要很多资源，而该方式可以在很少次数的申请之后得到满足。<br>（这段指数增长的策略可以根据实际情况通过修改源码来修改）</p>\n<h4 id=\"资源回收策略\"><a href=\"#资源回收策略\" class=\"headerlink\" title=\"资源回收策略\"></a>资源回收策略</h4><p>当application的executor空闲时间超过spark.dynamicAllocation.executorIdleTimeout（默认60s）后，就会被回收。</p>\n<h2 id=\"配置思路\"><a href=\"#配置思路\" class=\"headerlink\" title=\"配置思路\"></a>配置思路</h2><h3 id=\"启动-external-shuffle-service\"><a href=\"#启动-external-shuffle-service\" class=\"headerlink\" title=\"启动 external shuffle service\"></a>启动 external shuffle service</h3><p>要使用这一特性有两个前提条件。首先，你的应用必须设置 spark.dynamicAllocation.enabled 为 true。其次，你必须在每个节点上启动一个外部混洗服务（external shuffle service），并在你的应用中将 spark.shuffle.service.enabled 设为true。外部混洗服务的目的就是为了在删除执行器的时候，能够保留其输出的混洗文件（本文后续有更详细的描述）。启用外部混洗的方式在各个集群管理器上各不相同：</p>\n<p>在Spark独立部署的集群中，你只需要在worker启动前设置 spark.shuffle.server.enabled 为true即可。</p>\n<p>在YARN模式下，混洗服务需要按以下步骤在各个NodeManager上启动：</p>\n<ol>\n<li>首先按照YARN profile 构建Spark。如果你已经有打好包的Spark，可以忽略这一步。</li>\n<li>找到 spark-<version>-yarn-shuffle.jar。如果你是自定义编译，其位置应该在 ${SPARK_HOME}/network/yarn/target/scala-<version>，否则应该可以在 lib 目录下找到这个jar包。</version></version></li>\n<li>将该jar包添加到NodeManager的classpath路径中。</li>\n<li>配置各个节点上的yarn-site.xml，将 spark_shuffle 添加到 yarn.nodemanager.aux-services 中，然后将 yarn.nodemanager.aux-services.spark_shuffle.class 设为 org.apache.spark.network.yarn.YarnShuffleService，并将 spark.shuffle.service.enabled 设为 true。</li>\n<li>最后重启各节点上的NodeManager。</li>\n</ol>\n<p>所有相关的配置都是可选的，并且都在 spark.dynamicAllocation.<em> 和 spark.shuffle.service.</em> 命名空间下。更详细请参考：<a href=\"http://spark.apache.org/docs/latest/configuration.html#dynamic-allocation\" target=\"_blank\" rel=\"external\">configurations page</a>。</p>\n<h3 id=\"外部混洗服务external-shuffle-service\"><a href=\"#外部混洗服务external-shuffle-service\" class=\"headerlink\" title=\"外部混洗服务external shuffle service\"></a>外部混洗服务external shuffle service</h3><p>非动态分配模式下，执行器可能的退出原因有执行失败或者相关Spark应用已经退出。不管是哪种原因，执行器的所有状态都已经不再需要，可以丢弃掉。但在动态分配的情形下，执行器有可能在Spark应用运行期间被移除。这时候，如果Spark应用尝试去访问该执行器存储的状态，就必须重算这一部分数据。因此，Spark需要一种机制，能够优雅地关闭执行器，同时还保留其状态数据。</p>\n<p>这种需求对于混洗操作尤其重要。混洗过程中，Spark执行器首先将map输出写到本地磁盘，同时执行器本身又是一个文件服务器，这样其他执行器就能够通过该执行器获得对应的map结果数据。一旦有某些任务执行时间过长，动态分配有可能在混洗结束前移除任务异常的执行器，而这些被移除的执行器对应的数据将会被重新计算，但这些重算其实是不必要的。</p>\n<p>要解决这一问题，就需要用到一个外部混洗服务（external shuffle service），该服务在Spark 1.2引入。该服务在每个节点上都会启动一个不依赖于任何Spark应用或执行器的独立进程。一旦该服务启用，Spark执行器不再从各个执行器上获取shuffle文件，转而从这个service获取。这意味着，任何执行器输出的混洗状态数据都可能存留时间比对应的执行器进程还长。</p>\n<p>除了混洗文件之外，执行器也会在磁盘或者内存中缓存数。一旦执行器被移除，其缓存数据将无法访问。这个问题目前还没有解决。或许在未来的版本中，可能会采用外部混洗服务类似的方法，将缓存数据保存在堆外存储中以解决这一问题。</p>\n<h2 id=\"配置说明\"><a href=\"#配置说明\" class=\"headerlink\" title=\"配置说明\"></a>配置说明</h2><p>配置文件：<br>$SPARK_HOME/conf/spark-defaults.conf<br>$HADOOP_HOME/conf/yarn-site.xml</p>\n<h3 id=\"Spark配置说明\"><a href=\"#Spark配置说明\" class=\"headerlink\" title=\"Spark配置说明\"></a>Spark配置说明</h3><p>在spark-defaults.conf 中添加<br><figure class=\"highlight bash\"><figcaption><span>spark-defaults.conf</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">spark.shuffle.service.enabled <span class=\"literal\">true</span>   <span class=\"comment\"># 开启外部shuffle服务，开启这个服务可以保护executor的shuffle文件，安全移除executor，在Yarn模式下这个shuffle服务以org.apache.spark.yarn.network.YarnShuffleService实现</span></div><div class=\"line\">spark.shuffle.service.port 7337 <span class=\"comment\"># Shuffle Service服务端口，必须和yarn-site中的一致</span></div><div class=\"line\">spark.dynamicAllocation.enabled <span class=\"literal\">true</span>  <span class=\"comment\"># 开启动态资源分配</span></div><div class=\"line\">spark.dynamicAllocation.minExecutors 1  <span class=\"comment\"># 每个Application最小分配的executor数</span></div><div class=\"line\">spark.dynamicAllocation.maxExecutors 30  <span class=\"comment\"># 每个Application最大并发分配的executor数</span></div><div class=\"line\">spark.dynamicAllocation.schedulerBacklogTimeout 1s <span class=\"comment\"># 任务待时间（超时便申请新资源)默认60秒</span></div><div class=\"line\">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s <span class=\"comment\">#  再次请求等待时间，默认60秒</span></div><div class=\"line\">spark.dynamicAllocation.executorIdleTimeout <span class=\"comment\"># executor闲置时间（超过释放资源）默认600秒</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"yarn的配置\"><a href=\"#yarn的配置\" class=\"headerlink\" title=\"yarn的配置\"></a>yarn的配置</h3><h4 id=\"添加相应的jar包spark-yarn-shuffle-jar\"><a href=\"#添加相应的jar包spark-yarn-shuffle-jar\" class=\"headerlink\" title=\"添加相应的jar包spark--yarn-shuffle.jar\"></a>添加相应的jar包spark-<version>-yarn-shuffle.jar</version></h4><p>如果是自己编译的spark，可以在$SPARK_HOME/network/yarn/target/scala-<version>下面找到<br>是预编译的，直接在$SPARK_HOME/lib/下面找到<br>找到jar包后，将其添加到每个nodemanager的classpath下面(或者直接放到yarn的lib目录中,${HADOOP_HOME}/share/hadoop/yarn/lib/)</version></p>\n<h4 id=\"配置yarn-site-xml文件\"><a href=\"#配置yarn-site-xml文件\" class=\"headerlink\" title=\"配置yarn-site.xml文件\"></a>配置yarn-site.xml文件</h4><p>在所有节点的yarn-site.xml中，为yarn.nodemanager.aux-services配置项新增spark_shuffle这个值（注意是新增，在原有value的基础上逗号分隔新增即可）<br><figure class=\"highlight xml\"><figcaption><span>yarn-site.xml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>spark.shuffle.service.enabled<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"重启所有的节点\"><a href=\"#重启所有的节点\" class=\"headerlink\" title=\"重启所有的节点\"></a>重启所有的节点</h4><h3 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h3><p>当开启了动态资源分配（spark.dynamicAllocation.enabled），num-executor选项将不再兼容，如果设置了num-executor，那么动态资源分配将被关闭</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"http://spark.apache.org/docs/1.6.2/job-scheduling.html#dynamic-resource-allocation\" target=\"_blank\" rel=\"external\">spark1.6.2作业调度官网</a><br><a href=\"http://ifeve.com/spark-schedule/\" target=\"_blank\" rel=\"external\">spark1.6.2作业调度翻译版</a><br><a href=\"http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/\" target=\"_blank\" rel=\"external\">Apache Spark Resource Management and YARN App Models</a><br><a href=\"https://issues.apache.org/jira/browse/YARN-1197\" target=\"_blank\" rel=\"external\">jira/browse/YARN-1197–Support changing resources of an allocated container</a><br><a href=\"http://hejunhao.me/archives/675\" target=\"_blank\" rel=\"external\">Spark集群资源动态分配</a><br><a href=\"http://blog.sina.com.cn/s/blog_a29dec8d0102vfwx.html\" target=\"_blank\" rel=\"external\">spark动态资源分配在yarn（hadoop）的配置</a><br><a href=\"https://www.linkedin.com/pulse/spark-executors%E5%9C%A8yarn%E4%B8%8A%E7%9A%84%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D-victor-wang\" target=\"_blank\" rel=\"external\">Spark Executors在YARN上的动态分配</a></p>\n","excerpt":"<p>本文解决遇到的以下问题：<br><em>spark作业提交到yarn的时候，如果用户(wzfw)所在队列本来有500个executor的权限，但是他跑一个简单的程序根本不需要这么多的资源，只需要200个核就足够了，那他如果申请了400个核的话，是否需要全部分配给他？</em></p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>目前我们所有的spark程序的分配都是靠参数设置固定的Executor数量进行资源预分配的，如果用户op在yarn的资源队列里可以申请到200个资源，那它就算跑占用资源很少的程序也能申请到200个核，这是不合理的</p>\n<p>比如简单跑如下SparkPi程序，申请20个核：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 20 lib/spark-examples-1.6.2-hadoop2.6.0.jar</div></pre></td></tr></table></figure></p>\n<p>yarn中资源占用情况如下：<br><img src=\"/2016/09/18/Spark在yarn中的资源申请与分配/spark-yarn-allocation.png\" alt=\"spark-yarn-allocation.png\" title=\"\"><br>可以看到，我就跑了一个SparkPi啊，竟然用了43G的内存，这样很不合理！</p>\n<h2 id=\"总述\"><a href=\"#总述\" class=\"headerlink\" title=\"总述\"></a>总述</h2><p>Spark在yarn集群上运行的时候，一方面默认通过num-executors参数设置固定的Executor数量，每个application会独占所有预分配的资源直到整个生命周期的结束。Spark1.2后开始引入动态资源分配（Dynamic Resource Allocation）机制，支持资源弹性分配。</p>\n<p>对于已知的业务负载，使用固定的集群资源配置是相对容易的；对于未知的业务负载，使用动态的集群资源分配方式可以满足负载的动态变化，这样集群的资源利用和业务负载的处理效率都会更加灵活。</p>\n<p>动态资源分配测试在Spark1.2仅支持Yarn模式，从Spark1.6开始，支持standalone、Yarn、Mesos.这个特性默认是禁用的。<br>","more":"</p>\n<h2 id=\"动态资源分配的思想\"><a href=\"#动态资源分配的思想\" class=\"headerlink\" title=\"动态资源分配的思想\"></a>动态资源分配的思想</h2><p>简单来说，就是基于负载来动态调节Spark应用的资源占用，你的应用会在资源空闲的时候将其释放给集群，而后续用到的时候再重新申请。</p>\n<h3 id=\"动态资源分配策略\"><a href=\"#动态资源分配策略\" class=\"headerlink\" title=\"动态资源分配策略\"></a>动态资源分配策略</h3><p>其实没有一个固定的方法可以预测一个executor后续是否马上会被分配去执行任务，或者一个新分配的执行器实际上是空闲的，所以我们需要一些试探性的方法，来决定是否申请或移除一个执行器。策略分为<strong>请求策略</strong>与<strong>移除策略</strong>：</p>\n<h4 id=\"请求策略\"><a href=\"#请求策略\" class=\"headerlink\" title=\"请求策略\"></a>请求策略</h4><p>开启动态分配策略后，application会在task因没有足够资源被挂起的时候去动态申请资源，这种情况意味着该application现有的executor无法满足所有task并行运行。spark一轮一轮的申请资源，当有task挂起或等待spark.dynamicAllocation.schedulerBacklogTimeout(默认1s)时间的时候，会开始动态资源分配；之后会每隔spark.dynamicAllocation.sustainedSchedulerBacklogTimeout(默认1s)时间申请一次，直到申请到足够的资源。<strong>每次申请的资源量是指数增长的，即1,2,4,8等</strong>。<br>之所以采用指数增长，出于两方面考虑：其一，开始申请的少是考虑到可能application会马上得到满足；其次要成倍增加，是为了如果application需要很多资源，而该方式可以在很少次数的申请之后得到满足。<br>（这段指数增长的策略可以根据实际情况通过修改源码来修改）</p>\n<h4 id=\"资源回收策略\"><a href=\"#资源回收策略\" class=\"headerlink\" title=\"资源回收策略\"></a>资源回收策略</h4><p>当application的executor空闲时间超过spark.dynamicAllocation.executorIdleTimeout（默认60s）后，就会被回收。</p>\n<h2 id=\"配置思路\"><a href=\"#配置思路\" class=\"headerlink\" title=\"配置思路\"></a>配置思路</h2><h3 id=\"启动-external-shuffle-service\"><a href=\"#启动-external-shuffle-service\" class=\"headerlink\" title=\"启动 external shuffle service\"></a>启动 external shuffle service</h3><p>要使用这一特性有两个前提条件。首先，你的应用必须设置 spark.dynamicAllocation.enabled 为 true。其次，你必须在每个节点上启动一个外部混洗服务（external shuffle service），并在你的应用中将 spark.shuffle.service.enabled 设为true。外部混洗服务的目的就是为了在删除执行器的时候，能够保留其输出的混洗文件（本文后续有更详细的描述）。启用外部混洗的方式在各个集群管理器上各不相同：</p>\n<p>在Spark独立部署的集群中，你只需要在worker启动前设置 spark.shuffle.server.enabled 为true即可。</p>\n<p>在YARN模式下，混洗服务需要按以下步骤在各个NodeManager上启动：</p>\n<ol>\n<li>首先按照YARN profile 构建Spark。如果你已经有打好包的Spark，可以忽略这一步。</li>\n<li>找到 spark-<version>-yarn-shuffle.jar。如果你是自定义编译，其位置应该在 ${SPARK_HOME}/network/yarn/target/scala-<version>，否则应该可以在 lib 目录下找到这个jar包。</li>\n<li>将该jar包添加到NodeManager的classpath路径中。</li>\n<li>配置各个节点上的yarn-site.xml，将 spark_shuffle 添加到 yarn.nodemanager.aux-services 中，然后将 yarn.nodemanager.aux-services.spark_shuffle.class 设为 org.apache.spark.network.yarn.YarnShuffleService，并将 spark.shuffle.service.enabled 设为 true。</li>\n<li>最后重启各节点上的NodeManager。</li>\n</ol>\n<p>所有相关的配置都是可选的，并且都在 spark.dynamicAllocation.<em> 和 spark.shuffle.service.</em> 命名空间下。更详细请参考：<a href=\"http://spark.apache.org/docs/latest/configuration.html#dynamic-allocation\">configurations page</a>。</p>\n<h3 id=\"外部混洗服务external-shuffle-service\"><a href=\"#外部混洗服务external-shuffle-service\" class=\"headerlink\" title=\"外部混洗服务external shuffle service\"></a>外部混洗服务external shuffle service</h3><p>非动态分配模式下，执行器可能的退出原因有执行失败或者相关Spark应用已经退出。不管是哪种原因，执行器的所有状态都已经不再需要，可以丢弃掉。但在动态分配的情形下，执行器有可能在Spark应用运行期间被移除。这时候，如果Spark应用尝试去访问该执行器存储的状态，就必须重算这一部分数据。因此，Spark需要一种机制，能够优雅地关闭执行器，同时还保留其状态数据。</p>\n<p>这种需求对于混洗操作尤其重要。混洗过程中，Spark执行器首先将map输出写到本地磁盘，同时执行器本身又是一个文件服务器，这样其他执行器就能够通过该执行器获得对应的map结果数据。一旦有某些任务执行时间过长，动态分配有可能在混洗结束前移除任务异常的执行器，而这些被移除的执行器对应的数据将会被重新计算，但这些重算其实是不必要的。</p>\n<p>要解决这一问题，就需要用到一个外部混洗服务（external shuffle service），该服务在Spark 1.2引入。该服务在每个节点上都会启动一个不依赖于任何Spark应用或执行器的独立进程。一旦该服务启用，Spark执行器不再从各个执行器上获取shuffle文件，转而从这个service获取。这意味着，任何执行器输出的混洗状态数据都可能存留时间比对应的执行器进程还长。</p>\n<p>除了混洗文件之外，执行器也会在磁盘或者内存中缓存数。一旦执行器被移除，其缓存数据将无法访问。这个问题目前还没有解决。或许在未来的版本中，可能会采用外部混洗服务类似的方法，将缓存数据保存在堆外存储中以解决这一问题。</p>\n<h2 id=\"配置说明\"><a href=\"#配置说明\" class=\"headerlink\" title=\"配置说明\"></a>配置说明</h2><p>配置文件：<br>$SPARK_HOME/conf/spark-defaults.conf<br>$HADOOP_HOME/conf/yarn-site.xml</p>\n<h3 id=\"Spark配置说明\"><a href=\"#Spark配置说明\" class=\"headerlink\" title=\"Spark配置说明\"></a>Spark配置说明</h3><p>在spark-defaults.conf 中添加<br><figure class=\"highlight bash\"><figcaption><span>spark-defaults.conf</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">spark.shuffle.service.enabled <span class=\"literal\">true</span>   <span class=\"comment\"># 开启外部shuffle服务，开启这个服务可以保护executor的shuffle文件，安全移除executor，在Yarn模式下这个shuffle服务以org.apache.spark.yarn.network.YarnShuffleService实现</span></div><div class=\"line\">spark.shuffle.service.port 7337 <span class=\"comment\"># Shuffle Service服务端口，必须和yarn-site中的一致</span></div><div class=\"line\">spark.dynamicAllocation.enabled <span class=\"literal\">true</span>  <span class=\"comment\"># 开启动态资源分配</span></div><div class=\"line\">spark.dynamicAllocation.minExecutors 1  <span class=\"comment\"># 每个Application最小分配的executor数</span></div><div class=\"line\">spark.dynamicAllocation.maxExecutors 30  <span class=\"comment\"># 每个Application最大并发分配的executor数</span></div><div class=\"line\">spark.dynamicAllocation.schedulerBacklogTimeout 1s <span class=\"comment\"># 任务待时间（超时便申请新资源)默认60秒</span></div><div class=\"line\">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s <span class=\"comment\">#  再次请求等待时间，默认60秒</span></div><div class=\"line\">spark.dynamicAllocation.executorIdleTimeout <span class=\"comment\"># executor闲置时间（超过释放资源）默认600秒</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"yarn的配置\"><a href=\"#yarn的配置\" class=\"headerlink\" title=\"yarn的配置\"></a>yarn的配置</h3><h4 id=\"添加相应的jar包spark-yarn-shuffle-jar\"><a href=\"#添加相应的jar包spark-yarn-shuffle-jar\" class=\"headerlink\" title=\"添加相应的jar包spark--yarn-shuffle.jar\"></a>添加相应的jar包spark-<version>-yarn-shuffle.jar</h4><p>如果是自己编译的spark，可以在$SPARK_HOME/network/yarn/target/scala-<version>下面找到<br>是预编译的，直接在$SPARK_HOME/lib/下面找到<br>找到jar包后，将其添加到每个nodemanager的classpath下面(或者直接放到yarn的lib目录中,${HADOOP_HOME}/share/hadoop/yarn/lib/)</p>\n<h4 id=\"配置yarn-site-xml文件\"><a href=\"#配置yarn-site-xml文件\" class=\"headerlink\" title=\"配置yarn-site.xml文件\"></a>配置yarn-site.xml文件</h4><p>在所有节点的yarn-site.xml中，为yarn.nodemanager.aux-services配置项新增spark_shuffle这个值（注意是新增，在原有value的基础上逗号分隔新增即可）<br><figure class=\"highlight xml\"><figcaption><span>yarn-site.xml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>spark.shuffle.service.enabled<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"重启所有的节点\"><a href=\"#重启所有的节点\" class=\"headerlink\" title=\"重启所有的节点\"></a>重启所有的节点</h4><h3 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h3><p>当开启了动态资源分配（spark.dynamicAllocation.enabled），num-executor选项将不再兼容，如果设置了num-executor，那么动态资源分配将被关闭</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"http://spark.apache.org/docs/1.6.2/job-scheduling.html#dynamic-resource-allocation\">spark1.6.2作业调度官网</a><br><a href=\"http://ifeve.com/spark-schedule/\">spark1.6.2作业调度翻译版</a><br><a href=\"http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/\">Apache Spark Resource Management and YARN App Models</a><br><a href=\"https://issues.apache.org/jira/browse/YARN-1197\">jira/browse/YARN-1197–Support changing resources of an allocated container</a><br><a href=\"http://hejunhao.me/archives/675\">Spark集群资源动态分配</a><br><a href=\"http://blog.sina.com.cn/s/blog_a29dec8d0102vfwx.html\">spark动态资源分配在yarn（hadoop）的配置</a><br><a href=\"https://www.linkedin.com/pulse/spark-executors%E5%9C%A8yarn%E4%B8%8A%E7%9A%84%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D-victor-wang\">Spark Executors在YARN上的动态分配</a></p>"},{"title":"Spark在Kerberos下连接使用Hbase的配置","toc":false,"date":"2016-09-18T09:06:06.000Z","_content":"\n复制HBase目录下的lib文件到spark目录/lib/hbase。spark依赖此lib，但直接指定到Hbase下的lib目录的话又会出错\n清单如下：guava-12.0.1.jar htrace-core-3.1.0-incubating.jar protobuf-java-2.5.0.jar   这三个jar加上以hbase开头所有jar，其它就不必了，全部复制会引起报错。\n``` bash \ncd $SPARK_HOME/lib/hbase\ncp /usr/lib/hbase/lib/hbase-* ./\ncp /usr/lib/hbase/lib/guava-12.0.1.jar ./\ncp /usr/lib/hbase/lib/htrace-core-3.1.0-incubating.jar ./\ncp /usr/lib/hbase/lib/protobuf-java-2.5.0.jar ./\n```\n然后在spark客户端配置如下：\n也就是增加到classpath\n``` bash spark-default.conf\nspark.driver.extraClassPath /usr/lib/hadoop/lib/*:/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/lib/hbase/*\n```\n就可以了","source":"_posts/2016-09-18-Spark在Kerberos下连接使用Hbase的配置.md","raw":"---\ntitle: Spark在Kerberos下连接使用Hbase的配置\ntoc: false\ndate: 2016-09-18 17:06:06\ntags: [spark, hbase, kerberos]\ncategories: spark\n---\n\n复制HBase目录下的lib文件到spark目录/lib/hbase。spark依赖此lib，但直接指定到Hbase下的lib目录的话又会出错\n清单如下：guava-12.0.1.jar htrace-core-3.1.0-incubating.jar protobuf-java-2.5.0.jar   这三个jar加上以hbase开头所有jar，其它就不必了，全部复制会引起报错。\n``` bash \ncd $SPARK_HOME/lib/hbase\ncp /usr/lib/hbase/lib/hbase-* ./\ncp /usr/lib/hbase/lib/guava-12.0.1.jar ./\ncp /usr/lib/hbase/lib/htrace-core-3.1.0-incubating.jar ./\ncp /usr/lib/hbase/lib/protobuf-java-2.5.0.jar ./\n```\n然后在spark客户端配置如下：\n也就是增加到classpath\n``` bash spark-default.conf\nspark.driver.extraClassPath /usr/lib/hadoop/lib/*:/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/lib/hbase/*\n```\n就可以了","slug":"Spark在Kerberos下连接使用Hbase的配置","published":1,"updated":"2016-09-18T09:58:24.636Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfv9000npsguzcp13fmk","content":"<p>复制HBase目录下的lib文件到spark目录/lib/hbase。spark依赖此lib，但直接指定到Hbase下的lib目录的话又会出错<br>清单如下：guava-12.0.1.jar htrace-core-3.1.0-incubating.jar protobuf-java-2.5.0.jar   这三个jar加上以hbase开头所有jar，其它就不必了，全部复制会引起报错。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">cd</span> <span class=\"variable\">$SPARK_HOME</span>/lib/hbase</div><div class=\"line\">cp /usr/lib/hbase/lib/hbase-* ./</div><div class=\"line\">cp /usr/lib/hbase/lib/guava-12.0.1.jar ./</div><div class=\"line\">cp /usr/lib/hbase/lib/htrace-core-3.1.0-incubating.jar ./</div><div class=\"line\">cp /usr/lib/hbase/lib/protobuf-java-2.5.0.jar ./</div></pre></td></tr></table></figure></p>\n<p>然后在spark客户端配置如下：<br>也就是增加到classpath<br><figure class=\"highlight bash\"><figcaption><span>spark-default.conf</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">spark.driver.extraClassPath /usr/lib/hadoop/lib/*:/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/lib/hbase/*</div></pre></td></tr></table></figure></p>\n<p>就可以了</p>\n","excerpt":"","more":"<p>复制HBase目录下的lib文件到spark目录/lib/hbase。spark依赖此lib，但直接指定到Hbase下的lib目录的话又会出错<br>清单如下：guava-12.0.1.jar htrace-core-3.1.0-incubating.jar protobuf-java-2.5.0.jar   这三个jar加上以hbase开头所有jar，其它就不必了，全部复制会引起报错。<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">cd</span> <span class=\"variable\">$SPARK_HOME</span>/lib/hbase</div><div class=\"line\">cp /usr/lib/hbase/lib/hbase-* ./</div><div class=\"line\">cp /usr/lib/hbase/lib/guava-12.0.1.jar ./</div><div class=\"line\">cp /usr/lib/hbase/lib/htrace-core-3.1.0-incubating.jar ./</div><div class=\"line\">cp /usr/lib/hbase/lib/protobuf-java-2.5.0.jar ./</div></pre></td></tr></table></figure></p>\n<p>然后在spark客户端配置如下：<br>也就是增加到classpath<br><figure class=\"highlight bash\"><figcaption><span>spark-default.conf</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">spark.driver.extraClassPath /usr/lib/hadoop/lib/*:/usr/op/sparkKerbersTest/spark-1.6.2-bin-hadoop2.6/lib/hbase/*</div></pre></td></tr></table></figure></p>\n<p>就可以了</p>\n"},{"title":"scala中@的用法","toc":false,"date":"2016-09-23T03:24:02.000Z","_content":"\n有些场景，比如模式匹配会遇到scala代码中有@符号，比如\n``` scala\ncase x @ Some(Nil) => x\n```\n现将网友的答案总结一下，并持续更新：\n### 绑定在模式匹配中，取出对应的原来输入值\n比如：\n\n``` scala\nval o: Option[Int] = Some(5)\n\n// o: Option[Int] = Some(5)\n\no match {\n  case Some(x) => println(x)\n  case None =>\n}\n// 输出:  \n5\n\no match {\n  case x @ Some(_) => println(x)\n  case None =>\n}\n// 输出\nSome(5)\n```\n如上案例，有些情况下，模式匹配后你并不想取出他的值，而是取出他本来的自己（Some(5)），这种情况下就用 @；并且@可以用于各个级别\n\n### @可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式\n听起来很绕口，看如下代码：\n\n``` scala\nval d@(c@Some(a), Some(b)) = (Some(1), Some(2))\n```\n结果竟然产生了四个值：\n``` scala\nd: (Some[Int], Some[Int]) = (Some(1),Some(2))\nc: Some[Int] = Some(1)\na: Int = 1\nb: Int = 2\n```\n如上所述，说明定义d和c是两个匹配模式，a和b是两个数字\n\n``` scala\n(Some(1), Some(2)) match {\n  case d@(c@Some(a), Some(b)) => println(a, b, c, d)\n}\n```\n结果如下：\n``` scala\n(1,2,Some(1),(Some(1),Some(2)))\n```\n\n再跑一个例子\n``` scala\nscala> for (t@Some(u) <- Seq(Some(1))) println(t, u)\n(Some(1),1)\n```\n\n再跑一个例子\n\n``` scala\nscala> val List(x, xs @ _*) = List(1, 2, 3)\nx: Int = 1\nxs: Seq[Int] = List(2, 3)\n```\n\n### 引用\nhttp://stackoverflow.com/questions/2359014/scala-operator\n","source":"_posts/2016-09-23-scala中-的用法.md","raw":"---\ntitle: scala中@的用法\ntoc: false\ndate: 2016-09-23 11:24:02\ntags: scala\ncategories: scala\n---\n\n有些场景，比如模式匹配会遇到scala代码中有@符号，比如\n``` scala\ncase x @ Some(Nil) => x\n```\n现将网友的答案总结一下，并持续更新：\n### 绑定在模式匹配中，取出对应的原来输入值\n比如：\n\n``` scala\nval o: Option[Int] = Some(5)\n\n// o: Option[Int] = Some(5)\n\no match {\n  case Some(x) => println(x)\n  case None =>\n}\n// 输出:  \n5\n\no match {\n  case x @ Some(_) => println(x)\n  case None =>\n}\n// 输出\nSome(5)\n```\n如上案例，有些情况下，模式匹配后你并不想取出他的值，而是取出他本来的自己（Some(5)），这种情况下就用 @；并且@可以用于各个级别\n\n### @可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式\n听起来很绕口，看如下代码：\n\n``` scala\nval d@(c@Some(a), Some(b)) = (Some(1), Some(2))\n```\n结果竟然产生了四个值：\n``` scala\nd: (Some[Int], Some[Int]) = (Some(1),Some(2))\nc: Some[Int] = Some(1)\na: Int = 1\nb: Int = 2\n```\n如上所述，说明定义d和c是两个匹配模式，a和b是两个数字\n\n``` scala\n(Some(1), Some(2)) match {\n  case d@(c@Some(a), Some(b)) => println(a, b, c, d)\n}\n```\n结果如下：\n``` scala\n(1,2,Some(1),(Some(1),Some(2)))\n```\n\n再跑一个例子\n``` scala\nscala> for (t@Some(u) <- Seq(Some(1))) println(t, u)\n(Some(1),1)\n```\n\n再跑一个例子\n\n``` scala\nscala> val List(x, xs @ _*) = List(1, 2, 3)\nx: Int = 1\nxs: Seq[Int] = List(2, 3)\n```\n\n### 引用\nhttp://stackoverflow.com/questions/2359014/scala-operator\n","slug":"scala中-的用法","published":1,"updated":"2016-09-26T06:16:19.924Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfvc000rpsgueg4ycqeu","content":"<p>有些场景，比如模式匹配会遇到scala代码中有@符号，比如<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">case</span> x @ <span class=\"type\">Some</span>(<span class=\"type\">Nil</span>) =&gt; x</div></pre></td></tr></table></figure></p>\n<p>现将网友的答案总结一下，并持续更新：</p>\n<h3 id=\"绑定在模式匹配中，取出对应的原来输入值\"><a href=\"#绑定在模式匹配中，取出对应的原来输入值\" class=\"headerlink\" title=\"绑定在模式匹配中，取出对应的原来输入值\"></a>绑定在模式匹配中，取出对应的原来输入值</h3><p>比如：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> o: <span class=\"type\">Option</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Some</span>(<span class=\"number\">5</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// o: Option[Int] = Some(5)</span></div><div class=\"line\"></div><div class=\"line\">o <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(x) =&gt; println(x)</div><div class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"comment\">// 输出:  </span></div><div class=\"line\"><span class=\"number\">5</span></div><div class=\"line\"></div><div class=\"line\">o <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> x @ <span class=\"type\">Some</span>(_) =&gt; println(x)</div><div class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"comment\">// 输出</span></div><div class=\"line\"><span class=\"type\">Some</span>(<span class=\"number\">5</span>)</div></pre></td></tr></table></figure>\n<p>如上案例，有些情况下，模式匹配后你并不想取出他的值，而是取出他本来的自己（Some(5)），这种情况下就用 @；并且@可以用于各个级别</p>\n<h3 id=\"可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式\"><a href=\"#可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式\" class=\"headerlink\" title=\"@可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式\"></a>@可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式</h3><p>听起来很绕口，看如下代码：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> d@(c<span class=\"meta\">@Some</span>(a), <span class=\"type\">Some</span>(b)) = (<span class=\"type\">Some</span>(<span class=\"number\">1</span>), <span class=\"type\">Some</span>(<span class=\"number\">2</span>))</div></pre></td></tr></table></figure>\n<p>结果竟然产生了四个值：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">d: (<span class=\"type\">Some</span>[<span class=\"type\">Int</span>], <span class=\"type\">Some</span>[<span class=\"type\">Int</span>]) = (<span class=\"type\">Some</span>(<span class=\"number\">1</span>),<span class=\"type\">Some</span>(<span class=\"number\">2</span>))</div><div class=\"line\">c: <span class=\"type\">Some</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Some</span>(<span class=\"number\">1</span>)</div><div class=\"line\">a: <span class=\"type\">Int</span> = <span class=\"number\">1</span></div><div class=\"line\">b: <span class=\"type\">Int</span> = <span class=\"number\">2</span></div></pre></td></tr></table></figure></p>\n<p>如上所述，说明定义d和c是两个匹配模式，a和b是两个数字</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">(<span class=\"type\">Some</span>(<span class=\"number\">1</span>), <span class=\"type\">Some</span>(<span class=\"number\">2</span>)) <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> d@(c<span class=\"meta\">@Some</span>(a), <span class=\"type\">Some</span>(b)) =&gt; println(a, b, c, d)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>结果如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"type\">Some</span>(<span class=\"number\">1</span>),(<span class=\"type\">Some</span>(<span class=\"number\">1</span>),<span class=\"type\">Some</span>(<span class=\"number\">2</span>)))</div></pre></td></tr></table></figure></p>\n<p>再跑一个例子<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">for</span> (t<span class=\"meta\">@Some</span>(u) &lt;- <span class=\"type\">Seq</span>(<span class=\"type\">Some</span>(<span class=\"number\">1</span>))) println(t, u)</div><div class=\"line\">(<span class=\"type\">Some</span>(<span class=\"number\">1</span>),<span class=\"number\">1</span>)</div></pre></td></tr></table></figure></p>\n<p>再跑一个例子</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> <span class=\"type\">List</span>(x, xs @ _*) = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>)</div><div class=\"line\">x: <span class=\"type\">Int</span> = <span class=\"number\">1</span></div><div class=\"line\">xs: <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">3</span>)</div></pre></td></tr></table></figure>\n<h3 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h3><p><a href=\"http://stackoverflow.com/questions/2359014/scala-operator\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/2359014/scala-operator</a></p>\n","excerpt":"","more":"<p>有些场景，比如模式匹配会遇到scala代码中有@符号，比如<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">case</span> x @ <span class=\"type\">Some</span>(<span class=\"type\">Nil</span>) =&gt; x</div></pre></td></tr></table></figure></p>\n<p>现将网友的答案总结一下，并持续更新：</p>\n<h3 id=\"绑定在模式匹配中，取出对应的原来输入值\"><a href=\"#绑定在模式匹配中，取出对应的原来输入值\" class=\"headerlink\" title=\"绑定在模式匹配中，取出对应的原来输入值\"></a>绑定在模式匹配中，取出对应的原来输入值</h3><p>比如：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> o: <span class=\"type\">Option</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Some</span>(<span class=\"number\">5</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// o: Option[Int] = Some(5)</span></div><div class=\"line\"></div><div class=\"line\">o <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(x) =&gt; println(x)</div><div class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"comment\">// 输出:  </span></div><div class=\"line\"><span class=\"number\">5</span></div><div class=\"line\"></div><div class=\"line\">o <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> x @ <span class=\"type\">Some</span>(_) =&gt; println(x)</div><div class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"comment\">// 输出</span></div><div class=\"line\"><span class=\"type\">Some</span>(<span class=\"number\">5</span>)</div></pre></td></tr></table></figure>\n<p>如上案例，有些情况下，模式匹配后你并不想取出他的值，而是取出他本来的自己（Some(5)），这种情况下就用 @；并且@可以用于各个级别</p>\n<h3 id=\"可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式\"><a href=\"#可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式\" class=\"headerlink\" title=\"@可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式\"></a>@可以用来将名称和一个匹配的模式绑定，然后这个值作为匹配模式</h3><p>听起来很绕口，看如下代码：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> d@(c<span class=\"meta\">@Some</span>(a), <span class=\"type\">Some</span>(b)) = (<span class=\"type\">Some</span>(<span class=\"number\">1</span>), <span class=\"type\">Some</span>(<span class=\"number\">2</span>))</div></pre></td></tr></table></figure>\n<p>结果竟然产生了四个值：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">d: (<span class=\"type\">Some</span>[<span class=\"type\">Int</span>], <span class=\"type\">Some</span>[<span class=\"type\">Int</span>]) = (<span class=\"type\">Some</span>(<span class=\"number\">1</span>),<span class=\"type\">Some</span>(<span class=\"number\">2</span>))</div><div class=\"line\">c: <span class=\"type\">Some</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Some</span>(<span class=\"number\">1</span>)</div><div class=\"line\">a: <span class=\"type\">Int</span> = <span class=\"number\">1</span></div><div class=\"line\">b: <span class=\"type\">Int</span> = <span class=\"number\">2</span></div></pre></td></tr></table></figure></p>\n<p>如上所述，说明定义d和c是两个匹配模式，a和b是两个数字</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">(<span class=\"type\">Some</span>(<span class=\"number\">1</span>), <span class=\"type\">Some</span>(<span class=\"number\">2</span>)) <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> d@(c<span class=\"meta\">@Some</span>(a), <span class=\"type\">Some</span>(b)) =&gt; println(a, b, c, d)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>结果如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"type\">Some</span>(<span class=\"number\">1</span>),(<span class=\"type\">Some</span>(<span class=\"number\">1</span>),<span class=\"type\">Some</span>(<span class=\"number\">2</span>)))</div></pre></td></tr></table></figure></p>\n<p>再跑一个例子<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">for</span> (t<span class=\"meta\">@Some</span>(u) &lt;- <span class=\"type\">Seq</span>(<span class=\"type\">Some</span>(<span class=\"number\">1</span>))) println(t, u)</div><div class=\"line\">(<span class=\"type\">Some</span>(<span class=\"number\">1</span>),<span class=\"number\">1</span>)</div></pre></td></tr></table></figure></p>\n<p>再跑一个例子</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> <span class=\"type\">List</span>(x, xs @ _*) = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>)</div><div class=\"line\">x: <span class=\"type\">Int</span> = <span class=\"number\">1</span></div><div class=\"line\">xs: <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">3</span>)</div></pre></td></tr></table></figure>\n<h3 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h3><p><a href=\"http://stackoverflow.com/questions/2359014/scala-operator\">http://stackoverflow.com/questions/2359014/scala-operator</a></p>\n"},{"title":"livy-server初探1——简介与提交脚本以及LivyServer类","date":"2016-09-13T01:51:25.000Z","toc":true,"_content":"\n[Livy server](http://livy.io/)是针对Spark的开源的REST接口，使得我们可以通过REST接口来实现与Spark交互,之前应该是Hue框架的一个功能模块，现在已经独立出来啦。具有如下功能：\n1） 可以与scala、python、R shell客户端交互，执行一些代码片段\n2） 可以提交整个Spark Job,支持scala、python、java编写的Spark job。\n\n## Welcome to Livy\n\n下面是官网文档中我对 Welcome to Livy的翻译：\n\nLivy通过提供REST服务来简化与Spark集群的交互。它可以通过job或者代码片段的方式来提交Spark任务，并同步或者异步地获得任务的结果，以及管理spark context，上述功能通过简单的REST接口或者RPC服务来实现。livy也可以简化Spark与一些应用程序之间的交互，使得Spark可以用于一些web应用(比如Hue)。更多的功能包括：\n\n- 拥有长期运行的Spark Contexts供多用户提交各种的Spark job；\n- 不同的任务和用户可以共享cached RDD或者DataFrames；\n- 多个SC可以按计划同时运行，为了使得SC具有更好的容错性和并发性，可以将SC运行在yarn/Mesos等集群中；\n- 可以通过java/scala客户端的API来提交预编译好的jar包或代码片段\n- 支持一定的安全机制\n- Apache-licensed 100%开源\n\n与ReadMe中的文档结合再补充几条：\n\n- 支持Scala，Python，R Shell的交互；\n- 支持 Scala，Java，Python的批量提交；\n- 不需要你对你自己的代码增加任何改变；\n\n官网和github逛了一整子后不禁感叹，新东西总是缺乏底层的文档的，所以要了解它就要阅读源码了。\n\n## 从./bin/livy-server进入\n``` bash \nusage=\"Usage: livy-server (start|stop)\"\n\n# 指定LIVY_HOME与LIVY_CONF_DIR，上述 `export LIVY_HOME=$(cd $(dirname $0)/.. && pwd)`这种写法值得学习，代表将LIVY_HOME环境变量设为本脚本的父目录，通过这种写法，增强了脚本的可移植性，另外注明一点，dirname这个命令在命令行里是不能用的，只有写在脚本中才能起作用。\nexport LIVY_HOME=$(cd $(dirname $0)/.. && pwd)\nLIVY_CONF_DIR=${LIVY_CONF_DIR:-\"$LIVY_HOME/conf\"}\n\n# 运行所有的livy-env.sh中的环境变量，并使用set -a 表示输出所有的环境变量的改变\nif [ -f \"${LIVY_CONF_DIR}/livy-env.sh\" ]; then\n  # Promote all variable declarations to environment (exported) variables\n  set -a\n  . \"${LIVY_CONF_DIR}/livy-env.sh\"\n  set +a\nfi\n```\n\n接下来可以看到调用了 start_livy_server，以及stop的代码(其实就是ps -p到livy的那个进程，然后kill掉，值得借鉴)：\n\n``` bash\noption=$1\n\ncase $option in\n\n  (start)\n    start_livy_server \"new\"\n    ;;\n\n  (\"\")\n    # make it compatible with previous version of livy-server\n    start_livy_server \"old\"\n    ;;\n\n  (stop)\n    if [ -f $pid ]; then\n      TARGET_ID=\"$(cat \"$pid\")\"\n      if [[ $(ps -p \"$TARGET_ID\" -o comm=) =~ \"java\" ]]; then\n        echo \"stopping livy_server\"\n        kill \"$TARGET_ID\" && rm -f \"$pid\"\n      else\n        echo \"no livy_server to stop\"\n      fi\n    else\n      echo \"no livy_server to stop\"\n    fi\n    ;;\n\n  (*)\n    echo $usage\n    exit 1\n    ;;\n\nesac\n```\n接下来就是start_livy_server函数了，它做了下面几件事情：\n\n1. 找到livy的jar包；\n2. 设置LIVY_CLASSPATH并将SPARK与HADOOP以及YARN的CONF_DIR加入到classpath中；\n3. 如果是`./bin/livy-server`启动的程序，就直接运行 \"$RUNNER $LIVY_SERVER_JAVA_OPTS -cp $LIVY_CLASSPATH:$CLASSPATH com.cloudera.livy.server.LivyServer\"\n4. 如果是`./bin/livy-server start`启动的程序，则增加了日志记录，以方便查看，所以推荐新版本使用带start参数的方式\n\n## com.cloudera.livy.server.LivyServer\n\n从后面进入：原来是创建了一个 LivyServer的server，然后start和join启动\n\n``` scala LivyServer.scala\nobject LivyServer {\n\n  def main(args: Array[String]): Unit = {\n    val server = new LivyServer()\n    try {\n      server.start()\n      server.join()\n    } finally {\n      server.stop()\n    }\n  }\n\n}\n```\n### LiveServer的属性\n\nLivyServer的属性不多，（与spark源码相比）：\n\n``` scala LivyServer.scala\nimport LivyConf._\n\n  private var server: WebServer = _\n  private var _serverUrl: Option[String] = None\n  // make livyConf accessible for testing\n  private[livy] var livyConf: LivyConf = _\n\n  private var kinitFailCount: Int = 0\n  private var executor: ScheduledExecutorService = _\n```\n\n### start()函数\n然后是start()函数了\n首先，从配置文件中读取配置信息（这一块内容自己写得时候可以借用）：\n\n- 从配置信息中得到host和port\n\n``` scala LivyServer.scala\nlivyConf = new LivyConf().loadFromFile(\"livy.conf\")\nval host = livyConf.get(SERVER_HOST)\nval port = livyConf.getInt(SERVER_PORT)\n# 这个而没有看懂\nval multipartConfig = MultipartConfig(\n    maxFileSize = Some(livyConf.getLong(LivyConf.FILE_UPLOAD_MAX_SIZE))\n  ).toMultipartConfigElement\n```\n\n- 测试SparkHome是否设置成功\n\n如下代码，这里使用了require方法对参数进行先决条件检测(值得借鉴)\n``` scala LivyServer.scala\n// Make sure the `spark-submit` program exists, otherwise much of livy won't work.\ntestSparkHome(livyConf)\n...\n\n/**\n* Sets the spark-submit path if it's not configured in the LivyConf\n*/\nprivate[server] def testSparkHome(livyConf: LivyConf): Unit = {\nval sparkHome = livyConf.sparkHome().getOrElse {\n  throw new IllegalArgumentException(\"Livy requires the SPARK_HOME environment variable\")\n}\n\nrequire(new File(sparkHome).isDirectory(), \"SPARK_HOME path does not exist\")\n}\n```\n\n- 测试spark-submit命令可用，否则livy无法工作(值得借鉴):\n\n这里的代码写得太精彩了！先是定义一个`$SPAKR_HOME/bin/spark-sumbit --version`的命令，使用java的ProcessBuilder，然后可以得到exitCode和重定向的标准输出结果，如果结果是\"version ...\"的话，就代表执行成功，输出结果；然后对这个version进行正则匹配，如果是1.6到2.0版本之间，就返回true，否则就说明spark版本不支持；\n\n``` scala LivyServer.scala\ntestSparkSubmit(livyConf)\n...\n\n/**\n* Test that the configured `spark-submit` executable exists.\n*\n* @param livyConf\n*/\nprivate[server] def testSparkSubmit(livyConf: LivyConf): Unit = {\ntry {\n  testSparkVersion(sparkSubmitVersion(livyConf))\n} catch {\n  case e: IOException =>\n    throw new IOException(\"Failed to run spark-submit executable\", e)\n}\n}\n\n...\n/**\n* Return the version of the configured `spark-submit` version.\n*\n* @param livyConf\n* @return the version\n*/\nprivate def sparkSubmitVersion(livyConf: LivyConf): String = {\nval sparkSubmit = livyConf.sparkSubmit()\nval pb = new ProcessBuilder(sparkSubmit, \"--version\")\npb.redirectErrorStream(true)\npb.redirectInput(ProcessBuilder.Redirect.PIPE)\n\nif (LivyConf.TEST_MODE) {\n  pb.environment().put(\"LIVY_TEST_CLASSPATH\", sys.props(\"java.class.path\"))\n}\n\nval process = new LineBufferedProcess(pb.start())\nval exitCode = process.waitFor()\nval output = process.inputIterator.mkString(\"\\n\")\n\nval regex = \"\"\"version (.*)\"\"\".r.unanchored\n\noutput match {\n  case regex(version) => version\n  case _ =>\n    throw new IOException(f\"Unable to determine spark-submit version [$exitCode]:\\n$output\")\n}\n}\n\n/**\n* Throw an exception if Spark version is not supported.\n* @param version Spark version\n*/\nprivate[server] def testSparkVersion(version: String): Unit = {\nval versionPattern = \"\"\"(\\d)+\\.(\\d)+(?:\\.\\d*)?\"\"\".r\n// This is exclusive. Version which equals to this will be rejected.\nval maxVersion = (2, 0)\nval minVersion = (1, 6)\n\nval supportedVersion = version match {\n  case versionPattern(major, minor) =>\n    val v = (major.toInt, minor.toInt)\n    v >= minVersion && v < maxVersion\n  case _ => false\n}\nrequire(supportedVersion, s\"Unsupported Spark version $version.\")\n}\n```\n\n- 通过livy.spark.master是否以yarn开头判断是否需要初始化YarnClient\n\n``` scala LivyServer.scala\n// Initialize YarnClient ASAP to save time.\nif (livyConf.isRunningOnYarn()) {\n  Future { SparkYarnApp.yarnClient }\n}\n\n// SparkYarnApp.scala\n// YarnClient is thread safe. Create once, share it across threads.\nlazy val yarnClient = {\n  val c = YarnClient.createYarnClient() // 这里调用的是yarn提供的API\n  c.init(new YarnConfiguration())\n  c.start()\n  c\n}\n```\n\n- 接下来启动WebServer\n\n这个webServer是Jetty的WebServer，通过设置是否使用ssl的配置来判断启动的是http server还是 https server，也会判断有没有Kerberos，设置IP，端口，日志等。（以后写得时候得查看Jetty的API和文档）\n\n``` scala LivyServer.scala\nserver = new WebServer(livyConf, host, port)\nserver.context.setResourceBase(\"src/main/com/cloudera/livy/server\")\nserver.context.addEventListener(\n  new ServletContextListener() with MetricsBootstrap with ServletApiImplicits {\n\n    private def mount(sc: ServletContext, servlet: Servlet, mappings: String*): Unit = {\n      val registration = sc.addServlet(servlet.getClass().getName(), servlet)\n      registration.addMapping(mappings: _*)\n      registration.setMultipartConfig(multipartConfig)\n    }\n\n    override def contextDestroyed(sce: ServletContextEvent): Unit = {\n\n    }\n\n    override def contextInitialized(sce: ServletContextEvent): Unit = {\n      try {\n        val context = sce.getServletContext()\n        context.initParameters(org.scalatra.EnvironmentKey) = livyConf.get(ENVIRONMENT)\n        mount(context, new InteractiveSessionServlet(livyConf), \"/sessions/*\")\n        mount(context, new BatchSessionServlet(livyConf), \"/batches/*\")\n        context.mountMetricsAdminServlet(\"/\")\n      } catch {\n        case e: Throwable =>\n          error(\"Exception thrown when initializing server\", e)\n          sys.exit(1)\n      }\n    }\n\n  })\n```\n\n","source":"_posts/2016-09-13-livy-server初探1——简介与提交脚本以及LivyServer类.md","raw":"---\ntitle: livy-server初探1——简介与提交脚本以及LivyServer类\ndate: 2016-09-13 09:51:25\ntoc: true\ntags:\n- spark\n- livy\ncategories: spark\n---\n\n[Livy server](http://livy.io/)是针对Spark的开源的REST接口，使得我们可以通过REST接口来实现与Spark交互,之前应该是Hue框架的一个功能模块，现在已经独立出来啦。具有如下功能：\n1） 可以与scala、python、R shell客户端交互，执行一些代码片段\n2） 可以提交整个Spark Job,支持scala、python、java编写的Spark job。\n\n## Welcome to Livy\n\n下面是官网文档中我对 Welcome to Livy的翻译：\n\nLivy通过提供REST服务来简化与Spark集群的交互。它可以通过job或者代码片段的方式来提交Spark任务，并同步或者异步地获得任务的结果，以及管理spark context，上述功能通过简单的REST接口或者RPC服务来实现。livy也可以简化Spark与一些应用程序之间的交互，使得Spark可以用于一些web应用(比如Hue)。更多的功能包括：\n\n- 拥有长期运行的Spark Contexts供多用户提交各种的Spark job；\n- 不同的任务和用户可以共享cached RDD或者DataFrames；\n- 多个SC可以按计划同时运行，为了使得SC具有更好的容错性和并发性，可以将SC运行在yarn/Mesos等集群中；\n- 可以通过java/scala客户端的API来提交预编译好的jar包或代码片段\n- 支持一定的安全机制\n- Apache-licensed 100%开源\n\n与ReadMe中的文档结合再补充几条：\n\n- 支持Scala，Python，R Shell的交互；\n- 支持 Scala，Java，Python的批量提交；\n- 不需要你对你自己的代码增加任何改变；\n\n官网和github逛了一整子后不禁感叹，新东西总是缺乏底层的文档的，所以要了解它就要阅读源码了。\n\n## 从./bin/livy-server进入\n``` bash \nusage=\"Usage: livy-server (start|stop)\"\n\n# 指定LIVY_HOME与LIVY_CONF_DIR，上述 `export LIVY_HOME=$(cd $(dirname $0)/.. && pwd)`这种写法值得学习，代表将LIVY_HOME环境变量设为本脚本的父目录，通过这种写法，增强了脚本的可移植性，另外注明一点，dirname这个命令在命令行里是不能用的，只有写在脚本中才能起作用。\nexport LIVY_HOME=$(cd $(dirname $0)/.. && pwd)\nLIVY_CONF_DIR=${LIVY_CONF_DIR:-\"$LIVY_HOME/conf\"}\n\n# 运行所有的livy-env.sh中的环境变量，并使用set -a 表示输出所有的环境变量的改变\nif [ -f \"${LIVY_CONF_DIR}/livy-env.sh\" ]; then\n  # Promote all variable declarations to environment (exported) variables\n  set -a\n  . \"${LIVY_CONF_DIR}/livy-env.sh\"\n  set +a\nfi\n```\n\n接下来可以看到调用了 start_livy_server，以及stop的代码(其实就是ps -p到livy的那个进程，然后kill掉，值得借鉴)：\n\n``` bash\noption=$1\n\ncase $option in\n\n  (start)\n    start_livy_server \"new\"\n    ;;\n\n  (\"\")\n    # make it compatible with previous version of livy-server\n    start_livy_server \"old\"\n    ;;\n\n  (stop)\n    if [ -f $pid ]; then\n      TARGET_ID=\"$(cat \"$pid\")\"\n      if [[ $(ps -p \"$TARGET_ID\" -o comm=) =~ \"java\" ]]; then\n        echo \"stopping livy_server\"\n        kill \"$TARGET_ID\" && rm -f \"$pid\"\n      else\n        echo \"no livy_server to stop\"\n      fi\n    else\n      echo \"no livy_server to stop\"\n    fi\n    ;;\n\n  (*)\n    echo $usage\n    exit 1\n    ;;\n\nesac\n```\n接下来就是start_livy_server函数了，它做了下面几件事情：\n\n1. 找到livy的jar包；\n2. 设置LIVY_CLASSPATH并将SPARK与HADOOP以及YARN的CONF_DIR加入到classpath中；\n3. 如果是`./bin/livy-server`启动的程序，就直接运行 \"$RUNNER $LIVY_SERVER_JAVA_OPTS -cp $LIVY_CLASSPATH:$CLASSPATH com.cloudera.livy.server.LivyServer\"\n4. 如果是`./bin/livy-server start`启动的程序，则增加了日志记录，以方便查看，所以推荐新版本使用带start参数的方式\n\n## com.cloudera.livy.server.LivyServer\n\n从后面进入：原来是创建了一个 LivyServer的server，然后start和join启动\n\n``` scala LivyServer.scala\nobject LivyServer {\n\n  def main(args: Array[String]): Unit = {\n    val server = new LivyServer()\n    try {\n      server.start()\n      server.join()\n    } finally {\n      server.stop()\n    }\n  }\n\n}\n```\n### LiveServer的属性\n\nLivyServer的属性不多，（与spark源码相比）：\n\n``` scala LivyServer.scala\nimport LivyConf._\n\n  private var server: WebServer = _\n  private var _serverUrl: Option[String] = None\n  // make livyConf accessible for testing\n  private[livy] var livyConf: LivyConf = _\n\n  private var kinitFailCount: Int = 0\n  private var executor: ScheduledExecutorService = _\n```\n\n### start()函数\n然后是start()函数了\n首先，从配置文件中读取配置信息（这一块内容自己写得时候可以借用）：\n\n- 从配置信息中得到host和port\n\n``` scala LivyServer.scala\nlivyConf = new LivyConf().loadFromFile(\"livy.conf\")\nval host = livyConf.get(SERVER_HOST)\nval port = livyConf.getInt(SERVER_PORT)\n# 这个而没有看懂\nval multipartConfig = MultipartConfig(\n    maxFileSize = Some(livyConf.getLong(LivyConf.FILE_UPLOAD_MAX_SIZE))\n  ).toMultipartConfigElement\n```\n\n- 测试SparkHome是否设置成功\n\n如下代码，这里使用了require方法对参数进行先决条件检测(值得借鉴)\n``` scala LivyServer.scala\n// Make sure the `spark-submit` program exists, otherwise much of livy won't work.\ntestSparkHome(livyConf)\n...\n\n/**\n* Sets the spark-submit path if it's not configured in the LivyConf\n*/\nprivate[server] def testSparkHome(livyConf: LivyConf): Unit = {\nval sparkHome = livyConf.sparkHome().getOrElse {\n  throw new IllegalArgumentException(\"Livy requires the SPARK_HOME environment variable\")\n}\n\nrequire(new File(sparkHome).isDirectory(), \"SPARK_HOME path does not exist\")\n}\n```\n\n- 测试spark-submit命令可用，否则livy无法工作(值得借鉴):\n\n这里的代码写得太精彩了！先是定义一个`$SPAKR_HOME/bin/spark-sumbit --version`的命令，使用java的ProcessBuilder，然后可以得到exitCode和重定向的标准输出结果，如果结果是\"version ...\"的话，就代表执行成功，输出结果；然后对这个version进行正则匹配，如果是1.6到2.0版本之间，就返回true，否则就说明spark版本不支持；\n\n``` scala LivyServer.scala\ntestSparkSubmit(livyConf)\n...\n\n/**\n* Test that the configured `spark-submit` executable exists.\n*\n* @param livyConf\n*/\nprivate[server] def testSparkSubmit(livyConf: LivyConf): Unit = {\ntry {\n  testSparkVersion(sparkSubmitVersion(livyConf))\n} catch {\n  case e: IOException =>\n    throw new IOException(\"Failed to run spark-submit executable\", e)\n}\n}\n\n...\n/**\n* Return the version of the configured `spark-submit` version.\n*\n* @param livyConf\n* @return the version\n*/\nprivate def sparkSubmitVersion(livyConf: LivyConf): String = {\nval sparkSubmit = livyConf.sparkSubmit()\nval pb = new ProcessBuilder(sparkSubmit, \"--version\")\npb.redirectErrorStream(true)\npb.redirectInput(ProcessBuilder.Redirect.PIPE)\n\nif (LivyConf.TEST_MODE) {\n  pb.environment().put(\"LIVY_TEST_CLASSPATH\", sys.props(\"java.class.path\"))\n}\n\nval process = new LineBufferedProcess(pb.start())\nval exitCode = process.waitFor()\nval output = process.inputIterator.mkString(\"\\n\")\n\nval regex = \"\"\"version (.*)\"\"\".r.unanchored\n\noutput match {\n  case regex(version) => version\n  case _ =>\n    throw new IOException(f\"Unable to determine spark-submit version [$exitCode]:\\n$output\")\n}\n}\n\n/**\n* Throw an exception if Spark version is not supported.\n* @param version Spark version\n*/\nprivate[server] def testSparkVersion(version: String): Unit = {\nval versionPattern = \"\"\"(\\d)+\\.(\\d)+(?:\\.\\d*)?\"\"\".r\n// This is exclusive. Version which equals to this will be rejected.\nval maxVersion = (2, 0)\nval minVersion = (1, 6)\n\nval supportedVersion = version match {\n  case versionPattern(major, minor) =>\n    val v = (major.toInt, minor.toInt)\n    v >= minVersion && v < maxVersion\n  case _ => false\n}\nrequire(supportedVersion, s\"Unsupported Spark version $version.\")\n}\n```\n\n- 通过livy.spark.master是否以yarn开头判断是否需要初始化YarnClient\n\n``` scala LivyServer.scala\n// Initialize YarnClient ASAP to save time.\nif (livyConf.isRunningOnYarn()) {\n  Future { SparkYarnApp.yarnClient }\n}\n\n// SparkYarnApp.scala\n// YarnClient is thread safe. Create once, share it across threads.\nlazy val yarnClient = {\n  val c = YarnClient.createYarnClient() // 这里调用的是yarn提供的API\n  c.init(new YarnConfiguration())\n  c.start()\n  c\n}\n```\n\n- 接下来启动WebServer\n\n这个webServer是Jetty的WebServer，通过设置是否使用ssl的配置来判断启动的是http server还是 https server，也会判断有没有Kerberos，设置IP，端口，日志等。（以后写得时候得查看Jetty的API和文档）\n\n``` scala LivyServer.scala\nserver = new WebServer(livyConf, host, port)\nserver.context.setResourceBase(\"src/main/com/cloudera/livy/server\")\nserver.context.addEventListener(\n  new ServletContextListener() with MetricsBootstrap with ServletApiImplicits {\n\n    private def mount(sc: ServletContext, servlet: Servlet, mappings: String*): Unit = {\n      val registration = sc.addServlet(servlet.getClass().getName(), servlet)\n      registration.addMapping(mappings: _*)\n      registration.setMultipartConfig(multipartConfig)\n    }\n\n    override def contextDestroyed(sce: ServletContextEvent): Unit = {\n\n    }\n\n    override def contextInitialized(sce: ServletContextEvent): Unit = {\n      try {\n        val context = sce.getServletContext()\n        context.initParameters(org.scalatra.EnvironmentKey) = livyConf.get(ENVIRONMENT)\n        mount(context, new InteractiveSessionServlet(livyConf), \"/sessions/*\")\n        mount(context, new BatchSessionServlet(livyConf), \"/batches/*\")\n        context.mountMetricsAdminServlet(\"/\")\n      } catch {\n        case e: Throwable =>\n          error(\"Exception thrown when initializing server\", e)\n          sys.exit(1)\n      }\n    }\n\n  })\n```\n\n","slug":"livy-server初探1——简介与提交脚本以及LivyServer类","published":1,"updated":"2016-09-26T06:44:42.956Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfvf000spsgukjuactol","content":"<p><a href=\"http://livy.io/\" target=\"_blank\" rel=\"external\">Livy server</a>是针对Spark的开源的REST接口，使得我们可以通过REST接口来实现与Spark交互,之前应该是Hue框架的一个功能模块，现在已经独立出来啦。具有如下功能：<br>1） 可以与scala、python、R shell客户端交互，执行一些代码片段<br>2） 可以提交整个Spark Job,支持scala、python、java编写的Spark job。</p>\n<h2 id=\"Welcome-to-Livy\"><a href=\"#Welcome-to-Livy\" class=\"headerlink\" title=\"Welcome to Livy\"></a>Welcome to Livy</h2><p>下面是官网文档中我对 Welcome to Livy的翻译：</p>\n<p>Livy通过提供REST服务来简化与Spark集群的交互。它可以通过job或者代码片段的方式来提交Spark任务，并同步或者异步地获得任务的结果，以及管理spark context，上述功能通过简单的REST接口或者RPC服务来实现。livy也可以简化Spark与一些应用程序之间的交互，使得Spark可以用于一些web应用(比如Hue)。更多的功能包括：</p>\n<ul>\n<li>拥有长期运行的Spark Contexts供多用户提交各种的Spark job；</li>\n<li>不同的任务和用户可以共享cached RDD或者DataFrames；</li>\n<li>多个SC可以按计划同时运行，为了使得SC具有更好的容错性和并发性，可以将SC运行在yarn/Mesos等集群中；</li>\n<li>可以通过java/scala客户端的API来提交预编译好的jar包或代码片段</li>\n<li>支持一定的安全机制</li>\n<li>Apache-licensed 100%开源</li>\n</ul>\n<p>与ReadMe中的文档结合再补充几条：</p>\n<ul>\n<li>支持Scala，Python，R Shell的交互；</li>\n<li>支持 Scala，Java，Python的批量提交；</li>\n<li>不需要你对你自己的代码增加任何改变；</li>\n</ul>\n<p>官网和github逛了一整子后不禁感叹，新东西总是缺乏底层的文档的，所以要了解它就要阅读源码了。</p>\n<h2 id=\"从-bin-livy-server进入\"><a href=\"#从-bin-livy-server进入\" class=\"headerlink\" title=\"从./bin/livy-server进入\"></a>从./bin/livy-server进入</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">usage=<span class=\"string\">\"Usage: livy-server (start|stop)\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 指定LIVY_HOME与LIVY_CONF_DIR，上述 `export LIVY_HOME=$(cd $(dirname $0)/.. &amp;&amp; pwd)`这种写法值得学习，代表将LIVY_HOME环境变量设为本脚本的父目录，通过这种写法，增强了脚本的可移植性，另外注明一点，dirname这个命令在命令行里是不能用的，只有写在脚本中才能起作用。</span></div><div class=\"line\"><span class=\"built_in\">export</span> LIVY_HOME=$(<span class=\"built_in\">cd</span> $(dirname <span class=\"variable\">$0</span>)/.. &amp;&amp; <span class=\"built_in\">pwd</span>)</div><div class=\"line\">LIVY_CONF_DIR=<span class=\"variable\">$&#123;LIVY_CONF_DIR:-\"$LIVY_HOME/conf\"&#125;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 运行所有的livy-env.sh中的环境变量，并使用set -a 表示输出所有的环境变量的改变</span></div><div class=\"line\"><span class=\"keyword\">if</span> [ <span class=\"_\">-f</span> <span class=\"string\">\"<span class=\"variable\">$&#123;LIVY_CONF_DIR&#125;</span>/livy-env.sh\"</span> ]; <span class=\"keyword\">then</span></div><div class=\"line\">  <span class=\"comment\"># Promote all variable declarations to environment (exported) variables</span></div><div class=\"line\">  <span class=\"built_in\">set</span> <span class=\"_\">-a</span></div><div class=\"line\">  . <span class=\"string\">\"<span class=\"variable\">$&#123;LIVY_CONF_DIR&#125;</span>/livy-env.sh\"</span></div><div class=\"line\">  <span class=\"built_in\">set</span> +a</div><div class=\"line\"><span class=\"keyword\">fi</span></div></pre></td></tr></table></figure>\n<p>接下来可以看到调用了 start_livy_server，以及stop的代码(其实就是ps -p到livy的那个进程，然后kill掉，值得借鉴)：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">option=<span class=\"variable\">$1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"variable\">$option</span> <span class=\"keyword\">in</span></div><div class=\"line\"></div><div class=\"line\">  (start)</div><div class=\"line\">    start_livy_server <span class=\"string\">\"new\"</span></div><div class=\"line\">    ;;</div><div class=\"line\"></div><div class=\"line\">  (<span class=\"string\">\"\"</span>)</div><div class=\"line\">    <span class=\"comment\"># make it compatible with previous version of livy-server</span></div><div class=\"line\">    start_livy_server <span class=\"string\">\"old\"</span></div><div class=\"line\">    ;;</div><div class=\"line\"></div><div class=\"line\">  (stop)</div><div class=\"line\">    <span class=\"keyword\">if</span> [ <span class=\"_\">-f</span> <span class=\"variable\">$pid</span> ]; <span class=\"keyword\">then</span></div><div class=\"line\">      TARGET_ID=<span class=\"string\">\"<span class=\"variable\">$(cat \"$pid\")</span>\"</span></div><div class=\"line\">      <span class=\"keyword\">if</span> [[ $(ps -p <span class=\"string\">\"<span class=\"variable\">$TARGET_ID</span>\"</span> -o comm=) =~ <span class=\"string\">\"java\"</span> ]]; <span class=\"keyword\">then</span></div><div class=\"line\">        <span class=\"built_in\">echo</span> <span class=\"string\">\"stopping livy_server\"</span></div><div class=\"line\">        <span class=\"built_in\">kill</span> <span class=\"string\">\"<span class=\"variable\">$TARGET_ID</span>\"</span> &amp;&amp; rm <span class=\"_\">-f</span> <span class=\"string\">\"<span class=\"variable\">$pid</span>\"</span></div><div class=\"line\">      <span class=\"keyword\">else</span></div><div class=\"line\">        <span class=\"built_in\">echo</span> <span class=\"string\">\"no livy_server to stop\"</span></div><div class=\"line\">      <span class=\"keyword\">fi</span></div><div class=\"line\">    <span class=\"keyword\">else</span></div><div class=\"line\">      <span class=\"built_in\">echo</span> <span class=\"string\">\"no livy_server to stop\"</span></div><div class=\"line\">    <span class=\"keyword\">fi</span></div><div class=\"line\">    ;;</div><div class=\"line\"></div><div class=\"line\">  (*)</div><div class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"variable\">$usage</span></div><div class=\"line\">    <span class=\"built_in\">exit</span> 1</div><div class=\"line\">    ;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">esac</span></div></pre></td></tr></table></figure>\n<p>接下来就是start_livy_server函数了，它做了下面几件事情：</p>\n<ol>\n<li>找到livy的jar包；</li>\n<li>设置LIVY_CLASSPATH并将SPARK与HADOOP以及YARN的CONF_DIR加入到classpath中；</li>\n<li>如果是<code>./bin/livy-server</code>启动的程序，就直接运行 “$RUNNER $LIVY_SERVER_JAVA_OPTS -cp $LIVY_CLASSPATH:$CLASSPATH com.cloudera.livy.server.LivyServer”</li>\n<li>如果是<code>./bin/livy-server start</code>启动的程序，则增加了日志记录，以方便查看，所以推荐新版本使用带start参数的方式</li>\n</ol>\n<h2 id=\"com-cloudera-livy-server-LivyServer\"><a href=\"#com-cloudera-livy-server-LivyServer\" class=\"headerlink\" title=\"com.cloudera.livy.server.LivyServer\"></a>com.cloudera.livy.server.LivyServer</h2><p>从后面进入：原来是创建了一个 LivyServer的server，然后start和join启动</p>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">LivyServer</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> server = <span class=\"keyword\">new</span> <span class=\"type\">LivyServer</span>()</div><div class=\"line\">    <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">      server.start()</div><div class=\"line\">      server.join()</div><div class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">      server.stop()</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"LiveServer的属性\"><a href=\"#LiveServer的属性\" class=\"headerlink\" title=\"LiveServer的属性\"></a>LiveServer的属性</h3><p>LivyServer的属性不多，（与spark源码相比）：</p>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">LivyConf</span>._</div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> server: <span class=\"type\">WebServer</span> = _</div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> _serverUrl: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">None</span></div><div class=\"line\">  <span class=\"comment\">// make livyConf accessible for testing</span></div><div class=\"line\">  <span class=\"keyword\">private</span>[livy] <span class=\"keyword\">var</span> livyConf: <span class=\"type\">LivyConf</span> = _</div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> kinitFailCount: <span class=\"type\">Int</span> = <span class=\"number\">0</span></div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> executor: <span class=\"type\">ScheduledExecutorService</span> = _</div></pre></td></tr></table></figure>\n<h3 id=\"start-函数\"><a href=\"#start-函数\" class=\"headerlink\" title=\"start()函数\"></a>start()函数</h3><p>然后是start()函数了<br>首先，从配置文件中读取配置信息（这一块内容自己写得时候可以借用）：</p>\n<ul>\n<li>从配置信息中得到host和port</li>\n</ul>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">livyConf = <span class=\"keyword\">new</span> <span class=\"type\">LivyConf</span>().loadFromFile(<span class=\"string\">\"livy.conf\"</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> host = livyConf.get(<span class=\"type\">SERVER_HOST</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> port = livyConf.getInt(<span class=\"type\">SERVER_PORT</span>)</div><div class=\"line\"># 这个而没有看懂</div><div class=\"line\"><span class=\"keyword\">val</span> multipartConfig = <span class=\"type\">MultipartConfig</span>(</div><div class=\"line\">    maxFileSize = <span class=\"type\">Some</span>(livyConf.getLong(<span class=\"type\">LivyConf</span>.<span class=\"type\">FILE_UPLOAD_MAX_SIZE</span>))</div><div class=\"line\">  ).toMultipartConfigElement</div></pre></td></tr></table></figure>\n<ul>\n<li>测试SparkHome是否设置成功</li>\n</ul>\n<p>如下代码，这里使用了require方法对参数进行先决条件检测(值得借鉴)<br><figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Make sure the `spark-submit` program exists, otherwise much of livy won't work.</span></div><div class=\"line\">testSparkHome(livyConf)</div><div class=\"line\">...</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">* Sets the spark-submit path if it's not configured in the LivyConf</div><div class=\"line\">*/</div><div class=\"line\"><span class=\"keyword\">private</span>[server] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">testSparkHome</span></span>(livyConf: <span class=\"type\">LivyConf</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\"><span class=\"keyword\">val</span> sparkHome = livyConf.sparkHome().getOrElse &#123;</div><div class=\"line\">  <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalArgumentException</span>(<span class=\"string\">\"Livy requires the SPARK_HOME environment variable\"</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">require(<span class=\"keyword\">new</span> <span class=\"type\">File</span>(sparkHome).isDirectory(), <span class=\"string\">\"SPARK_HOME path does not exist\"</span>)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<ul>\n<li>测试spark-submit命令可用，否则livy无法工作(值得借鉴):</li>\n</ul>\n<p>这里的代码写得太精彩了！先是定义一个<code>$SPAKR_HOME/bin/spark-sumbit --version</code>的命令，使用java的ProcessBuilder，然后可以得到exitCode和重定向的标准输出结果，如果结果是”version …”的话，就代表执行成功，输出结果；然后对这个version进行正则匹配，如果是1.6到2.0版本之间，就返回true，否则就说明spark版本不支持；</p>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div></pre></td><td class=\"code\"><pre><div class=\"line\">testSparkSubmit(livyConf)</div><div class=\"line\">...</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">* Test that the configured `spark-submit` executable exists.</div><div class=\"line\">*</div><div class=\"line\">* @param livyConf</div><div class=\"line\">*/</div><div class=\"line\"><span class=\"keyword\">private</span>[server] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">testSparkSubmit</span></span>(livyConf: <span class=\"type\">LivyConf</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  testSparkVersion(sparkSubmitVersion(livyConf))</div><div class=\"line\">&#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> e: <span class=\"type\">IOException</span> =&gt;</div><div class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IOException</span>(<span class=\"string\">\"Failed to run spark-submit executable\"</span>, e)</div><div class=\"line\">&#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">...</div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">* Return the version of the configured `spark-submit` version.</div><div class=\"line\">*</div><div class=\"line\">* @param livyConf</div><div class=\"line\">* @return the version</div><div class=\"line\">*/</div><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sparkSubmitVersion</span></span>(livyConf: <span class=\"type\">LivyConf</span>): <span class=\"type\">String</span> = &#123;</div><div class=\"line\"><span class=\"keyword\">val</span> sparkSubmit = livyConf.sparkSubmit()</div><div class=\"line\"><span class=\"keyword\">val</span> pb = <span class=\"keyword\">new</span> <span class=\"type\">ProcessBuilder</span>(sparkSubmit, <span class=\"string\">\"--version\"</span>)</div><div class=\"line\">pb.redirectErrorStream(<span class=\"literal\">true</span>)</div><div class=\"line\">pb.redirectInput(<span class=\"type\">ProcessBuilder</span>.<span class=\"type\">Redirect</span>.<span class=\"type\">PIPE</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"type\">LivyConf</span>.<span class=\"type\">TEST_MODE</span>) &#123;</div><div class=\"line\">  pb.environment().put(<span class=\"string\">\"LIVY_TEST_CLASSPATH\"</span>, sys.props(<span class=\"string\">\"java.class.path\"</span>))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> process = <span class=\"keyword\">new</span> <span class=\"type\">LineBufferedProcess</span>(pb.start())</div><div class=\"line\"><span class=\"keyword\">val</span> exitCode = process.waitFor()</div><div class=\"line\"><span class=\"keyword\">val</span> output = process.inputIterator.mkString(<span class=\"string\">\"\\n\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> regex = <span class=\"string\">\"\"</span><span class=\"string\">\"version (.*)\"</span><span class=\"string\">\"\"</span>.r.unanchored</div><div class=\"line\"></div><div class=\"line\">output <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> regex(version) =&gt; version</div><div class=\"line\">  <span class=\"keyword\">case</span> _ =&gt;</div><div class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IOException</span>(<span class=\"string\">f\"Unable to determine spark-submit version [<span class=\"subst\">$exitCode</span>]:\\n<span class=\"subst\">$output</span>\"</span>)</div><div class=\"line\">&#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">* Throw an exception if Spark version is not supported.</div><div class=\"line\">* @param version Spark version</div><div class=\"line\">*/</div><div class=\"line\"><span class=\"keyword\">private</span>[server] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">testSparkVersion</span></span>(version: <span class=\"type\">String</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\"><span class=\"keyword\">val</span> versionPattern = <span class=\"string\">\"\"</span><span class=\"string\">\"(\\d)+\\.(\\d)+(?:\\.\\d*)?\"</span><span class=\"string\">\"\"</span>.r</div><div class=\"line\"><span class=\"comment\">// This is exclusive. Version which equals to this will be rejected.</span></div><div class=\"line\"><span class=\"keyword\">val</span> maxVersion = (<span class=\"number\">2</span>, <span class=\"number\">0</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> minVersion = (<span class=\"number\">1</span>, <span class=\"number\">6</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> supportedVersion = version <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> versionPattern(major, minor) =&gt;</div><div class=\"line\">    <span class=\"keyword\">val</span> v = (major.toInt, minor.toInt)</div><div class=\"line\">    v &gt;= minVersion &amp;&amp; v &lt; maxVersion</div><div class=\"line\">  <span class=\"keyword\">case</span> _ =&gt; <span class=\"literal\">false</span></div><div class=\"line\">&#125;</div><div class=\"line\">require(supportedVersion, <span class=\"string\">s\"Unsupported Spark version <span class=\"subst\">$version</span>.\"</span>)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>通过livy.spark.master是否以yarn开头判断是否需要初始化YarnClient</li>\n</ul>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Initialize YarnClient ASAP to save time.</span></div><div class=\"line\"><span class=\"keyword\">if</span> (livyConf.isRunningOnYarn()) &#123;</div><div class=\"line\">  <span class=\"type\">Future</span> &#123; <span class=\"type\">SparkYarnApp</span>.yarnClient &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// SparkYarnApp.scala</span></div><div class=\"line\"><span class=\"comment\">// YarnClient is thread safe. Create once, share it across threads.</span></div><div class=\"line\"><span class=\"keyword\">lazy</span> <span class=\"keyword\">val</span> yarnClient = &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> c = <span class=\"type\">YarnClient</span>.createYarnClient() <span class=\"comment\">// 这里调用的是yarn提供的API</span></div><div class=\"line\">  c.init(<span class=\"keyword\">new</span> <span class=\"type\">YarnConfiguration</span>())</div><div class=\"line\">  c.start()</div><div class=\"line\">  c</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>接下来启动WebServer</li>\n</ul>\n<p>这个webServer是Jetty的WebServer，通过设置是否使用ssl的配置来判断启动的是http server还是 https server，也会判断有没有Kerberos，设置IP，端口，日志等。（以后写得时候得查看Jetty的API和文档）</p>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">server = <span class=\"keyword\">new</span> <span class=\"type\">WebServer</span>(livyConf, host, port)</div><div class=\"line\">server.context.setResourceBase(<span class=\"string\">\"src/main/com/cloudera/livy/server\"</span>)</div><div class=\"line\">server.context.addEventListener(</div><div class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">ServletContextListener</span>() <span class=\"keyword\">with</span> <span class=\"type\">MetricsBootstrap</span> <span class=\"keyword\">with</span> <span class=\"type\">ServletApiImplicits</span> &#123;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mount</span></span>(sc: <span class=\"type\">ServletContext</span>, servlet: <span class=\"type\">Servlet</span>, mappings: <span class=\"type\">String</span>*): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> registration = sc.addServlet(servlet.getClass().getName(), servlet)</div><div class=\"line\">      registration.addMapping(mappings: _*)</div><div class=\"line\">      registration.setMultipartConfig(multipartConfig)</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">contextDestroyed</span></span>(sce: <span class=\"type\">ServletContextEvent</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\"></div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">contextInitialized</span></span>(sce: <span class=\"type\">ServletContextEvent</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">      <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">val</span> context = sce.getServletContext()</div><div class=\"line\">        context.initParameters(org.scalatra.<span class=\"type\">EnvironmentKey</span>) = livyConf.get(<span class=\"type\">ENVIRONMENT</span>)</div><div class=\"line\">        mount(context, <span class=\"keyword\">new</span> <span class=\"type\">InteractiveSessionServlet</span>(livyConf), <span class=\"string\">\"/sessions/*\"</span>)</div><div class=\"line\">        mount(context, <span class=\"keyword\">new</span> <span class=\"type\">BatchSessionServlet</span>(livyConf), <span class=\"string\">\"/batches/*\"</span>)</div><div class=\"line\">        context.mountMetricsAdminServlet(<span class=\"string\">\"/\"</span>)</div><div class=\"line\">      &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">case</span> e: <span class=\"type\">Throwable</span> =&gt;</div><div class=\"line\">          error(<span class=\"string\">\"Exception thrown when initializing server\"</span>, e)</div><div class=\"line\">          sys.exit(<span class=\"number\">1</span>)</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">  &#125;)</div></pre></td></tr></table></figure>\n","excerpt":"","more":"<p><a href=\"http://livy.io/\">Livy server</a>是针对Spark的开源的REST接口，使得我们可以通过REST接口来实现与Spark交互,之前应该是Hue框架的一个功能模块，现在已经独立出来啦。具有如下功能：<br>1） 可以与scala、python、R shell客户端交互，执行一些代码片段<br>2） 可以提交整个Spark Job,支持scala、python、java编写的Spark job。</p>\n<h2 id=\"Welcome-to-Livy\"><a href=\"#Welcome-to-Livy\" class=\"headerlink\" title=\"Welcome to Livy\"></a>Welcome to Livy</h2><p>下面是官网文档中我对 Welcome to Livy的翻译：</p>\n<p>Livy通过提供REST服务来简化与Spark集群的交互。它可以通过job或者代码片段的方式来提交Spark任务，并同步或者异步地获得任务的结果，以及管理spark context，上述功能通过简单的REST接口或者RPC服务来实现。livy也可以简化Spark与一些应用程序之间的交互，使得Spark可以用于一些web应用(比如Hue)。更多的功能包括：</p>\n<ul>\n<li>拥有长期运行的Spark Contexts供多用户提交各种的Spark job；</li>\n<li>不同的任务和用户可以共享cached RDD或者DataFrames；</li>\n<li>多个SC可以按计划同时运行，为了使得SC具有更好的容错性和并发性，可以将SC运行在yarn/Mesos等集群中；</li>\n<li>可以通过java/scala客户端的API来提交预编译好的jar包或代码片段</li>\n<li>支持一定的安全机制</li>\n<li>Apache-licensed 100%开源</li>\n</ul>\n<p>与ReadMe中的文档结合再补充几条：</p>\n<ul>\n<li>支持Scala，Python，R Shell的交互；</li>\n<li>支持 Scala，Java，Python的批量提交；</li>\n<li>不需要你对你自己的代码增加任何改变；</li>\n</ul>\n<p>官网和github逛了一整子后不禁感叹，新东西总是缺乏底层的文档的，所以要了解它就要阅读源码了。</p>\n<h2 id=\"从-bin-livy-server进入\"><a href=\"#从-bin-livy-server进入\" class=\"headerlink\" title=\"从./bin/livy-server进入\"></a>从./bin/livy-server进入</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">usage=<span class=\"string\">\"Usage: livy-server (start|stop)\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 指定LIVY_HOME与LIVY_CONF_DIR，上述 `export LIVY_HOME=$(cd $(dirname $0)/.. &amp;&amp; pwd)`这种写法值得学习，代表将LIVY_HOME环境变量设为本脚本的父目录，通过这种写法，增强了脚本的可移植性，另外注明一点，dirname这个命令在命令行里是不能用的，只有写在脚本中才能起作用。</span></div><div class=\"line\"><span class=\"built_in\">export</span> LIVY_HOME=$(<span class=\"built_in\">cd</span> $(dirname <span class=\"variable\">$0</span>)/.. &amp;&amp; <span class=\"built_in\">pwd</span>)</div><div class=\"line\">LIVY_CONF_DIR=<span class=\"variable\">$&#123;LIVY_CONF_DIR:-\"$LIVY_HOME/conf\"&#125;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 运行所有的livy-env.sh中的环境变量，并使用set -a 表示输出所有的环境变量的改变</span></div><div class=\"line\"><span class=\"keyword\">if</span> [ <span class=\"_\">-f</span> <span class=\"string\">\"<span class=\"variable\">$&#123;LIVY_CONF_DIR&#125;</span>/livy-env.sh\"</span> ]; <span class=\"keyword\">then</span></div><div class=\"line\">  <span class=\"comment\"># Promote all variable declarations to environment (exported) variables</span></div><div class=\"line\">  <span class=\"built_in\">set</span> <span class=\"_\">-a</span></div><div class=\"line\">  . <span class=\"string\">\"<span class=\"variable\">$&#123;LIVY_CONF_DIR&#125;</span>/livy-env.sh\"</span></div><div class=\"line\">  <span class=\"built_in\">set</span> +a</div><div class=\"line\"><span class=\"keyword\">fi</span></div></pre></td></tr></table></figure>\n<p>接下来可以看到调用了 start_livy_server，以及stop的代码(其实就是ps -p到livy的那个进程，然后kill掉，值得借鉴)：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">option=<span class=\"variable\">$1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"variable\">$option</span> <span class=\"keyword\">in</span></div><div class=\"line\"></div><div class=\"line\">  (start)</div><div class=\"line\">    start_livy_server <span class=\"string\">\"new\"</span></div><div class=\"line\">    ;;</div><div class=\"line\"></div><div class=\"line\">  (<span class=\"string\">\"\"</span>)</div><div class=\"line\">    <span class=\"comment\"># make it compatible with previous version of livy-server</span></div><div class=\"line\">    start_livy_server <span class=\"string\">\"old\"</span></div><div class=\"line\">    ;;</div><div class=\"line\"></div><div class=\"line\">  (stop)</div><div class=\"line\">    <span class=\"keyword\">if</span> [ <span class=\"_\">-f</span> <span class=\"variable\">$pid</span> ]; <span class=\"keyword\">then</span></div><div class=\"line\">      TARGET_ID=<span class=\"string\">\"<span class=\"variable\">$(cat \"$pid\")</span>\"</span></div><div class=\"line\">      <span class=\"keyword\">if</span> [[ $(ps -p <span class=\"string\">\"<span class=\"variable\">$TARGET_ID</span>\"</span> -o comm=) =~ <span class=\"string\">\"java\"</span> ]]; <span class=\"keyword\">then</span></div><div class=\"line\">        <span class=\"built_in\">echo</span> <span class=\"string\">\"stopping livy_server\"</span></div><div class=\"line\">        <span class=\"built_in\">kill</span> <span class=\"string\">\"<span class=\"variable\">$TARGET_ID</span>\"</span> &amp;&amp; rm <span class=\"_\">-f</span> <span class=\"string\">\"<span class=\"variable\">$pid</span>\"</span></div><div class=\"line\">      <span class=\"keyword\">else</span></div><div class=\"line\">        <span class=\"built_in\">echo</span> <span class=\"string\">\"no livy_server to stop\"</span></div><div class=\"line\">      <span class=\"keyword\">fi</span></div><div class=\"line\">    <span class=\"keyword\">else</span></div><div class=\"line\">      <span class=\"built_in\">echo</span> <span class=\"string\">\"no livy_server to stop\"</span></div><div class=\"line\">    <span class=\"keyword\">fi</span></div><div class=\"line\">    ;;</div><div class=\"line\"></div><div class=\"line\">  (*)</div><div class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"variable\">$usage</span></div><div class=\"line\">    <span class=\"built_in\">exit</span> 1</div><div class=\"line\">    ;;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">esac</span></div></pre></td></tr></table></figure>\n<p>接下来就是start_livy_server函数了，它做了下面几件事情：</p>\n<ol>\n<li>找到livy的jar包；</li>\n<li>设置LIVY_CLASSPATH并将SPARK与HADOOP以及YARN的CONF_DIR加入到classpath中；</li>\n<li>如果是<code>./bin/livy-server</code>启动的程序，就直接运行 “$RUNNER $LIVY_SERVER_JAVA_OPTS -cp $LIVY_CLASSPATH:$CLASSPATH com.cloudera.livy.server.LivyServer”</li>\n<li>如果是<code>./bin/livy-server start</code>启动的程序，则增加了日志记录，以方便查看，所以推荐新版本使用带start参数的方式</li>\n</ol>\n<h2 id=\"com-cloudera-livy-server-LivyServer\"><a href=\"#com-cloudera-livy-server-LivyServer\" class=\"headerlink\" title=\"com.cloudera.livy.server.LivyServer\"></a>com.cloudera.livy.server.LivyServer</h2><p>从后面进入：原来是创建了一个 LivyServer的server，然后start和join启动</p>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">LivyServer</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> server = <span class=\"keyword\">new</span> <span class=\"type\">LivyServer</span>()</div><div class=\"line\">    <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">      server.start()</div><div class=\"line\">      server.join()</div><div class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">      server.stop()</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"LiveServer的属性\"><a href=\"#LiveServer的属性\" class=\"headerlink\" title=\"LiveServer的属性\"></a>LiveServer的属性</h3><p>LivyServer的属性不多，（与spark源码相比）：</p>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">LivyConf</span>._</div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> server: <span class=\"type\">WebServer</span> = _</div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> _serverUrl: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">None</span></div><div class=\"line\">  <span class=\"comment\">// make livyConf accessible for testing</span></div><div class=\"line\">  <span class=\"keyword\">private</span>[livy] <span class=\"keyword\">var</span> livyConf: <span class=\"type\">LivyConf</span> = _</div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> kinitFailCount: <span class=\"type\">Int</span> = <span class=\"number\">0</span></div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> executor: <span class=\"type\">ScheduledExecutorService</span> = _</div></pre></td></tr></table></figure>\n<h3 id=\"start-函数\"><a href=\"#start-函数\" class=\"headerlink\" title=\"start()函数\"></a>start()函数</h3><p>然后是start()函数了<br>首先，从配置文件中读取配置信息（这一块内容自己写得时候可以借用）：</p>\n<ul>\n<li>从配置信息中得到host和port</li>\n</ul>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">livyConf = <span class=\"keyword\">new</span> <span class=\"type\">LivyConf</span>().loadFromFile(<span class=\"string\">\"livy.conf\"</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> host = livyConf.get(<span class=\"type\">SERVER_HOST</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> port = livyConf.getInt(<span class=\"type\">SERVER_PORT</span>)</div><div class=\"line\"># 这个而没有看懂</div><div class=\"line\"><span class=\"keyword\">val</span> multipartConfig = <span class=\"type\">MultipartConfig</span>(</div><div class=\"line\">    maxFileSize = <span class=\"type\">Some</span>(livyConf.getLong(<span class=\"type\">LivyConf</span>.<span class=\"type\">FILE_UPLOAD_MAX_SIZE</span>))</div><div class=\"line\">  ).toMultipartConfigElement</div></pre></td></tr></table></figure>\n<ul>\n<li>测试SparkHome是否设置成功</li>\n</ul>\n<p>如下代码，这里使用了require方法对参数进行先决条件检测(值得借鉴)<br><figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Make sure the `spark-submit` program exists, otherwise much of livy won't work.</span></div><div class=\"line\">testSparkHome(livyConf)</div><div class=\"line\">...</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">* Sets the spark-submit path if it's not configured in the LivyConf</div><div class=\"line\">*/</span></div><div class=\"line\"><span class=\"keyword\">private</span>[server] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">testSparkHome</span></span>(livyConf: <span class=\"type\">LivyConf</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\"><span class=\"keyword\">val</span> sparkHome = livyConf.sparkHome().getOrElse &#123;</div><div class=\"line\">  <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalArgumentException</span>(<span class=\"string\">\"Livy requires the SPARK_HOME environment variable\"</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">require(<span class=\"keyword\">new</span> <span class=\"type\">File</span>(sparkHome).isDirectory(), <span class=\"string\">\"SPARK_HOME path does not exist\"</span>)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<ul>\n<li>测试spark-submit命令可用，否则livy无法工作(值得借鉴):</li>\n</ul>\n<p>这里的代码写得太精彩了！先是定义一个<code>$SPAKR_HOME/bin/spark-sumbit --version</code>的命令，使用java的ProcessBuilder，然后可以得到exitCode和重定向的标准输出结果，如果结果是”version …”的话，就代表执行成功，输出结果；然后对这个version进行正则匹配，如果是1.6到2.0版本之间，就返回true，否则就说明spark版本不支持；</p>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div></pre></td><td class=\"code\"><pre><div class=\"line\">testSparkSubmit(livyConf)</div><div class=\"line\">...</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">* Test that the configured `spark-submit` executable exists.</div><div class=\"line\">*</div><div class=\"line\">* @param livyConf</div><div class=\"line\">*/</span></div><div class=\"line\"><span class=\"keyword\">private</span>[server] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">testSparkSubmit</span></span>(livyConf: <span class=\"type\">LivyConf</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  testSparkVersion(sparkSubmitVersion(livyConf))</div><div class=\"line\">&#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> e: <span class=\"type\">IOException</span> =&gt;</div><div class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IOException</span>(<span class=\"string\">\"Failed to run spark-submit executable\"</span>, e)</div><div class=\"line\">&#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">...</div><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">* Return the version of the configured `spark-submit` version.</div><div class=\"line\">*</div><div class=\"line\">* @param livyConf</div><div class=\"line\">* @return the version</div><div class=\"line\">*/</span></div><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sparkSubmitVersion</span></span>(livyConf: <span class=\"type\">LivyConf</span>): <span class=\"type\">String</span> = &#123;</div><div class=\"line\"><span class=\"keyword\">val</span> sparkSubmit = livyConf.sparkSubmit()</div><div class=\"line\"><span class=\"keyword\">val</span> pb = <span class=\"keyword\">new</span> <span class=\"type\">ProcessBuilder</span>(sparkSubmit, <span class=\"string\">\"--version\"</span>)</div><div class=\"line\">pb.redirectErrorStream(<span class=\"literal\">true</span>)</div><div class=\"line\">pb.redirectInput(<span class=\"type\">ProcessBuilder</span>.<span class=\"type\">Redirect</span>.<span class=\"type\">PIPE</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"type\">LivyConf</span>.<span class=\"type\">TEST_MODE</span>) &#123;</div><div class=\"line\">  pb.environment().put(<span class=\"string\">\"LIVY_TEST_CLASSPATH\"</span>, sys.props(<span class=\"string\">\"java.class.path\"</span>))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> process = <span class=\"keyword\">new</span> <span class=\"type\">LineBufferedProcess</span>(pb.start())</div><div class=\"line\"><span class=\"keyword\">val</span> exitCode = process.waitFor()</div><div class=\"line\"><span class=\"keyword\">val</span> output = process.inputIterator.mkString(<span class=\"string\">\"\\n\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> regex = <span class=\"string\">\"\"</span><span class=\"string\">\"version (.*)\"</span><span class=\"string\">\"\"</span>.r.unanchored</div><div class=\"line\"></div><div class=\"line\">output <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> regex(version) =&gt; version</div><div class=\"line\">  <span class=\"keyword\">case</span> _ =&gt;</div><div class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IOException</span>(<span class=\"string\">f\"Unable to determine spark-submit version [<span class=\"subst\">$exitCode</span>]:\\n<span class=\"subst\">$output</span>\"</span>)</div><div class=\"line\">&#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">* Throw an exception if Spark version is not supported.</div><div class=\"line\">* @param version Spark version</div><div class=\"line\">*/</span></div><div class=\"line\"><span class=\"keyword\">private</span>[server] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">testSparkVersion</span></span>(version: <span class=\"type\">String</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\"><span class=\"keyword\">val</span> versionPattern = <span class=\"string\">\"\"</span><span class=\"string\">\"(\\d)+\\.(\\d)+(?:\\.\\d*)?\"</span><span class=\"string\">\"\"</span>.r</div><div class=\"line\"><span class=\"comment\">// This is exclusive. Version which equals to this will be rejected.</span></div><div class=\"line\"><span class=\"keyword\">val</span> maxVersion = (<span class=\"number\">2</span>, <span class=\"number\">0</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> minVersion = (<span class=\"number\">1</span>, <span class=\"number\">6</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> supportedVersion = version <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> versionPattern(major, minor) =&gt;</div><div class=\"line\">    <span class=\"keyword\">val</span> v = (major.toInt, minor.toInt)</div><div class=\"line\">    v &gt;= minVersion &amp;&amp; v &lt; maxVersion</div><div class=\"line\">  <span class=\"keyword\">case</span> _ =&gt; <span class=\"literal\">false</span></div><div class=\"line\">&#125;</div><div class=\"line\">require(supportedVersion, <span class=\"string\">s\"Unsupported Spark version <span class=\"subst\">$version</span>.\"</span>)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>通过livy.spark.master是否以yarn开头判断是否需要初始化YarnClient</li>\n</ul>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Initialize YarnClient ASAP to save time.</span></div><div class=\"line\"><span class=\"keyword\">if</span> (livyConf.isRunningOnYarn()) &#123;</div><div class=\"line\">  <span class=\"type\">Future</span> &#123; <span class=\"type\">SparkYarnApp</span>.yarnClient &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// SparkYarnApp.scala</span></div><div class=\"line\"><span class=\"comment\">// YarnClient is thread safe. Create once, share it across threads.</span></div><div class=\"line\"><span class=\"keyword\">lazy</span> <span class=\"keyword\">val</span> yarnClient = &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> c = <span class=\"type\">YarnClient</span>.createYarnClient() <span class=\"comment\">// 这里调用的是yarn提供的API</span></div><div class=\"line\">  c.init(<span class=\"keyword\">new</span> <span class=\"type\">YarnConfiguration</span>())</div><div class=\"line\">  c.start()</div><div class=\"line\">  c</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>接下来启动WebServer</li>\n</ul>\n<p>这个webServer是Jetty的WebServer，通过设置是否使用ssl的配置来判断启动的是http server还是 https server，也会判断有没有Kerberos，设置IP，端口，日志等。（以后写得时候得查看Jetty的API和文档）</p>\n<figure class=\"highlight scala\"><figcaption><span>LivyServer.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">server = <span class=\"keyword\">new</span> <span class=\"type\">WebServer</span>(livyConf, host, port)</div><div class=\"line\">server.context.setResourceBase(<span class=\"string\">\"src/main/com/cloudera/livy/server\"</span>)</div><div class=\"line\">server.context.addEventListener(</div><div class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">ServletContextListener</span>() <span class=\"keyword\">with</span> <span class=\"type\">MetricsBootstrap</span> <span class=\"keyword\">with</span> <span class=\"type\">ServletApiImplicits</span> &#123;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mount</span></span>(sc: <span class=\"type\">ServletContext</span>, servlet: <span class=\"type\">Servlet</span>, mappings: <span class=\"type\">String</span>*): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> registration = sc.addServlet(servlet.getClass().getName(), servlet)</div><div class=\"line\">      registration.addMapping(mappings: _*)</div><div class=\"line\">      registration.setMultipartConfig(multipartConfig)</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">contextDestroyed</span></span>(sce: <span class=\"type\">ServletContextEvent</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\"></div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">contextInitialized</span></span>(sce: <span class=\"type\">ServletContextEvent</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">      <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">val</span> context = sce.getServletContext()</div><div class=\"line\">        context.initParameters(org.scalatra.<span class=\"type\">EnvironmentKey</span>) = livyConf.get(<span class=\"type\">ENVIRONMENT</span>)</div><div class=\"line\">        mount(context, <span class=\"keyword\">new</span> <span class=\"type\">InteractiveSessionServlet</span>(livyConf), <span class=\"string\">\"/sessions/*\"</span>)</div><div class=\"line\">        mount(context, <span class=\"keyword\">new</span> <span class=\"type\">BatchSessionServlet</span>(livyConf), <span class=\"string\">\"/batches/*\"</span>)</div><div class=\"line\">        context.mountMetricsAdminServlet(<span class=\"string\">\"/\"</span>)</div><div class=\"line\">      &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">case</span> e: <span class=\"type\">Throwable</span> =&gt;</div><div class=\"line\">          error(<span class=\"string\">\"Exception thrown when initializing server\"</span>, e)</div><div class=\"line\">          sys.exit(<span class=\"number\">1</span>)</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">  &#125;)</div></pre></td></tr></table></figure>\n"},{"title":"flume的http监控参数说明","toc":true,"date":"2016-05-18T11:29:20.000Z","_content":"\n如果要启用http监控，需要在启动flume的时候添加如下命令：\n\n\tbin/flume-ng agent --conf conf --conf-file conf/avroToHdfs2.conf --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=8533 -Dflume.root.logger=INFO,console\n\n这样，在flume所在IP的8533端口，就可以接收到如下json串，可以在chrome浏览器上安装 JSONView这个插件，使得阅读这个json串更加方便。\n\n对于一个source，一个channel和一个sink的agent监控如下：\n\n其中 channel的ChannelFillPercentage值是比较具有明显特征的属性，代表channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。如果这个值缓慢增加，增加到一定程度，就会出现数据丢失的情况。\n\n结果: 其中src-1是子自定义的source名称\n\n\t{\n\t\"SOURCE.src-1\":{\n\t\t\"OpenConnectionCount\":\"0\",\t\t//目前与客户端或sink保持连接的总数量(目前只有avro source展现该度量)\n\t\t\"Type\":\"SOURCE\",\t\t\t\t\t\n\t\t\"AppendBatchAcceptedCount\":\"1355\",\t//成功提交到channel的批次的总数量\n\t\t\"AppendBatchReceivedCount\":\"1355\",\t//接收到事件批次的总数量\n\t\t\"EventAcceptedCount\":\"28286\",\t//成功写出到channel的事件总数量，且source返回success给创建事件的sink或RPC客户端系统\n\t\t\"AppendReceivedCount\":\"0\",\t\t//每批只有一个事件的事件总数量(与RPC调用中的一个append调用相等)\n\t\t\"StopTime\":\"0\",\t\t\t//source停止时自Epoch以来的毫秒值时间\n\t\t\"StartTime\":\"1442566410435\",\t//source启动时自Epoch以来的毫秒值时间\n\t\t\"EventReceivedCount\":\"28286\",\t//目前为止source已经接收到的事件总数量\n\t\t\"AppendAcceptedCount\":\"0\"\t\t//单独传入的事件到Channel且成功返回的事件总数量\n\t},\n\t\"CHANNEL.ch-1\":{\n\t\t\"EventPutSuccessCount\":\"28286\",\t//成功写入channel且提交的事件总数量\n\t\t\"ChannelFillPercentage\":\"0.0\",\t//channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。\n\t\t\"Type\":\"CHANNEL\",\n\t\t\"StopTime\":\"0\",\t\t\t//channel停止时自Epoch以来的毫秒值时间\n\t\t\"EventPutAttemptCount\":\"28286\",\t//Source尝试写入Channe的事件总数量\n\t\t\"ChannelSize\":\"0\",\t\t\t//目前channel中事件的总数量\n\t\t\"StartTime\":\"1442566410326\",\t//channel启动时自Epoch以来的毫秒值时间\n\t\t\"EventTakeSuccessCount\":\"28286\",\t//sink成功读取的事件的总数量\n\t\t\"ChannelCapacity\":\"1000000\",       //channel的容量\n\t\t\"EventTakeAttemptCount\":\"313734329512\" //sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据\n\t},\n\t\"SINK.sink-1\":{\n\t\t\"Type\":\"SINK\",\n\t\t\"ConnectionClosedCount\":\"0\",\t//下一阶段或存储系统关闭的连接数量(如在HDFS中关闭一个文件)\n\t\t\"EventDrainSuccessCount\":\"28286\",\t//sink成功写出到存储的事件总数量\n\t\t\"KafkaEventSendTimer\":\"482493\",    \n\t\t\"BatchCompleteCount\":\"0\",\t\t//与最大批量尺寸相等的批量的数量\n\t\t\"ConnectionFailedCount\":\"0\",\t//下一阶段或存储系统由于错误关闭的连接数量（如HDFS上一个新创建的文件因为超时而关闭）\n\t\t\"EventDrainAttemptCount\":\"0\",\t//sink尝试写出到存储的事件总数量\n\t\t\"ConnectionCreatedCount\":\"0\",\t//下一个阶段或存储系统创建的连接数量（如HDFS创建一个新文件）\n\t\t\"BatchEmptyCount\":\"0\",\t\t//空的批量的数量，如果数量很大表示souce写数据比sink清理数据慢速度慢很多\n\t\t\"StopTime\":\"0\",\t\t\t\n\t\t\"RollbackCount\":\"9\",\t\t\t//\n\t\t\"StartTime\":\"1442566411897\",\n\t\t\"BatchUnderflowCount\":\"0\"\t\t//比sink配置使用的最大批量尺寸更小的批量的数量，如果该值很高也表示sink比souce更快\n\t}\n\t}\n","source":"_posts/2016-10-18-flume的http监控参数说明.md","raw":"---\ntitle: flume的http监控参数说明\ntoc: true\ndate: 2016-5-18 19:29:20\ntags: flume\ncategories: flume\n---\n\n如果要启用http监控，需要在启动flume的时候添加如下命令：\n\n\tbin/flume-ng agent --conf conf --conf-file conf/avroToHdfs2.conf --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=8533 -Dflume.root.logger=INFO,console\n\n这样，在flume所在IP的8533端口，就可以接收到如下json串，可以在chrome浏览器上安装 JSONView这个插件，使得阅读这个json串更加方便。\n\n对于一个source，一个channel和一个sink的agent监控如下：\n\n其中 channel的ChannelFillPercentage值是比较具有明显特征的属性，代表channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。如果这个值缓慢增加，增加到一定程度，就会出现数据丢失的情况。\n\n结果: 其中src-1是子自定义的source名称\n\n\t{\n\t\"SOURCE.src-1\":{\n\t\t\"OpenConnectionCount\":\"0\",\t\t//目前与客户端或sink保持连接的总数量(目前只有avro source展现该度量)\n\t\t\"Type\":\"SOURCE\",\t\t\t\t\t\n\t\t\"AppendBatchAcceptedCount\":\"1355\",\t//成功提交到channel的批次的总数量\n\t\t\"AppendBatchReceivedCount\":\"1355\",\t//接收到事件批次的总数量\n\t\t\"EventAcceptedCount\":\"28286\",\t//成功写出到channel的事件总数量，且source返回success给创建事件的sink或RPC客户端系统\n\t\t\"AppendReceivedCount\":\"0\",\t\t//每批只有一个事件的事件总数量(与RPC调用中的一个append调用相等)\n\t\t\"StopTime\":\"0\",\t\t\t//source停止时自Epoch以来的毫秒值时间\n\t\t\"StartTime\":\"1442566410435\",\t//source启动时自Epoch以来的毫秒值时间\n\t\t\"EventReceivedCount\":\"28286\",\t//目前为止source已经接收到的事件总数量\n\t\t\"AppendAcceptedCount\":\"0\"\t\t//单独传入的事件到Channel且成功返回的事件总数量\n\t},\n\t\"CHANNEL.ch-1\":{\n\t\t\"EventPutSuccessCount\":\"28286\",\t//成功写入channel且提交的事件总数量\n\t\t\"ChannelFillPercentage\":\"0.0\",\t//channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。\n\t\t\"Type\":\"CHANNEL\",\n\t\t\"StopTime\":\"0\",\t\t\t//channel停止时自Epoch以来的毫秒值时间\n\t\t\"EventPutAttemptCount\":\"28286\",\t//Source尝试写入Channe的事件总数量\n\t\t\"ChannelSize\":\"0\",\t\t\t//目前channel中事件的总数量\n\t\t\"StartTime\":\"1442566410326\",\t//channel启动时自Epoch以来的毫秒值时间\n\t\t\"EventTakeSuccessCount\":\"28286\",\t//sink成功读取的事件的总数量\n\t\t\"ChannelCapacity\":\"1000000\",       //channel的容量\n\t\t\"EventTakeAttemptCount\":\"313734329512\" //sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据\n\t},\n\t\"SINK.sink-1\":{\n\t\t\"Type\":\"SINK\",\n\t\t\"ConnectionClosedCount\":\"0\",\t//下一阶段或存储系统关闭的连接数量(如在HDFS中关闭一个文件)\n\t\t\"EventDrainSuccessCount\":\"28286\",\t//sink成功写出到存储的事件总数量\n\t\t\"KafkaEventSendTimer\":\"482493\",    \n\t\t\"BatchCompleteCount\":\"0\",\t\t//与最大批量尺寸相等的批量的数量\n\t\t\"ConnectionFailedCount\":\"0\",\t//下一阶段或存储系统由于错误关闭的连接数量（如HDFS上一个新创建的文件因为超时而关闭）\n\t\t\"EventDrainAttemptCount\":\"0\",\t//sink尝试写出到存储的事件总数量\n\t\t\"ConnectionCreatedCount\":\"0\",\t//下一个阶段或存储系统创建的连接数量（如HDFS创建一个新文件）\n\t\t\"BatchEmptyCount\":\"0\",\t\t//空的批量的数量，如果数量很大表示souce写数据比sink清理数据慢速度慢很多\n\t\t\"StopTime\":\"0\",\t\t\t\n\t\t\"RollbackCount\":\"9\",\t\t\t//\n\t\t\"StartTime\":\"1442566411897\",\n\t\t\"BatchUnderflowCount\":\"0\"\t\t//比sink配置使用的最大批量尺寸更小的批量的数量，如果该值很高也表示sink比souce更快\n\t}\n\t}\n","slug":"flume的http监控参数说明","published":1,"updated":"2016-10-18T11:30:29.383Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfvh000vpsguobe3gpj7","content":"<p>如果要启用http监控，需要在启动flume的时候添加如下命令：</p>\n<pre><code>bin/flume-ng agent --conf conf --conf-file conf/avroToHdfs2.conf --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=8533 -Dflume.root.logger=INFO,console\n</code></pre><p>这样，在flume所在IP的8533端口，就可以接收到如下json串，可以在chrome浏览器上安装 JSONView这个插件，使得阅读这个json串更加方便。</p>\n<p>对于一个source，一个channel和一个sink的agent监控如下：</p>\n<p>其中 channel的ChannelFillPercentage值是比较具有明显特征的属性，代表channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。如果这个值缓慢增加，增加到一定程度，就会出现数据丢失的情况。</p>\n<p>结果: 其中src-1是子自定义的source名称</p>\n<pre><code>{\n&quot;SOURCE.src-1&quot;:{\n    &quot;OpenConnectionCount&quot;:&quot;0&quot;,        //目前与客户端或sink保持连接的总数量(目前只有avro source展现该度量)\n    &quot;Type&quot;:&quot;SOURCE&quot;,                    \n    &quot;AppendBatchAcceptedCount&quot;:&quot;1355&quot;,    //成功提交到channel的批次的总数量\n    &quot;AppendBatchReceivedCount&quot;:&quot;1355&quot;,    //接收到事件批次的总数量\n    &quot;EventAcceptedCount&quot;:&quot;28286&quot;,    //成功写出到channel的事件总数量，且source返回success给创建事件的sink或RPC客户端系统\n    &quot;AppendReceivedCount&quot;:&quot;0&quot;,        //每批只有一个事件的事件总数量(与RPC调用中的一个append调用相等)\n    &quot;StopTime&quot;:&quot;0&quot;,            //source停止时自Epoch以来的毫秒值时间\n    &quot;StartTime&quot;:&quot;1442566410435&quot;,    //source启动时自Epoch以来的毫秒值时间\n    &quot;EventReceivedCount&quot;:&quot;28286&quot;,    //目前为止source已经接收到的事件总数量\n    &quot;AppendAcceptedCount&quot;:&quot;0&quot;        //单独传入的事件到Channel且成功返回的事件总数量\n},\n&quot;CHANNEL.ch-1&quot;:{\n    &quot;EventPutSuccessCount&quot;:&quot;28286&quot;,    //成功写入channel且提交的事件总数量\n    &quot;ChannelFillPercentage&quot;:&quot;0.0&quot;,    //channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。\n    &quot;Type&quot;:&quot;CHANNEL&quot;,\n    &quot;StopTime&quot;:&quot;0&quot;,            //channel停止时自Epoch以来的毫秒值时间\n    &quot;EventPutAttemptCount&quot;:&quot;28286&quot;,    //Source尝试写入Channe的事件总数量\n    &quot;ChannelSize&quot;:&quot;0&quot;,            //目前channel中事件的总数量\n    &quot;StartTime&quot;:&quot;1442566410326&quot;,    //channel启动时自Epoch以来的毫秒值时间\n    &quot;EventTakeSuccessCount&quot;:&quot;28286&quot;,    //sink成功读取的事件的总数量\n    &quot;ChannelCapacity&quot;:&quot;1000000&quot;,       //channel的容量\n    &quot;EventTakeAttemptCount&quot;:&quot;313734329512&quot; //sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据\n},\n&quot;SINK.sink-1&quot;:{\n    &quot;Type&quot;:&quot;SINK&quot;,\n    &quot;ConnectionClosedCount&quot;:&quot;0&quot;,    //下一阶段或存储系统关闭的连接数量(如在HDFS中关闭一个文件)\n    &quot;EventDrainSuccessCount&quot;:&quot;28286&quot;,    //sink成功写出到存储的事件总数量\n    &quot;KafkaEventSendTimer&quot;:&quot;482493&quot;,    \n    &quot;BatchCompleteCount&quot;:&quot;0&quot;,        //与最大批量尺寸相等的批量的数量\n    &quot;ConnectionFailedCount&quot;:&quot;0&quot;,    //下一阶段或存储系统由于错误关闭的连接数量（如HDFS上一个新创建的文件因为超时而关闭）\n    &quot;EventDrainAttemptCount&quot;:&quot;0&quot;,    //sink尝试写出到存储的事件总数量\n    &quot;ConnectionCreatedCount&quot;:&quot;0&quot;,    //下一个阶段或存储系统创建的连接数量（如HDFS创建一个新文件）\n    &quot;BatchEmptyCount&quot;:&quot;0&quot;,        //空的批量的数量，如果数量很大表示souce写数据比sink清理数据慢速度慢很多\n    &quot;StopTime&quot;:&quot;0&quot;,            \n    &quot;RollbackCount&quot;:&quot;9&quot;,            //\n    &quot;StartTime&quot;:&quot;1442566411897&quot;,\n    &quot;BatchUnderflowCount&quot;:&quot;0&quot;        //比sink配置使用的最大批量尺寸更小的批量的数量，如果该值很高也表示sink比souce更快\n}\n}\n</code></pre>","excerpt":"","more":"<p>如果要启用http监控，需要在启动flume的时候添加如下命令：</p>\n<pre><code>bin/flume-ng agent --conf conf --conf-file conf/avroToHdfs2.conf --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=8533 -Dflume.root.logger=INFO,console\n</code></pre><p>这样，在flume所在IP的8533端口，就可以接收到如下json串，可以在chrome浏览器上安装 JSONView这个插件，使得阅读这个json串更加方便。</p>\n<p>对于一个source，一个channel和一个sink的agent监控如下：</p>\n<p>其中 channel的ChannelFillPercentage值是比较具有明显特征的属性，代表channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。如果这个值缓慢增加，增加到一定程度，就会出现数据丢失的情况。</p>\n<p>结果: 其中src-1是子自定义的source名称</p>\n<pre><code>{\n&quot;SOURCE.src-1&quot;:{\n    &quot;OpenConnectionCount&quot;:&quot;0&quot;,        //目前与客户端或sink保持连接的总数量(目前只有avro source展现该度量)\n    &quot;Type&quot;:&quot;SOURCE&quot;,                    \n    &quot;AppendBatchAcceptedCount&quot;:&quot;1355&quot;,    //成功提交到channel的批次的总数量\n    &quot;AppendBatchReceivedCount&quot;:&quot;1355&quot;,    //接收到事件批次的总数量\n    &quot;EventAcceptedCount&quot;:&quot;28286&quot;,    //成功写出到channel的事件总数量，且source返回success给创建事件的sink或RPC客户端系统\n    &quot;AppendReceivedCount&quot;:&quot;0&quot;,        //每批只有一个事件的事件总数量(与RPC调用中的一个append调用相等)\n    &quot;StopTime&quot;:&quot;0&quot;,            //source停止时自Epoch以来的毫秒值时间\n    &quot;StartTime&quot;:&quot;1442566410435&quot;,    //source启动时自Epoch以来的毫秒值时间\n    &quot;EventReceivedCount&quot;:&quot;28286&quot;,    //目前为止source已经接收到的事件总数量\n    &quot;AppendAcceptedCount&quot;:&quot;0&quot;        //单独传入的事件到Channel且成功返回的事件总数量\n},\n&quot;CHANNEL.ch-1&quot;:{\n    &quot;EventPutSuccessCount&quot;:&quot;28286&quot;,    //成功写入channel且提交的事件总数量\n    &quot;ChannelFillPercentage&quot;:&quot;0.0&quot;,    //channel满时的百分比，重点就在这个值，一般情况下，这个值小于0.01就代表很通畅，sink的速度比source的速度快，如果这个值超过了5，就代表肯定是sink的速度不够快，需要对sink进行调优，或者需要控制source的速率。\n    &quot;Type&quot;:&quot;CHANNEL&quot;,\n    &quot;StopTime&quot;:&quot;0&quot;,            //channel停止时自Epoch以来的毫秒值时间\n    &quot;EventPutAttemptCount&quot;:&quot;28286&quot;,    //Source尝试写入Channe的事件总数量\n    &quot;ChannelSize&quot;:&quot;0&quot;,            //目前channel中事件的总数量\n    &quot;StartTime&quot;:&quot;1442566410326&quot;,    //channel启动时自Epoch以来的毫秒值时间\n    &quot;EventTakeSuccessCount&quot;:&quot;28286&quot;,    //sink成功读取的事件的总数量\n    &quot;ChannelCapacity&quot;:&quot;1000000&quot;,       //channel的容量\n    &quot;EventTakeAttemptCount&quot;:&quot;313734329512&quot; //sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据\n},\n&quot;SINK.sink-1&quot;:{\n    &quot;Type&quot;:&quot;SINK&quot;,\n    &quot;ConnectionClosedCount&quot;:&quot;0&quot;,    //下一阶段或存储系统关闭的连接数量(如在HDFS中关闭一个文件)\n    &quot;EventDrainSuccessCount&quot;:&quot;28286&quot;,    //sink成功写出到存储的事件总数量\n    &quot;KafkaEventSendTimer&quot;:&quot;482493&quot;,    \n    &quot;BatchCompleteCount&quot;:&quot;0&quot;,        //与最大批量尺寸相等的批量的数量\n    &quot;ConnectionFailedCount&quot;:&quot;0&quot;,    //下一阶段或存储系统由于错误关闭的连接数量（如HDFS上一个新创建的文件因为超时而关闭）\n    &quot;EventDrainAttemptCount&quot;:&quot;0&quot;,    //sink尝试写出到存储的事件总数量\n    &quot;ConnectionCreatedCount&quot;:&quot;0&quot;,    //下一个阶段或存储系统创建的连接数量（如HDFS创建一个新文件）\n    &quot;BatchEmptyCount&quot;:&quot;0&quot;,        //空的批量的数量，如果数量很大表示souce写数据比sink清理数据慢速度慢很多\n    &quot;StopTime&quot;:&quot;0&quot;,            \n    &quot;RollbackCount&quot;:&quot;9&quot;,            //\n    &quot;StartTime&quot;:&quot;1442566411897&quot;,\n    &quot;BatchUnderflowCount&quot;:&quot;0&quot;        //比sink配置使用的最大批量尺寸更小的批量的数量，如果该值很高也表示sink比souce更快\n}\n}\n</code></pre>"},{"title":"Java中的“钩子”","toc":true,"date":"2016-07-18T11:31:32.000Z","_content":"\n最近看银辉大哥写的对hdfs中小文件打包成大文件的程序的时候，发现他在代码中巧妙地运用了“钩子”，是用匿名内部类来实现的，感觉很酷，所以决定好好向大神学习一下使用匿名内部类实现钩子的用法：\n\n#### 题目\n\n为了让我能够快速了解这个方法的使用，银辉大哥首先给我出个题：\n比如上医院看病，一般会有　挂号，问诊，开药，付费，拿药　几个过程，但是不同的病科室不同，大夫不同，药方不同，付费方式不同，取药方式不同。写一个程序，打印不同的看病流程：如一个人感冒：挂呼吸科，看张大夫，开了砒霜，支付宝支付，快递拿药。\n另外一个人胃痛，挂了内科，看了王大夫，开了鹤顶红，没有付钱，直接抢药。\n\n#### 模板方法\n\n模板方法模式（Template Method）：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。该模式使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。\n使用场景：\n1、一次性实现一个算法的不变的部分，并将可变的行为留给子类来实现。\n2、各子类中公共的行为应被提取出来并集中到一个公共父类中以避免代码重复。即“重分解以一般化”，首先识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。\n3、控制子类扩展。模板方法只在特定点调用“Hook Method（钩子方法）”操作，这样就只允许在这些点进行扩展。\n\n{% asset_img hook.jpg %}\n\n\n#### 实现代码\n\n##### interface GoHospital.java:\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tpublic interface GoHospital extends Runnable{\n\t    /**\n\t     * 挂号\n\t     * @return 科室名\n\t     */\n\t    void onRegister(ActionHook register);\n\n\t    /**\n\t     * 问诊\n\t     * @return 病名\n\t     */\n\t    void onInterview(ActionHook interview);\n\n\t    /**\n\t     * 开药\n\t     * @return 药名\n\t     */\n\t    void onMedicine(ActionHook medicine);\n\n\t    /**\n\t     * 付费\n\t     * @return 付了多少钱\n\t     */\n\t    void onPay(ActionHook pay);\n\n\t    /**\n\t     * 返回取药方式\n\t     * @return 取药方式\n\t     */\n\t    void onGetMedicine(ActionHook getMedicine);\n\n\t}\n\n###### AbsGoHospital.java\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tabstract class AbsGoHospital implements GoHospital{\n\t    private ActionHook register;\n\t    private ActionHook intterview;\n\t    private ActionHook medicine;\n\t    private ActionHook pay;\n\t    private ActionHook getMedicine;\n\n\t    public void run(){\n\t        if (register != null) {\n\t            boolean isRegisterOk = register.exec();\n\t            if (isRegisterOk && intterview != null) {\n\t                boolean isIntterviewOk = intterview.exec(isRegisterOk);\n\t                if (isIntterviewOk && medicine != null) {\n\t                    boolean isMedicineOK = medicine.exec();\n\t                    if (isMedicineOK && pay != null) {\n\t                        boolean isPayOk = pay.exec();\n\t                        if (isPayOk && getMedicine != null) {\n\t                            boolean isGetMedicineOk = getMedicine.exec();\n\t                        }\n\t                    }\n\t                }\n\t            }\n\t        }\n\n\t    }\n\n\n\n\t    /**\n\t     * 挂号\n\t     * @return 科室名\n\t     */\n\t    public void onRegister(ActionHook register) {\n\t        this.register = register;\n\t    }\n\n\t    /**\n\t     * 问诊\n\t     * @return 病名\n\t     */\n\t    public void onInterview(ActionHook interview) {\n\t        this.intterview = interview;\n\t    }\n\n\t    /**\n\t     * 开药\n\t     * @return 药名\n\t     */\n\t    public void onMedicine(ActionHook medicine) {\n\t        this.medicine = medicine;\n\t    }\n\n\t    /**\n\t     * 付费\n\t     * @return 付了多少钱\n\t     */\n\t    public void onPay(ActionHook pay) {\n\t        this.pay = pay;\n\t    }\n\n\t    /**\n\t     * 返回取药方式\n\t     * @return 取药方式\n\t     */\n\t    public void onGetMedicine(ActionHook getMedicine) {\n\t        this.getMedicine = getMedicine;\n\t    }\n\n\t}\n\n\n###### 定义一个钩子\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tpublic interface ActionHook {\n\t    /**\n\t     * 钩子逻辑\n\t     * @param args 任意参数\n\t     */\n\t    boolean exec(Object ... args);\n\t}\n\n\n###### 定义GoHostpital的实现类\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tpublic class XiaogangGoHopital extends AbsGoHospital {\n\t    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过\n\t    private String name;\n\t    private String diease;\n\t    private String paymentPre;\n\t    private String getMedicineWay;\n\n\t    public XiaogangGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {\n\t        this.name = name;\n\t        this.diease = diease;\n\t        this.paymentPre = paymentPre;\n\t        this.getMedicineWay = getMedicineWay;\n\t    }\n\n\t    @Override\n\t    public String toString() {\n\t        return \"XiaogangGoHopital{\" +\n\t                \"name='\" + name + '\\'' +\n\t                \", diease='\" + diease + '\\'' +\n\t                \", paymentPre='\" + paymentPre + '\\'' +\n\t                \", getMedicineWay='\" + getMedicineWay + '\\'' +\n\t                '}';\n\t    }\n\n\t    public String getDiease() {\n\t        return diease;\n\t    }\n\n\t    public void setDiease(String diease) {\n\t        this.diease = diease;\n\t    }\n\n\t    public String getPaymentPre() {\n\t        return paymentPre;\n\t    }\n\n\t    public void setPaymentPre(String paymentPre) {\n\t        this.paymentPre = paymentPre;\n\t    }\n\n\t    public String getGetMedicineWay() {\n\t        return getMedicineWay;\n\t    }\n\n\t    public void setGetMedicineWay(String getMedicineWay) {\n\t        this.getMedicineWay = getMedicineWay;\n\t    }\n\n\t    public String getName() {\n\t        return name;\n\t    }\n\n\t    public void setName(String name) {\n\t        this.name = name;\n\t    }\n\n\t    @Override\n\t    public void onRegister(final ActionHook register) {\n\t        super.onRegister(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                register.exec(args);\n\t                System.out.println(name + \"骑电瓶车去的\");\n\t                return true;\n\t            }\n\t        });\n\t    }\n\t}\n\n\n###### LiGoHopital 实现类\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tpublic class LiGoHopital extends AbsGoHospital {\n\t    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过\n\t    private String name;\n\t    private String diease;\n\t    private String paymentPre;\n\t    private String getMedicineWay;\n\n\t    public LiGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {\n\t        this.name = name;\n\t        this.diease = diease;\n\t        this.paymentPre = paymentPre;\n\t        this.getMedicineWay = getMedicineWay;\n\t    }\n\n\t    @Override\n\t    public String toString() {\n\t        return \"XiaogangGoHopital{\" +\n\t                \"name='\" + name + '\\'' +\n\t                \", diease='\" + diease + '\\'' +\n\t                \", paymentPre='\" + paymentPre + '\\'' +\n\t                \", getMedicineWay='\" + getMedicineWay + '\\'' +\n\t                '}';\n\t    }\n\n\t    public String getDiease() {\n\t        return diease;\n\t    }\n\n\t    public void setDiease(String diease) {\n\t        this.diease = diease;\n\t    }\n\n\t    public String getPaymentPre() {\n\t        return paymentPre;\n\t    }\n\n\t    public void setPaymentPre(String paymentPre) {\n\t        this.paymentPre = paymentPre;\n\t    }\n\n\t    public String getGetMedicineWay() {\n\t        return getMedicineWay;\n\t    }\n\n\t    public void setGetMedicineWay(String getMedicineWay) {\n\t        this.getMedicineWay = getMedicineWay;\n\t    }\n\n\t    public String getName() {\n\t        return name;\n\t    }\n\n\t    public void setName(String name) {\n\t        this.name = name;\n\t    }\n\n\t    @Override\n\t    public void onRegister(final ActionHook register) {\n\t        super.onRegister(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                register.exec(args);\n\t                System.out.println(name + \"ta开车去的\");\n\t                return true;\n\t            }\n\t        });\n\t    }\n\n\t    @Override\n\t    public void onPay(final ActionHook pay) {\n\t        super.onPay(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                pay.exec();\n\t                System.out.println(\"他不喜欢付钱\");\n\t                return true;\n\t            }\n\t        });\n\t    }\n\t}\n\n\n###### mainz测试函数\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tpublic class Main {\n\t    public static void main(String[] args) {\n\t        XiaogangGoHopital xiaogangGoHopital = new XiaogangGoHopital(\"Gang\", \"jiba\", \"zhifubao\", \"shunfeng\");\n\t        xiaogangGoHopital.onRegister(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"=====\\n外科\");\n\t                return true;\n\t            }\n\t        });\n\t        xiaogangGoHopital.onInterview(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(args[0]);\n\t                System.out.println(\"右臂肌肉损伤\");\n\t                return true;\n\t            }\n\t        });\n\t        xiaogangGoHopital.onMedicine(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"钙片\");\n\t                return true;\n\t            }\n\t        });\n\t        xiaogangGoHopital.onPay(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"支付宝付了 123 元\");\n\t                return true;\n\t            }\n\t        });\n\t        xiaogangGoHopital.onGetMedicine(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"顺风快递\");\n\t                return true;\n\t            }\n\t        });\n\n\t        Thread thread = new Thread(xiaogangGoHopital);\n\t        thread.start();\n\n\n\n\t        LiGoHopital liGoHopital = new LiGoHopital(\"Li\", \"jiba\", \"zhifubao\", \"shunfeng\");\n\t        liGoHopital.onRegister(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"=====\\n外科\");\n\t                return true;\n\t            }\n\t        });\n\t        liGoHopital.onInterview(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"右臂肌肉损伤\");\n\t                return true;\n\t            }\n\t        });\n\t        liGoHopital.onMedicine(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"钙片\");\n\t                return true;\n\t            }\n\t        });\n\t        liGoHopital.onPay(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"支付宝付了 123 元\");\n\t                return true;\n\t            }\n\t        });\n\t        liGoHopital.onGetMedicine(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"顺风快递\");\n\t                return true;\n\t            }\n\t        });\n\n\t        Thread thread2 = new Thread(liGoHopital);\n\t        thread2.start();\n\n\t    }\n\t}\n\n\n#### 测试结果\n\n结果如下：\n\n\t=====\n\t外科\n\tGang骑电瓶车去的\n\ttrue\n\t右臂肌肉损伤\n\t钙片\n\t支付宝付了 123 元\n\t顺风快递\n\t=====\n\t外科\n\tLita开车去的\n\t右臂肌肉损伤\n\t钙片\n\t支付宝付了 123 元\n\t他不喜欢付钱\n\t顺风快递\n\n#### 总结\n\n由于工作任务蛮重，所以实现地很简单，详细代码可以参考hdfs小文件问题的归档程序；\n理解设计模式，或者实现技巧，才是第一步，以后能够把它熟练运用才是最重要的！","source":"_posts/2016-10-18-Java中的“钩子”.md","raw":"---\ntitle: Java中的“钩子”\ntoc: true\ndate: 2016-7-18 19:31:32\ntags: java\ncategories: java\n---\n\n最近看银辉大哥写的对hdfs中小文件打包成大文件的程序的时候，发现他在代码中巧妙地运用了“钩子”，是用匿名内部类来实现的，感觉很酷，所以决定好好向大神学习一下使用匿名内部类实现钩子的用法：\n\n#### 题目\n\n为了让我能够快速了解这个方法的使用，银辉大哥首先给我出个题：\n比如上医院看病，一般会有　挂号，问诊，开药，付费，拿药　几个过程，但是不同的病科室不同，大夫不同，药方不同，付费方式不同，取药方式不同。写一个程序，打印不同的看病流程：如一个人感冒：挂呼吸科，看张大夫，开了砒霜，支付宝支付，快递拿药。\n另外一个人胃痛，挂了内科，看了王大夫，开了鹤顶红，没有付钱，直接抢药。\n\n#### 模板方法\n\n模板方法模式（Template Method）：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。该模式使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。\n使用场景：\n1、一次性实现一个算法的不变的部分，并将可变的行为留给子类来实现。\n2、各子类中公共的行为应被提取出来并集中到一个公共父类中以避免代码重复。即“重分解以一般化”，首先识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。\n3、控制子类扩展。模板方法只在特定点调用“Hook Method（钩子方法）”操作，这样就只允许在这些点进行扩展。\n\n{% asset_img hook.jpg %}\n\n\n#### 实现代码\n\n##### interface GoHospital.java:\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tpublic interface GoHospital extends Runnable{\n\t    /**\n\t     * 挂号\n\t     * @return 科室名\n\t     */\n\t    void onRegister(ActionHook register);\n\n\t    /**\n\t     * 问诊\n\t     * @return 病名\n\t     */\n\t    void onInterview(ActionHook interview);\n\n\t    /**\n\t     * 开药\n\t     * @return 药名\n\t     */\n\t    void onMedicine(ActionHook medicine);\n\n\t    /**\n\t     * 付费\n\t     * @return 付了多少钱\n\t     */\n\t    void onPay(ActionHook pay);\n\n\t    /**\n\t     * 返回取药方式\n\t     * @return 取药方式\n\t     */\n\t    void onGetMedicine(ActionHook getMedicine);\n\n\t}\n\n###### AbsGoHospital.java\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tabstract class AbsGoHospital implements GoHospital{\n\t    private ActionHook register;\n\t    private ActionHook intterview;\n\t    private ActionHook medicine;\n\t    private ActionHook pay;\n\t    private ActionHook getMedicine;\n\n\t    public void run(){\n\t        if (register != null) {\n\t            boolean isRegisterOk = register.exec();\n\t            if (isRegisterOk && intterview != null) {\n\t                boolean isIntterviewOk = intterview.exec(isRegisterOk);\n\t                if (isIntterviewOk && medicine != null) {\n\t                    boolean isMedicineOK = medicine.exec();\n\t                    if (isMedicineOK && pay != null) {\n\t                        boolean isPayOk = pay.exec();\n\t                        if (isPayOk && getMedicine != null) {\n\t                            boolean isGetMedicineOk = getMedicine.exec();\n\t                        }\n\t                    }\n\t                }\n\t            }\n\t        }\n\n\t    }\n\n\n\n\t    /**\n\t     * 挂号\n\t     * @return 科室名\n\t     */\n\t    public void onRegister(ActionHook register) {\n\t        this.register = register;\n\t    }\n\n\t    /**\n\t     * 问诊\n\t     * @return 病名\n\t     */\n\t    public void onInterview(ActionHook interview) {\n\t        this.intterview = interview;\n\t    }\n\n\t    /**\n\t     * 开药\n\t     * @return 药名\n\t     */\n\t    public void onMedicine(ActionHook medicine) {\n\t        this.medicine = medicine;\n\t    }\n\n\t    /**\n\t     * 付费\n\t     * @return 付了多少钱\n\t     */\n\t    public void onPay(ActionHook pay) {\n\t        this.pay = pay;\n\t    }\n\n\t    /**\n\t     * 返回取药方式\n\t     * @return 取药方式\n\t     */\n\t    public void onGetMedicine(ActionHook getMedicine) {\n\t        this.getMedicine = getMedicine;\n\t    }\n\n\t}\n\n\n###### 定义一个钩子\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tpublic interface ActionHook {\n\t    /**\n\t     * 钩子逻辑\n\t     * @param args 任意参数\n\t     */\n\t    boolean exec(Object ... args);\n\t}\n\n\n###### 定义GoHostpital的实现类\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tpublic class XiaogangGoHopital extends AbsGoHospital {\n\t    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过\n\t    private String name;\n\t    private String diease;\n\t    private String paymentPre;\n\t    private String getMedicineWay;\n\n\t    public XiaogangGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {\n\t        this.name = name;\n\t        this.diease = diease;\n\t        this.paymentPre = paymentPre;\n\t        this.getMedicineWay = getMedicineWay;\n\t    }\n\n\t    @Override\n\t    public String toString() {\n\t        return \"XiaogangGoHopital{\" +\n\t                \"name='\" + name + '\\'' +\n\t                \", diease='\" + diease + '\\'' +\n\t                \", paymentPre='\" + paymentPre + '\\'' +\n\t                \", getMedicineWay='\" + getMedicineWay + '\\'' +\n\t                '}';\n\t    }\n\n\t    public String getDiease() {\n\t        return diease;\n\t    }\n\n\t    public void setDiease(String diease) {\n\t        this.diease = diease;\n\t    }\n\n\t    public String getPaymentPre() {\n\t        return paymentPre;\n\t    }\n\n\t    public void setPaymentPre(String paymentPre) {\n\t        this.paymentPre = paymentPre;\n\t    }\n\n\t    public String getGetMedicineWay() {\n\t        return getMedicineWay;\n\t    }\n\n\t    public void setGetMedicineWay(String getMedicineWay) {\n\t        this.getMedicineWay = getMedicineWay;\n\t    }\n\n\t    public String getName() {\n\t        return name;\n\t    }\n\n\t    public void setName(String name) {\n\t        this.name = name;\n\t    }\n\n\t    @Override\n\t    public void onRegister(final ActionHook register) {\n\t        super.onRegister(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                register.exec(args);\n\t                System.out.println(name + \"骑电瓶车去的\");\n\t                return true;\n\t            }\n\t        });\n\t    }\n\t}\n\n\n###### LiGoHopital 实现类\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tpublic class LiGoHopital extends AbsGoHospital {\n\t    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过\n\t    private String name;\n\t    private String diease;\n\t    private String paymentPre;\n\t    private String getMedicineWay;\n\n\t    public LiGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {\n\t        this.name = name;\n\t        this.diease = diease;\n\t        this.paymentPre = paymentPre;\n\t        this.getMedicineWay = getMedicineWay;\n\t    }\n\n\t    @Override\n\t    public String toString() {\n\t        return \"XiaogangGoHopital{\" +\n\t                \"name='\" + name + '\\'' +\n\t                \", diease='\" + diease + '\\'' +\n\t                \", paymentPre='\" + paymentPre + '\\'' +\n\t                \", getMedicineWay='\" + getMedicineWay + '\\'' +\n\t                '}';\n\t    }\n\n\t    public String getDiease() {\n\t        return diease;\n\t    }\n\n\t    public void setDiease(String diease) {\n\t        this.diease = diease;\n\t    }\n\n\t    public String getPaymentPre() {\n\t        return paymentPre;\n\t    }\n\n\t    public void setPaymentPre(String paymentPre) {\n\t        this.paymentPre = paymentPre;\n\t    }\n\n\t    public String getGetMedicineWay() {\n\t        return getMedicineWay;\n\t    }\n\n\t    public void setGetMedicineWay(String getMedicineWay) {\n\t        this.getMedicineWay = getMedicineWay;\n\t    }\n\n\t    public String getName() {\n\t        return name;\n\t    }\n\n\t    public void setName(String name) {\n\t        this.name = name;\n\t    }\n\n\t    @Override\n\t    public void onRegister(final ActionHook register) {\n\t        super.onRegister(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                register.exec(args);\n\t                System.out.println(name + \"ta开车去的\");\n\t                return true;\n\t            }\n\t        });\n\t    }\n\n\t    @Override\n\t    public void onPay(final ActionHook pay) {\n\t        super.onPay(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                pay.exec();\n\t                System.out.println(\"他不喜欢付钱\");\n\t                return true;\n\t            }\n\t        });\n\t    }\n\t}\n\n\n###### mainz测试函数\n\n\t/**\n\t * Created by Adam on 2016/5/27.\n\t */\n\tpublic class Main {\n\t    public static void main(String[] args) {\n\t        XiaogangGoHopital xiaogangGoHopital = new XiaogangGoHopital(\"Gang\", \"jiba\", \"zhifubao\", \"shunfeng\");\n\t        xiaogangGoHopital.onRegister(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"=====\\n外科\");\n\t                return true;\n\t            }\n\t        });\n\t        xiaogangGoHopital.onInterview(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(args[0]);\n\t                System.out.println(\"右臂肌肉损伤\");\n\t                return true;\n\t            }\n\t        });\n\t        xiaogangGoHopital.onMedicine(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"钙片\");\n\t                return true;\n\t            }\n\t        });\n\t        xiaogangGoHopital.onPay(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"支付宝付了 123 元\");\n\t                return true;\n\t            }\n\t        });\n\t        xiaogangGoHopital.onGetMedicine(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"顺风快递\");\n\t                return true;\n\t            }\n\t        });\n\n\t        Thread thread = new Thread(xiaogangGoHopital);\n\t        thread.start();\n\n\n\n\t        LiGoHopital liGoHopital = new LiGoHopital(\"Li\", \"jiba\", \"zhifubao\", \"shunfeng\");\n\t        liGoHopital.onRegister(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"=====\\n外科\");\n\t                return true;\n\t            }\n\t        });\n\t        liGoHopital.onInterview(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"右臂肌肉损伤\");\n\t                return true;\n\t            }\n\t        });\n\t        liGoHopital.onMedicine(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"钙片\");\n\t                return true;\n\t            }\n\t        });\n\t        liGoHopital.onPay(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"支付宝付了 123 元\");\n\t                return true;\n\t            }\n\t        });\n\t        liGoHopital.onGetMedicine(new ActionHook() {\n\t            @Override\n\t            public boolean exec(Object... args) {\n\t                System.out.println(\"顺风快递\");\n\t                return true;\n\t            }\n\t        });\n\n\t        Thread thread2 = new Thread(liGoHopital);\n\t        thread2.start();\n\n\t    }\n\t}\n\n\n#### 测试结果\n\n结果如下：\n\n\t=====\n\t外科\n\tGang骑电瓶车去的\n\ttrue\n\t右臂肌肉损伤\n\t钙片\n\t支付宝付了 123 元\n\t顺风快递\n\t=====\n\t外科\n\tLita开车去的\n\t右臂肌肉损伤\n\t钙片\n\t支付宝付了 123 元\n\t他不喜欢付钱\n\t顺风快递\n\n#### 总结\n\n由于工作任务蛮重，所以实现地很简单，详细代码可以参考hdfs小文件问题的归档程序；\n理解设计模式，或者实现技巧，才是第一步，以后能够把它熟练运用才是最重要的！","slug":"Java中的“钩子”","published":1,"updated":"2016-10-18T11:34:03.743Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfvl000zpsgu4w8afrnz","content":"<p>最近看银辉大哥写的对hdfs中小文件打包成大文件的程序的时候，发现他在代码中巧妙地运用了“钩子”，是用匿名内部类来实现的，感觉很酷，所以决定好好向大神学习一下使用匿名内部类实现钩子的用法：</p>\n<h4 id=\"题目\"><a href=\"#题目\" class=\"headerlink\" title=\"题目\"></a>题目</h4><p>为了让我能够快速了解这个方法的使用，银辉大哥首先给我出个题：<br>比如上医院看病，一般会有　挂号，问诊，开药，付费，拿药　几个过程，但是不同的病科室不同，大夫不同，药方不同，付费方式不同，取药方式不同。写一个程序，打印不同的看病流程：如一个人感冒：挂呼吸科，看张大夫，开了砒霜，支付宝支付，快递拿药。<br>另外一个人胃痛，挂了内科，看了王大夫，开了鹤顶红，没有付钱，直接抢药。</p>\n<h4 id=\"模板方法\"><a href=\"#模板方法\" class=\"headerlink\" title=\"模板方法\"></a>模板方法</h4><p>模板方法模式（Template Method）：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。该模式使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。<br>使用场景：<br>1、一次性实现一个算法的不变的部分，并将可变的行为留给子类来实现。<br>2、各子类中公共的行为应被提取出来并集中到一个公共父类中以避免代码重复。即“重分解以一般化”，首先识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。<br>3、控制子类扩展。模板方法只在特定点调用“Hook Method（钩子方法）”操作，这样就只允许在这些点进行扩展。</p>\n<img src=\"/2016/07/18/Java中的“钩子”/hook.jpg\" alt=\"hook.jpg\" title=\"\">\n<h4 id=\"实现代码\"><a href=\"#实现代码\" class=\"headerlink\" title=\"实现代码\"></a>实现代码</h4><h5 id=\"interface-GoHospital-java\"><a href=\"#interface-GoHospital-java\" class=\"headerlink\" title=\"interface GoHospital.java:\"></a>interface GoHospital.java:</h5><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\npublic interface GoHospital extends Runnable{\n    /**\n     * 挂号\n     * @return 科室名\n     */\n    void onRegister(ActionHook register);\n\n    /**\n     * 问诊\n     * @return 病名\n     */\n    void onInterview(ActionHook interview);\n\n    /**\n     * 开药\n     * @return 药名\n     */\n    void onMedicine(ActionHook medicine);\n\n    /**\n     * 付费\n     * @return 付了多少钱\n     */\n    void onPay(ActionHook pay);\n\n    /**\n     * 返回取药方式\n     * @return 取药方式\n     */\n    void onGetMedicine(ActionHook getMedicine);\n\n}\n</code></pre><h6 id=\"AbsGoHospital-java\"><a href=\"#AbsGoHospital-java\" class=\"headerlink\" title=\"AbsGoHospital.java\"></a>AbsGoHospital.java</h6><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\nabstract class AbsGoHospital implements GoHospital{\n    private ActionHook register;\n    private ActionHook intterview;\n    private ActionHook medicine;\n    private ActionHook pay;\n    private ActionHook getMedicine;\n\n    public void run(){\n        if (register != null) {\n            boolean isRegisterOk = register.exec();\n            if (isRegisterOk &amp;&amp; intterview != null) {\n                boolean isIntterviewOk = intterview.exec(isRegisterOk);\n                if (isIntterviewOk &amp;&amp; medicine != null) {\n                    boolean isMedicineOK = medicine.exec();\n                    if (isMedicineOK &amp;&amp; pay != null) {\n                        boolean isPayOk = pay.exec();\n                        if (isPayOk &amp;&amp; getMedicine != null) {\n                            boolean isGetMedicineOk = getMedicine.exec();\n                        }\n                    }\n                }\n            }\n        }\n\n    }\n\n\n\n    /**\n     * 挂号\n     * @return 科室名\n     */\n    public void onRegister(ActionHook register) {\n        this.register = register;\n    }\n\n    /**\n     * 问诊\n     * @return 病名\n     */\n    public void onInterview(ActionHook interview) {\n        this.intterview = interview;\n    }\n\n    /**\n     * 开药\n     * @return 药名\n     */\n    public void onMedicine(ActionHook medicine) {\n        this.medicine = medicine;\n    }\n\n    /**\n     * 付费\n     * @return 付了多少钱\n     */\n    public void onPay(ActionHook pay) {\n        this.pay = pay;\n    }\n\n    /**\n     * 返回取药方式\n     * @return 取药方式\n     */\n    public void onGetMedicine(ActionHook getMedicine) {\n        this.getMedicine = getMedicine;\n    }\n\n}\n</code></pre><h6 id=\"定义一个钩子\"><a href=\"#定义一个钩子\" class=\"headerlink\" title=\"定义一个钩子\"></a>定义一个钩子</h6><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\npublic interface ActionHook {\n    /**\n     * 钩子逻辑\n     * @param args 任意参数\n     */\n    boolean exec(Object ... args);\n}\n</code></pre><h6 id=\"定义GoHostpital的实现类\"><a href=\"#定义GoHostpital的实现类\" class=\"headerlink\" title=\"定义GoHostpital的实现类\"></a>定义GoHostpital的实现类</h6><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\npublic class XiaogangGoHopital extends AbsGoHospital {\n    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过\n    private String name;\n    private String diease;\n    private String paymentPre;\n    private String getMedicineWay;\n\n    public XiaogangGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {\n        this.name = name;\n        this.diease = diease;\n        this.paymentPre = paymentPre;\n        this.getMedicineWay = getMedicineWay;\n    }\n\n    @Override\n    public String toString() {\n        return &quot;XiaogangGoHopital{&quot; +\n                &quot;name=&apos;&quot; + name + &apos;\\&apos;&apos; +\n                &quot;, diease=&apos;&quot; + diease + &apos;\\&apos;&apos; +\n                &quot;, paymentPre=&apos;&quot; + paymentPre + &apos;\\&apos;&apos; +\n                &quot;, getMedicineWay=&apos;&quot; + getMedicineWay + &apos;\\&apos;&apos; +\n                &apos;}&apos;;\n    }\n\n    public String getDiease() {\n        return diease;\n    }\n\n    public void setDiease(String diease) {\n        this.diease = diease;\n    }\n\n    public String getPaymentPre() {\n        return paymentPre;\n    }\n\n    public void setPaymentPre(String paymentPre) {\n        this.paymentPre = paymentPre;\n    }\n\n    public String getGetMedicineWay() {\n        return getMedicineWay;\n    }\n\n    public void setGetMedicineWay(String getMedicineWay) {\n        this.getMedicineWay = getMedicineWay;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    @Override\n    public void onRegister(final ActionHook register) {\n        super.onRegister(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                register.exec(args);\n                System.out.println(name + &quot;骑电瓶车去的&quot;);\n                return true;\n            }\n        });\n    }\n}\n</code></pre><h6 id=\"LiGoHopital-实现类\"><a href=\"#LiGoHopital-实现类\" class=\"headerlink\" title=\"LiGoHopital 实现类\"></a>LiGoHopital 实现类</h6><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\npublic class LiGoHopital extends AbsGoHospital {\n    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过\n    private String name;\n    private String diease;\n    private String paymentPre;\n    private String getMedicineWay;\n\n    public LiGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {\n        this.name = name;\n        this.diease = diease;\n        this.paymentPre = paymentPre;\n        this.getMedicineWay = getMedicineWay;\n    }\n\n    @Override\n    public String toString() {\n        return &quot;XiaogangGoHopital{&quot; +\n                &quot;name=&apos;&quot; + name + &apos;\\&apos;&apos; +\n                &quot;, diease=&apos;&quot; + diease + &apos;\\&apos;&apos; +\n                &quot;, paymentPre=&apos;&quot; + paymentPre + &apos;\\&apos;&apos; +\n                &quot;, getMedicineWay=&apos;&quot; + getMedicineWay + &apos;\\&apos;&apos; +\n                &apos;}&apos;;\n    }\n\n    public String getDiease() {\n        return diease;\n    }\n\n    public void setDiease(String diease) {\n        this.diease = diease;\n    }\n\n    public String getPaymentPre() {\n        return paymentPre;\n    }\n\n    public void setPaymentPre(String paymentPre) {\n        this.paymentPre = paymentPre;\n    }\n\n    public String getGetMedicineWay() {\n        return getMedicineWay;\n    }\n\n    public void setGetMedicineWay(String getMedicineWay) {\n        this.getMedicineWay = getMedicineWay;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    @Override\n    public void onRegister(final ActionHook register) {\n        super.onRegister(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                register.exec(args);\n                System.out.println(name + &quot;ta开车去的&quot;);\n                return true;\n            }\n        });\n    }\n\n    @Override\n    public void onPay(final ActionHook pay) {\n        super.onPay(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                pay.exec();\n                System.out.println(&quot;他不喜欢付钱&quot;);\n                return true;\n            }\n        });\n    }\n}\n</code></pre><h6 id=\"mainz测试函数\"><a href=\"#mainz测试函数\" class=\"headerlink\" title=\"mainz测试函数\"></a>mainz测试函数</h6><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\npublic class Main {\n    public static void main(String[] args) {\n        XiaogangGoHopital xiaogangGoHopital = new XiaogangGoHopital(&quot;Gang&quot;, &quot;jiba&quot;, &quot;zhifubao&quot;, &quot;shunfeng&quot;);\n        xiaogangGoHopital.onRegister(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;=====\\n外科&quot;);\n                return true;\n            }\n        });\n        xiaogangGoHopital.onInterview(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(args[0]);\n                System.out.println(&quot;右臂肌肉损伤&quot;);\n                return true;\n            }\n        });\n        xiaogangGoHopital.onMedicine(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;钙片&quot;);\n                return true;\n            }\n        });\n        xiaogangGoHopital.onPay(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;支付宝付了 123 元&quot;);\n                return true;\n            }\n        });\n        xiaogangGoHopital.onGetMedicine(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;顺风快递&quot;);\n                return true;\n            }\n        });\n\n        Thread thread = new Thread(xiaogangGoHopital);\n        thread.start();\n\n\n\n        LiGoHopital liGoHopital = new LiGoHopital(&quot;Li&quot;, &quot;jiba&quot;, &quot;zhifubao&quot;, &quot;shunfeng&quot;);\n        liGoHopital.onRegister(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;=====\\n外科&quot;);\n                return true;\n            }\n        });\n        liGoHopital.onInterview(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;右臂肌肉损伤&quot;);\n                return true;\n            }\n        });\n        liGoHopital.onMedicine(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;钙片&quot;);\n                return true;\n            }\n        });\n        liGoHopital.onPay(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;支付宝付了 123 元&quot;);\n                return true;\n            }\n        });\n        liGoHopital.onGetMedicine(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;顺风快递&quot;);\n                return true;\n            }\n        });\n\n        Thread thread2 = new Thread(liGoHopital);\n        thread2.start();\n\n    }\n}\n</code></pre><h4 id=\"测试结果\"><a href=\"#测试结果\" class=\"headerlink\" title=\"测试结果\"></a>测试结果</h4><p>结果如下：</p>\n<pre><code>=====\n外科\nGang骑电瓶车去的\ntrue\n右臂肌肉损伤\n钙片\n支付宝付了 123 元\n顺风快递\n=====\n外科\nLita开车去的\n右臂肌肉损伤\n钙片\n支付宝付了 123 元\n他不喜欢付钱\n顺风快递\n</code></pre><h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>由于工作任务蛮重，所以实现地很简单，详细代码可以参考hdfs小文件问题的归档程序；<br>理解设计模式，或者实现技巧，才是第一步，以后能够把它熟练运用才是最重要的！</p>\n","excerpt":"","more":"<p>最近看银辉大哥写的对hdfs中小文件打包成大文件的程序的时候，发现他在代码中巧妙地运用了“钩子”，是用匿名内部类来实现的，感觉很酷，所以决定好好向大神学习一下使用匿名内部类实现钩子的用法：</p>\n<h4 id=\"题目\"><a href=\"#题目\" class=\"headerlink\" title=\"题目\"></a>题目</h4><p>为了让我能够快速了解这个方法的使用，银辉大哥首先给我出个题：<br>比如上医院看病，一般会有　挂号，问诊，开药，付费，拿药　几个过程，但是不同的病科室不同，大夫不同，药方不同，付费方式不同，取药方式不同。写一个程序，打印不同的看病流程：如一个人感冒：挂呼吸科，看张大夫，开了砒霜，支付宝支付，快递拿药。<br>另外一个人胃痛，挂了内科，看了王大夫，开了鹤顶红，没有付钱，直接抢药。</p>\n<h4 id=\"模板方法\"><a href=\"#模板方法\" class=\"headerlink\" title=\"模板方法\"></a>模板方法</h4><p>模板方法模式（Template Method）：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。该模式使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。<br>使用场景：<br>1、一次性实现一个算法的不变的部分，并将可变的行为留给子类来实现。<br>2、各子类中公共的行为应被提取出来并集中到一个公共父类中以避免代码重复。即“重分解以一般化”，首先识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。<br>3、控制子类扩展。模板方法只在特定点调用“Hook Method（钩子方法）”操作，这样就只允许在这些点进行扩展。</p>\n<img src=\"/2016/07/18/Java中的“钩子”/hook.jpg\" alt=\"hook.jpg\" title=\"\">\n<h4 id=\"实现代码\"><a href=\"#实现代码\" class=\"headerlink\" title=\"实现代码\"></a>实现代码</h4><h5 id=\"interface-GoHospital-java\"><a href=\"#interface-GoHospital-java\" class=\"headerlink\" title=\"interface GoHospital.java:\"></a>interface GoHospital.java:</h5><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\npublic interface GoHospital extends Runnable{\n    /**\n     * 挂号\n     * @return 科室名\n     */\n    void onRegister(ActionHook register);\n\n    /**\n     * 问诊\n     * @return 病名\n     */\n    void onInterview(ActionHook interview);\n\n    /**\n     * 开药\n     * @return 药名\n     */\n    void onMedicine(ActionHook medicine);\n\n    /**\n     * 付费\n     * @return 付了多少钱\n     */\n    void onPay(ActionHook pay);\n\n    /**\n     * 返回取药方式\n     * @return 取药方式\n     */\n    void onGetMedicine(ActionHook getMedicine);\n\n}\n</code></pre><h6 id=\"AbsGoHospital-java\"><a href=\"#AbsGoHospital-java\" class=\"headerlink\" title=\"AbsGoHospital.java\"></a>AbsGoHospital.java</h6><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\nabstract class AbsGoHospital implements GoHospital{\n    private ActionHook register;\n    private ActionHook intterview;\n    private ActionHook medicine;\n    private ActionHook pay;\n    private ActionHook getMedicine;\n\n    public void run(){\n        if (register != null) {\n            boolean isRegisterOk = register.exec();\n            if (isRegisterOk &amp;&amp; intterview != null) {\n                boolean isIntterviewOk = intterview.exec(isRegisterOk);\n                if (isIntterviewOk &amp;&amp; medicine != null) {\n                    boolean isMedicineOK = medicine.exec();\n                    if (isMedicineOK &amp;&amp; pay != null) {\n                        boolean isPayOk = pay.exec();\n                        if (isPayOk &amp;&amp; getMedicine != null) {\n                            boolean isGetMedicineOk = getMedicine.exec();\n                        }\n                    }\n                }\n            }\n        }\n\n    }\n\n\n\n    /**\n     * 挂号\n     * @return 科室名\n     */\n    public void onRegister(ActionHook register) {\n        this.register = register;\n    }\n\n    /**\n     * 问诊\n     * @return 病名\n     */\n    public void onInterview(ActionHook interview) {\n        this.intterview = interview;\n    }\n\n    /**\n     * 开药\n     * @return 药名\n     */\n    public void onMedicine(ActionHook medicine) {\n        this.medicine = medicine;\n    }\n\n    /**\n     * 付费\n     * @return 付了多少钱\n     */\n    public void onPay(ActionHook pay) {\n        this.pay = pay;\n    }\n\n    /**\n     * 返回取药方式\n     * @return 取药方式\n     */\n    public void onGetMedicine(ActionHook getMedicine) {\n        this.getMedicine = getMedicine;\n    }\n\n}\n</code></pre><h6 id=\"定义一个钩子\"><a href=\"#定义一个钩子\" class=\"headerlink\" title=\"定义一个钩子\"></a>定义一个钩子</h6><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\npublic interface ActionHook {\n    /**\n     * 钩子逻辑\n     * @param args 任意参数\n     */\n    boolean exec(Object ... args);\n}\n</code></pre><h6 id=\"定义GoHostpital的实现类\"><a href=\"#定义GoHostpital的实现类\" class=\"headerlink\" title=\"定义GoHostpital的实现类\"></a>定义GoHostpital的实现类</h6><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\npublic class XiaogangGoHopital extends AbsGoHospital {\n    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过\n    private String name;\n    private String diease;\n    private String paymentPre;\n    private String getMedicineWay;\n\n    public XiaogangGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {\n        this.name = name;\n        this.diease = diease;\n        this.paymentPre = paymentPre;\n        this.getMedicineWay = getMedicineWay;\n    }\n\n    @Override\n    public String toString() {\n        return &quot;XiaogangGoHopital{&quot; +\n                &quot;name=&apos;&quot; + name + &apos;\\&apos;&apos; +\n                &quot;, diease=&apos;&quot; + diease + &apos;\\&apos;&apos; +\n                &quot;, paymentPre=&apos;&quot; + paymentPre + &apos;\\&apos;&apos; +\n                &quot;, getMedicineWay=&apos;&quot; + getMedicineWay + &apos;\\&apos;&apos; +\n                &apos;}&apos;;\n    }\n\n    public String getDiease() {\n        return diease;\n    }\n\n    public void setDiease(String diease) {\n        this.diease = diease;\n    }\n\n    public String getPaymentPre() {\n        return paymentPre;\n    }\n\n    public void setPaymentPre(String paymentPre) {\n        this.paymentPre = paymentPre;\n    }\n\n    public String getGetMedicineWay() {\n        return getMedicineWay;\n    }\n\n    public void setGetMedicineWay(String getMedicineWay) {\n        this.getMedicineWay = getMedicineWay;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    @Override\n    public void onRegister(final ActionHook register) {\n        super.onRegister(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                register.exec(args);\n                System.out.println(name + &quot;骑电瓶车去的&quot;);\n                return true;\n            }\n        });\n    }\n}\n</code></pre><h6 id=\"LiGoHopital-实现类\"><a href=\"#LiGoHopital-实现类\" class=\"headerlink\" title=\"LiGoHopital 实现类\"></a>LiGoHopital 实现类</h6><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\npublic class LiGoHopital extends AbsGoHospital {\n    // todo 这里的字段，方法都可以用enum扩展，由于本例为了学习，所以略过\n    private String name;\n    private String diease;\n    private String paymentPre;\n    private String getMedicineWay;\n\n    public LiGoHopital(String name, String diease, String paymentPre, String getMedicineWay) {\n        this.name = name;\n        this.diease = diease;\n        this.paymentPre = paymentPre;\n        this.getMedicineWay = getMedicineWay;\n    }\n\n    @Override\n    public String toString() {\n        return &quot;XiaogangGoHopital{&quot; +\n                &quot;name=&apos;&quot; + name + &apos;\\&apos;&apos; +\n                &quot;, diease=&apos;&quot; + diease + &apos;\\&apos;&apos; +\n                &quot;, paymentPre=&apos;&quot; + paymentPre + &apos;\\&apos;&apos; +\n                &quot;, getMedicineWay=&apos;&quot; + getMedicineWay + &apos;\\&apos;&apos; +\n                &apos;}&apos;;\n    }\n\n    public String getDiease() {\n        return diease;\n    }\n\n    public void setDiease(String diease) {\n        this.diease = diease;\n    }\n\n    public String getPaymentPre() {\n        return paymentPre;\n    }\n\n    public void setPaymentPre(String paymentPre) {\n        this.paymentPre = paymentPre;\n    }\n\n    public String getGetMedicineWay() {\n        return getMedicineWay;\n    }\n\n    public void setGetMedicineWay(String getMedicineWay) {\n        this.getMedicineWay = getMedicineWay;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    @Override\n    public void onRegister(final ActionHook register) {\n        super.onRegister(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                register.exec(args);\n                System.out.println(name + &quot;ta开车去的&quot;);\n                return true;\n            }\n        });\n    }\n\n    @Override\n    public void onPay(final ActionHook pay) {\n        super.onPay(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                pay.exec();\n                System.out.println(&quot;他不喜欢付钱&quot;);\n                return true;\n            }\n        });\n    }\n}\n</code></pre><h6 id=\"mainz测试函数\"><a href=\"#mainz测试函数\" class=\"headerlink\" title=\"mainz测试函数\"></a>mainz测试函数</h6><pre><code>/**\n * Created by Adam on 2016/5/27.\n */\npublic class Main {\n    public static void main(String[] args) {\n        XiaogangGoHopital xiaogangGoHopital = new XiaogangGoHopital(&quot;Gang&quot;, &quot;jiba&quot;, &quot;zhifubao&quot;, &quot;shunfeng&quot;);\n        xiaogangGoHopital.onRegister(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;=====\\n外科&quot;);\n                return true;\n            }\n        });\n        xiaogangGoHopital.onInterview(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(args[0]);\n                System.out.println(&quot;右臂肌肉损伤&quot;);\n                return true;\n            }\n        });\n        xiaogangGoHopital.onMedicine(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;钙片&quot;);\n                return true;\n            }\n        });\n        xiaogangGoHopital.onPay(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;支付宝付了 123 元&quot;);\n                return true;\n            }\n        });\n        xiaogangGoHopital.onGetMedicine(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;顺风快递&quot;);\n                return true;\n            }\n        });\n\n        Thread thread = new Thread(xiaogangGoHopital);\n        thread.start();\n\n\n\n        LiGoHopital liGoHopital = new LiGoHopital(&quot;Li&quot;, &quot;jiba&quot;, &quot;zhifubao&quot;, &quot;shunfeng&quot;);\n        liGoHopital.onRegister(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;=====\\n外科&quot;);\n                return true;\n            }\n        });\n        liGoHopital.onInterview(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;右臂肌肉损伤&quot;);\n                return true;\n            }\n        });\n        liGoHopital.onMedicine(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;钙片&quot;);\n                return true;\n            }\n        });\n        liGoHopital.onPay(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;支付宝付了 123 元&quot;);\n                return true;\n            }\n        });\n        liGoHopital.onGetMedicine(new ActionHook() {\n            @Override\n            public boolean exec(Object... args) {\n                System.out.println(&quot;顺风快递&quot;);\n                return true;\n            }\n        });\n\n        Thread thread2 = new Thread(liGoHopital);\n        thread2.start();\n\n    }\n}\n</code></pre><h4 id=\"测试结果\"><a href=\"#测试结果\" class=\"headerlink\" title=\"测试结果\"></a>测试结果</h4><p>结果如下：</p>\n<pre><code>=====\n外科\nGang骑电瓶车去的\ntrue\n右臂肌肉损伤\n钙片\n支付宝付了 123 元\n顺风快递\n=====\n外科\nLita开车去的\n右臂肌肉损伤\n钙片\n支付宝付了 123 元\n他不喜欢付钱\n顺风快递\n</code></pre><h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>由于工作任务蛮重，所以实现地很简单，详细代码可以参考hdfs小文件问题的归档程序；<br>理解设计模式，或者实现技巧，才是第一步，以后能够把它熟练运用才是最重要的！</p>\n"},{"layout":"false","title":"景区位置服务项目说明文档","toc":true,"date":"2016-10-18T06:20:54.000Z","_content":"\n## 总体架构\n总体架构图如下：\n{% asset_img 1.png %}\n\n如上图：主要分为三大部分：\n#### 上游数据\n由东方国信提供各个省份的Oidd数据，发送至kafka集群\n#### 处理逻辑\n1. Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；\n2. 景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；\n\n#### 下游系统\n处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。\n\n## 处理逻辑\n### 景区用户识别端详解\n\n景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑\n分别实现如下功能：\n\n#### 配置信息读取\nkafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取\n\n#### 接收Kakfa消息流\n接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；\n\n#### 对所有的号码根据mdn生成key\n\n#### 对mdn重复数据进行去重\n注意这里是在每台不同的机器下进行的去重，而不是整体的去重\n\n#### 建立Redis连接\n在每台机器上建立Redis连接池，\n\n#### 动态读取Mysql订阅信息\n对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -> PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]\n\n#### 对源数据进行过滤，不在景区所属城市的数据丢掉\n\n#### 读取Redis中已经保存的景区用户信息列表\n从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除\n\n#### 判断数据是否需要推送\n数据是否需要推送需要满足：\n\n1. 不在Redis中：与Redis中数据进行对比，在Redis中的数据删除\n2. 在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回\"\";\n3. 更新Redis \n\n#### 数据统一发送\n将处理好的数据按照一定的规则统一批量发送至flume","source":"_posts/2016-10-18-景区位置服务项目说明文档.md","raw":"---\nlayout: false\ntitle: 景区位置服务项目说明文档\ntoc: true\ndate: 2016-10-18 14:20:54\ntags: \n- spark streaming\n- flume\n- kafka\n- 大数据开发\n- redis\ncategories: spark开发\n---\n\n## 总体架构\n总体架构图如下：\n{% asset_img 1.png %}\n\n如上图：主要分为三大部分：\n#### 上游数据\n由东方国信提供各个省份的Oidd数据，发送至kafka集群\n#### 处理逻辑\n1. Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；\n2. 景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；\n\n#### 下游系统\n处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。\n\n## 处理逻辑\n### 景区用户识别端详解\n\n景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑\n分别实现如下功能：\n\n#### 配置信息读取\nkafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取\n\n#### 接收Kakfa消息流\n接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；\n\n#### 对所有的号码根据mdn生成key\n\n#### 对mdn重复数据进行去重\n注意这里是在每台不同的机器下进行的去重，而不是整体的去重\n\n#### 建立Redis连接\n在每台机器上建立Redis连接池，\n\n#### 动态读取Mysql订阅信息\n对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -> PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]\n\n#### 对源数据进行过滤，不在景区所属城市的数据丢掉\n\n#### 读取Redis中已经保存的景区用户信息列表\n从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除\n\n#### 判断数据是否需要推送\n数据是否需要推送需要满足：\n\n1. 不在Redis中：与Redis中数据进行对比，在Redis中的数据删除\n2. 在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回\"\";\n3. 更新Redis \n\n#### 数据统一发送\n将处理好的数据按照一定的规则统一批量发送至flume","slug":"景区位置服务项目说明文档","published":1,"updated":"2017-06-28T13:29:13.096Z","comments":1,"photos":[],"link":"","_id":"cjldkwfvp0013psgunn584gz5","content":"<h2 id=\"总体架构\"><a href=\"#总体架构\" class=\"headerlink\" title=\"总体架构\"></a>总体架构</h2><p>总体架构图如下：<br></p>\n<p>如上图：主要分为三大部分：</p>\n<h4 id=\"上游数据\"><a href=\"#上游数据\" class=\"headerlink\" title=\"上游数据\"></a>上游数据</h4><p>由东方国信提供各个省份的Oidd数据，发送至kafka集群</p>\n<h4 id=\"处理逻辑\"><a href=\"#处理逻辑\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h4><ol>\n<li>Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；</li>\n<li>景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；</li>\n</ol>\n<h4 id=\"下游系统\"><a href=\"#下游系统\" class=\"headerlink\" title=\"下游系统\"></a>下游系统</h4><p>处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。</p>\n<h2 id=\"处理逻辑-1\"><a href=\"#处理逻辑-1\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h2><h3 id=\"景区用户识别端详解\"><a href=\"#景区用户识别端详解\" class=\"headerlink\" title=\"景区用户识别端详解\"></a>景区用户识别端详解</h3><p>景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑<br>分别实现如下功能：</p>\n<h4 id=\"配置信息读取\"><a href=\"#配置信息读取\" class=\"headerlink\" title=\"配置信息读取\"></a>配置信息读取</h4><p>kafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取</p>\n<h4 id=\"接收Kakfa消息流\"><a href=\"#接收Kakfa消息流\" class=\"headerlink\" title=\"接收Kakfa消息流\"></a>接收Kakfa消息流</h4><p>接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；</p>\n<h4 id=\"对所有的号码根据mdn生成key\"><a href=\"#对所有的号码根据mdn生成key\" class=\"headerlink\" title=\"对所有的号码根据mdn生成key\"></a>对所有的号码根据mdn生成key</h4><h4 id=\"对mdn重复数据进行去重\"><a href=\"#对mdn重复数据进行去重\" class=\"headerlink\" title=\"对mdn重复数据进行去重\"></a>对mdn重复数据进行去重</h4><p>注意这里是在每台不同的机器下进行的去重，而不是整体的去重</p>\n<h4 id=\"建立Redis连接\"><a href=\"#建立Redis连接\" class=\"headerlink\" title=\"建立Redis连接\"></a>建立Redis连接</h4><p>在每台机器上建立Redis连接池，</p>\n<h4 id=\"动态读取Mysql订阅信息\"><a href=\"#动态读取Mysql订阅信息\" class=\"headerlink\" title=\"动态读取Mysql订阅信息\"></a>动态读取Mysql订阅信息</h4><p>对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -&gt; PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]</p>\n<h4 id=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"><a href=\"#对源数据进行过滤，不在景区所属城市的数据丢掉\" class=\"headerlink\" title=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"></a>对源数据进行过滤，不在景区所属城市的数据丢掉</h4><h4 id=\"读取Redis中已经保存的景区用户信息列表\"><a href=\"#读取Redis中已经保存的景区用户信息列表\" class=\"headerlink\" title=\"读取Redis中已经保存的景区用户信息列表\"></a>读取Redis中已经保存的景区用户信息列表</h4><p>从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除</p>\n<h4 id=\"判断数据是否需要推送\"><a href=\"#判断数据是否需要推送\" class=\"headerlink\" title=\"判断数据是否需要推送\"></a>判断数据是否需要推送</h4><p>数据是否需要推送需要满足：</p>\n<ol>\n<li>不在Redis中：与Redis中数据进行对比，在Redis中的数据删除</li>\n<li>在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回””;</li>\n<li>更新Redis </li>\n</ol>\n<h4 id=\"数据统一发送\"><a href=\"#数据统一发送\" class=\"headerlink\" title=\"数据统一发送\"></a>数据统一发送</h4><p>将处理好的数据按照一定的规则统一批量发送至flume</p>\n","excerpt":"","more":"<h2 id=\"总体架构\"><a href=\"#总体架构\" class=\"headerlink\" title=\"总体架构\"></a>总体架构</h2><p>总体架构图如下：<br></p>\n<p>如上图：主要分为三大部分：</p>\n<h4 id=\"上游数据\"><a href=\"#上游数据\" class=\"headerlink\" title=\"上游数据\"></a>上游数据</h4><p>由东方国信提供各个省份的Oidd数据，发送至kafka集群</p>\n<h4 id=\"处理逻辑\"><a href=\"#处理逻辑\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h4><ol>\n<li>Web微服务端：web微服务接受位置数据订阅，并将订阅的景区信息存储至Mysql中；</li>\n<li>景区用户识别端：由Spark Streaming程序根据订阅信息实时对上游数据进行处理，识别出到达景区的最新电话号码列表，发送至下游；</li>\n</ol>\n<h4 id=\"下游系统\"><a href=\"#下游系统\" class=\"headerlink\" title=\"下游系统\"></a>下游系统</h4><p>处理逻辑将处理结果通过FTP共享接口发送至下游大数据营销系统，由大数据营销系统实现推荐等。</p>\n<h2 id=\"处理逻辑-1\"><a href=\"#处理逻辑-1\" class=\"headerlink\" title=\"处理逻辑\"></a>处理逻辑</h2><h3 id=\"景区用户识别端详解\"><a href=\"#景区用户识别端详解\" class=\"headerlink\" title=\"景区用户识别端详解\"></a>景区用户识别端详解</h3><p>景区用户识别端详解逻辑由Spark Streaming程序实现，运行部署在yarn集群中，是一个实时的容错的处理逻辑<br>分别实现如下功能：</p>\n<h4 id=\"配置信息读取\"><a href=\"#配置信息读取\" class=\"headerlink\" title=\"配置信息读取\"></a>配置信息读取</h4><p>kafka，flume，redis，mysql等配置信息配置在数据库中，系统载入试进行自动化预读取</p>\n<h4 id=\"接收Kakfa消息流\"><a href=\"#接收Kakfa消息流\" class=\"headerlink\" title=\"接收Kakfa消息流\"></a>接收Kakfa消息流</h4><p>接收由国信发送至kafka的消息流，并将其分发到不同的机器，设置并发度；</p>\n<h4 id=\"对所有的号码根据mdn生成key\"><a href=\"#对所有的号码根据mdn生成key\" class=\"headerlink\" title=\"对所有的号码根据mdn生成key\"></a>对所有的号码根据mdn生成key</h4><h4 id=\"对mdn重复数据进行去重\"><a href=\"#对mdn重复数据进行去重\" class=\"headerlink\" title=\"对mdn重复数据进行去重\"></a>对mdn重复数据进行去重</h4><p>注意这里是在每台不同的机器下进行的去重，而不是整体的去重</p>\n<h4 id=\"建立Redis连接\"><a href=\"#建立Redis连接\" class=\"headerlink\" title=\"建立Redis连接\"></a>建立Redis连接</h4><p>在每台机器上建立Redis连接池，</p>\n<h4 id=\"动态读取Mysql订阅信息\"><a href=\"#动态读取Mysql订阅信息\" class=\"headerlink\" title=\"动态读取Mysql订阅信息\"></a>动态读取Mysql订阅信息</h4><p>对每一批数据，动态从mysql数据库中载入订阅的景区信息L0，并将其转换成 cityCode -&gt; PositionSubData的 HashMap util.HashMap[Int, util.LinkedList[PositionSubData]]</p>\n<h4 id=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"><a href=\"#对源数据进行过滤，不在景区所属城市的数据丢掉\" class=\"headerlink\" title=\"对源数据进行过滤，不在景区所属城市的数据丢掉\"></a>对源数据进行过滤，不在景区所属城市的数据丢掉</h4><h4 id=\"读取Redis中已经保存的景区用户信息列表\"><a href=\"#读取Redis中已经保存的景区用户信息列表\" class=\"headerlink\" title=\"读取Redis中已经保存的景区用户信息列表\"></a>读取Redis中已经保存的景区用户信息列表</h4><p>从Redis中获取之前保存的号码列表L1，并且对过期的数据进行双向删除</p>\n<h4 id=\"判断数据是否需要推送\"><a href=\"#判断数据是否需要推送\" class=\"headerlink\" title=\"判断数据是否需要推送\"></a>判断数据是否需要推送</h4><p>数据是否需要推送需要满足：</p>\n<ol>\n<li>不在Redis中：与Redis中数据进行对比，在Redis中的数据删除</li>\n<li>在景区里：通过判断数据是否在景区里，如果在景区，则返回它对应的locationId_intervalTime_spId，否则返回””;</li>\n<li>更新Redis </li>\n</ol>\n<h4 id=\"数据统一发送\"><a href=\"#数据统一发送\" class=\"headerlink\" title=\"数据统一发送\"></a>数据统一发送</h4><p>将处理好的数据按照一定的规则统一批量发送至flume</p>\n"},{"title":"记HashMap遇到的java.util.ConcurrentModificationException的bug","toc":true,"date":"2016-10-27T02:22:51.000Z","_content":"\n#### 问题背景\n\nspark Streaming 实时程序在联调期间稳定运行了两天，以为问题不大了，第二天早上的时候打开一看，竟然挂了，定位到代码，原来我的程序实时读取redis的数据为一个HashMap，直到挂的时候，Redis中数据一直在增大，共 6083条：\n\nspark相关代码如下：\n\n``` scala\n // 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除\n ...\nval sentinelPool = InternalRedisClient.getSentinelPool\nvar phoneSet: util.Map[String, String] = new util.HashMap[String, String]()\n//            printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)\n\nvar jedis1: Jedis = null\ntry {\n  jedis1 = sentinelPool.getResource\n  //              printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)\n\n  phoneSet = jedis1.hgetAll(redisHashKey)\n  //              printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)\n\n  printLog.debug(\"phoneSet_1: \" + phoneSet)\n} finally {\n  if (jedis1 != null) {\n    printLog.debug(\"close jedis1\")\n    jedis1.close()\n  }\n}\n\nif (!phoneSet.isEmpty) { \n  for (eachPhoneKV: (String, String) <- phoneSet) { // 就在这里挂掉了\n    val expirationDate: Int = eachPhoneKV._2.split(\"\\\\|\")(4).toInt\n    val today: Int = getNowDate.toInt\n    if (today > expirationDate) {\n      phoneSet.remove(eachPhoneKV._1)\n      var jedis2: Jedis = null\n      try {\n        jedis2 = sentinelPool.getResource\n        jedis2.hdel(redisHashKey, eachPhoneKV._1)\n      } finally {\n        if (jedis2 != null) {\n          printLog.debug(\"close jedis2\")\n          jedis2.close()\n        }\n      }\n    }\n  }\n  printLog.debug(\"phoneSet_filtedByData: \" + phoneSet)\n}\n```\n具体异常粘信息如下：\n``` java\nUser class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 4 times, most recent failure: Lost task 4.3 in stage 0.0 (TID 170, NM-304-HW-XH628V3-BIGDATA-063): java.util.ConcurrentModificationException\nat java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)\nat java.util.HashMap$EntryIterator.next(HashMap.java:962)\nat java.util.HashMap$EntryIterator.next(HashMap.java:960)\nat scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:267)\nat scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:264)\nat scala.collection.Iterator$class.foreach(Iterator.scala:727)\nat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\nat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\nat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\nat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\nat com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:871)\nat com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:677)\n```\n\n#### 解决方案\n\n项目太紧张，来不及详细分析java的源码了，根据经验redis中应该六千多条数据应该不是很大的，HashMap完全可以一次读取，从网上查到原因是因为remove操作导致的，在Iterator遍历过程中调用HashMap的remove方法会crash，有两个解决办法：\n\n1. 一个解决办法是用一个ArrayList记录要删除的key,然后再遍历这个ArrayList,调用HashMap的remove方法以ArrayList的元素为key进行删除；这个方法需要额外的空间和时间，虽然也浪费的不多，但总感觉不够优雅；\n2. 创建一个Iterator<Map.Entry<Integer, String>> iterator = map.entrySet().iterator();，然后用这个 iterator.remove()方法进行删除，这个是可以删除的；\n\n我使用的是方法二，修改后的代码如下：\n``` scala\nvar phoneMap: util.Map[String, String] = new util.HashMap[String, String]()\n// printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)\n\nvar jedis1: Jedis = null\ntry {\n  jedis1 = sentinelPool.getResource\n  // printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)\n\n  phoneMap = jedis1.hgetAll(redisHashKey)\n  // printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)\n\n  printLog.debug(\"phoneSet_1: \" + phoneMap)\n} finally {\n  if (jedis1 != null) {\n    printLog.debug(\"close jedis1\")\n    jedis1.close()\n  }\n}\n\n// 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除\nif (!phoneMap.isEmpty) {\n  val iterator: util.Iterator[Entry[String, String]] = phoneMap.entrySet().iterator()\n  while (iterator.hasNext) {\n    val eachPhoneKV: Entry[String, String] = iterator.next()\n    val mdn = eachPhoneKV.getKey\n    val redisValue = eachPhoneKV.getValue\n\n    var expirationDate: Int = 0\n    try {\n      expirationDate = redisValue.split(\"\\\\|\")(4).toInt\n      val today: Int = getNowDate.toInt\n      if (today > expirationDate) {\n        printLog.info(\"delete this data for expirationDate:\" + eachPhoneKV)\n        iterator.remove()\n        // phoneMap.remove(mdn) // 这一句是错误的，因为无法据此删除\n        var jedis2: Jedis = null\n        try {\n          jedis2 = sentinelPool.getResource\n          jedis2.hdel(redisHashKey, mdn)\n        } finally {\n          if (jedis2 != null) {\n            printLog.debug(\"close jedis2\")\n            jedis2.close()\n          }\n        }\n      }\n    } catch {\n      // 如果解析发生异常，则redis中删掉这个key，并且在phoneMap中同时删除\n      case ex: Exception => {\n        printLog.error(\"redis error data and del it: \" + eachPhoneKV + \" error: \" + ex)\n        iterator.remove()\n        var jedis4: Jedis = null\n        try {\n          jedis4 = sentinelPool.getResource\n          jedis4.hdel(redisHashKey, mdn)\n        } finally {\n          if (jedis4 != null) {\n            printLog.debug(\"close jedis4\")\n            jedis4.close()\n          }\n        }\n      }\n    }\n  }\n```\n然后测试，打包，部署，OK，解决。\n\n#### 原理说明\n\n遍历HashMap有三种方法，分别是: \n``` java\nfor(Map.Entry<Integer, String> entry : map.entrySet()){}  // scala中为 <-\n\nfor(Integer key : keySet){}\n\nIterator<Map.Entry<Integer, String>> it = map.entrySet().iterator();\n        while(it.hasNext()){}\n```\n其实上面的三种遍历方式从根本上讲都是使用的迭代器，之所以出现不同的结果是由于remove操作的实现不同决定的。\n\n首先前两种方法都在调用nextEntry方法的同一个地方抛出了异常，虽然remove成功了，但是在迭代器遍历下一个元素的时候抛出异常：\n``` java\nfinal Entry<K,V> nextEntry() {\n    if (modCount != expectedModCount)\n        throw new ConcurrentModificationException();\n    Entry<K,V> e = next;\n    ...\n}\n```\n这里modCount是表示map中的元素被修改了几次(在移除，新加元素时此值都会自增)，而expectedModCount是表示期望的修改次数，在迭代器构造的时候这两个值是相等，如果在遍历过程中这两个值出现了不同步就会抛出ConcurrentModificationException异常。\n\n1、HashMap的remove方法实现\n``` java\npublic V remove(Object key) {\n    Entry<K,V> e = removeEntryForKey(key);\n    return (e == null ? null : e.value);\n}\n```\n\n2、HashMap.KeySet的remove方法实现\n``` java\npublic boolean remove(Object o) {\n    return HashMap.this.removeEntryForKey(o) != null;\n}\n```\n\n3、HashMap.HashIterator的remove方法实现\n``` java\npublic void remove() {\n   if (current == null)\n        throw new IllegalStateException();\n   if (modCount != expectedModCount)\n        throw new ConcurrentModificationException();\n   Object k = current.key;\n   current = null;\n   HashMap.this.removeEntryForKey(k);\n   expectedModCount = modCount;\n}\n```\n以上三种实现方式都通过调用HashMap.removeEntryForKey方法来实现删除key的操作。在removeEntryForKey方法内只要移除了key modCount就会执行一次自增操作，此时modCount就与expectedModCount不一致了，上面三种remove实现中，只有第三种iterator的remove方法在调用完removeEntryForKey方法后同步了expectedModCount值与modCount相同，所以在遍历下个元素调用nextEntry方法时，iterator方式不会抛异常。\n\n``` java\nfinal Entry<K,V> removeEntryForKey(Object key) {\n    int hash = (key == null) ? 0 : hash(key.hashCode());\n    int i = indexFor(hash, table.length);\n    Entry<K,V> prev = table[i];\n    Entry<K,V> e = prev;\n\n    while (e != null) {\n        Entry<K,V> next = e.next;\n        Object k;\n        if (e.hash == hash &&\n            ((k = e.key) == key || (key != null && key.equals(k)))) {\n            modCount++;\n            size--;\n            if (prev == e)\n                table[i] = next;\n            else\n                prev.next = next;\n            e.recordRemoval(this);\n            return e;\n        }\n        prev = e;\n        e = next;\n    }\n\n    return e;\n}\n```\n\n#### 其它思考\n\n1、如果是遍历过程中增加或修改数据呢？\n增加或修改数据只能通过Map的put方法实现，在遍历过程中修改数据可以，但如果增加新key就会在下次循环时抛异常，因为在添加新key时modCount也会自增。\n\n2、有些集合类也有同样的遍历问题，如ArrayList，通过Iterator方式可正确遍历完成remove操作，直接调用list的remove方法就会抛异常。\n``` java\n//会抛ConcurrentModificationException异常\nfor(String str : list){\n\tlist.remove(str);\n}\n\n//正确遍历移除方式\nIterator<String> it = list.iterator();\nwhile(it.hasNext()){\n\tit.next();\n\tit.remove();\n}\n```\n\n3、jdk为什么这样设计，只允许通过iterator进行remove操作？\n\nHashMap和keySet的remove方法都可以通过传递key参数删除任意的元素，而iterator只能删除当前元素(current);\n对于通过HashMap的remove方法来说，一旦删除的元素是iterator对象中next所正在引用的，如果没有通过modCount与 expectedModCount的比较实现快速失败抛出异常，下次循环该元素将成为current指向，此时iterator就遍历了一个已移除的过期数据，所以一定要判断这两个值是否一致。\n\n4、这是一个坑，如果IDE能提示就好了，下次注意\n\n#### 引用\n\nhttp://afredlyj.github.io/posts/hashmap-concurrentmodificationexception.html\nhttp://dumbee.net/archives/41\nhttp://blog.csdn.net/wzy_1988/article/details/51423583","source":"_posts/2016-10-27-记HashMap遇到的java-util-ConcurrentModificationException的bug.md","raw":"---\ntitle: 记HashMap遇到的java.util.ConcurrentModificationException的bug\ntoc: true\ndate: 2016-10-27 10:22:51\ntags: java\ncategories: java\n---\n\n#### 问题背景\n\nspark Streaming 实时程序在联调期间稳定运行了两天，以为问题不大了，第二天早上的时候打开一看，竟然挂了，定位到代码，原来我的程序实时读取redis的数据为一个HashMap，直到挂的时候，Redis中数据一直在增大，共 6083条：\n\nspark相关代码如下：\n\n``` scala\n // 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除\n ...\nval sentinelPool = InternalRedisClient.getSentinelPool\nvar phoneSet: util.Map[String, String] = new util.HashMap[String, String]()\n//            printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)\n\nvar jedis1: Jedis = null\ntry {\n  jedis1 = sentinelPool.getResource\n  //              printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)\n\n  phoneSet = jedis1.hgetAll(redisHashKey)\n  //              printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)\n\n  printLog.debug(\"phoneSet_1: \" + phoneSet)\n} finally {\n  if (jedis1 != null) {\n    printLog.debug(\"close jedis1\")\n    jedis1.close()\n  }\n}\n\nif (!phoneSet.isEmpty) { \n  for (eachPhoneKV: (String, String) <- phoneSet) { // 就在这里挂掉了\n    val expirationDate: Int = eachPhoneKV._2.split(\"\\\\|\")(4).toInt\n    val today: Int = getNowDate.toInt\n    if (today > expirationDate) {\n      phoneSet.remove(eachPhoneKV._1)\n      var jedis2: Jedis = null\n      try {\n        jedis2 = sentinelPool.getResource\n        jedis2.hdel(redisHashKey, eachPhoneKV._1)\n      } finally {\n        if (jedis2 != null) {\n          printLog.debug(\"close jedis2\")\n          jedis2.close()\n        }\n      }\n    }\n  }\n  printLog.debug(\"phoneSet_filtedByData: \" + phoneSet)\n}\n```\n具体异常粘信息如下：\n``` java\nUser class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 4 times, most recent failure: Lost task 4.3 in stage 0.0 (TID 170, NM-304-HW-XH628V3-BIGDATA-063): java.util.ConcurrentModificationException\nat java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)\nat java.util.HashMap$EntryIterator.next(HashMap.java:962)\nat java.util.HashMap$EntryIterator.next(HashMap.java:960)\nat scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:267)\nat scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:264)\nat scala.collection.Iterator$class.foreach(Iterator.scala:727)\nat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\nat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\nat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\nat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\nat com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:871)\nat com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:677)\n```\n\n#### 解决方案\n\n项目太紧张，来不及详细分析java的源码了，根据经验redis中应该六千多条数据应该不是很大的，HashMap完全可以一次读取，从网上查到原因是因为remove操作导致的，在Iterator遍历过程中调用HashMap的remove方法会crash，有两个解决办法：\n\n1. 一个解决办法是用一个ArrayList记录要删除的key,然后再遍历这个ArrayList,调用HashMap的remove方法以ArrayList的元素为key进行删除；这个方法需要额外的空间和时间，虽然也浪费的不多，但总感觉不够优雅；\n2. 创建一个Iterator<Map.Entry<Integer, String>> iterator = map.entrySet().iterator();，然后用这个 iterator.remove()方法进行删除，这个是可以删除的；\n\n我使用的是方法二，修改后的代码如下：\n``` scala\nvar phoneMap: util.Map[String, String] = new util.HashMap[String, String]()\n// printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)\n\nvar jedis1: Jedis = null\ntry {\n  jedis1 = sentinelPool.getResource\n  // printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)\n\n  phoneMap = jedis1.hgetAll(redisHashKey)\n  // printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)\n\n  printLog.debug(\"phoneSet_1: \" + phoneMap)\n} finally {\n  if (jedis1 != null) {\n    printLog.debug(\"close jedis1\")\n    jedis1.close()\n  }\n}\n\n// 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除\nif (!phoneMap.isEmpty) {\n  val iterator: util.Iterator[Entry[String, String]] = phoneMap.entrySet().iterator()\n  while (iterator.hasNext) {\n    val eachPhoneKV: Entry[String, String] = iterator.next()\n    val mdn = eachPhoneKV.getKey\n    val redisValue = eachPhoneKV.getValue\n\n    var expirationDate: Int = 0\n    try {\n      expirationDate = redisValue.split(\"\\\\|\")(4).toInt\n      val today: Int = getNowDate.toInt\n      if (today > expirationDate) {\n        printLog.info(\"delete this data for expirationDate:\" + eachPhoneKV)\n        iterator.remove()\n        // phoneMap.remove(mdn) // 这一句是错误的，因为无法据此删除\n        var jedis2: Jedis = null\n        try {\n          jedis2 = sentinelPool.getResource\n          jedis2.hdel(redisHashKey, mdn)\n        } finally {\n          if (jedis2 != null) {\n            printLog.debug(\"close jedis2\")\n            jedis2.close()\n          }\n        }\n      }\n    } catch {\n      // 如果解析发生异常，则redis中删掉这个key，并且在phoneMap中同时删除\n      case ex: Exception => {\n        printLog.error(\"redis error data and del it: \" + eachPhoneKV + \" error: \" + ex)\n        iterator.remove()\n        var jedis4: Jedis = null\n        try {\n          jedis4 = sentinelPool.getResource\n          jedis4.hdel(redisHashKey, mdn)\n        } finally {\n          if (jedis4 != null) {\n            printLog.debug(\"close jedis4\")\n            jedis4.close()\n          }\n        }\n      }\n    }\n  }\n```\n然后测试，打包，部署，OK，解决。\n\n#### 原理说明\n\n遍历HashMap有三种方法，分别是: \n``` java\nfor(Map.Entry<Integer, String> entry : map.entrySet()){}  // scala中为 <-\n\nfor(Integer key : keySet){}\n\nIterator<Map.Entry<Integer, String>> it = map.entrySet().iterator();\n        while(it.hasNext()){}\n```\n其实上面的三种遍历方式从根本上讲都是使用的迭代器，之所以出现不同的结果是由于remove操作的实现不同决定的。\n\n首先前两种方法都在调用nextEntry方法的同一个地方抛出了异常，虽然remove成功了，但是在迭代器遍历下一个元素的时候抛出异常：\n``` java\nfinal Entry<K,V> nextEntry() {\n    if (modCount != expectedModCount)\n        throw new ConcurrentModificationException();\n    Entry<K,V> e = next;\n    ...\n}\n```\n这里modCount是表示map中的元素被修改了几次(在移除，新加元素时此值都会自增)，而expectedModCount是表示期望的修改次数，在迭代器构造的时候这两个值是相等，如果在遍历过程中这两个值出现了不同步就会抛出ConcurrentModificationException异常。\n\n1、HashMap的remove方法实现\n``` java\npublic V remove(Object key) {\n    Entry<K,V> e = removeEntryForKey(key);\n    return (e == null ? null : e.value);\n}\n```\n\n2、HashMap.KeySet的remove方法实现\n``` java\npublic boolean remove(Object o) {\n    return HashMap.this.removeEntryForKey(o) != null;\n}\n```\n\n3、HashMap.HashIterator的remove方法实现\n``` java\npublic void remove() {\n   if (current == null)\n        throw new IllegalStateException();\n   if (modCount != expectedModCount)\n        throw new ConcurrentModificationException();\n   Object k = current.key;\n   current = null;\n   HashMap.this.removeEntryForKey(k);\n   expectedModCount = modCount;\n}\n```\n以上三种实现方式都通过调用HashMap.removeEntryForKey方法来实现删除key的操作。在removeEntryForKey方法内只要移除了key modCount就会执行一次自增操作，此时modCount就与expectedModCount不一致了，上面三种remove实现中，只有第三种iterator的remove方法在调用完removeEntryForKey方法后同步了expectedModCount值与modCount相同，所以在遍历下个元素调用nextEntry方法时，iterator方式不会抛异常。\n\n``` java\nfinal Entry<K,V> removeEntryForKey(Object key) {\n    int hash = (key == null) ? 0 : hash(key.hashCode());\n    int i = indexFor(hash, table.length);\n    Entry<K,V> prev = table[i];\n    Entry<K,V> e = prev;\n\n    while (e != null) {\n        Entry<K,V> next = e.next;\n        Object k;\n        if (e.hash == hash &&\n            ((k = e.key) == key || (key != null && key.equals(k)))) {\n            modCount++;\n            size--;\n            if (prev == e)\n                table[i] = next;\n            else\n                prev.next = next;\n            e.recordRemoval(this);\n            return e;\n        }\n        prev = e;\n        e = next;\n    }\n\n    return e;\n}\n```\n\n#### 其它思考\n\n1、如果是遍历过程中增加或修改数据呢？\n增加或修改数据只能通过Map的put方法实现，在遍历过程中修改数据可以，但如果增加新key就会在下次循环时抛异常，因为在添加新key时modCount也会自增。\n\n2、有些集合类也有同样的遍历问题，如ArrayList，通过Iterator方式可正确遍历完成remove操作，直接调用list的remove方法就会抛异常。\n``` java\n//会抛ConcurrentModificationException异常\nfor(String str : list){\n\tlist.remove(str);\n}\n\n//正确遍历移除方式\nIterator<String> it = list.iterator();\nwhile(it.hasNext()){\n\tit.next();\n\tit.remove();\n}\n```\n\n3、jdk为什么这样设计，只允许通过iterator进行remove操作？\n\nHashMap和keySet的remove方法都可以通过传递key参数删除任意的元素，而iterator只能删除当前元素(current);\n对于通过HashMap的remove方法来说，一旦删除的元素是iterator对象中next所正在引用的，如果没有通过modCount与 expectedModCount的比较实现快速失败抛出异常，下次循环该元素将成为current指向，此时iterator就遍历了一个已移除的过期数据，所以一定要判断这两个值是否一致。\n\n4、这是一个坑，如果IDE能提示就好了，下次注意\n\n#### 引用\n\nhttp://afredlyj.github.io/posts/hashmap-concurrentmodificationexception.html\nhttp://dumbee.net/archives/41\nhttp://blog.csdn.net/wzy_1988/article/details/51423583","slug":"记HashMap遇到的java-util-ConcurrentModificationException的bug","published":1,"updated":"2016-11-24T08:22:20.615Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfvt0017psgu94s2e79t","content":"<h4 id=\"问题背景\"><a href=\"#问题背景\" class=\"headerlink\" title=\"问题背景\"></a>问题背景</h4><p>spark Streaming 实时程序在联调期间稳定运行了两天，以为问题不大了，第二天早上的时候打开一看，竟然挂了，定位到代码，原来我的程序实时读取redis的数据为一个HashMap，直到挂的时候，Redis中数据一直在增大，共 6083条：</p>\n<p>spark相关代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div></pre></td><td class=\"code\"><pre><div class=\"line\"> <span class=\"comment\">// 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除</span></div><div class=\"line\"> ...</div><div class=\"line\"><span class=\"keyword\">val</span> sentinelPool = <span class=\"type\">InternalRedisClient</span>.getSentinelPool</div><div class=\"line\"><span class=\"keyword\">var</span> phoneSet: util.<span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"keyword\">new</span> util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]()</div><div class=\"line\"><span class=\"comment\">//            printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">var</span> jedis1: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  jedis1 = sentinelPool.getResource</div><div class=\"line\">  <span class=\"comment\">//              printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\">  phoneSet = jedis1.hgetAll(redisHashKey)</div><div class=\"line\">  <span class=\"comment\">//              printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\">  printLog.debug(<span class=\"string\">\"phoneSet_1: \"</span> + phoneSet)</div><div class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (jedis1 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    printLog.debug(<span class=\"string\">\"close jedis1\"</span>)</div><div class=\"line\">    jedis1.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> (!phoneSet.isEmpty) &#123; </div><div class=\"line\">  <span class=\"keyword\">for</span> (eachPhoneKV: (<span class=\"type\">String</span>, <span class=\"type\">String</span>) &lt;- phoneSet) &#123; <span class=\"comment\">// 就在这里挂掉了</span></div><div class=\"line\">    <span class=\"keyword\">val</span> expirationDate: <span class=\"type\">Int</span> = eachPhoneKV._2.split(<span class=\"string\">\"\\\\|\"</span>)(<span class=\"number\">4</span>).toInt</div><div class=\"line\">    <span class=\"keyword\">val</span> today: <span class=\"type\">Int</span> = getNowDate.toInt</div><div class=\"line\">    <span class=\"keyword\">if</span> (today &gt; expirationDate) &#123;</div><div class=\"line\">      phoneSet.remove(eachPhoneKV._1)</div><div class=\"line\">      <span class=\"keyword\">var</span> jedis2: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\">      <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">        jedis2 = sentinelPool.getResource</div><div class=\"line\">        jedis2.hdel(redisHashKey, eachPhoneKV._1)</div><div class=\"line\">      &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (jedis2 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">          printLog.debug(<span class=\"string\">\"close jedis2\"</span>)</div><div class=\"line\">          jedis2.close()</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  printLog.debug(<span class=\"string\">\"phoneSet_filtedByData: \"</span> + phoneSet)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>具体异常粘信息如下：<br><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 4 times, most recent failure: Lost task 4.3 in stage 0.0 (TID 170, NM-304-HW-XH628V3-BIGDATA-063): java.util.ConcurrentModificationException</div><div class=\"line\">at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)</div><div class=\"line\">at java.util.HashMap$EntryIterator.next(HashMap.java:962)</div><div class=\"line\">at java.util.HashMap$EntryIterator.next(HashMap.java:960)</div><div class=\"line\">at scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:267)</div><div class=\"line\">at scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:264)</div><div class=\"line\">at scala.collection.Iterator$class.foreach(Iterator.scala:727)</div><div class=\"line\">at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)</div><div class=\"line\">at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)</div><div class=\"line\">at scala.collection.AbstractIterable.foreach(Iterable.scala:54)</div><div class=\"line\">at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)</div><div class=\"line\">at com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:871)</div><div class=\"line\">at com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:677)</div></pre></td></tr></table></figure></p>\n<h4 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h4><p>项目太紧张，来不及详细分析java的源码了，根据经验redis中应该六千多条数据应该不是很大的，HashMap完全可以一次读取，从网上查到原因是因为remove操作导致的，在Iterator遍历过程中调用HashMap的remove方法会crash，有两个解决办法：</p>\n<ol>\n<li>一个解决办法是用一个ArrayList记录要删除的key,然后再遍历这个ArrayList,调用HashMap的remove方法以ArrayList的元素为key进行删除；这个方法需要额外的空间和时间，虽然也浪费的不多，但总感觉不够优雅；</li>\n<li>创建一个Iterator<map.entry<integer, string=\"\">&gt; iterator = map.entrySet().iterator();，然后用这个 iterator.remove()方法进行删除，这个是可以删除的；</map.entry<integer,></li>\n</ol>\n<p>我使用的是方法二，修改后的代码如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> phoneMap: util.<span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"keyword\">new</span> util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]()</div><div class=\"line\"><span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">var</span> jedis1: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  jedis1 = sentinelPool.getResource</div><div class=\"line\">  <span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\">  phoneMap = jedis1.hgetAll(redisHashKey)</div><div class=\"line\">  <span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\">  printLog.debug(<span class=\"string\">\"phoneSet_1: \"</span> + phoneMap)</div><div class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (jedis1 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    printLog.debug(<span class=\"string\">\"close jedis1\"</span>)</div><div class=\"line\">    jedis1.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除</span></div><div class=\"line\"><span class=\"keyword\">if</span> (!phoneMap.isEmpty) &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> iterator: util.<span class=\"type\">Iterator</span>[<span class=\"type\">Entry</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]] = phoneMap.entrySet().iterator()</div><div class=\"line\">  <span class=\"keyword\">while</span> (iterator.hasNext) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> eachPhoneKV: <span class=\"type\">Entry</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = iterator.next()</div><div class=\"line\">    <span class=\"keyword\">val</span> mdn = eachPhoneKV.getKey</div><div class=\"line\">    <span class=\"keyword\">val</span> redisValue = eachPhoneKV.getValue</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">var</span> expirationDate: <span class=\"type\">Int</span> = <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">      expirationDate = redisValue.split(<span class=\"string\">\"\\\\|\"</span>)(<span class=\"number\">4</span>).toInt</div><div class=\"line\">      <span class=\"keyword\">val</span> today: <span class=\"type\">Int</span> = getNowDate.toInt</div><div class=\"line\">      <span class=\"keyword\">if</span> (today &gt; expirationDate) &#123;</div><div class=\"line\">        printLog.info(<span class=\"string\">\"delete this data for expirationDate:\"</span> + eachPhoneKV)</div><div class=\"line\">        iterator.remove()</div><div class=\"line\">        <span class=\"comment\">// phoneMap.remove(mdn) // 这一句是错误的，因为无法据此删除</span></div><div class=\"line\">        <span class=\"keyword\">var</span> jedis2: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">          jedis2 = sentinelPool.getResource</div><div class=\"line\">          jedis2.hdel(redisHashKey, mdn)</div><div class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">          <span class=\"keyword\">if</span> (jedis2 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">            printLog.debug(<span class=\"string\">\"close jedis2\"</span>)</div><div class=\"line\">            jedis2.close()</div><div class=\"line\">          &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">      <span class=\"comment\">// 如果解析发生异常，则redis中删掉这个key，并且在phoneMap中同时删除</span></div><div class=\"line\">      <span class=\"keyword\">case</span> ex: <span class=\"type\">Exception</span> =&gt; &#123;</div><div class=\"line\">        printLog.error(<span class=\"string\">\"redis error data and del it: \"</span> + eachPhoneKV + <span class=\"string\">\" error: \"</span> + ex)</div><div class=\"line\">        iterator.remove()</div><div class=\"line\">        <span class=\"keyword\">var</span> jedis4: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">          jedis4 = sentinelPool.getResource</div><div class=\"line\">          jedis4.hdel(redisHashKey, mdn)</div><div class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">          <span class=\"keyword\">if</span> (jedis4 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">            printLog.debug(<span class=\"string\">\"close jedis4\"</span>)</div><div class=\"line\">            jedis4.close()</div><div class=\"line\">          &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure></p>\n<p>然后测试，打包，部署，OK，解决。</p>\n<h4 id=\"原理说明\"><a href=\"#原理说明\" class=\"headerlink\" title=\"原理说明\"></a>原理说明</h4><p>遍历HashMap有三种方法，分别是:<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span>(Map.Entry&lt;Integer, String&gt; entry : map.entrySet())&#123;&#125;  <span class=\"comment\">// scala中为 &lt;-</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span>(Integer key : keySet)&#123;&#125;</div><div class=\"line\"></div><div class=\"line\">Iterator&lt;Map.Entry&lt;Integer, String&gt;&gt; it = map.entrySet().iterator();</div><div class=\"line\">        <span class=\"keyword\">while</span>(it.hasNext())&#123;&#125;</div></pre></td></tr></table></figure></p>\n<p>其实上面的三种遍历方式从根本上讲都是使用的迭代器，之所以出现不同的结果是由于remove操作的实现不同决定的。</p>\n<p>首先前两种方法都在调用nextEntry方法的同一个地方抛出了异常，虽然remove成功了，但是在迭代器遍历下一个元素的时候抛出异常：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> Entry&lt;K,V&gt; <span class=\"title\">nextEntry</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (modCount != expectedModCount)</div><div class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ConcurrentModificationException();</div><div class=\"line\">    Entry&lt;K,V&gt; e = next;</div><div class=\"line\">    ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>这里modCount是表示map中的元素被修改了几次(在移除，新加元素时此值都会自增)，而expectedModCount是表示期望的修改次数，在迭代器构造的时候这两个值是相等，如果在遍历过程中这两个值出现了不同步就会抛出ConcurrentModificationException异常。</p>\n<p>1、HashMap的remove方法实现<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> V <span class=\"title\">remove</span><span class=\"params\">(Object key)</span> </span>&#123;</div><div class=\"line\">    Entry&lt;K,V&gt; e = removeEntryForKey(key);</div><div class=\"line\">    <span class=\"keyword\">return</span> (e == <span class=\"keyword\">null</span> ? <span class=\"keyword\">null</span> : e.value);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>2、HashMap.KeySet的remove方法实现<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">remove</span><span class=\"params\">(Object o)</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> HashMap.<span class=\"keyword\">this</span>.removeEntryForKey(o) != <span class=\"keyword\">null</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>3、HashMap.HashIterator的remove方法实现<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">remove</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">   <span class=\"keyword\">if</span> (current == <span class=\"keyword\">null</span>)</div><div class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalStateException();</div><div class=\"line\">   <span class=\"keyword\">if</span> (modCount != expectedModCount)</div><div class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ConcurrentModificationException();</div><div class=\"line\">   Object k = current.key;</div><div class=\"line\">   current = <span class=\"keyword\">null</span>;</div><div class=\"line\">   HashMap.<span class=\"keyword\">this</span>.removeEntryForKey(k);</div><div class=\"line\">   expectedModCount = modCount;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>以上三种实现方式都通过调用HashMap.removeEntryForKey方法来实现删除key的操作。在removeEntryForKey方法内只要移除了key modCount就会执行一次自增操作，此时modCount就与expectedModCount不一致了，上面三种remove实现中，只有第三种iterator的remove方法在调用完removeEntryForKey方法后同步了expectedModCount值与modCount相同，所以在遍历下个元素调用nextEntry方法时，iterator方式不会抛异常。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> Entry&lt;K,V&gt; <span class=\"title\">removeEntryForKey</span><span class=\"params\">(Object key)</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">int</span> hash = (key == <span class=\"keyword\">null</span>) ? <span class=\"number\">0</span> : hash(key.hashCode());</div><div class=\"line\">    <span class=\"keyword\">int</span> i = indexFor(hash, table.length);</div><div class=\"line\">    Entry&lt;K,V&gt; prev = table[i];</div><div class=\"line\">    Entry&lt;K,V&gt; e = prev;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">while</span> (e != <span class=\"keyword\">null</span>) &#123;</div><div class=\"line\">        Entry&lt;K,V&gt; next = e.next;</div><div class=\"line\">        Object k;</div><div class=\"line\">        <span class=\"keyword\">if</span> (e.hash == hash &amp;&amp;</div><div class=\"line\">            ((k = e.key) == key || (key != <span class=\"keyword\">null</span> &amp;&amp; key.equals(k)))) &#123;</div><div class=\"line\">            modCount++;</div><div class=\"line\">            size--;</div><div class=\"line\">            <span class=\"keyword\">if</span> (prev == e)</div><div class=\"line\">                table[i] = next;</div><div class=\"line\">            <span class=\"keyword\">else</span></div><div class=\"line\">                prev.next = next;</div><div class=\"line\">            e.recordRemoval(<span class=\"keyword\">this</span>);</div><div class=\"line\">            <span class=\"keyword\">return</span> e;</div><div class=\"line\">        &#125;</div><div class=\"line\">        prev = e;</div><div class=\"line\">        e = next;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">return</span> e;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"其它思考\"><a href=\"#其它思考\" class=\"headerlink\" title=\"其它思考\"></a>其它思考</h4><p>1、如果是遍历过程中增加或修改数据呢？<br>增加或修改数据只能通过Map的put方法实现，在遍历过程中修改数据可以，但如果增加新key就会在下次循环时抛异常，因为在添加新key时modCount也会自增。</p>\n<p>2、有些集合类也有同样的遍历问题，如ArrayList，通过Iterator方式可正确遍历完成remove操作，直接调用list的remove方法就会抛异常。<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">//会抛ConcurrentModificationException异常</span></div><div class=\"line\"><span class=\"keyword\">for</span>(String str : list)&#123;</div><div class=\"line\">\tlist.remove(str);</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">//正确遍历移除方式</span></div><div class=\"line\">Iterator&lt;String&gt; it = list.iterator();</div><div class=\"line\"><span class=\"keyword\">while</span>(it.hasNext())&#123;</div><div class=\"line\">\tit.next();</div><div class=\"line\">\tit.remove();</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>3、jdk为什么这样设计，只允许通过iterator进行remove操作？</p>\n<p>HashMap和keySet的remove方法都可以通过传递key参数删除任意的元素，而iterator只能删除当前元素(current);<br>对于通过HashMap的remove方法来说，一旦删除的元素是iterator对象中next所正在引用的，如果没有通过modCount与 expectedModCount的比较实现快速失败抛出异常，下次循环该元素将成为current指向，此时iterator就遍历了一个已移除的过期数据，所以一定要判断这两个值是否一致。</p>\n<p>4、这是一个坑，如果IDE能提示就好了，下次注意</p>\n<h4 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h4><p><a href=\"http://afredlyj.github.io/posts/hashmap-concurrentmodificationexception.html\" target=\"_blank\" rel=\"external\">http://afredlyj.github.io/posts/hashmap-concurrentmodificationexception.html</a><br><a href=\"http://dumbee.net/archives/41\" target=\"_blank\" rel=\"external\">http://dumbee.net/archives/41</a><br><a href=\"http://blog.csdn.net/wzy_1988/article/details/51423583\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/wzy_1988/article/details/51423583</a></p>\n","excerpt":"","more":"<h4 id=\"问题背景\"><a href=\"#问题背景\" class=\"headerlink\" title=\"问题背景\"></a>问题背景</h4><p>spark Streaming 实时程序在联调期间稳定运行了两天，以为问题不大了，第二天早上的时候打开一看，竟然挂了，定位到代码，原来我的程序实时读取redis的数据为一个HashMap，直到挂的时候，Redis中数据一直在增大，共 6083条：</p>\n<p>spark相关代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div></pre></td><td class=\"code\"><pre><div class=\"line\"> <span class=\"comment\">// 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除</span></div><div class=\"line\"> ...</div><div class=\"line\"><span class=\"keyword\">val</span> sentinelPool = <span class=\"type\">InternalRedisClient</span>.getSentinelPool</div><div class=\"line\"><span class=\"keyword\">var</span> phoneSet: util.<span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"keyword\">new</span> util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]()</div><div class=\"line\"><span class=\"comment\">//            printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">var</span> jedis1: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  jedis1 = sentinelPool.getResource</div><div class=\"line\">  <span class=\"comment\">//              printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\">  phoneSet = jedis1.hgetAll(redisHashKey)</div><div class=\"line\">  <span class=\"comment\">//              printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\">  printLog.debug(<span class=\"string\">\"phoneSet_1: \"</span> + phoneSet)</div><div class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (jedis1 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    printLog.debug(<span class=\"string\">\"close jedis1\"</span>)</div><div class=\"line\">    jedis1.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> (!phoneSet.isEmpty) &#123; </div><div class=\"line\">  <span class=\"keyword\">for</span> (eachPhoneKV: (<span class=\"type\">String</span>, <span class=\"type\">String</span>) &lt;- phoneSet) &#123; <span class=\"comment\">// 就在这里挂掉了</span></div><div class=\"line\">    <span class=\"keyword\">val</span> expirationDate: <span class=\"type\">Int</span> = eachPhoneKV._2.split(<span class=\"string\">\"\\\\|\"</span>)(<span class=\"number\">4</span>).toInt</div><div class=\"line\">    <span class=\"keyword\">val</span> today: <span class=\"type\">Int</span> = getNowDate.toInt</div><div class=\"line\">    <span class=\"keyword\">if</span> (today &gt; expirationDate) &#123;</div><div class=\"line\">      phoneSet.remove(eachPhoneKV._1)</div><div class=\"line\">      <span class=\"keyword\">var</span> jedis2: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\">      <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">        jedis2 = sentinelPool.getResource</div><div class=\"line\">        jedis2.hdel(redisHashKey, eachPhoneKV._1)</div><div class=\"line\">      &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (jedis2 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">          printLog.debug(<span class=\"string\">\"close jedis2\"</span>)</div><div class=\"line\">          jedis2.close()</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  printLog.debug(<span class=\"string\">\"phoneSet_filtedByData: \"</span> + phoneSet)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>具体异常粘信息如下：<br><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 4 times, most recent failure: Lost task 4.3 in stage 0.0 (TID 170, NM-304-HW-XH628V3-BIGDATA-063): java.util.ConcurrentModificationException</div><div class=\"line\">at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)</div><div class=\"line\">at java.util.HashMap$EntryIterator.next(HashMap.java:962)</div><div class=\"line\">at java.util.HashMap$EntryIterator.next(HashMap.java:960)</div><div class=\"line\">at scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:267)</div><div class=\"line\">at scala.collection.convert.Wrappers$JMapWrapperLike$$anon$2.next(Wrappers.scala:264)</div><div class=\"line\">at scala.collection.Iterator$class.foreach(Iterator.scala:727)</div><div class=\"line\">at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)</div><div class=\"line\">at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)</div><div class=\"line\">at scala.collection.AbstractIterable.foreach(Iterable.scala:54)</div><div class=\"line\">at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)</div><div class=\"line\">at com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:871)</div><div class=\"line\">at com.chinatelecom.bigdata.oidd2.Location$$anonfun$2$$anonfun$3.apply(Location.scala:677)</div></pre></td></tr></table></figure></p>\n<h4 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h4><p>项目太紧张，来不及详细分析java的源码了，根据经验redis中应该六千多条数据应该不是很大的，HashMap完全可以一次读取，从网上查到原因是因为remove操作导致的，在Iterator遍历过程中调用HashMap的remove方法会crash，有两个解决办法：</p>\n<ol>\n<li>一个解决办法是用一个ArrayList记录要删除的key,然后再遍历这个ArrayList,调用HashMap的remove方法以ArrayList的元素为key进行删除；这个方法需要额外的空间和时间，虽然也浪费的不多，但总感觉不够优雅；</li>\n<li>创建一个Iterator<Map.Entry<Integer, String>&gt; iterator = map.entrySet().iterator();，然后用这个 iterator.remove()方法进行删除，这个是可以删除的；</li>\n</ol>\n<p>我使用的是方法二，修改后的代码如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> phoneMap: util.<span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"keyword\">new</span> util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]()</div><div class=\"line\"><span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">var</span> jedis1: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  jedis1 = sentinelPool.getResource</div><div class=\"line\">  <span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\">  phoneMap = jedis1.hgetAll(redisHashKey)</div><div class=\"line\">  <span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"></div><div class=\"line\">  printLog.debug(<span class=\"string\">\"phoneSet_1: \"</span> + phoneMap)</div><div class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (jedis1 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    printLog.debug(<span class=\"string\">\"close jedis1\"</span>)</div><div class=\"line\">    jedis1.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 1. 从L1中删除过期的号码，同时Redis中的对应该K-V也删除</span></div><div class=\"line\"><span class=\"keyword\">if</span> (!phoneMap.isEmpty) &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> iterator: util.<span class=\"type\">Iterator</span>[<span class=\"type\">Entry</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]] = phoneMap.entrySet().iterator()</div><div class=\"line\">  <span class=\"keyword\">while</span> (iterator.hasNext) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> eachPhoneKV: <span class=\"type\">Entry</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = iterator.next()</div><div class=\"line\">    <span class=\"keyword\">val</span> mdn = eachPhoneKV.getKey</div><div class=\"line\">    <span class=\"keyword\">val</span> redisValue = eachPhoneKV.getValue</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">var</span> expirationDate: <span class=\"type\">Int</span> = <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">      expirationDate = redisValue.split(<span class=\"string\">\"\\\\|\"</span>)(<span class=\"number\">4</span>).toInt</div><div class=\"line\">      <span class=\"keyword\">val</span> today: <span class=\"type\">Int</span> = getNowDate.toInt</div><div class=\"line\">      <span class=\"keyword\">if</span> (today &gt; expirationDate) &#123;</div><div class=\"line\">        printLog.info(<span class=\"string\">\"delete this data for expirationDate:\"</span> + eachPhoneKV)</div><div class=\"line\">        iterator.remove()</div><div class=\"line\">        <span class=\"comment\">// phoneMap.remove(mdn) // 这一句是错误的，因为无法据此删除</span></div><div class=\"line\">        <span class=\"keyword\">var</span> jedis2: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">          jedis2 = sentinelPool.getResource</div><div class=\"line\">          jedis2.hdel(redisHashKey, mdn)</div><div class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">          <span class=\"keyword\">if</span> (jedis2 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">            printLog.debug(<span class=\"string\">\"close jedis2\"</span>)</div><div class=\"line\">            jedis2.close()</div><div class=\"line\">          &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">      <span class=\"comment\">// 如果解析发生异常，则redis中删掉这个key，并且在phoneMap中同时删除</span></div><div class=\"line\">      <span class=\"keyword\">case</span> ex: <span class=\"type\">Exception</span> =&gt; &#123;</div><div class=\"line\">        printLog.error(<span class=\"string\">\"redis error data and del it: \"</span> + eachPhoneKV + <span class=\"string\">\" error: \"</span> + ex)</div><div class=\"line\">        iterator.remove()</div><div class=\"line\">        <span class=\"keyword\">var</span> jedis4: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">          jedis4 = sentinelPool.getResource</div><div class=\"line\">          jedis4.hdel(redisHashKey, mdn)</div><div class=\"line\">        &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">          <span class=\"keyword\">if</span> (jedis4 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">            printLog.debug(<span class=\"string\">\"close jedis4\"</span>)</div><div class=\"line\">            jedis4.close()</div><div class=\"line\">          &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure></p>\n<p>然后测试，打包，部署，OK，解决。</p>\n<h4 id=\"原理说明\"><a href=\"#原理说明\" class=\"headerlink\" title=\"原理说明\"></a>原理说明</h4><p>遍历HashMap有三种方法，分别是:<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span>(Map.Entry&lt;Integer, String&gt; entry : map.entrySet())&#123;&#125;  <span class=\"comment\">// scala中为 &lt;-</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">for</span>(Integer key : keySet)&#123;&#125;</div><div class=\"line\"></div><div class=\"line\">Iterator&lt;Map.Entry&lt;Integer, String&gt;&gt; it = map.entrySet().iterator();</div><div class=\"line\">        <span class=\"keyword\">while</span>(it.hasNext())&#123;&#125;</div></pre></td></tr></table></figure></p>\n<p>其实上面的三种遍历方式从根本上讲都是使用的迭代器，之所以出现不同的结果是由于remove操作的实现不同决定的。</p>\n<p>首先前两种方法都在调用nextEntry方法的同一个地方抛出了异常，虽然remove成功了，但是在迭代器遍历下一个元素的时候抛出异常：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> Entry&lt;K,V&gt; <span class=\"title\">nextEntry</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (modCount != expectedModCount)</div><div class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ConcurrentModificationException();</div><div class=\"line\">    Entry&lt;K,V&gt; e = next;</div><div class=\"line\">    ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>这里modCount是表示map中的元素被修改了几次(在移除，新加元素时此值都会自增)，而expectedModCount是表示期望的修改次数，在迭代器构造的时候这两个值是相等，如果在遍历过程中这两个值出现了不同步就会抛出ConcurrentModificationException异常。</p>\n<p>1、HashMap的remove方法实现<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> V <span class=\"title\">remove</span><span class=\"params\">(Object key)</span> </span>&#123;</div><div class=\"line\">    Entry&lt;K,V&gt; e = removeEntryForKey(key);</div><div class=\"line\">    <span class=\"keyword\">return</span> (e == <span class=\"keyword\">null</span> ? <span class=\"keyword\">null</span> : e.value);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>2、HashMap.KeySet的remove方法实现<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">remove</span><span class=\"params\">(Object o)</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> HashMap.<span class=\"keyword\">this</span>.removeEntryForKey(o) != <span class=\"keyword\">null</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>3、HashMap.HashIterator的remove方法实现<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">remove</span><span class=\"params\">()</span> </span>&#123;</div><div class=\"line\">   <span class=\"keyword\">if</span> (current == <span class=\"keyword\">null</span>)</div><div class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalStateException();</div><div class=\"line\">   <span class=\"keyword\">if</span> (modCount != expectedModCount)</div><div class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> ConcurrentModificationException();</div><div class=\"line\">   Object k = current.key;</div><div class=\"line\">   current = <span class=\"keyword\">null</span>;</div><div class=\"line\">   HashMap.<span class=\"keyword\">this</span>.removeEntryForKey(k);</div><div class=\"line\">   expectedModCount = modCount;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>以上三种实现方式都通过调用HashMap.removeEntryForKey方法来实现删除key的操作。在removeEntryForKey方法内只要移除了key modCount就会执行一次自增操作，此时modCount就与expectedModCount不一致了，上面三种remove实现中，只有第三种iterator的remove方法在调用完removeEntryForKey方法后同步了expectedModCount值与modCount相同，所以在遍历下个元素调用nextEntry方法时，iterator方式不会抛异常。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">final</span> Entry&lt;K,V&gt; <span class=\"title\">removeEntryForKey</span><span class=\"params\">(Object key)</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">int</span> hash = (key == <span class=\"keyword\">null</span>) ? <span class=\"number\">0</span> : hash(key.hashCode());</div><div class=\"line\">    <span class=\"keyword\">int</span> i = indexFor(hash, table.length);</div><div class=\"line\">    Entry&lt;K,V&gt; prev = table[i];</div><div class=\"line\">    Entry&lt;K,V&gt; e = prev;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">while</span> (e != <span class=\"keyword\">null</span>) &#123;</div><div class=\"line\">        Entry&lt;K,V&gt; next = e.next;</div><div class=\"line\">        Object k;</div><div class=\"line\">        <span class=\"keyword\">if</span> (e.hash == hash &amp;&amp;</div><div class=\"line\">            ((k = e.key) == key || (key != <span class=\"keyword\">null</span> &amp;&amp; key.equals(k)))) &#123;</div><div class=\"line\">            modCount++;</div><div class=\"line\">            size--;</div><div class=\"line\">            <span class=\"keyword\">if</span> (prev == e)</div><div class=\"line\">                table[i] = next;</div><div class=\"line\">            <span class=\"keyword\">else</span></div><div class=\"line\">                prev.next = next;</div><div class=\"line\">            e.recordRemoval(<span class=\"keyword\">this</span>);</div><div class=\"line\">            <span class=\"keyword\">return</span> e;</div><div class=\"line\">        &#125;</div><div class=\"line\">        prev = e;</div><div class=\"line\">        e = next;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">return</span> e;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"其它思考\"><a href=\"#其它思考\" class=\"headerlink\" title=\"其它思考\"></a>其它思考</h4><p>1、如果是遍历过程中增加或修改数据呢？<br>增加或修改数据只能通过Map的put方法实现，在遍历过程中修改数据可以，但如果增加新key就会在下次循环时抛异常，因为在添加新key时modCount也会自增。</p>\n<p>2、有些集合类也有同样的遍历问题，如ArrayList，通过Iterator方式可正确遍历完成remove操作，直接调用list的remove方法就会抛异常。<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">//会抛ConcurrentModificationException异常</span></div><div class=\"line\"><span class=\"keyword\">for</span>(String str : list)&#123;</div><div class=\"line\">\tlist.remove(str);</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">//正确遍历移除方式</span></div><div class=\"line\">Iterator&lt;String&gt; it = list.iterator();</div><div class=\"line\"><span class=\"keyword\">while</span>(it.hasNext())&#123;</div><div class=\"line\">\tit.next();</div><div class=\"line\">\tit.remove();</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>3、jdk为什么这样设计，只允许通过iterator进行remove操作？</p>\n<p>HashMap和keySet的remove方法都可以通过传递key参数删除任意的元素，而iterator只能删除当前元素(current);<br>对于通过HashMap的remove方法来说，一旦删除的元素是iterator对象中next所正在引用的，如果没有通过modCount与 expectedModCount的比较实现快速失败抛出异常，下次循环该元素将成为current指向，此时iterator就遍历了一个已移除的过期数据，所以一定要判断这两个值是否一致。</p>\n<p>4、这是一个坑，如果IDE能提示就好了，下次注意</p>\n<h4 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h4><p><a href=\"http://afredlyj.github.io/posts/hashmap-concurrentmodificationexception.html\">http://afredlyj.github.io/posts/hashmap-concurrentmodificationexception.html</a><br><a href=\"http://dumbee.net/archives/41\">http://dumbee.net/archives/41</a><br><a href=\"http://blog.csdn.net/wzy_1988/article/details/51423583\">http://blog.csdn.net/wzy_1988/article/details/51423583</a></p>\n"},{"title":"scala通过slick连接数据库","toc":true,"date":"2016-10-31T11:20:33.000Z","_content":"（持续更新）\n由于Spark是由scala语言开发的，scala语言可以使用到所有java语言中的特性，所以spark连接数据库（比如Mysql）有很多种方法，这里记录两种我使用到的高级用法以及一些教训，分别是：\n1. 使用Slick优雅地连接数据库；\n2. 如何使用SparkStreaming实时地获取数据库中的内容；\n3. 连接数据库过程中的踩坑集锦。\n\n## 使用Slick优雅地连接数据库\n\n如果使用scala语言，当然可以想到的是，通过java连接数据库的方式连接数据库是没有问题的，但是scala语言有没有自己更加优雅地方法连接数据库呢？答案是肯定的，非常推荐使用：Slick\n\n### Slick简介\nSlick 是 TypeSafe 推出的 Scala 数据库访问库。开发者可以使用 Scala 语言风格来编写数据查询，而不是用 SQL 。 Slick 对于 Scala 来说，有如 LINQ 至于 C#，或者类似于其它平台上的 ORM 系统，它使用应用使用数据库有如使用 Scala 内置的集合类型（比如列表，集合等）一样方便。当然如有需要你还是可以直接使用 SQL 语句来查询数据库。\n使用 Slick 而不直接使用 SQL 语句，可以使用编译器帮助发现一些类型错误，同时 Slick 可以为不同的后台数据库类型生成查询。它具有一些如下的特性：\n\n1. Scala \n\n所有查询，表格和字段映射，以及类型都采用普通的 Scala 语法。\n``` scala\nclass Coffees(tag: Tag) extends Table[(String, Double)](tag, \"COFFEES\") {\n    def name = column[String](\"COF_NAME\", O.PrimaryKey)\n    def price = column[Double](\"PRICE\")\n    def * = (name, price)\n}\nval coffees = TableQuery[Coffees]\n```\n数据访问接口类型 Scala 的集合类型\n``` scala\n// Query that only returns the \"name\" column\ncoffees.map(_.name)\n\n// Query that does a \"where price < 10.0\"\ncoffees.filter(_.price < 10.0)\n```\n\n2. 类型安全\n\n你使用的 IDE 可以帮助你写代码 在编译时而无需到运行时就可以发现一些错误\n``` scala\n// The result of \"select PRICE from COFFEES\" is a Seq of Double\n// because of the type safe column definitions\nval coffeeNames: Seq[Double] = coffees.map(_.price).list\n\n// Query builders are type safe:\ncoffees.filter(_.price < 10.0)\n// Using a string in the filter would result in a compilation error\n```\n\n3. 可以组合\n\n查询接口为函数，这些函数可以多次组合和重用。可以使用函数式的方式来访问数据库\n\n``` scala\n// Create a query for coffee names with a price less than 10, sorted by name\ncoffees.filter(_.price < 10.0).sortBy(_.name).map(_.name)\n// The generated SQL is equivalent to:\n// select name from COFFEES where PRICE < 10.0 order by NAME\n```\n\n4. 支持几乎所有常见的数据库\n\n\t- DB2 (via slick-extensions)\n\t- Derby/JavaDB\n\t- H2\n\t- HSQLDB/HyperSQL\n\t- Microsoft Access\n\t- Microsoft SQL Server (via slick-extensions)\n\t- MySQL\n\t- Oracle (via slick-extensions)\n\t- PostgreSQL\n\t- SQLite\n\n对于其它的一些数据库类型 Slick 也提供了有限的支持。\n\n关于Slick的具体教程以及API，可以参阅 \n[Slick官网](http://slick.lightbend.com/)\n[极客学院Slick中文教程](http://wiki.jikexueyuan.com/project/slick-guide/)\n以及google\n\n### 如何使用到Spark项目中\n以我自己摸索的方法为例，当然会有更多方法\n\n#### 配置文件中配置数据库连接信息\nSlick默认会读取项目顶层的配置文件，当然配置文件的路径可以手动指定，默认在顶层的配置文件路径下，我的配置文件在 `resources/application.conf`中：\n\n``` conf resources/application.conf\n// todo: 使其变成可以从外部文件载入\n// 开发环境Mysql\nmysql = {\n  url = \"jdbc:mysql://someIp:3306/someDb?useUnicode=true&characterEncoding=utf-8\"\n  driver = \"com.mysql.jdbc.Driver\"\n  connectionPool = disabled\n  keepAliveConnection = true\n  databaseName = \"someDb\"\n  user = \"user\"\n  password = \"password\"\n}\n```\n#### 定义数据库表格对应的case class\n\n比如：该AppFrame是我定义的一个应用框架的样例类，与数据库中的字段有对应关系\nPS：AppFrame的设置是为了将一切配置写到数据库中，这样可以实现项目的热切换，一套程序可以不需要重新编译而使用到不同的环境不同的策略中，亲测有效。\n\n``` scala bean/AppFrame.scala\n/**\n  * Author: wangxiaogang\n  * Date: 2016/10/1\n  * Email: wangxiaogang@chinatelecom.cn\n  * 应用的整体描述，包括 appId, app名称，输入类型，输入类型详细配置表，输出类型，输出类型详细配置表\n  */\ncase class AppFrame (\n               id: Int,\n               name: String,\n               inputStr: String,\n               // 代表对应的输入在所在类型的输入中的id\n               inputId: Int,\n               outputStr: String,\n               outId: Int,\n               redisStr: String,\n               redisId: Int,\n               sqlPoolStr: String,\n               sqlPoolId:Int\n               ){}\n```\n\n#### 使用Slick编写读取数据库的逻辑\n\n这里就是slick的优势，非常简单\n\n``` scala dao/MysqlDao.scala\n/**\n  * Author: wangxiaogang\n  * Date: 2016/9/29\n  * Email: wangxiaogang@chinatelecom.cn\n  * 与Mysql数据库的交互类，使用了Slick方式\n  */\nobject MysqlDao {\n  /**\n    * 通过appId获取到应用的整体描述，包括 appId, 输入类型，输入类型详细配置表，输出类型，输出类型详细配置表\n    * 这里简单试验大数据实时处理框架的构思是否可行\n    *\n    * @param appId\n    * @return\n    * @todo : 目前只配置输入类型与输出类型的配置，以后尽量把所有数据处理方案都能以配置的形式写入数据库中\n    */\n  def getAppFrame(appId: Int): AppFrame = {\n    implicit val getResult = GetResult(r =>\n      AppFrame(r.nextInt(), r.nextString(), r.nextString(), r.nextInt(), r.nextString(), r.nextInt(), r.nextString(),\n        r.nextInt(),  r.nextString(), r.nextInt()))\n    val q = sql\"\"\"SELECT * FROM appframe WHERE id = $appId\"\"\".as[AppFrame]\n\n    val db = Database.forConfig(\"mysql\")\n    try {\n      val fu = db.run(q)\n      Await.result(fu, 10 seconds).head\n    } finally {\n      db.close()\n    }\n  }\n  ...\n}\n```\n如上，就这么简单几行，就能连接数据库了，并且将其转化为对应的样例类，是不是超级好用\n当然，我只是使用了一点皮毛，它还有很多有用的特性我没有使用到。\n\n## 引用\nhttp://slick.lightbend.com/\nhttp://wiki.jikexueyuan.com/project/slick-guide/\n","source":"_posts/2016-10-31-scala通过slick连接数据库.md","raw":"---\ntitle: scala通过slick连接数据库\ntoc: true\ndate: 2016-10-31 19:20:33\ntags: \n- spark\n- scala\n- 持续更新\ncategories: \n- spark开发\n---\n（持续更新）\n由于Spark是由scala语言开发的，scala语言可以使用到所有java语言中的特性，所以spark连接数据库（比如Mysql）有很多种方法，这里记录两种我使用到的高级用法以及一些教训，分别是：\n1. 使用Slick优雅地连接数据库；\n2. 如何使用SparkStreaming实时地获取数据库中的内容；\n3. 连接数据库过程中的踩坑集锦。\n\n## 使用Slick优雅地连接数据库\n\n如果使用scala语言，当然可以想到的是，通过java连接数据库的方式连接数据库是没有问题的，但是scala语言有没有自己更加优雅地方法连接数据库呢？答案是肯定的，非常推荐使用：Slick\n\n### Slick简介\nSlick 是 TypeSafe 推出的 Scala 数据库访问库。开发者可以使用 Scala 语言风格来编写数据查询，而不是用 SQL 。 Slick 对于 Scala 来说，有如 LINQ 至于 C#，或者类似于其它平台上的 ORM 系统，它使用应用使用数据库有如使用 Scala 内置的集合类型（比如列表，集合等）一样方便。当然如有需要你还是可以直接使用 SQL 语句来查询数据库。\n使用 Slick 而不直接使用 SQL 语句，可以使用编译器帮助发现一些类型错误，同时 Slick 可以为不同的后台数据库类型生成查询。它具有一些如下的特性：\n\n1. Scala \n\n所有查询，表格和字段映射，以及类型都采用普通的 Scala 语法。\n``` scala\nclass Coffees(tag: Tag) extends Table[(String, Double)](tag, \"COFFEES\") {\n    def name = column[String](\"COF_NAME\", O.PrimaryKey)\n    def price = column[Double](\"PRICE\")\n    def * = (name, price)\n}\nval coffees = TableQuery[Coffees]\n```\n数据访问接口类型 Scala 的集合类型\n``` scala\n// Query that only returns the \"name\" column\ncoffees.map(_.name)\n\n// Query that does a \"where price < 10.0\"\ncoffees.filter(_.price < 10.0)\n```\n\n2. 类型安全\n\n你使用的 IDE 可以帮助你写代码 在编译时而无需到运行时就可以发现一些错误\n``` scala\n// The result of \"select PRICE from COFFEES\" is a Seq of Double\n// because of the type safe column definitions\nval coffeeNames: Seq[Double] = coffees.map(_.price).list\n\n// Query builders are type safe:\ncoffees.filter(_.price < 10.0)\n// Using a string in the filter would result in a compilation error\n```\n\n3. 可以组合\n\n查询接口为函数，这些函数可以多次组合和重用。可以使用函数式的方式来访问数据库\n\n``` scala\n// Create a query for coffee names with a price less than 10, sorted by name\ncoffees.filter(_.price < 10.0).sortBy(_.name).map(_.name)\n// The generated SQL is equivalent to:\n// select name from COFFEES where PRICE < 10.0 order by NAME\n```\n\n4. 支持几乎所有常见的数据库\n\n\t- DB2 (via slick-extensions)\n\t- Derby/JavaDB\n\t- H2\n\t- HSQLDB/HyperSQL\n\t- Microsoft Access\n\t- Microsoft SQL Server (via slick-extensions)\n\t- MySQL\n\t- Oracle (via slick-extensions)\n\t- PostgreSQL\n\t- SQLite\n\n对于其它的一些数据库类型 Slick 也提供了有限的支持。\n\n关于Slick的具体教程以及API，可以参阅 \n[Slick官网](http://slick.lightbend.com/)\n[极客学院Slick中文教程](http://wiki.jikexueyuan.com/project/slick-guide/)\n以及google\n\n### 如何使用到Spark项目中\n以我自己摸索的方法为例，当然会有更多方法\n\n#### 配置文件中配置数据库连接信息\nSlick默认会读取项目顶层的配置文件，当然配置文件的路径可以手动指定，默认在顶层的配置文件路径下，我的配置文件在 `resources/application.conf`中：\n\n``` conf resources/application.conf\n// todo: 使其变成可以从外部文件载入\n// 开发环境Mysql\nmysql = {\n  url = \"jdbc:mysql://someIp:3306/someDb?useUnicode=true&characterEncoding=utf-8\"\n  driver = \"com.mysql.jdbc.Driver\"\n  connectionPool = disabled\n  keepAliveConnection = true\n  databaseName = \"someDb\"\n  user = \"user\"\n  password = \"password\"\n}\n```\n#### 定义数据库表格对应的case class\n\n比如：该AppFrame是我定义的一个应用框架的样例类，与数据库中的字段有对应关系\nPS：AppFrame的设置是为了将一切配置写到数据库中，这样可以实现项目的热切换，一套程序可以不需要重新编译而使用到不同的环境不同的策略中，亲测有效。\n\n``` scala bean/AppFrame.scala\n/**\n  * Author: wangxiaogang\n  * Date: 2016/10/1\n  * Email: wangxiaogang@chinatelecom.cn\n  * 应用的整体描述，包括 appId, app名称，输入类型，输入类型详细配置表，输出类型，输出类型详细配置表\n  */\ncase class AppFrame (\n               id: Int,\n               name: String,\n               inputStr: String,\n               // 代表对应的输入在所在类型的输入中的id\n               inputId: Int,\n               outputStr: String,\n               outId: Int,\n               redisStr: String,\n               redisId: Int,\n               sqlPoolStr: String,\n               sqlPoolId:Int\n               ){}\n```\n\n#### 使用Slick编写读取数据库的逻辑\n\n这里就是slick的优势，非常简单\n\n``` scala dao/MysqlDao.scala\n/**\n  * Author: wangxiaogang\n  * Date: 2016/9/29\n  * Email: wangxiaogang@chinatelecom.cn\n  * 与Mysql数据库的交互类，使用了Slick方式\n  */\nobject MysqlDao {\n  /**\n    * 通过appId获取到应用的整体描述，包括 appId, 输入类型，输入类型详细配置表，输出类型，输出类型详细配置表\n    * 这里简单试验大数据实时处理框架的构思是否可行\n    *\n    * @param appId\n    * @return\n    * @todo : 目前只配置输入类型与输出类型的配置，以后尽量把所有数据处理方案都能以配置的形式写入数据库中\n    */\n  def getAppFrame(appId: Int): AppFrame = {\n    implicit val getResult = GetResult(r =>\n      AppFrame(r.nextInt(), r.nextString(), r.nextString(), r.nextInt(), r.nextString(), r.nextInt(), r.nextString(),\n        r.nextInt(),  r.nextString(), r.nextInt()))\n    val q = sql\"\"\"SELECT * FROM appframe WHERE id = $appId\"\"\".as[AppFrame]\n\n    val db = Database.forConfig(\"mysql\")\n    try {\n      val fu = db.run(q)\n      Await.result(fu, 10 seconds).head\n    } finally {\n      db.close()\n    }\n  }\n  ...\n}\n```\n如上，就这么简单几行，就能连接数据库了，并且将其转化为对应的样例类，是不是超级好用\n当然，我只是使用了一点皮毛，它还有很多有用的特性我没有使用到。\n\n## 引用\nhttp://slick.lightbend.com/\nhttp://wiki.jikexueyuan.com/project/slick-guide/\n","slug":"scala通过slick连接数据库","published":1,"updated":"2016-11-24T09:05:21.561Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfvw001bpsguv3sgqym2","content":"<p>（持续更新）<br>由于Spark是由scala语言开发的，scala语言可以使用到所有java语言中的特性，所以spark连接数据库（比如Mysql）有很多种方法，这里记录两种我使用到的高级用法以及一些教训，分别是：</p>\n<ol>\n<li>使用Slick优雅地连接数据库；</li>\n<li>如何使用SparkStreaming实时地获取数据库中的内容；</li>\n<li>连接数据库过程中的踩坑集锦。</li>\n</ol>\n<h2 id=\"使用Slick优雅地连接数据库\"><a href=\"#使用Slick优雅地连接数据库\" class=\"headerlink\" title=\"使用Slick优雅地连接数据库\"></a>使用Slick优雅地连接数据库</h2><p>如果使用scala语言，当然可以想到的是，通过java连接数据库的方式连接数据库是没有问题的，但是scala语言有没有自己更加优雅地方法连接数据库呢？答案是肯定的，非常推荐使用：Slick</p>\n<h3 id=\"Slick简介\"><a href=\"#Slick简介\" class=\"headerlink\" title=\"Slick简介\"></a>Slick简介</h3><p>Slick 是 TypeSafe 推出的 Scala 数据库访问库。开发者可以使用 Scala 语言风格来编写数据查询，而不是用 SQL 。 Slick 对于 Scala 来说，有如 LINQ 至于 C#，或者类似于其它平台上的 ORM 系统，它使用应用使用数据库有如使用 Scala 内置的集合类型（比如列表，集合等）一样方便。当然如有需要你还是可以直接使用 SQL 语句来查询数据库。<br>使用 Slick 而不直接使用 SQL 语句，可以使用编译器帮助发现一些类型错误，同时 Slick 可以为不同的后台数据库类型生成查询。它具有一些如下的特性：</p>\n<ol>\n<li>Scala </li>\n</ol>\n<p>所有查询，表格和字段映射，以及类型都采用普通的 Scala 语法。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Coffees</span>(<span class=\"params\">tag: <span class=\"type\">Tag</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Table</span>[(<span class=\"type\">String</span>, <span class=\"type\">Double</span>)](<span class=\"params\">tag, \"<span class=\"type\">COFFEES</span>\"</span>) </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">name</span> </span>= column[<span class=\"type\">String</span>](<span class=\"string\">\"COF_NAME\"</span>, <span class=\"type\">O</span>.<span class=\"type\">PrimaryKey</span>)</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">price</span> </span>= column[<span class=\"type\">Double</span>](<span class=\"string\">\"PRICE\"</span>)</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">*</span> </span>= (name, price)</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"keyword\">val</span> coffees = <span class=\"type\">TableQuery</span>[<span class=\"type\">Coffees</span>]</div></pre></td></tr></table></figure></p>\n<p>数据访问接口类型 Scala 的集合类型<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Query that only returns the \"name\" column</span></div><div class=\"line\">coffees.map(_.name)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Query that does a \"where price &lt; 10.0\"</span></div><div class=\"line\">coffees.filter(_.price &lt; <span class=\"number\">10.0</span>)</div></pre></td></tr></table></figure></p>\n<ol>\n<li>类型安全</li>\n</ol>\n<p>你使用的 IDE 可以帮助你写代码 在编译时而无需到运行时就可以发现一些错误<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// The result of \"select PRICE from COFFEES\" is a Seq of Double</span></div><div class=\"line\"><span class=\"comment\">// because of the type safe column definitions</span></div><div class=\"line\"><span class=\"keyword\">val</span> coffeeNames: <span class=\"type\">Seq</span>[<span class=\"type\">Double</span>] = coffees.map(_.price).list</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Query builders are type safe:</span></div><div class=\"line\">coffees.filter(_.price &lt; <span class=\"number\">10.0</span>)</div><div class=\"line\"><span class=\"comment\">// Using a string in the filter would result in a compilation error</span></div></pre></td></tr></table></figure></p>\n<ol>\n<li>可以组合</li>\n</ol>\n<p>查询接口为函数，这些函数可以多次组合和重用。可以使用函数式的方式来访问数据库</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Create a query for coffee names with a price less than 10, sorted by name</span></div><div class=\"line\">coffees.filter(_.price &lt; <span class=\"number\">10.0</span>).sortBy(_.name).map(_.name)</div><div class=\"line\"><span class=\"comment\">// The generated SQL is equivalent to:</span></div><div class=\"line\"><span class=\"comment\">// select name from COFFEES where PRICE &lt; 10.0 order by NAME</span></div></pre></td></tr></table></figure>\n<ol>\n<li><p>支持几乎所有常见的数据库</p>\n<ul>\n<li>DB2 (via slick-extensions)</li>\n<li>Derby/JavaDB</li>\n<li>H2</li>\n<li>HSQLDB/HyperSQL</li>\n<li>Microsoft Access</li>\n<li>Microsoft SQL Server (via slick-extensions)</li>\n<li>MySQL</li>\n<li>Oracle (via slick-extensions)</li>\n<li>PostgreSQL</li>\n<li>SQLite</li>\n</ul>\n</li>\n</ol>\n<p>对于其它的一些数据库类型 Slick 也提供了有限的支持。</p>\n<p>关于Slick的具体教程以及API，可以参阅<br><a href=\"http://slick.lightbend.com/\" target=\"_blank\" rel=\"external\">Slick官网</a><br><a href=\"http://wiki.jikexueyuan.com/project/slick-guide/\" target=\"_blank\" rel=\"external\">极客学院Slick中文教程</a><br>以及google</p>\n<h3 id=\"如何使用到Spark项目中\"><a href=\"#如何使用到Spark项目中\" class=\"headerlink\" title=\"如何使用到Spark项目中\"></a>如何使用到Spark项目中</h3><p>以我自己摸索的方法为例，当然会有更多方法</p>\n<h4 id=\"配置文件中配置数据库连接信息\"><a href=\"#配置文件中配置数据库连接信息\" class=\"headerlink\" title=\"配置文件中配置数据库连接信息\"></a>配置文件中配置数据库连接信息</h4><p>Slick默认会读取项目顶层的配置文件，当然配置文件的路径可以手动指定，默认在顶层的配置文件路径下，我的配置文件在 <code>resources/application.conf</code>中：</p>\n<figure class=\"highlight plain\"><figcaption><span>resources/application.conf</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">// todo: 使其变成可以从外部文件载入</div><div class=\"line\">// 开发环境Mysql</div><div class=\"line\">mysql = &#123;</div><div class=\"line\">  url = &quot;jdbc:mysql://someIp:3306/someDb?useUnicode=true&amp;characterEncoding=utf-8&quot;</div><div class=\"line\">  driver = &quot;com.mysql.jdbc.Driver&quot;</div><div class=\"line\">  connectionPool = disabled</div><div class=\"line\">  keepAliveConnection = true</div><div class=\"line\">  databaseName = &quot;someDb&quot;</div><div class=\"line\">  user = &quot;user&quot;</div><div class=\"line\">  password = &quot;password&quot;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"定义数据库表格对应的case-class\"><a href=\"#定义数据库表格对应的case-class\" class=\"headerlink\" title=\"定义数据库表格对应的case class\"></a>定义数据库表格对应的case class</h4><p>比如：该AppFrame是我定义的一个应用框架的样例类，与数据库中的字段有对应关系<br>PS：AppFrame的设置是为了将一切配置写到数据库中，这样可以实现项目的热切换，一套程序可以不需要重新编译而使用到不同的环境不同的策略中，亲测有效。</p>\n<figure class=\"highlight scala\"><figcaption><span>bean/AppFrame.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">  * Author: wangxiaogang</div><div class=\"line\">  * Date: 2016/10/1</div><div class=\"line\">  * Email: wangxiaogang@chinatelecom.cn</div><div class=\"line\">  * 应用的整体描述，包括 appId, app名称，输入类型，输入类型详细配置表，输出类型，输出类型详细配置表</div><div class=\"line\">  */</div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">AppFrame</span> (<span class=\"params\"></span></span></div><div class=\"line\">               id: <span class=\"type\">Int</span>,</div><div class=\"line\">               name: <span class=\"type\">String</span>,</div><div class=\"line\">               inputStr: <span class=\"type\">String</span>,</div><div class=\"line\">               // 代表对应的输入在所在类型的输入中的id</div><div class=\"line\">               inputId: <span class=\"type\">Int</span>,</div><div class=\"line\">               outputStr: <span class=\"type\">String</span>,</div><div class=\"line\">               outId: <span class=\"type\">Int</span>,</div><div class=\"line\">               redisStr: <span class=\"type\">String</span>,</div><div class=\"line\">               redisId: <span class=\"type\">Int</span>,</div><div class=\"line\">               sqlPoolStr: <span class=\"type\">String</span>,</div><div class=\"line\">               sqlPoolId:<span class=\"type\">Int</span></div><div class=\"line\">               )&#123;&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"使用Slick编写读取数据库的逻辑\"><a href=\"#使用Slick编写读取数据库的逻辑\" class=\"headerlink\" title=\"使用Slick编写读取数据库的逻辑\"></a>使用Slick编写读取数据库的逻辑</h4><p>这里就是slick的优势，非常简单</p>\n<figure class=\"highlight scala\"><figcaption><span>dao/MysqlDao.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">  * Author: wangxiaogang</div><div class=\"line\">  * Date: 2016/9/29</div><div class=\"line\">  * Email: wangxiaogang@chinatelecom.cn</div><div class=\"line\">  * 与Mysql数据库的交互类，使用了Slick方式</div><div class=\"line\">  */</div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">MysqlDao</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">/**</span></div><div class=\"line\">    * 通过appId获取到应用的整体描述，包括 appId, 输入类型，输入类型详细配置表，输出类型，输出类型详细配置表</div><div class=\"line\">    * 这里简单试验大数据实时处理框架的构思是否可行</div><div class=\"line\">    *</div><div class=\"line\">    * @param appId</div><div class=\"line\">    * @return</div><div class=\"line\">    * @todo : 目前只配置输入类型与输出类型的配置，以后尽量把所有数据处理方案都能以配置的形式写入数据库中</div><div class=\"line\">    */</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getAppFrame</span></span>(appId: <span class=\"type\">Int</span>): <span class=\"type\">AppFrame</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">implicit</span> <span class=\"keyword\">val</span> getResult = <span class=\"type\">GetResult</span>(r =&gt;</div><div class=\"line\">      <span class=\"type\">AppFrame</span>(r.nextInt(), r.nextString(), r.nextString(), r.nextInt(), r.nextString(), r.nextInt(), r.nextString(),</div><div class=\"line\">        r.nextInt(),  r.nextString(), r.nextInt()))</div><div class=\"line\">    <span class=\"keyword\">val</span> q = <span class=\"string\">sql\"\"</span><span class=\"string\">\"SELECT * FROM appframe WHERE id = $appId\"</span><span class=\"string\">\"\"</span>.as[<span class=\"type\">AppFrame</span>]</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> db = <span class=\"type\">Database</span>.forConfig(<span class=\"string\">\"mysql\"</span>)</div><div class=\"line\">    <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> fu = db.run(q)</div><div class=\"line\">      <span class=\"type\">Await</span>.result(fu, <span class=\"number\">10</span> seconds).head</div><div class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">      db.close()</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>如上，就这么简单几行，就能连接数据库了，并且将其转化为对应的样例类，是不是超级好用<br>当然，我只是使用了一点皮毛，它还有很多有用的特性我没有使用到。</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"http://slick.lightbend.com/\" target=\"_blank\" rel=\"external\">http://slick.lightbend.com/</a><br><a href=\"http://wiki.jikexueyuan.com/project/slick-guide/\" target=\"_blank\" rel=\"external\">http://wiki.jikexueyuan.com/project/slick-guide/</a></p>\n","excerpt":"","more":"<p>（持续更新）<br>由于Spark是由scala语言开发的，scala语言可以使用到所有java语言中的特性，所以spark连接数据库（比如Mysql）有很多种方法，这里记录两种我使用到的高级用法以及一些教训，分别是：</p>\n<ol>\n<li>使用Slick优雅地连接数据库；</li>\n<li>如何使用SparkStreaming实时地获取数据库中的内容；</li>\n<li>连接数据库过程中的踩坑集锦。</li>\n</ol>\n<h2 id=\"使用Slick优雅地连接数据库\"><a href=\"#使用Slick优雅地连接数据库\" class=\"headerlink\" title=\"使用Slick优雅地连接数据库\"></a>使用Slick优雅地连接数据库</h2><p>如果使用scala语言，当然可以想到的是，通过java连接数据库的方式连接数据库是没有问题的，但是scala语言有没有自己更加优雅地方法连接数据库呢？答案是肯定的，非常推荐使用：Slick</p>\n<h3 id=\"Slick简介\"><a href=\"#Slick简介\" class=\"headerlink\" title=\"Slick简介\"></a>Slick简介</h3><p>Slick 是 TypeSafe 推出的 Scala 数据库访问库。开发者可以使用 Scala 语言风格来编写数据查询，而不是用 SQL 。 Slick 对于 Scala 来说，有如 LINQ 至于 C#，或者类似于其它平台上的 ORM 系统，它使用应用使用数据库有如使用 Scala 内置的集合类型（比如列表，集合等）一样方便。当然如有需要你还是可以直接使用 SQL 语句来查询数据库。<br>使用 Slick 而不直接使用 SQL 语句，可以使用编译器帮助发现一些类型错误，同时 Slick 可以为不同的后台数据库类型生成查询。它具有一些如下的特性：</p>\n<ol>\n<li>Scala </li>\n</ol>\n<p>所有查询，表格和字段映射，以及类型都采用普通的 Scala 语法。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Coffees</span>(<span class=\"params\">tag: <span class=\"type\">Tag</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Table</span>[(<span class=\"type\">String</span>, <span class=\"type\">Double</span>)](<span class=\"params\">tag, \"<span class=\"type\">COFFEES</span>\"</span>) </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">name</span> </span>= column[<span class=\"type\">String</span>](<span class=\"string\">\"COF_NAME\"</span>, <span class=\"type\">O</span>.<span class=\"type\">PrimaryKey</span>)</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">price</span> </span>= column[<span class=\"type\">Double</span>](<span class=\"string\">\"PRICE\"</span>)</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">*</span> </span>= (name, price)</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"keyword\">val</span> coffees = <span class=\"type\">TableQuery</span>[<span class=\"type\">Coffees</span>]</div></pre></td></tr></table></figure></p>\n<p>数据访问接口类型 Scala 的集合类型<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Query that only returns the \"name\" column</span></div><div class=\"line\">coffees.map(_.name)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Query that does a \"where price &lt; 10.0\"</span></div><div class=\"line\">coffees.filter(_.price &lt; <span class=\"number\">10.0</span>)</div></pre></td></tr></table></figure></p>\n<ol>\n<li>类型安全</li>\n</ol>\n<p>你使用的 IDE 可以帮助你写代码 在编译时而无需到运行时就可以发现一些错误<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// The result of \"select PRICE from COFFEES\" is a Seq of Double</span></div><div class=\"line\"><span class=\"comment\">// because of the type safe column definitions</span></div><div class=\"line\"><span class=\"keyword\">val</span> coffeeNames: <span class=\"type\">Seq</span>[<span class=\"type\">Double</span>] = coffees.map(_.price).list</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Query builders are type safe:</span></div><div class=\"line\">coffees.filter(_.price &lt; <span class=\"number\">10.0</span>)</div><div class=\"line\"><span class=\"comment\">// Using a string in the filter would result in a compilation error</span></div></pre></td></tr></table></figure></p>\n<ol>\n<li>可以组合</li>\n</ol>\n<p>查询接口为函数，这些函数可以多次组合和重用。可以使用函数式的方式来访问数据库</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Create a query for coffee names with a price less than 10, sorted by name</span></div><div class=\"line\">coffees.filter(_.price &lt; <span class=\"number\">10.0</span>).sortBy(_.name).map(_.name)</div><div class=\"line\"><span class=\"comment\">// The generated SQL is equivalent to:</span></div><div class=\"line\"><span class=\"comment\">// select name from COFFEES where PRICE &lt; 10.0 order by NAME</span></div></pre></td></tr></table></figure>\n<ol>\n<li><p>支持几乎所有常见的数据库</p>\n<ul>\n<li>DB2 (via slick-extensions)</li>\n<li>Derby/JavaDB</li>\n<li>H2</li>\n<li>HSQLDB/HyperSQL</li>\n<li>Microsoft Access</li>\n<li>Microsoft SQL Server (via slick-extensions)</li>\n<li>MySQL</li>\n<li>Oracle (via slick-extensions)</li>\n<li>PostgreSQL</li>\n<li>SQLite</li>\n</ul>\n</li>\n</ol>\n<p>对于其它的一些数据库类型 Slick 也提供了有限的支持。</p>\n<p>关于Slick的具体教程以及API，可以参阅<br><a href=\"http://slick.lightbend.com/\">Slick官网</a><br><a href=\"http://wiki.jikexueyuan.com/project/slick-guide/\">极客学院Slick中文教程</a><br>以及google</p>\n<h3 id=\"如何使用到Spark项目中\"><a href=\"#如何使用到Spark项目中\" class=\"headerlink\" title=\"如何使用到Spark项目中\"></a>如何使用到Spark项目中</h3><p>以我自己摸索的方法为例，当然会有更多方法</p>\n<h4 id=\"配置文件中配置数据库连接信息\"><a href=\"#配置文件中配置数据库连接信息\" class=\"headerlink\" title=\"配置文件中配置数据库连接信息\"></a>配置文件中配置数据库连接信息</h4><p>Slick默认会读取项目顶层的配置文件，当然配置文件的路径可以手动指定，默认在顶层的配置文件路径下，我的配置文件在 <code>resources/application.conf</code>中：</p>\n<figure class=\"highlight plain\"><figcaption><span>resources/application.conf</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">// todo: 使其变成可以从外部文件载入</div><div class=\"line\">// 开发环境Mysql</div><div class=\"line\">mysql = &#123;</div><div class=\"line\">  url = &quot;jdbc:mysql://someIp:3306/someDb?useUnicode=true&amp;characterEncoding=utf-8&quot;</div><div class=\"line\">  driver = &quot;com.mysql.jdbc.Driver&quot;</div><div class=\"line\">  connectionPool = disabled</div><div class=\"line\">  keepAliveConnection = true</div><div class=\"line\">  databaseName = &quot;someDb&quot;</div><div class=\"line\">  user = &quot;user&quot;</div><div class=\"line\">  password = &quot;password&quot;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"定义数据库表格对应的case-class\"><a href=\"#定义数据库表格对应的case-class\" class=\"headerlink\" title=\"定义数据库表格对应的case class\"></a>定义数据库表格对应的case class</h4><p>比如：该AppFrame是我定义的一个应用框架的样例类，与数据库中的字段有对应关系<br>PS：AppFrame的设置是为了将一切配置写到数据库中，这样可以实现项目的热切换，一套程序可以不需要重新编译而使用到不同的环境不同的策略中，亲测有效。</p>\n<figure class=\"highlight scala\"><figcaption><span>bean/AppFrame.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">  * Author: wangxiaogang</div><div class=\"line\">  * Date: 2016/10/1</div><div class=\"line\">  * Email: wangxiaogang@chinatelecom.cn</div><div class=\"line\">  * 应用的整体描述，包括 appId, app名称，输入类型，输入类型详细配置表，输出类型，输出类型详细配置表</div><div class=\"line\">  */</span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">AppFrame</span> (<span class=\"params\"></div><div class=\"line\">               id: <span class=\"type\">Int</span>,</div><div class=\"line\">               name: <span class=\"type\">String</span>,</div><div class=\"line\">               inputStr: <span class=\"type\">String</span>,</div><div class=\"line\">               // 代表对应的输入在所在类型的输入中的id</div><div class=\"line\">               inputId: <span class=\"type\">Int</span>,</div><div class=\"line\">               outputStr: <span class=\"type\">String</span>,</div><div class=\"line\">               outId: <span class=\"type\">Int</span>,</div><div class=\"line\">               redisStr: <span class=\"type\">String</span>,</div><div class=\"line\">               redisId: <span class=\"type\">Int</span>,</div><div class=\"line\">               sqlPoolStr: <span class=\"type\">String</span>,</div><div class=\"line\">               sqlPoolId:<span class=\"type\">Int</span></div><div class=\"line\">               </span>)</span>&#123;&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"使用Slick编写读取数据库的逻辑\"><a href=\"#使用Slick编写读取数据库的逻辑\" class=\"headerlink\" title=\"使用Slick编写读取数据库的逻辑\"></a>使用Slick编写读取数据库的逻辑</h4><p>这里就是slick的优势，非常简单</p>\n<figure class=\"highlight scala\"><figcaption><span>dao/MysqlDao.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">  * Author: wangxiaogang</div><div class=\"line\">  * Date: 2016/9/29</div><div class=\"line\">  * Email: wangxiaogang@chinatelecom.cn</div><div class=\"line\">  * 与Mysql数据库的交互类，使用了Slick方式</div><div class=\"line\">  */</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">MysqlDao</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">/**</div><div class=\"line\">    * 通过appId获取到应用的整体描述，包括 appId, 输入类型，输入类型详细配置表，输出类型，输出类型详细配置表</div><div class=\"line\">    * 这里简单试验大数据实时处理框架的构思是否可行</div><div class=\"line\">    *</div><div class=\"line\">    * @param appId</div><div class=\"line\">    * @return</div><div class=\"line\">    * @todo : 目前只配置输入类型与输出类型的配置，以后尽量把所有数据处理方案都能以配置的形式写入数据库中</div><div class=\"line\">    */</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getAppFrame</span></span>(appId: <span class=\"type\">Int</span>): <span class=\"type\">AppFrame</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">implicit</span> <span class=\"keyword\">val</span> getResult = <span class=\"type\">GetResult</span>(r =&gt;</div><div class=\"line\">      <span class=\"type\">AppFrame</span>(r.nextInt(), r.nextString(), r.nextString(), r.nextInt(), r.nextString(), r.nextInt(), r.nextString(),</div><div class=\"line\">        r.nextInt(),  r.nextString(), r.nextInt()))</div><div class=\"line\">    <span class=\"keyword\">val</span> q = <span class=\"string\">sql\"\"</span><span class=\"string\">\"SELECT * FROM appframe WHERE id = $appId\"</span><span class=\"string\">\"\"</span>.as[<span class=\"type\">AppFrame</span>]</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> db = <span class=\"type\">Database</span>.forConfig(<span class=\"string\">\"mysql\"</span>)</div><div class=\"line\">    <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> fu = db.run(q)</div><div class=\"line\">      <span class=\"type\">Await</span>.result(fu, <span class=\"number\">10</span> seconds).head</div><div class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">      db.close()</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>如上，就这么简单几行，就能连接数据库了，并且将其转化为对应的样例类，是不是超级好用<br>当然，我只是使用了一点皮毛，它还有很多有用的特性我没有使用到。</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"http://slick.lightbend.com/\">http://slick.lightbend.com/</a><br><a href=\"http://wiki.jikexueyuan.com/project/slick-guide/\">http://wiki.jikexueyuan.com/project/slick-guide/</a></p>\n"},{"title":"Spark踩坑之Streaming程序实时读写哨兵模式的Redis","toc":true,"date":"2016-11-24T02:51:50.000Z","_content":"\n## 背景\nSpark Streaming程序使用Redis保存出现在景区中的用户，用以识别用户是否是第一次进入景区，要分别进行读操作和写操作，为了程序的高可用性，Redis我们使用的是哨兵模式（满满的，都是坑）；\n\nRedis哨兵模式，顾名思义，就是Redis有三台机器作为一主两备，然后有三个哨兵（三个端口），告诉你Master是哪一台，然后你再去访问master，这样的话，一台机器由于负载发生了主备切换，对我们来说，对外的端口是统一的（哨兵）；\n\n我们Redis的版本是3.2.4，连接Redis，我们的程序使用了开源的jedis作为redis的连接器，jedis的版本是2.8.1，关于jedis的使用，我参考的是jedis源码的单元测试程序，这个可以在github中找到。\n\n## 踩坑历程\n\n### hgetAll不能读取太大量的数据\n在设计程序的时候，我们使用了hash作为数据的存储结构，该hash可以存储40亿条记录，一开始，为了减少对Redis的压力，我们选择的策略是对于每个batch，我们都统一读取全部的数据，然后在内存中筛选计算的方法，可以极大减少对redis的访问；\n\n``` scala\nval sentinelPool = InternalRedisClient.getSentinelPool\nvar phoneMap: util.Map[String, String] = new util.HashMap[String, String]()\n// printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)\nvar jedis1: Jedis = null\ntry {\n  jedis1 = sentinelPool.getResource\n  // printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)\n  // 这里当redis中该HSet数据量很大的时候（五十万条），一次读取需要很长时间，对性能影响很大，\n  // 故当数据条数大于一定值的时候，不用此方法\n  phoneMap = jedis1.hgetAll(redisHashKey)\n  // printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)\n  printLog.debug(\"phoneSet_1: \" + phoneMap)\n} finally {\n  if (jedis1 != null) {\n    printLog.debug(\"close jedis1\")\n    jedis1.close()\n  }\n}\n```\n\n```scala\n\n但是当程序运行一段时间后，发现延迟很高，我们也没有想到数据量会有这么大，发现当数据量超过10W条这个级别后，每次读取全部数据的耗时将会很大，而且对redis的压力反而变大了，所以我们弃用了一次读取所有数据的方法，但是，如果数据量少与10万条级别的话，一次读取，也是值得考虑的；\n\n### jedis的sentinel连接池在spark Streaming中连接池无法释放的问题\n\n我们一开始采用的是通过jedis的sentinel连接池的方法连接redis；\n在连接哨兵模式的redis的时候，由于SparkStreaming程序中每个container并不会关闭，导致在spark的transform方法中，jedis sentinel 连接池在Streaming下出现连接池资源释放不了的bug，为了解决这个问题，我们弃用了jedis的sentinel连接池，每10秒手动询问redis的哨兵Master的地址，然后手动与redis进行连接，最终解决了这个问题。\n手动实现jedis sentinel的连接的代码如下：\n\n``` scala\n// 由于redis sentinel 建立连接池不释放的坑，最后弃用连接池，自己实现寻找master的逻辑\nval masterHostPort = getRedisMasterHostPortList(redisConf)\nval masterHost = masterHostPort(0)\nval masterPort = masterHostPort(1).toInt\n...\n// 得到master后，就可以直接建立redis连接了\nval paRdd3: Iterator[(String, (String, String))] = paRdd2.flatMap { eachKV =>\n。。。\n\tvar jedis1: Jedis = null\n\tvar redis1Val: String = null\n\ttry {\n\t  jedis1 = new Jedis(masterHost, masterPort)\n\t  redis1Val = jedis1.hget(redisHashKey, mdn)\n\t} catch {\n\t  case e: Exception => {\n\t    printLog.error(\"jedis1 error：e\" + e)\n\t  }\n\t} finally {\n\t  if (jedis1 != null) {\n\t    jedis1.close()\n\t  }\n\t}\n}\n\n```\n询问哨兵Master在哪里的代码如下：\n\n``` scala\n/**\n    * 得到redis sentinel 模式的master信息\n    *\n    * @param redisConf\n    * @return\n    */\n  def getRedisMasterHostPortList(redisConf: RedisConf): util.List[String] = {\n    printLog.debug(\"Trying to find master from available Sentinels...\")\n    //    @transient var masterFound: Boolean = false\n    var masterFound: Boolean = false\n    val sentinelList: Array[String] = redisConf.sentinels.split(\"\\\\|\")\n    for (sentinel <- sentinelList if !masterFound) {\n      var jedis: Jedis = null\n      try {\n        jedis = new Jedis(sentinel.split(\":\")(0), sentinel.split(\":\")(1).toInt)\n        val masterAddr: util.List[String] = jedis.sentinelGetMasterAddrByName(redisConf.masterName)\n        // connected to sentinel...\n        if (masterAddr == null || masterAddr.size != 2) {\n          printLog.error(\"Can not get master addr, master name: \" + redisConf.masterName + \". Sentinel: \" + sentinel + \".\")\n        } else {\n          masterFound = true\n          printLog.debug(\"masterAddr: \" + masterAddr)\n          return masterAddr\n        }\n      } catch {\n        case e: JedisException => {\n          // resolves #1036, it should handle JedisException there's another chance\n          // of raising JedisDataException\n          printLog.error(\"Cannot get master address from sentinel running @ \" + sentinel + \". Reason: \" + e + \". Trying next one.\")\n        }\n      } finally {\n        if (jedis != null) {\n          jedis.close()\n        }\n      }\n    }\n    return null\n  }\n```\n\n### Redis的压力负载\n\n后来我们对程序进行了加压测试，发现当压力增大后redis的主备切换非常频繁，原来在redis中，slaver会定时跟master通信，询问其是否健康，如果不健康，slaver就会强制进行主备切换，当redis访问频繁的时候，master来不及及时响应slaver的请求，就会导致主备切换频繁，解决这个问题的方法是修改询问时间，默认是 5s，适度调高就可以了。\n\n当然redis还有一些其它优化参数，这里不做讨论；目前可以支持四千万的数据，\n\n## 引用\nhttps://github.com/xetorthio/jedis","source":"_posts/2016-11-24-Spark踩坑之Streaming程序实时读写哨兵模式的Redis.md","raw":"---\ntitle: Spark踩坑之Streaming程序实时读写哨兵模式的Redis\ntoc: true\ndate: 2016-11-24 10:51:50\ntags: \n- spark streaming\n- redis\n- spark开发\ncategories: spark开发\n---\n\n## 背景\nSpark Streaming程序使用Redis保存出现在景区中的用户，用以识别用户是否是第一次进入景区，要分别进行读操作和写操作，为了程序的高可用性，Redis我们使用的是哨兵模式（满满的，都是坑）；\n\nRedis哨兵模式，顾名思义，就是Redis有三台机器作为一主两备，然后有三个哨兵（三个端口），告诉你Master是哪一台，然后你再去访问master，这样的话，一台机器由于负载发生了主备切换，对我们来说，对外的端口是统一的（哨兵）；\n\n我们Redis的版本是3.2.4，连接Redis，我们的程序使用了开源的jedis作为redis的连接器，jedis的版本是2.8.1，关于jedis的使用，我参考的是jedis源码的单元测试程序，这个可以在github中找到。\n\n## 踩坑历程\n\n### hgetAll不能读取太大量的数据\n在设计程序的时候，我们使用了hash作为数据的存储结构，该hash可以存储40亿条记录，一开始，为了减少对Redis的压力，我们选择的策略是对于每个batch，我们都统一读取全部的数据，然后在内存中筛选计算的方法，可以极大减少对redis的访问；\n\n``` scala\nval sentinelPool = InternalRedisClient.getSentinelPool\nvar phoneMap: util.Map[String, String] = new util.HashMap[String, String]()\n// printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)\nvar jedis1: Jedis = null\ntry {\n  jedis1 = sentinelPool.getResource\n  // printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)\n  // 这里当redis中该HSet数据量很大的时候（五十万条），一次读取需要很长时间，对性能影响很大，\n  // 故当数据条数大于一定值的时候，不用此方法\n  phoneMap = jedis1.hgetAll(redisHashKey)\n  // printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)\n  printLog.debug(\"phoneSet_1: \" + phoneMap)\n} finally {\n  if (jedis1 != null) {\n    printLog.debug(\"close jedis1\")\n    jedis1.close()\n  }\n}\n```\n\n```scala\n\n但是当程序运行一段时间后，发现延迟很高，我们也没有想到数据量会有这么大，发现当数据量超过10W条这个级别后，每次读取全部数据的耗时将会很大，而且对redis的压力反而变大了，所以我们弃用了一次读取所有数据的方法，但是，如果数据量少与10万条级别的话，一次读取，也是值得考虑的；\n\n### jedis的sentinel连接池在spark Streaming中连接池无法释放的问题\n\n我们一开始采用的是通过jedis的sentinel连接池的方法连接redis；\n在连接哨兵模式的redis的时候，由于SparkStreaming程序中每个container并不会关闭，导致在spark的transform方法中，jedis sentinel 连接池在Streaming下出现连接池资源释放不了的bug，为了解决这个问题，我们弃用了jedis的sentinel连接池，每10秒手动询问redis的哨兵Master的地址，然后手动与redis进行连接，最终解决了这个问题。\n手动实现jedis sentinel的连接的代码如下：\n\n``` scala\n// 由于redis sentinel 建立连接池不释放的坑，最后弃用连接池，自己实现寻找master的逻辑\nval masterHostPort = getRedisMasterHostPortList(redisConf)\nval masterHost = masterHostPort(0)\nval masterPort = masterHostPort(1).toInt\n...\n// 得到master后，就可以直接建立redis连接了\nval paRdd3: Iterator[(String, (String, String))] = paRdd2.flatMap { eachKV =>\n。。。\n\tvar jedis1: Jedis = null\n\tvar redis1Val: String = null\n\ttry {\n\t  jedis1 = new Jedis(masterHost, masterPort)\n\t  redis1Val = jedis1.hget(redisHashKey, mdn)\n\t} catch {\n\t  case e: Exception => {\n\t    printLog.error(\"jedis1 error：e\" + e)\n\t  }\n\t} finally {\n\t  if (jedis1 != null) {\n\t    jedis1.close()\n\t  }\n\t}\n}\n\n```\n询问哨兵Master在哪里的代码如下：\n\n``` scala\n/**\n    * 得到redis sentinel 模式的master信息\n    *\n    * @param redisConf\n    * @return\n    */\n  def getRedisMasterHostPortList(redisConf: RedisConf): util.List[String] = {\n    printLog.debug(\"Trying to find master from available Sentinels...\")\n    //    @transient var masterFound: Boolean = false\n    var masterFound: Boolean = false\n    val sentinelList: Array[String] = redisConf.sentinels.split(\"\\\\|\")\n    for (sentinel <- sentinelList if !masterFound) {\n      var jedis: Jedis = null\n      try {\n        jedis = new Jedis(sentinel.split(\":\")(0), sentinel.split(\":\")(1).toInt)\n        val masterAddr: util.List[String] = jedis.sentinelGetMasterAddrByName(redisConf.masterName)\n        // connected to sentinel...\n        if (masterAddr == null || masterAddr.size != 2) {\n          printLog.error(\"Can not get master addr, master name: \" + redisConf.masterName + \". Sentinel: \" + sentinel + \".\")\n        } else {\n          masterFound = true\n          printLog.debug(\"masterAddr: \" + masterAddr)\n          return masterAddr\n        }\n      } catch {\n        case e: JedisException => {\n          // resolves #1036, it should handle JedisException there's another chance\n          // of raising JedisDataException\n          printLog.error(\"Cannot get master address from sentinel running @ \" + sentinel + \". Reason: \" + e + \". Trying next one.\")\n        }\n      } finally {\n        if (jedis != null) {\n          jedis.close()\n        }\n      }\n    }\n    return null\n  }\n```\n\n### Redis的压力负载\n\n后来我们对程序进行了加压测试，发现当压力增大后redis的主备切换非常频繁，原来在redis中，slaver会定时跟master通信，询问其是否健康，如果不健康，slaver就会强制进行主备切换，当redis访问频繁的时候，master来不及及时响应slaver的请求，就会导致主备切换频繁，解决这个问题的方法是修改询问时间，默认是 5s，适度调高就可以了。\n\n当然redis还有一些其它优化参数，这里不做讨论；目前可以支持四千万的数据，\n\n## 引用\nhttps://github.com/xetorthio/jedis","slug":"Spark踩坑之Streaming程序实时读写哨兵模式的Redis","published":1,"updated":"2017-05-22T07:18:06.184Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfw2001fpsguk3uhonyf","content":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>Spark Streaming程序使用Redis保存出现在景区中的用户，用以识别用户是否是第一次进入景区，要分别进行读操作和写操作，为了程序的高可用性，Redis我们使用的是哨兵模式（满满的，都是坑）；</p>\n<p>Redis哨兵模式，顾名思义，就是Redis有三台机器作为一主两备，然后有三个哨兵（三个端口），告诉你Master是哪一台，然后你再去访问master，这样的话，一台机器由于负载发生了主备切换，对我们来说，对外的端口是统一的（哨兵）；</p>\n<p>我们Redis的版本是3.2.4，连接Redis，我们的程序使用了开源的jedis作为redis的连接器，jedis的版本是2.8.1，关于jedis的使用，我参考的是jedis源码的单元测试程序，这个可以在github中找到。</p>\n<h2 id=\"踩坑历程\"><a href=\"#踩坑历程\" class=\"headerlink\" title=\"踩坑历程\"></a>踩坑历程</h2><h3 id=\"hgetAll不能读取太大量的数据\"><a href=\"#hgetAll不能读取太大量的数据\" class=\"headerlink\" title=\"hgetAll不能读取太大量的数据\"></a>hgetAll不能读取太大量的数据</h3><p>在设计程序的时候，我们使用了hash作为数据的存储结构，该hash可以存储40亿条记录，一开始，为了减少对Redis的压力，我们选择的策略是对于每个batch，我们都统一读取全部的数据，然后在内存中筛选计算的方法，可以极大减少对redis的访问；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> sentinelPool = <span class=\"type\">InternalRedisClient</span>.getSentinelPool</div><div class=\"line\"><span class=\"keyword\">var</span> phoneMap: util.<span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"keyword\">new</span> util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]()</div><div class=\"line\"><span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"><span class=\"keyword\">var</span> jedis1: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  jedis1 = sentinelPool.getResource</div><div class=\"line\">  <span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)</span></div><div class=\"line\">  <span class=\"comment\">// 这里当redis中该HSet数据量很大的时候（五十万条），一次读取需要很长时间，对性能影响很大，</span></div><div class=\"line\">  <span class=\"comment\">// 故当数据条数大于一定值的时候，不用此方法</span></div><div class=\"line\">  phoneMap = jedis1.hgetAll(redisHashKey)</div><div class=\"line\">  <span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)</span></div><div class=\"line\">  printLog.debug(<span class=\"string\">\"phoneSet_1: \"</span> + phoneMap)</div><div class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (jedis1 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    printLog.debug(<span class=\"string\">\"close jedis1\"</span>)</div><div class=\"line\">    jedis1.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">但是当程序运行一段时间后，发现延迟很高，我们也没有想到数据量会有这么大，发现当数据量超过<span class=\"number\">10</span>W条这个级别后，每次读取全部数据的耗时将会很大，而且对redis的压力反而变大了，所以我们弃用了一次读取所有数据的方法，但是，如果数据量少与<span class=\"number\">10</span>万条级别的话，一次读取，也是值得考虑的；</div><div class=\"line\"></div><div class=\"line\">### jedis的sentinel连接池在spark <span class=\"type\">Streaming</span>中连接池无法释放的问题</div><div class=\"line\"></div><div class=\"line\">我们一开始采用的是通过jedis的sentinel连接池的方法连接redis；</div><div class=\"line\">在连接哨兵模式的redis的时候，由于<span class=\"type\">SparkStreaming</span>程序中每个container并不会关闭，导致在spark的transform方法中，jedis sentinel 连接池在<span class=\"type\">Streaming</span>下出现连接池资源释放不了的bug，为了解决这个问题，我们弃用了jedis的sentinel连接池，每<span class=\"number\">10</span>秒手动询问redis的哨兵<span class=\"type\">Master</span>的地址，然后手动与redis进行连接，最终解决了这个问题。</div><div class=\"line\">手动实现jedis sentinel的连接的代码如下：</div><div class=\"line\"></div><div class=\"line\">``` scala</div><div class=\"line\"><span class=\"comment\">// 由于redis sentinel 建立连接池不释放的坑，最后弃用连接池，自己实现寻找master的逻辑</span></div><div class=\"line\"><span class=\"keyword\">val</span> masterHostPort = getRedisMasterHostPortList(redisConf)</div><div class=\"line\"><span class=\"keyword\">val</span> masterHost = masterHostPort(<span class=\"number\">0</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> masterPort = masterHostPort(<span class=\"number\">1</span>).toInt</div><div class=\"line\">...</div><div class=\"line\"><span class=\"comment\">// 得到master后，就可以直接建立redis连接了</span></div><div class=\"line\"><span class=\"keyword\">val</span> paRdd3: <span class=\"type\">Iterator</span>[(<span class=\"type\">String</span>, (<span class=\"type\">String</span>, <span class=\"type\">String</span>))] = paRdd2.flatMap &#123; eachKV =&gt;</div><div class=\"line\">。。。</div><div class=\"line\">\t<span class=\"keyword\">var</span> jedis1: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\">\t<span class=\"keyword\">var</span> redis1Val: <span class=\"type\">String</span> = <span class=\"literal\">null</span></div><div class=\"line\">\t<span class=\"keyword\">try</span> &#123;</div><div class=\"line\">\t  jedis1 = <span class=\"keyword\">new</span> <span class=\"type\">Jedis</span>(masterHost, masterPort)</div><div class=\"line\">\t  redis1Val = jedis1.hget(redisHashKey, mdn)</div><div class=\"line\">\t&#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">\t  <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt; &#123;</div><div class=\"line\">\t    printLog.error(<span class=\"string\">\"jedis1 error：e\"</span> + e)</div><div class=\"line\">\t  &#125;</div><div class=\"line\">\t&#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">\t  <span class=\"keyword\">if</span> (jedis1 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">\t    jedis1.close()</div><div class=\"line\">\t  &#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>询问哨兵Master在哪里的代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">    * 得到redis sentinel 模式的master信息</div><div class=\"line\">    *</div><div class=\"line\">    * @param redisConf</div><div class=\"line\">    * @return</div><div class=\"line\">    */</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getRedisMasterHostPortList</span></span>(redisConf: <span class=\"type\">RedisConf</span>): util.<span class=\"type\">List</span>[<span class=\"type\">String</span>] = &#123;</div><div class=\"line\">    printLog.debug(<span class=\"string\">\"Trying to find master from available Sentinels...\"</span>)</div><div class=\"line\">    <span class=\"comment\">//    @transient var masterFound: Boolean = false</span></div><div class=\"line\">    <span class=\"keyword\">var</span> masterFound: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span></div><div class=\"line\">    <span class=\"keyword\">val</span> sentinelList: <span class=\"type\">Array</span>[<span class=\"type\">String</span>] = redisConf.sentinels.split(<span class=\"string\">\"\\\\|\"</span>)</div><div class=\"line\">    <span class=\"keyword\">for</span> (sentinel &lt;- sentinelList <span class=\"keyword\">if</span> !masterFound) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> jedis: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\">      <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">        jedis = <span class=\"keyword\">new</span> <span class=\"type\">Jedis</span>(sentinel.split(<span class=\"string\">\":\"</span>)(<span class=\"number\">0</span>), sentinel.split(<span class=\"string\">\":\"</span>)(<span class=\"number\">1</span>).toInt)</div><div class=\"line\">        <span class=\"keyword\">val</span> masterAddr: util.<span class=\"type\">List</span>[<span class=\"type\">String</span>] = jedis.sentinelGetMasterAddrByName(redisConf.masterName)</div><div class=\"line\">        <span class=\"comment\">// connected to sentinel...</span></div><div class=\"line\">        <span class=\"keyword\">if</span> (masterAddr == <span class=\"literal\">null</span> || masterAddr.size != <span class=\"number\">2</span>) &#123;</div><div class=\"line\">          printLog.error(<span class=\"string\">\"Can not get master addr, master name: \"</span> + redisConf.masterName + <span class=\"string\">\". Sentinel: \"</span> + sentinel + <span class=\"string\">\".\"</span>)</div><div class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">          masterFound = <span class=\"literal\">true</span></div><div class=\"line\">          printLog.debug(<span class=\"string\">\"masterAddr: \"</span> + masterAddr)</div><div class=\"line\">          <span class=\"keyword\">return</span> masterAddr</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">case</span> e: <span class=\"type\">JedisException</span> =&gt; &#123;</div><div class=\"line\">          <span class=\"comment\">// resolves #1036, it should handle JedisException there's another chance</span></div><div class=\"line\">          <span class=\"comment\">// of raising JedisDataException</span></div><div class=\"line\">          printLog.error(<span class=\"string\">\"Cannot get master address from sentinel running @ \"</span> + sentinel + <span class=\"string\">\". Reason: \"</span> + e + <span class=\"string\">\". Trying next one.\"</span>)</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (jedis != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">          jedis.close()</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">null</span></div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<h3 id=\"Redis的压力负载\"><a href=\"#Redis的压力负载\" class=\"headerlink\" title=\"Redis的压力负载\"></a>Redis的压力负载</h3><p>后来我们对程序进行了加压测试，发现当压力增大后redis的主备切换非常频繁，原来在redis中，slaver会定时跟master通信，询问其是否健康，如果不健康，slaver就会强制进行主备切换，当redis访问频繁的时候，master来不及及时响应slaver的请求，就会导致主备切换频繁，解决这个问题的方法是修改询问时间，默认是 5s，适度调高就可以了。</p>\n<p>当然redis还有一些其它优化参数，这里不做讨论；目前可以支持四千万的数据，</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"https://github.com/xetorthio/jedis\" target=\"_blank\" rel=\"external\">https://github.com/xetorthio/jedis</a></p>\n","excerpt":"","more":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>Spark Streaming程序使用Redis保存出现在景区中的用户，用以识别用户是否是第一次进入景区，要分别进行读操作和写操作，为了程序的高可用性，Redis我们使用的是哨兵模式（满满的，都是坑）；</p>\n<p>Redis哨兵模式，顾名思义，就是Redis有三台机器作为一主两备，然后有三个哨兵（三个端口），告诉你Master是哪一台，然后你再去访问master，这样的话，一台机器由于负载发生了主备切换，对我们来说，对外的端口是统一的（哨兵）；</p>\n<p>我们Redis的版本是3.2.4，连接Redis，我们的程序使用了开源的jedis作为redis的连接器，jedis的版本是2.8.1，关于jedis的使用，我参考的是jedis源码的单元测试程序，这个可以在github中找到。</p>\n<h2 id=\"踩坑历程\"><a href=\"#踩坑历程\" class=\"headerlink\" title=\"踩坑历程\"></a>踩坑历程</h2><h3 id=\"hgetAll不能读取太大量的数据\"><a href=\"#hgetAll不能读取太大量的数据\" class=\"headerlink\" title=\"hgetAll不能读取太大量的数据\"></a>hgetAll不能读取太大量的数据</h3><p>在设计程序的时候，我们使用了hash作为数据的存储结构，该hash可以存储40亿条记录，一开始，为了减少对Redis的压力，我们选择的策略是对于每个batch，我们都统一读取全部的数据，然后在内存中筛选计算的方法，可以极大减少对redis的访问；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> sentinelPool = <span class=\"type\">InternalRedisClient</span>.getSentinelPool</div><div class=\"line\"><span class=\"keyword\">var</span> phoneMap: util.<span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"keyword\">new</span> util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]()</div><div class=\"line\"><span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle0: \" + sentinelPool.getNumIdle + \" Active0: \" + sentinelPool.getNumActive)</span></div><div class=\"line\"><span class=\"keyword\">var</span> jedis1: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  jedis1 = sentinelPool.getResource</div><div class=\"line\">  <span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle1: \" + sentinelPool.getNumIdle + \" Active1: \" + sentinelPool.getNumActive)</span></div><div class=\"line\">  <span class=\"comment\">// 这里当redis中该HSet数据量很大的时候（五十万条），一次读取需要很长时间，对性能影响很大，</span></div><div class=\"line\">  <span class=\"comment\">// 故当数据条数大于一定值的时候，不用此方法</span></div><div class=\"line\">  phoneMap = jedis1.hgetAll(redisHashKey)</div><div class=\"line\">  <span class=\"comment\">// printLog.debug( \"sentinelPool NumIdle3: \" + sentinelPool.getNumIdle + \" Active3: \" + sentinelPool.getNumActive)</span></div><div class=\"line\">  printLog.debug(<span class=\"string\">\"phoneSet_1: \"</span> + phoneMap)</div><div class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (jedis1 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    printLog.debug(<span class=\"string\">\"close jedis1\"</span>)</div><div class=\"line\">    jedis1.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">但是当程序运行一段时间后，发现延迟很高，我们也没有想到数据量会有这么大，发现当数据量超过<span class=\"number\">10</span>W条这个级别后，每次读取全部数据的耗时将会很大，而且对redis的压力反而变大了，所以我们弃用了一次读取所有数据的方法，但是，如果数据量少与<span class=\"number\">10</span>万条级别的话，一次读取，也是值得考虑的；</div><div class=\"line\"></div><div class=\"line\">### jedis的sentinel连接池在spark <span class=\"type\">Streaming</span>中连接池无法释放的问题</div><div class=\"line\"></div><div class=\"line\">我们一开始采用的是通过jedis的sentinel连接池的方法连接redis；</div><div class=\"line\">在连接哨兵模式的redis的时候，由于<span class=\"type\">SparkStreaming</span>程序中每个container并不会关闭，导致在spark的transform方法中，jedis sentinel 连接池在<span class=\"type\">Streaming</span>下出现连接池资源释放不了的bug，为了解决这个问题，我们弃用了jedis的sentinel连接池，每<span class=\"number\">10</span>秒手动询问redis的哨兵<span class=\"type\">Master</span>的地址，然后手动与redis进行连接，最终解决了这个问题。</div><div class=\"line\">手动实现jedis sentinel的连接的代码如下：</div><div class=\"line\"></div><div class=\"line\">``` scala</div><div class=\"line\"><span class=\"comment\">// 由于redis sentinel 建立连接池不释放的坑，最后弃用连接池，自己实现寻找master的逻辑</span></div><div class=\"line\"><span class=\"keyword\">val</span> masterHostPort = getRedisMasterHostPortList(redisConf)</div><div class=\"line\"><span class=\"keyword\">val</span> masterHost = masterHostPort(<span class=\"number\">0</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> masterPort = masterHostPort(<span class=\"number\">1</span>).toInt</div><div class=\"line\">...</div><div class=\"line\"><span class=\"comment\">// 得到master后，就可以直接建立redis连接了</span></div><div class=\"line\"><span class=\"keyword\">val</span> paRdd3: <span class=\"type\">Iterator</span>[(<span class=\"type\">String</span>, (<span class=\"type\">String</span>, <span class=\"type\">String</span>))] = paRdd2.flatMap &#123; eachKV =&gt;</div><div class=\"line\">。。。</div><div class=\"line\">\t<span class=\"keyword\">var</span> jedis1: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\">\t<span class=\"keyword\">var</span> redis1Val: <span class=\"type\">String</span> = <span class=\"literal\">null</span></div><div class=\"line\">\t<span class=\"keyword\">try</span> &#123;</div><div class=\"line\">\t  jedis1 = <span class=\"keyword\">new</span> <span class=\"type\">Jedis</span>(masterHost, masterPort)</div><div class=\"line\">\t  redis1Val = jedis1.hget(redisHashKey, mdn)</div><div class=\"line\">\t&#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">\t  <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt; &#123;</div><div class=\"line\">\t    printLog.error(<span class=\"string\">\"jedis1 error：e\"</span> + e)</div><div class=\"line\">\t  &#125;</div><div class=\"line\">\t&#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">\t  <span class=\"keyword\">if</span> (jedis1 != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">\t    jedis1.close()</div><div class=\"line\">\t  &#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>询问哨兵Master在哪里的代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">    * 得到redis sentinel 模式的master信息</div><div class=\"line\">    *</div><div class=\"line\">    * @param redisConf</div><div class=\"line\">    * @return</div><div class=\"line\">    */</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getRedisMasterHostPortList</span></span>(redisConf: <span class=\"type\">RedisConf</span>): util.<span class=\"type\">List</span>[<span class=\"type\">String</span>] = &#123;</div><div class=\"line\">    printLog.debug(<span class=\"string\">\"Trying to find master from available Sentinels...\"</span>)</div><div class=\"line\">    <span class=\"comment\">//    @transient var masterFound: Boolean = false</span></div><div class=\"line\">    <span class=\"keyword\">var</span> masterFound: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span></div><div class=\"line\">    <span class=\"keyword\">val</span> sentinelList: <span class=\"type\">Array</span>[<span class=\"type\">String</span>] = redisConf.sentinels.split(<span class=\"string\">\"\\\\|\"</span>)</div><div class=\"line\">    <span class=\"keyword\">for</span> (sentinel &lt;- sentinelList <span class=\"keyword\">if</span> !masterFound) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> jedis: <span class=\"type\">Jedis</span> = <span class=\"literal\">null</span></div><div class=\"line\">      <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">        jedis = <span class=\"keyword\">new</span> <span class=\"type\">Jedis</span>(sentinel.split(<span class=\"string\">\":\"</span>)(<span class=\"number\">0</span>), sentinel.split(<span class=\"string\">\":\"</span>)(<span class=\"number\">1</span>).toInt)</div><div class=\"line\">        <span class=\"keyword\">val</span> masterAddr: util.<span class=\"type\">List</span>[<span class=\"type\">String</span>] = jedis.sentinelGetMasterAddrByName(redisConf.masterName)</div><div class=\"line\">        <span class=\"comment\">// connected to sentinel...</span></div><div class=\"line\">        <span class=\"keyword\">if</span> (masterAddr == <span class=\"literal\">null</span> || masterAddr.size != <span class=\"number\">2</span>) &#123;</div><div class=\"line\">          printLog.error(<span class=\"string\">\"Can not get master addr, master name: \"</span> + redisConf.masterName + <span class=\"string\">\". Sentinel: \"</span> + sentinel + <span class=\"string\">\".\"</span>)</div><div class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">          masterFound = <span class=\"literal\">true</span></div><div class=\"line\">          printLog.debug(<span class=\"string\">\"masterAddr: \"</span> + masterAddr)</div><div class=\"line\">          <span class=\"keyword\">return</span> masterAddr</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">case</span> e: <span class=\"type\">JedisException</span> =&gt; &#123;</div><div class=\"line\">          <span class=\"comment\">// resolves #1036, it should handle JedisException there's another chance</span></div><div class=\"line\">          <span class=\"comment\">// of raising JedisDataException</span></div><div class=\"line\">          printLog.error(<span class=\"string\">\"Cannot get master address from sentinel running @ \"</span> + sentinel + <span class=\"string\">\". Reason: \"</span> + e + <span class=\"string\">\". Trying next one.\"</span>)</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (jedis != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">          jedis.close()</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">null</span></div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<h3 id=\"Redis的压力负载\"><a href=\"#Redis的压力负载\" class=\"headerlink\" title=\"Redis的压力负载\"></a>Redis的压力负载</h3><p>后来我们对程序进行了加压测试，发现当压力增大后redis的主备切换非常频繁，原来在redis中，slaver会定时跟master通信，询问其是否健康，如果不健康，slaver就会强制进行主备切换，当redis访问频繁的时候，master来不及及时响应slaver的请求，就会导致主备切换频繁，解决这个问题的方法是修改询问时间，默认是 5s，适度调高就可以了。</p>\n<p>当然redis还有一些其它优化参数，这里不做讨论；目前可以支持四千万的数据，</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"https://github.com/xetorthio/jedis\">https://github.com/xetorthio/jedis</a></p>\n"},{"title":"Spark踩坑之Streaming程序实时读取数据库","toc":false,"date":"2016-11-24T02:51:50.000Z","_content":"\n## 背景\n一个SparkStreaming的项目，由于需要从Mysql数据库中实时读取一些信息，然后生成特定的数据结构进行动态的处理，在此过程中踩了一些坑，谨记：\n\n#### 初始方案\n\n在rdd的每个partition中创建一个内部数据库连接池单例对象InternalMDBManager，然后使用一个连接池\n\n``` scala\nval dStream2: DStream[(String, (String, String))] = dStream1.transform {\n  rdd1 => {\n    val numPartitions = rdd1.getNumPartitions\n    printLog.info(\"numPartitions: \" + numPartitions)\n\n    val rdd3: RDD[(String, (String, String))] = rdd1.mapPartitions {\n      paRdd1 => {\n\n\n        /**\n          * Author: wangxiaogang\n          *\n          * 由于需要在spark中分布式读取Mysql的数据，所以需要创建可以分布式分发的单例对象连接池\n          */\n        object InternalMDBManager extends Serializable {\n          @transient private var pool: ComboPooledDataSource = _\n\n          /**\n            * 从连接池获取连接\n            *\n            * @return\n            */\n          def getConnection: Connection = {\n            try {\n              pool.getConnection()\n            } catch {\n              case ex: Exception => ex.printStackTrace()\n                null\n            }\n          }\n\n          def closeConnection(connection: Connection): Unit = {\n            if (!connection.isClosed) connection.close()\n          }\n\n          /**\n            * 创建连接池\n            *\n            * @param sqlPoolConf\n            */\n          def makePool(sqlPoolConf: SqlPoolConf): Unit = {\n            if (pool == null) {\n              try {\n                pool = new ComboPooledDataSource(true)\n                pool.setJdbcUrl(sqlPoolConf.jdbcUrl)\n                pool.setDriverClass(sqlPoolConf.driverClass)\n                pool.setUser(sqlPoolConf.user)\n                pool.setPassword(sqlPoolConf.password)\n                pool.setMaxPoolSize(sqlPoolConf.maxPoolSize)\n                pool.setMinPoolSize(sqlPoolConf.minPoolSize)\n                pool.setAcquireIncrement(sqlPoolConf.acquireIncrement)\n                pool.setInitialPoolSize(sqlPoolConf.initialPoolSize)\n                pool.setMaxIdleTime(sqlPoolConf.maxIdleTime)\n              } catch {\n                case ex: Exception => ex.printStackTrace()\n              }\n            }\n          }\n        }\n\n        InternalRedisClient.makeSentinelPool(redisConf)\n\n        InternalMDBManager.makePool(sqlPoolConf)\n        val sqlConn = sqlPool.getConnection\n        val sqlConn = InternalMDBManager.getConnection\n\n        。。。\n      }\n    }\n  }\n}\n```\n\n这个方法听起来是极好的，在每个jvm中创建一个连接池，然后不同的批数据使用共同的连接池，但是在实践的过程中发现，在foreachRDD中使用这种方法是可以的，网上有很多类似的例子；\n但是在transform方法中，如果在map，flatMap，filter等方法外面建立连接池，会出现连接池无法释放的问题，无论你如何使用finally释放，都释放不了；\n解决方法是**避免使用连接池，将数据库建立与释放操作封装到同一个函数里**，在我的问题里，因为对数据库的操作不会很频繁，所以不需要引入连接池，这样将会及时释放数据库资源。\n\n最终方案是如下：\n``` scala\ndef getPositionSubDataMap(sqlPoolConf: SqlPoolConf): util.HashMap[Int, util.LinkedList[PositionSubData]] = {\nval currentTime: Long = getCurrentTime\n// todo: 如果数据量比较大的话，判断时间语句直接放到mysql查询的时候\nval sqlStr =\n\"\"\"some sqls\"\"\"\n\n// 这里告诉我们，写代码的时候不要盲目建立资源池，不要简单的东西复杂化\nval url = sqlPoolConf.jdbcUrl\nval user = sqlPoolConf.user\nval password = sqlPoolConf.password\nvar sqlConn: Connection = null\n//    var pstmt: PreparedStatement = null\n//    var rs: ResultSet = null\nval positionSubDataMap: util.HashMap[Int, util.LinkedList[PositionSubData]] = new util.HashMap[Int, util.LinkedList[PositionSubData]]()\ntry {\n  sqlConn = DriverManager.getConnection(url, user, password)\n  val pstmt: PreparedStatement = sqlConn.prepareStatement(sqlStr)\n  val rs: ResultSet = pstmt.executeQuery()\n  //    var positionSubDataList = ArrayBuffer[PositionSubData]\n\n  while (rs.next()) {\n    val subId: Long = rs.getLong(\"sub_id\")\n    val spId: String = rs.getString(\"sp_id\")\n    val locationId: String = rs.getString(\"location_id\")\n    val provId: String = rs.getString(\"prov_id\")\n    val cityCode: String = rs.getString(\"city_code\")\n    val intervalTime: Int = rs.getInt(\"interv\")\n    val available: Int = rs.getInt(\"available\")\n    val startTime = rs.getLong(\"start_time\")\n    val endTime = rs.getLong(\"end_time\")\n    val centerLongitude: Double = rs.getDouble(\"center_longitude\")\n    val centerLatitude: Double = rs.getDouble(\"center_latitude\")\n    val radius: Int = rs.getInt(\"radius\")\n    val shape: String = rs.getString(\"shape\")\n\n    //      printLog.info(\"cityCode:\" + cityCode)\n\n    val positionSubData: PositionSubData = PositionSubData(subId, spId, locationId, provId, cityCode, intervalTime,\n      available, startTime, endTime, centerLongitude, centerLatitude, radius, shape)\n    //      printLog.info(\"positionSubData: \" + positionSubData)\n\n    if ((startTime < currentTime) && (endTime > currentTime)) {\n      //        positionSubDataList. += positionSubData\n      val cityCodeArray: Array[String] = cityCode.split(\"\\\\|\")\n      for (eachCityCode <- cityCodeArray) {\n        if (positionSubDataMap.get(eachCityCode.toInt) == null) {\n          // 代表以该城市为key没有其它景区\n          val positionSubDataList: util.LinkedList[PositionSubData] = new util.LinkedList[PositionSubData]\n          positionSubDataList.add(positionSubData)\n          //            printLog.info(\"1positionSubDataList：\" + positionSubDataList)\n          positionSubDataMap.put(eachCityCode.toInt, positionSubDataList)\n          //            printLog.info(\"1positionSubDataMap: \" + positionSubDataMap)\n        } else {\n          // 代表以该城市为key有其它景区，并且已经记录在案\n          val positionSubDataList: util.LinkedList[PositionSubData] = positionSubDataMap.get(eachCityCode.toInt)\n          positionSubDataList.add(positionSubData)\n          //            printLog.info(\"2positionSubDataList：\" + positionSubDataList)\n          //            printLog.info(\"2eachCityCode.toInt:\" + eachCityCode.toInt)\n          positionSubDataMap.put(eachCityCode.toInt, positionSubDataList)\n        }\n      }\n    }\n  }\n  if (rs != null) {\n    rs.close()\n  }\n  if (pstmt != null) {\n    pstmt.close()\n  }\n} catch {\n  case e: Exception => {\n    printLog.error(\"数据库连接错误：e\" + e)\n  }\n} finally {\n  if (sqlConn != null) {\n    sqlConn.close()\n  }\n}\npositionSubDataMap\n}\n```\n\n\n## 总结\n\n综上所述，Streaming程序中动态连接数据库要谨慎，要及时查看数据库的连接状态，看看数据库连接有没有被及时释放，它不会马上就报错，但随着连接数到达数据库的最高值的时候就会出错，检测不及时，等上了生产再出问题，就后悔莫及。\n\n## 参考链接\n在foreachRDD中建立连接池的例子\nhttp://www.cnblogs.com/xlturing/p/spark.html","source":"_posts/2016-11-24-Spark踩坑之Streaming程序实时读取数据库.md","raw":"---\ntitle: Spark踩坑之Streaming程序实时读取数据库\ntoc: false\ndate: 2016-11-24 10:51:50\n\ntags: \n- spark streaming\n- mysql\n- spark开发\ncategories: spark开发\n---\n\n## 背景\n一个SparkStreaming的项目，由于需要从Mysql数据库中实时读取一些信息，然后生成特定的数据结构进行动态的处理，在此过程中踩了一些坑，谨记：\n\n#### 初始方案\n\n在rdd的每个partition中创建一个内部数据库连接池单例对象InternalMDBManager，然后使用一个连接池\n\n``` scala\nval dStream2: DStream[(String, (String, String))] = dStream1.transform {\n  rdd1 => {\n    val numPartitions = rdd1.getNumPartitions\n    printLog.info(\"numPartitions: \" + numPartitions)\n\n    val rdd3: RDD[(String, (String, String))] = rdd1.mapPartitions {\n      paRdd1 => {\n\n\n        /**\n          * Author: wangxiaogang\n          *\n          * 由于需要在spark中分布式读取Mysql的数据，所以需要创建可以分布式分发的单例对象连接池\n          */\n        object InternalMDBManager extends Serializable {\n          @transient private var pool: ComboPooledDataSource = _\n\n          /**\n            * 从连接池获取连接\n            *\n            * @return\n            */\n          def getConnection: Connection = {\n            try {\n              pool.getConnection()\n            } catch {\n              case ex: Exception => ex.printStackTrace()\n                null\n            }\n          }\n\n          def closeConnection(connection: Connection): Unit = {\n            if (!connection.isClosed) connection.close()\n          }\n\n          /**\n            * 创建连接池\n            *\n            * @param sqlPoolConf\n            */\n          def makePool(sqlPoolConf: SqlPoolConf): Unit = {\n            if (pool == null) {\n              try {\n                pool = new ComboPooledDataSource(true)\n                pool.setJdbcUrl(sqlPoolConf.jdbcUrl)\n                pool.setDriverClass(sqlPoolConf.driverClass)\n                pool.setUser(sqlPoolConf.user)\n                pool.setPassword(sqlPoolConf.password)\n                pool.setMaxPoolSize(sqlPoolConf.maxPoolSize)\n                pool.setMinPoolSize(sqlPoolConf.minPoolSize)\n                pool.setAcquireIncrement(sqlPoolConf.acquireIncrement)\n                pool.setInitialPoolSize(sqlPoolConf.initialPoolSize)\n                pool.setMaxIdleTime(sqlPoolConf.maxIdleTime)\n              } catch {\n                case ex: Exception => ex.printStackTrace()\n              }\n            }\n          }\n        }\n\n        InternalRedisClient.makeSentinelPool(redisConf)\n\n        InternalMDBManager.makePool(sqlPoolConf)\n        val sqlConn = sqlPool.getConnection\n        val sqlConn = InternalMDBManager.getConnection\n\n        。。。\n      }\n    }\n  }\n}\n```\n\n这个方法听起来是极好的，在每个jvm中创建一个连接池，然后不同的批数据使用共同的连接池，但是在实践的过程中发现，在foreachRDD中使用这种方法是可以的，网上有很多类似的例子；\n但是在transform方法中，如果在map，flatMap，filter等方法外面建立连接池，会出现连接池无法释放的问题，无论你如何使用finally释放，都释放不了；\n解决方法是**避免使用连接池，将数据库建立与释放操作封装到同一个函数里**，在我的问题里，因为对数据库的操作不会很频繁，所以不需要引入连接池，这样将会及时释放数据库资源。\n\n最终方案是如下：\n``` scala\ndef getPositionSubDataMap(sqlPoolConf: SqlPoolConf): util.HashMap[Int, util.LinkedList[PositionSubData]] = {\nval currentTime: Long = getCurrentTime\n// todo: 如果数据量比较大的话，判断时间语句直接放到mysql查询的时候\nval sqlStr =\n\"\"\"some sqls\"\"\"\n\n// 这里告诉我们，写代码的时候不要盲目建立资源池，不要简单的东西复杂化\nval url = sqlPoolConf.jdbcUrl\nval user = sqlPoolConf.user\nval password = sqlPoolConf.password\nvar sqlConn: Connection = null\n//    var pstmt: PreparedStatement = null\n//    var rs: ResultSet = null\nval positionSubDataMap: util.HashMap[Int, util.LinkedList[PositionSubData]] = new util.HashMap[Int, util.LinkedList[PositionSubData]]()\ntry {\n  sqlConn = DriverManager.getConnection(url, user, password)\n  val pstmt: PreparedStatement = sqlConn.prepareStatement(sqlStr)\n  val rs: ResultSet = pstmt.executeQuery()\n  //    var positionSubDataList = ArrayBuffer[PositionSubData]\n\n  while (rs.next()) {\n    val subId: Long = rs.getLong(\"sub_id\")\n    val spId: String = rs.getString(\"sp_id\")\n    val locationId: String = rs.getString(\"location_id\")\n    val provId: String = rs.getString(\"prov_id\")\n    val cityCode: String = rs.getString(\"city_code\")\n    val intervalTime: Int = rs.getInt(\"interv\")\n    val available: Int = rs.getInt(\"available\")\n    val startTime = rs.getLong(\"start_time\")\n    val endTime = rs.getLong(\"end_time\")\n    val centerLongitude: Double = rs.getDouble(\"center_longitude\")\n    val centerLatitude: Double = rs.getDouble(\"center_latitude\")\n    val radius: Int = rs.getInt(\"radius\")\n    val shape: String = rs.getString(\"shape\")\n\n    //      printLog.info(\"cityCode:\" + cityCode)\n\n    val positionSubData: PositionSubData = PositionSubData(subId, spId, locationId, provId, cityCode, intervalTime,\n      available, startTime, endTime, centerLongitude, centerLatitude, radius, shape)\n    //      printLog.info(\"positionSubData: \" + positionSubData)\n\n    if ((startTime < currentTime) && (endTime > currentTime)) {\n      //        positionSubDataList. += positionSubData\n      val cityCodeArray: Array[String] = cityCode.split(\"\\\\|\")\n      for (eachCityCode <- cityCodeArray) {\n        if (positionSubDataMap.get(eachCityCode.toInt) == null) {\n          // 代表以该城市为key没有其它景区\n          val positionSubDataList: util.LinkedList[PositionSubData] = new util.LinkedList[PositionSubData]\n          positionSubDataList.add(positionSubData)\n          //            printLog.info(\"1positionSubDataList：\" + positionSubDataList)\n          positionSubDataMap.put(eachCityCode.toInt, positionSubDataList)\n          //            printLog.info(\"1positionSubDataMap: \" + positionSubDataMap)\n        } else {\n          // 代表以该城市为key有其它景区，并且已经记录在案\n          val positionSubDataList: util.LinkedList[PositionSubData] = positionSubDataMap.get(eachCityCode.toInt)\n          positionSubDataList.add(positionSubData)\n          //            printLog.info(\"2positionSubDataList：\" + positionSubDataList)\n          //            printLog.info(\"2eachCityCode.toInt:\" + eachCityCode.toInt)\n          positionSubDataMap.put(eachCityCode.toInt, positionSubDataList)\n        }\n      }\n    }\n  }\n  if (rs != null) {\n    rs.close()\n  }\n  if (pstmt != null) {\n    pstmt.close()\n  }\n} catch {\n  case e: Exception => {\n    printLog.error(\"数据库连接错误：e\" + e)\n  }\n} finally {\n  if (sqlConn != null) {\n    sqlConn.close()\n  }\n}\npositionSubDataMap\n}\n```\n\n\n## 总结\n\n综上所述，Streaming程序中动态连接数据库要谨慎，要及时查看数据库的连接状态，看看数据库连接有没有被及时释放，它不会马上就报错，但随着连接数到达数据库的最高值的时候就会出错，检测不及时，等上了生产再出问题，就后悔莫及。\n\n## 参考链接\n在foreachRDD中建立连接池的例子\nhttp://www.cnblogs.com/xlturing/p/spark.html","slug":"Spark踩坑之Streaming程序实时读取数据库","published":1,"updated":"2016-11-24T06:31:31.966Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfw5001jpsgu5bpebr1j","content":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>一个SparkStreaming的项目，由于需要从Mysql数据库中实时读取一些信息，然后生成特定的数据结构进行动态的处理，在此过程中踩了一些坑，谨记：</p>\n<h4 id=\"初始方案\"><a href=\"#初始方案\" class=\"headerlink\" title=\"初始方案\"></a>初始方案</h4><p>在rdd的每个partition中创建一个内部数据库连接池单例对象InternalMDBManager，然后使用一个连接池</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> dStream2: <span class=\"type\">DStream</span>[(<span class=\"type\">String</span>, (<span class=\"type\">String</span>, <span class=\"type\">String</span>))] = dStream1.transform &#123;</div><div class=\"line\">  rdd1 =&gt; &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> numPartitions = rdd1.getNumPartitions</div><div class=\"line\">    printLog.info(<span class=\"string\">\"numPartitions: \"</span> + numPartitions)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> rdd3: <span class=\"type\">RDD</span>[(<span class=\"type\">String</span>, (<span class=\"type\">String</span>, <span class=\"type\">String</span>))] = rdd1.mapPartitions &#123;</div><div class=\"line\">      paRdd1 =&gt; &#123;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/**</span></div><div class=\"line\">          * Author: wangxiaogang</div><div class=\"line\">          *</div><div class=\"line\">          * 由于需要在spark中分布式读取Mysql的数据，所以需要创建可以分布式分发的单例对象连接池</div><div class=\"line\">          */</div><div class=\"line\">        <span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">InternalMDBManager</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</div><div class=\"line\">          <span class=\"meta\">@transient</span> <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> pool: <span class=\"type\">ComboPooledDataSource</span> = _</div><div class=\"line\"></div><div class=\"line\">          <span class=\"comment\">/**</span></div><div class=\"line\">            * 从连接池获取连接</div><div class=\"line\">            *</div><div class=\"line\">            * @return</div><div class=\"line\">            */</div><div class=\"line\">          <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getConnection</span></span>: <span class=\"type\">Connection</span> = &#123;</div><div class=\"line\">            <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">              pool.getConnection()</div><div class=\"line\">            &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">              <span class=\"keyword\">case</span> ex: <span class=\"type\">Exception</span> =&gt; ex.printStackTrace()</div><div class=\"line\">                <span class=\"literal\">null</span></div><div class=\"line\">            &#125;</div><div class=\"line\">          &#125;</div><div class=\"line\"></div><div class=\"line\">          <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">closeConnection</span></span>(connection: <span class=\"type\">Connection</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">            <span class=\"keyword\">if</span> (!connection.isClosed) connection.close()</div><div class=\"line\">          &#125;</div><div class=\"line\"></div><div class=\"line\">          <span class=\"comment\">/**</span></div><div class=\"line\">            * 创建连接池</div><div class=\"line\">            *</div><div class=\"line\">            * @param sqlPoolConf</div><div class=\"line\">            */</div><div class=\"line\">          <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">makePool</span></span>(sqlPoolConf: <span class=\"type\">SqlPoolConf</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">            <span class=\"keyword\">if</span> (pool == <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">              <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">                pool = <span class=\"keyword\">new</span> <span class=\"type\">ComboPooledDataSource</span>(<span class=\"literal\">true</span>)</div><div class=\"line\">                pool.setJdbcUrl(sqlPoolConf.jdbcUrl)</div><div class=\"line\">                pool.setDriverClass(sqlPoolConf.driverClass)</div><div class=\"line\">                pool.setUser(sqlPoolConf.user)</div><div class=\"line\">                pool.setPassword(sqlPoolConf.password)</div><div class=\"line\">                pool.setMaxPoolSize(sqlPoolConf.maxPoolSize)</div><div class=\"line\">                pool.setMinPoolSize(sqlPoolConf.minPoolSize)</div><div class=\"line\">                pool.setAcquireIncrement(sqlPoolConf.acquireIncrement)</div><div class=\"line\">                pool.setInitialPoolSize(sqlPoolConf.initialPoolSize)</div><div class=\"line\">                pool.setMaxIdleTime(sqlPoolConf.maxIdleTime)</div><div class=\"line\">              &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">                <span class=\"keyword\">case</span> ex: <span class=\"type\">Exception</span> =&gt; ex.printStackTrace()</div><div class=\"line\">              &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">          &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"type\">InternalRedisClient</span>.makeSentinelPool(redisConf)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"type\">InternalMDBManager</span>.makePool(sqlPoolConf)</div><div class=\"line\">        <span class=\"keyword\">val</span> sqlConn = sqlPool.getConnection</div><div class=\"line\">        <span class=\"keyword\">val</span> sqlConn = <span class=\"type\">InternalMDBManager</span>.getConnection</div><div class=\"line\"></div><div class=\"line\">        。。。</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>这个方法听起来是极好的，在每个jvm中创建一个连接池，然后不同的批数据使用共同的连接池，但是在实践的过程中发现，在foreachRDD中使用这种方法是可以的，网上有很多类似的例子；<br>但是在transform方法中，如果在map，flatMap，filter等方法外面建立连接池，会出现连接池无法释放的问题，无论你如何使用finally释放，都释放不了；<br>解决方法是<strong>避免使用连接池，将数据库建立与释放操作封装到同一个函数里</strong>，在我的问题里，因为对数据库的操作不会很频繁，所以不需要引入连接池，这样将会及时释放数据库资源。</p>\n<p>最终方案是如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPositionSubDataMap</span></span>(sqlPoolConf: <span class=\"type\">SqlPoolConf</span>): util.<span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>]] = &#123;</div><div class=\"line\"><span class=\"keyword\">val</span> currentTime: <span class=\"type\">Long</span> = getCurrentTime</div><div class=\"line\"><span class=\"comment\">// todo: 如果数据量比较大的话，判断时间语句直接放到mysql查询的时候</span></div><div class=\"line\"><span class=\"keyword\">val</span> sqlStr =</div><div class=\"line\"><span class=\"string\">\"\"</span><span class=\"string\">\"some sqls\"</span><span class=\"string\">\"\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 这里告诉我们，写代码的时候不要盲目建立资源池，不要简单的东西复杂化</span></div><div class=\"line\"><span class=\"keyword\">val</span> url = sqlPoolConf.jdbcUrl</div><div class=\"line\"><span class=\"keyword\">val</span> user = sqlPoolConf.user</div><div class=\"line\"><span class=\"keyword\">val</span> password = sqlPoolConf.password</div><div class=\"line\"><span class=\"keyword\">var</span> sqlConn: <span class=\"type\">Connection</span> = <span class=\"literal\">null</span></div><div class=\"line\"><span class=\"comment\">//    var pstmt: PreparedStatement = null</span></div><div class=\"line\"><span class=\"comment\">//    var rs: ResultSet = null</span></div><div class=\"line\"><span class=\"keyword\">val</span> positionSubDataMap: util.<span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>]] = <span class=\"keyword\">new</span> util.<span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>]]()</div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  sqlConn = <span class=\"type\">DriverManager</span>.getConnection(url, user, password)</div><div class=\"line\">  <span class=\"keyword\">val</span> pstmt: <span class=\"type\">PreparedStatement</span> = sqlConn.prepareStatement(sqlStr)</div><div class=\"line\">  <span class=\"keyword\">val</span> rs: <span class=\"type\">ResultSet</span> = pstmt.executeQuery()</div><div class=\"line\">  <span class=\"comment\">//    var positionSubDataList = ArrayBuffer[PositionSubData]</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">while</span> (rs.next()) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> subId: <span class=\"type\">Long</span> = rs.getLong(<span class=\"string\">\"sub_id\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> spId: <span class=\"type\">String</span> = rs.getString(<span class=\"string\">\"sp_id\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> locationId: <span class=\"type\">String</span> = rs.getString(<span class=\"string\">\"location_id\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> provId: <span class=\"type\">String</span> = rs.getString(<span class=\"string\">\"prov_id\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> cityCode: <span class=\"type\">String</span> = rs.getString(<span class=\"string\">\"city_code\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> intervalTime: <span class=\"type\">Int</span> = rs.getInt(<span class=\"string\">\"interv\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> available: <span class=\"type\">Int</span> = rs.getInt(<span class=\"string\">\"available\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> startTime = rs.getLong(<span class=\"string\">\"start_time\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> endTime = rs.getLong(<span class=\"string\">\"end_time\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> centerLongitude: <span class=\"type\">Double</span> = rs.getDouble(<span class=\"string\">\"center_longitude\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> centerLatitude: <span class=\"type\">Double</span> = rs.getDouble(<span class=\"string\">\"center_latitude\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> radius: <span class=\"type\">Int</span> = rs.getInt(<span class=\"string\">\"radius\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> shape: <span class=\"type\">String</span> = rs.getString(<span class=\"string\">\"shape\"</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">//      printLog.info(\"cityCode:\" + cityCode)</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> positionSubData: <span class=\"type\">PositionSubData</span> = <span class=\"type\">PositionSubData</span>(subId, spId, locationId, provId, cityCode, intervalTime,</div><div class=\"line\">      available, startTime, endTime, centerLongitude, centerLatitude, radius, shape)</div><div class=\"line\">    <span class=\"comment\">//      printLog.info(\"positionSubData: \" + positionSubData)</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">if</span> ((startTime &lt; currentTime) &amp;&amp; (endTime &gt; currentTime)) &#123;</div><div class=\"line\">      <span class=\"comment\">//        positionSubDataList. += positionSubData</span></div><div class=\"line\">      <span class=\"keyword\">val</span> cityCodeArray: <span class=\"type\">Array</span>[<span class=\"type\">String</span>] = cityCode.split(<span class=\"string\">\"\\\\|\"</span>)</div><div class=\"line\">      <span class=\"keyword\">for</span> (eachCityCode &lt;- cityCodeArray) &#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (positionSubDataMap.get(eachCityCode.toInt) == <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">          <span class=\"comment\">// 代表以该城市为key没有其它景区</span></div><div class=\"line\">          <span class=\"keyword\">val</span> positionSubDataList: util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>] = <span class=\"keyword\">new</span> util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>]</div><div class=\"line\">          positionSubDataList.add(positionSubData)</div><div class=\"line\">          <span class=\"comment\">//            printLog.info(\"1positionSubDataList：\" + positionSubDataList)</span></div><div class=\"line\">          positionSubDataMap.put(eachCityCode.toInt, positionSubDataList)</div><div class=\"line\">          <span class=\"comment\">//            printLog.info(\"1positionSubDataMap: \" + positionSubDataMap)</span></div><div class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">          <span class=\"comment\">// 代表以该城市为key有其它景区，并且已经记录在案</span></div><div class=\"line\">          <span class=\"keyword\">val</span> positionSubDataList: util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>] = positionSubDataMap.get(eachCityCode.toInt)</div><div class=\"line\">          positionSubDataList.add(positionSubData)</div><div class=\"line\">          <span class=\"comment\">//            printLog.info(\"2positionSubDataList：\" + positionSubDataList)</span></div><div class=\"line\">          <span class=\"comment\">//            printLog.info(\"2eachCityCode.toInt:\" + eachCityCode.toInt)</span></div><div class=\"line\">          positionSubDataMap.put(eachCityCode.toInt, positionSubDataList)</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">if</span> (rs != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    rs.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">if</span> (pstmt != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    pstmt.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt; &#123;</div><div class=\"line\">    printLog.error(<span class=\"string\">\"数据库连接错误：e\"</span> + e)</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (sqlConn != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    sqlConn.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">positionSubDataMap</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>综上所述，Streaming程序中动态连接数据库要谨慎，要及时查看数据库的连接状态，看看数据库连接有没有被及时释放，它不会马上就报错，但随着连接数到达数据库的最高值的时候就会出错，检测不及时，等上了生产再出问题，就后悔莫及。</p>\n<h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><p>在foreachRDD中建立连接池的例子<br><a href=\"http://www.cnblogs.com/xlturing/p/spark.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/xlturing/p/spark.html</a></p>\n","excerpt":"","more":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>一个SparkStreaming的项目，由于需要从Mysql数据库中实时读取一些信息，然后生成特定的数据结构进行动态的处理，在此过程中踩了一些坑，谨记：</p>\n<h4 id=\"初始方案\"><a href=\"#初始方案\" class=\"headerlink\" title=\"初始方案\"></a>初始方案</h4><p>在rdd的每个partition中创建一个内部数据库连接池单例对象InternalMDBManager，然后使用一个连接池</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> dStream2: <span class=\"type\">DStream</span>[(<span class=\"type\">String</span>, (<span class=\"type\">String</span>, <span class=\"type\">String</span>))] = dStream1.transform &#123;</div><div class=\"line\">  rdd1 =&gt; &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> numPartitions = rdd1.getNumPartitions</div><div class=\"line\">    printLog.info(<span class=\"string\">\"numPartitions: \"</span> + numPartitions)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> rdd3: <span class=\"type\">RDD</span>[(<span class=\"type\">String</span>, (<span class=\"type\">String</span>, <span class=\"type\">String</span>))] = rdd1.mapPartitions &#123;</div><div class=\"line\">      paRdd1 =&gt; &#123;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">/**</div><div class=\"line\">          * Author: wangxiaogang</div><div class=\"line\">          *</div><div class=\"line\">          * 由于需要在spark中分布式读取Mysql的数据，所以需要创建可以分布式分发的单例对象连接池</div><div class=\"line\">          */</span></div><div class=\"line\">        <span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">InternalMDBManager</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</div><div class=\"line\">          <span class=\"meta\">@transient</span> <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> pool: <span class=\"type\">ComboPooledDataSource</span> = _</div><div class=\"line\"></div><div class=\"line\">          <span class=\"comment\">/**</div><div class=\"line\">            * 从连接池获取连接</div><div class=\"line\">            *</div><div class=\"line\">            * @return</div><div class=\"line\">            */</span></div><div class=\"line\">          <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getConnection</span></span>: <span class=\"type\">Connection</span> = &#123;</div><div class=\"line\">            <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">              pool.getConnection()</div><div class=\"line\">            &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">              <span class=\"keyword\">case</span> ex: <span class=\"type\">Exception</span> =&gt; ex.printStackTrace()</div><div class=\"line\">                <span class=\"literal\">null</span></div><div class=\"line\">            &#125;</div><div class=\"line\">          &#125;</div><div class=\"line\"></div><div class=\"line\">          <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">closeConnection</span></span>(connection: <span class=\"type\">Connection</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">            <span class=\"keyword\">if</span> (!connection.isClosed) connection.close()</div><div class=\"line\">          &#125;</div><div class=\"line\"></div><div class=\"line\">          <span class=\"comment\">/**</div><div class=\"line\">            * 创建连接池</div><div class=\"line\">            *</div><div class=\"line\">            * @param sqlPoolConf</div><div class=\"line\">            */</span></div><div class=\"line\">          <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">makePool</span></span>(sqlPoolConf: <span class=\"type\">SqlPoolConf</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">            <span class=\"keyword\">if</span> (pool == <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">              <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">                pool = <span class=\"keyword\">new</span> <span class=\"type\">ComboPooledDataSource</span>(<span class=\"literal\">true</span>)</div><div class=\"line\">                pool.setJdbcUrl(sqlPoolConf.jdbcUrl)</div><div class=\"line\">                pool.setDriverClass(sqlPoolConf.driverClass)</div><div class=\"line\">                pool.setUser(sqlPoolConf.user)</div><div class=\"line\">                pool.setPassword(sqlPoolConf.password)</div><div class=\"line\">                pool.setMaxPoolSize(sqlPoolConf.maxPoolSize)</div><div class=\"line\">                pool.setMinPoolSize(sqlPoolConf.minPoolSize)</div><div class=\"line\">                pool.setAcquireIncrement(sqlPoolConf.acquireIncrement)</div><div class=\"line\">                pool.setInitialPoolSize(sqlPoolConf.initialPoolSize)</div><div class=\"line\">                pool.setMaxIdleTime(sqlPoolConf.maxIdleTime)</div><div class=\"line\">              &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">                <span class=\"keyword\">case</span> ex: <span class=\"type\">Exception</span> =&gt; ex.printStackTrace()</div><div class=\"line\">              &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">          &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"type\">InternalRedisClient</span>.makeSentinelPool(redisConf)</div><div class=\"line\"></div><div class=\"line\">        <span class=\"type\">InternalMDBManager</span>.makePool(sqlPoolConf)</div><div class=\"line\">        <span class=\"keyword\">val</span> sqlConn = sqlPool.getConnection</div><div class=\"line\">        <span class=\"keyword\">val</span> sqlConn = <span class=\"type\">InternalMDBManager</span>.getConnection</div><div class=\"line\"></div><div class=\"line\">        。。。</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>这个方法听起来是极好的，在每个jvm中创建一个连接池，然后不同的批数据使用共同的连接池，但是在实践的过程中发现，在foreachRDD中使用这种方法是可以的，网上有很多类似的例子；<br>但是在transform方法中，如果在map，flatMap，filter等方法外面建立连接池，会出现连接池无法释放的问题，无论你如何使用finally释放，都释放不了；<br>解决方法是<strong>避免使用连接池，将数据库建立与释放操作封装到同一个函数里</strong>，在我的问题里，因为对数据库的操作不会很频繁，所以不需要引入连接池，这样将会及时释放数据库资源。</p>\n<p>最终方案是如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPositionSubDataMap</span></span>(sqlPoolConf: <span class=\"type\">SqlPoolConf</span>): util.<span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>]] = &#123;</div><div class=\"line\"><span class=\"keyword\">val</span> currentTime: <span class=\"type\">Long</span> = getCurrentTime</div><div class=\"line\"><span class=\"comment\">// todo: 如果数据量比较大的话，判断时间语句直接放到mysql查询的时候</span></div><div class=\"line\"><span class=\"keyword\">val</span> sqlStr =</div><div class=\"line\"><span class=\"string\">\"\"</span><span class=\"string\">\"some sqls\"</span><span class=\"string\">\"\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 这里告诉我们，写代码的时候不要盲目建立资源池，不要简单的东西复杂化</span></div><div class=\"line\"><span class=\"keyword\">val</span> url = sqlPoolConf.jdbcUrl</div><div class=\"line\"><span class=\"keyword\">val</span> user = sqlPoolConf.user</div><div class=\"line\"><span class=\"keyword\">val</span> password = sqlPoolConf.password</div><div class=\"line\"><span class=\"keyword\">var</span> sqlConn: <span class=\"type\">Connection</span> = <span class=\"literal\">null</span></div><div class=\"line\"><span class=\"comment\">//    var pstmt: PreparedStatement = null</span></div><div class=\"line\"><span class=\"comment\">//    var rs: ResultSet = null</span></div><div class=\"line\"><span class=\"keyword\">val</span> positionSubDataMap: util.<span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>]] = <span class=\"keyword\">new</span> util.<span class=\"type\">HashMap</span>[<span class=\"type\">Int</span>, util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>]]()</div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  sqlConn = <span class=\"type\">DriverManager</span>.getConnection(url, user, password)</div><div class=\"line\">  <span class=\"keyword\">val</span> pstmt: <span class=\"type\">PreparedStatement</span> = sqlConn.prepareStatement(sqlStr)</div><div class=\"line\">  <span class=\"keyword\">val</span> rs: <span class=\"type\">ResultSet</span> = pstmt.executeQuery()</div><div class=\"line\">  <span class=\"comment\">//    var positionSubDataList = ArrayBuffer[PositionSubData]</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">while</span> (rs.next()) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> subId: <span class=\"type\">Long</span> = rs.getLong(<span class=\"string\">\"sub_id\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> spId: <span class=\"type\">String</span> = rs.getString(<span class=\"string\">\"sp_id\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> locationId: <span class=\"type\">String</span> = rs.getString(<span class=\"string\">\"location_id\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> provId: <span class=\"type\">String</span> = rs.getString(<span class=\"string\">\"prov_id\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> cityCode: <span class=\"type\">String</span> = rs.getString(<span class=\"string\">\"city_code\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> intervalTime: <span class=\"type\">Int</span> = rs.getInt(<span class=\"string\">\"interv\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> available: <span class=\"type\">Int</span> = rs.getInt(<span class=\"string\">\"available\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> startTime = rs.getLong(<span class=\"string\">\"start_time\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> endTime = rs.getLong(<span class=\"string\">\"end_time\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> centerLongitude: <span class=\"type\">Double</span> = rs.getDouble(<span class=\"string\">\"center_longitude\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> centerLatitude: <span class=\"type\">Double</span> = rs.getDouble(<span class=\"string\">\"center_latitude\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> radius: <span class=\"type\">Int</span> = rs.getInt(<span class=\"string\">\"radius\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> shape: <span class=\"type\">String</span> = rs.getString(<span class=\"string\">\"shape\"</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">//      printLog.info(\"cityCode:\" + cityCode)</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> positionSubData: <span class=\"type\">PositionSubData</span> = <span class=\"type\">PositionSubData</span>(subId, spId, locationId, provId, cityCode, intervalTime,</div><div class=\"line\">      available, startTime, endTime, centerLongitude, centerLatitude, radius, shape)</div><div class=\"line\">    <span class=\"comment\">//      printLog.info(\"positionSubData: \" + positionSubData)</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">if</span> ((startTime &lt; currentTime) &amp;&amp; (endTime &gt; currentTime)) &#123;</div><div class=\"line\">      <span class=\"comment\">//        positionSubDataList. += positionSubData</span></div><div class=\"line\">      <span class=\"keyword\">val</span> cityCodeArray: <span class=\"type\">Array</span>[<span class=\"type\">String</span>] = cityCode.split(<span class=\"string\">\"\\\\|\"</span>)</div><div class=\"line\">      <span class=\"keyword\">for</span> (eachCityCode &lt;- cityCodeArray) &#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (positionSubDataMap.get(eachCityCode.toInt) == <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">          <span class=\"comment\">// 代表以该城市为key没有其它景区</span></div><div class=\"line\">          <span class=\"keyword\">val</span> positionSubDataList: util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>] = <span class=\"keyword\">new</span> util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>]</div><div class=\"line\">          positionSubDataList.add(positionSubData)</div><div class=\"line\">          <span class=\"comment\">//            printLog.info(\"1positionSubDataList：\" + positionSubDataList)</span></div><div class=\"line\">          positionSubDataMap.put(eachCityCode.toInt, positionSubDataList)</div><div class=\"line\">          <span class=\"comment\">//            printLog.info(\"1positionSubDataMap: \" + positionSubDataMap)</span></div><div class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">          <span class=\"comment\">// 代表以该城市为key有其它景区，并且已经记录在案</span></div><div class=\"line\">          <span class=\"keyword\">val</span> positionSubDataList: util.<span class=\"type\">LinkedList</span>[<span class=\"type\">PositionSubData</span>] = positionSubDataMap.get(eachCityCode.toInt)</div><div class=\"line\">          positionSubDataList.add(positionSubData)</div><div class=\"line\">          <span class=\"comment\">//            printLog.info(\"2positionSubDataList：\" + positionSubDataList)</span></div><div class=\"line\">          <span class=\"comment\">//            printLog.info(\"2eachCityCode.toInt:\" + eachCityCode.toInt)</span></div><div class=\"line\">          positionSubDataMap.put(eachCityCode.toInt, positionSubDataList)</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">if</span> (rs != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    rs.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">if</span> (pstmt != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    pstmt.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt; &#123;</div><div class=\"line\">    printLog.error(<span class=\"string\">\"数据库连接错误：e\"</span> + e)</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125; <span class=\"keyword\">finally</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (sqlConn != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    sqlConn.close()</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">positionSubDataMap</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>综上所述，Streaming程序中动态连接数据库要谨慎，要及时查看数据库的连接状态，看看数据库连接有没有被及时释放，它不会马上就报错，但随着连接数到达数据库的最高值的时候就会出错，检测不及时，等上了生产再出问题，就后悔莫及。</p>\n<h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><p>在foreachRDD中建立连接池的例子<br><a href=\"http://www.cnblogs.com/xlturing/p/spark.html\">http://www.cnblogs.com/xlturing/p/spark.html</a></p>\n"},{"title":"Spark踩坑之Streaming在Kerberos的hadoop中renew失败","toc":true,"date":"2016-11-24T08:24:50.000Z","_content":"\n## 问题描述：\nSparkStreaming任务的Kerberos环境下两天后出现 AMRMTOKEN INVALID\n\n早上回来，跑在yarn上面的Streaming程序莫名奇妙崩溃了，yarn logs 查看日志，700万行，发现在每个executor的最后报错如下：\n``` bash\n16/11/01 15:02:05 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]\n16/11/01 15:02:06 INFO spark.SecurityManager: Changing view acls to: wzfw\n16/11/01 15:02:06 INFO spark.SecurityManager: Changing modify acls to: wzfw\n16/11/01 15:02:06 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wzfw); users with modify permissions: Set(wzfw)\n...\n16/11/02 03:51:33 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM\n...\n16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM\n16/11/02 03:51:34 WARN executor.CoarseGrainedExecutorBackend: An unknown (NM-304-SA5212M4-BIGDATA-519:44457) driver disconnected.\n16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.142.116.19:44457 disassociated! Shutting down.\n```\n\n这个报错应该只是表象，原因应该出自driver，定位到driver： NM-304-SA5212M4-BIGDATA-519\n``` bash\n16/11/02 03:51:34 WARN executor.CoarseGrainedExecutorBackend: An unknown (NM-304-SA5212M4-BIGDATA-519:44457) driver disconnected.\n16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.142.116.19:44457 disassociated! Shutting down.\n16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM\n16/11/02 03:51:34 ERROR client.TransportClient: Failed to send RPC 5285936577870185909 to NM-304-SA5212M4-BIGDATA-519/10.142.116.19:44457: java.nio.channels.ClosedChannelException\njava.nio.channels.ClosedChannelException\n16/11/02 03:51:34 WARN netty.NettyRpcEndpointRef: Error sending message [message = Heartbeat(53,[Lscala.Tuple2;@519f65d9,BlockManagerId(53, NM-304-SA5212M4-BIGDATA-382, 29163))] in 1 attempts\njava.io.IOException: Failed to send RPC 5285936577870185909 to NM-304-SA5212M4-BIGDATA-519/10.142.116.19:44457: java.nio.channels.ClosedChannelException\n        at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)\n        at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)\n        at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)\n        at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)\n        at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)\n        at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)\n        at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)\n        at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)\n        at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)\n        at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)\n        at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)\n        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.nio.channels.ClosedChannelException\n```\n找到driver的日志：\n``` bash\nContainer: container_1477044851292_468337_02_000004 on NM-304-SA5212M4-BIGDATA-519_8041\n=========================================================================================\nLogType:stderr\nLog Upload Time:星期三 十一月 02 03:51:47 +0800 2016\nLogLength:194115722\nLog Contents:\n```\n根据时间定位：（vim中在driver中搜索 16\\/11\\/02 03:51:3）\n\n发现问题出题的源头：\n在 16/11/02 03:40 之前，driver的日志还是比较稳定的，但是在 此之后，频繁出现下述异常：\n\n``` bash\n16/11/01 15:11:43 INFO ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]\n16/11/01 15:11:43 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1477044851292_468337_000002\n16/11/01 15:11:45 INFO SecurityManager: Changing view acls to: wzfw\n16/11/01 15:11:45 INFO SecurityManager: Changing modify acls to: wzfw\n16/11/01 15:11:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wzfw); users with modify permissions: Set(wzfw)\n...\n16/11/02 03:41:17 WARN Client: Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid AMRMToken from appattempt_1477044851292_468337_000002\n16/11/02 03:41:17 INFO RetryInvocationHandler: Exception while invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm1. Trying to fail over immediately.\norg.apache.hadoop.security.token.SecretManager$InvalidToken: Invalid AMRMToken from appattempt_1477044851292_468337_000002\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)\n        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy19.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)\n        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:384)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid AMRMToken from appattempt_1477044851292_468337_000002\n        at org.apache.hadoop.ipc.Client.call(Client.java:1468)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy18.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n        ... 9 more\n16/11/02 03:41:17 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2\n16/11/02 03:41:17 INFO RetryInvocationHandler: Exception while invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm2 after 1 fail over attempts. Trying to fail over after sleeping for 22672ms.\njava.net.ConnectException: Call From NM-304-SA5212M4-BIGDATA-519/10.142.116.19 to NM-304-RH5885V3-BIGDATA-008:8030 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1472)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy18.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy19.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)\n        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:384)\nCaused by: java.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)\n        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)\n        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)\n        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1438)\n        ... 13 more\n16/11/02 03:41:20 INFO JobScheduler: Added jobs for time 1478029280000 ms\n```\n\n然后这些日志持续出现了十分钟左右，当然spark的stage是依旧在增加，依旧在运行\n\n``` bash\n16/11/02 03:51:27 INFO RetryInvocationHandler: Exception while invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm1 after 14 fail over attempts. Trying to fail over immediately.\norg.apache.hadoop.security.token.SecretManager$InvalidToken: Invalid AMRMToken from appattempt_1477044851292_468337_000002\n        at sun.reflect.GeneratedConstructorAccessor74.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)\n        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy19.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)\n        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:384)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid AMRMToken from appattempt_1477044851292_468337_000002\n        at org.apache.hadoop.ipc.Client.call(Client.java:1468)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy18.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n        ... 9 more\n16/11/02 03:51:27 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2\n16/11/02 03:51:27 INFO RetryInvocationHandler: Exception while invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm2 after 15 fail over attempts. Trying to fail over after sleeping for 31772ms.\njava.net.ConnectException: Call From NM-304-SA5212M4-BIGDATA-519/10.142.116.19 to NM-304-RH5885V3-BIGDATA-008:8030 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n        at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1472)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy18.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy19.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)\n        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:384)\nCaused by: java.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)\n        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)\n        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)\n        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1438)\n        ... 13 more\n```\n\n然后在最后收到这个信号就挂了：\n\n``` bash\n16/11/02 03:51:31 INFO WriteAheadLogManager  for Thread: Attempting to clear 0 old log files in hdfs://ns/user/wzfw/checkpoint/receivedBlockMetadata older than 1478029880000:\n16/11/02 03:51:31 INFO InputInfoTracker: remove old batch metadata: 1478029870000 ms\n16/11/02 03:51:32 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16\n16/11/02 03:51:32 WARN ApplicationMaster: Reporter thread fails 2 time(s) in a row.\njava.lang.reflect.UndeclaredThrowableException\n        at com.sun.proxy.$Proxy19.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)\n        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:384)\nCaused by: java.lang.InterruptedException: sleep interrupted\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:151)\n        ... 4 more\n16/11/02 03:51:32 ERROR ApplicationMaster: RECEIVED SIGNAL 15: SIGTERM\n16/11/02 03:51:33 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook\n16/11/02 03:51:33 INFO JobGenerator: Stopping JobGenerator immediately\n```\n后来又跑了一次，又因为同样的原因挂了，这次统计了时间：\nslaver：\n程序启动时间：16/11/02 22:58:27\n程序结束时间： 16/11/03 22:51:34\n\ndriver \n程序启动时间：16/11/02 22:58:18\n遇到Kerberos问题时间： 16/11/03 22:41:23\n程序结束时间：16/11/03 22:51:35\n\n上面的时间是有不正确的地方的，因为我启动的时间肯定不是16/11/02 22:58:18（那个时候我已经回家了），我是在 11/02号当天上午启动，所以一个很可能的原因是，在22:58的这个时候，因为种种原因，程序崩溃重启，换了一个driver。\n\n## 可能的原因\n\ngoogle一下，网上查到有类似的原因：\nstorm on yarn在这个问题中存在一些bug；\nhttps://community.cloudera.com/t5/Batch-Processing-and-Workflow/Long-running-yarn-app-storm-yarn-exits-with-Invalid-AMRMToken/td-p/26717\n\nhttp://www.codeflitting.com/blog/article/%E4%B8%BA%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E7%9A%84%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%85%8D%E7%BD%AE%20YARN\n\nhttps://issues.apache.org/jira/browse/YARN-3103\n\n根据日志以及网上资料，初步认为与Kerberos过期认证有关，AMRMToken无效，也就是说，在运行了一段时期（一天多）后，AM到RM的token无效了，然后无效很多次后，后续的任务分配不到container，所以程序就挂了，突然想到，出现这种错误的原因有以下几种；\n\n1. 大约12个小时以前，SparkStreaming程序因为后面的flume测试守护进程，导致换了一个driver，是否是由于driver改动后，导致token失效？\n\n2. yarn 2.6.0有一个相关的bug，https://issues.apache.org/jira/browse/YARN-3103 \nAMRMClientImpl.updateAMRMToken updates the token service before storing it to the credentials, so the token is mapped using the newly updated service rather than the empty service that was used when the RM created the original AMRM token. This leads to two AMRM tokens in the credentials and can still fail if the AMRMTokenSelector picks the wrong one.\nIn addition the AMRMClientImpl grabs the login user rather than the current user when security is enabled, so it's likely the UGI being updated is not the UGI that will be used when reconnecting to the RM.\nThe end result is that AMs can fail with invalid token errors when trying to reconnect to an RM after a new AMRM secret has been activated.\n大概意思是，更新token的时候会有两个token，如果选择了错误的token，就会导致出错\n\n3. hadoop设置仅允许 hdfs 用户的委派令牌保留最大生存期 7 （default）天，这始终是不够的\n需要修改一下yarn的配置，增加如下参数：\n\n将 ResourceManager 配置为对应 HDFS NameNode 的代理用户，以便在现有令牌超过其最大生存期时，ResourceManager 可以请求新的令牌。YARN 随后能够代表 hdfs 用户继续执行本地化和日志聚合\n\n``` xml yarn-site.xml\n<property>\n    <name>yarn.resourcemanager.proxy-user-privileges.enabled</name>\n    <value>true</value>\n</property>\n\n\n``` xml core-site.xml\n<property>\n    <name>hadoop.proxyuser.yarn.hosts</name>\n    <value>*</value>\n</property>\n\n<property>\n    <name>hadoop.proxyuser.yarn.groups</name>\n    <value>*</value>\n</property>\n```\n然后重启YARN和HDFS服务\n\n## 问题的暂时解决\n由于项目紧张，我写了一个脚本，每隔一分钟监控yarn上面运行的spark Streaming程序，如果挂了就重新启动起来，虽然也不影响后续的结果。后来观察，基本上是一天一挂或者两天一挂，不过这也太不靠谱了。\n\n## 问题解决\n后来发现都不是上述原因，同事的前爱奇艺的同事遇到过同样的问题：原因是：\n** NameNode采用了HA后，AM与Namenode通信使用的token的结构变为HA token,HA token中会有两个private token，代表两台namenode服务。 当AM更新token时，会调用hadoop客户端的addDelegationTokens更新token。但addDelegationTokens存在问题，其只会更新HA token，不会更新private token，而AM向NameNode发起请求时，会使用private token，导致出现异常。**\n\n发生这个问题需要以下几个条件：\n``` bash\n1. NameNode HA is enabled.\n2. Kerberos is enabled.\n3. HDFS Delegation Token (not Keytab or TGT) is used to communicate with NameNode.\n4. We want to update the HDFS Delegation Token for long running applicatons. HDFS Client will generate private tokens for each NameNode. When we update the HDFS Delegation Token, these private tokens will not be updated, which will cause token expired.\n```\n### hadoop修复方案\n这是hadoop的一个bug，在hadoop 2.9的新版本中修复了这个问题，所以，给hadoop打个patch就好了：\n\nHadoop: https://issues.apache.org/jira/browse/HDFS-9276 \nhadoop2.9修复了private token不更新问题\n\n### spark修复方案\n\nSpark： https://github.com/apache/spark/pull/9168/files\n主动使FileSystem对象关闭，再重新建立。FileSystem对象建立期间会重新设置private token\n\n由于hadoop变动对生产环境的影响很大，所以我们选择了spark的修复方案，修改了源码后打包编译上线，问题解决；\n\n感谢聪哥，也感谢那位在爱奇艺的发现并解决了这个问题的大神，没有你们，我要惨了。\n\n与Kerberos对干的日子很酸爽\n\n### 后记\n还有一个方案可以解决这个问题，可以不需要修改spark源码，提交的时候增加参数：\n```\n--conf spark.hadoop.fs.hdfs.impl.disable.cache=true\n```\n\n\n#### 引用\nhttp://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/","source":"_posts/2016-11-24-Spark踩坑之Streaming在Kerberos的hadoop中renew失败.md","raw":"---\ntitle: Spark踩坑之Streaming在Kerberos的hadoop中renew失败\ntoc: true\ndate: 2016-11-24 16:24:50\ntags: \n- spark streaming\n- kerberos\n- spark开发\ncategories: spark开发\n---\n\n## 问题描述：\nSparkStreaming任务的Kerberos环境下两天后出现 AMRMTOKEN INVALID\n\n早上回来，跑在yarn上面的Streaming程序莫名奇妙崩溃了，yarn logs 查看日志，700万行，发现在每个executor的最后报错如下：\n``` bash\n16/11/01 15:02:05 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]\n16/11/01 15:02:06 INFO spark.SecurityManager: Changing view acls to: wzfw\n16/11/01 15:02:06 INFO spark.SecurityManager: Changing modify acls to: wzfw\n16/11/01 15:02:06 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wzfw); users with modify permissions: Set(wzfw)\n...\n16/11/02 03:51:33 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM\n...\n16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM\n16/11/02 03:51:34 WARN executor.CoarseGrainedExecutorBackend: An unknown (NM-304-SA5212M4-BIGDATA-519:44457) driver disconnected.\n16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.142.116.19:44457 disassociated! Shutting down.\n```\n\n这个报错应该只是表象，原因应该出自driver，定位到driver： NM-304-SA5212M4-BIGDATA-519\n``` bash\n16/11/02 03:51:34 WARN executor.CoarseGrainedExecutorBackend: An unknown (NM-304-SA5212M4-BIGDATA-519:44457) driver disconnected.\n16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.142.116.19:44457 disassociated! Shutting down.\n16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM\n16/11/02 03:51:34 ERROR client.TransportClient: Failed to send RPC 5285936577870185909 to NM-304-SA5212M4-BIGDATA-519/10.142.116.19:44457: java.nio.channels.ClosedChannelException\njava.nio.channels.ClosedChannelException\n16/11/02 03:51:34 WARN netty.NettyRpcEndpointRef: Error sending message [message = Heartbeat(53,[Lscala.Tuple2;@519f65d9,BlockManagerId(53, NM-304-SA5212M4-BIGDATA-382, 29163))] in 1 attempts\njava.io.IOException: Failed to send RPC 5285936577870185909 to NM-304-SA5212M4-BIGDATA-519/10.142.116.19:44457: java.nio.channels.ClosedChannelException\n        at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)\n        at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)\n        at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)\n        at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)\n        at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)\n        at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)\n        at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)\n        at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)\n        at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)\n        at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)\n        at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)\n        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.nio.channels.ClosedChannelException\n```\n找到driver的日志：\n``` bash\nContainer: container_1477044851292_468337_02_000004 on NM-304-SA5212M4-BIGDATA-519_8041\n=========================================================================================\nLogType:stderr\nLog Upload Time:星期三 十一月 02 03:51:47 +0800 2016\nLogLength:194115722\nLog Contents:\n```\n根据时间定位：（vim中在driver中搜索 16\\/11\\/02 03:51:3）\n\n发现问题出题的源头：\n在 16/11/02 03:40 之前，driver的日志还是比较稳定的，但是在 此之后，频繁出现下述异常：\n\n``` bash\n16/11/01 15:11:43 INFO ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]\n16/11/01 15:11:43 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1477044851292_468337_000002\n16/11/01 15:11:45 INFO SecurityManager: Changing view acls to: wzfw\n16/11/01 15:11:45 INFO SecurityManager: Changing modify acls to: wzfw\n16/11/01 15:11:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wzfw); users with modify permissions: Set(wzfw)\n...\n16/11/02 03:41:17 WARN Client: Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid AMRMToken from appattempt_1477044851292_468337_000002\n16/11/02 03:41:17 INFO RetryInvocationHandler: Exception while invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm1. Trying to fail over immediately.\norg.apache.hadoop.security.token.SecretManager$InvalidToken: Invalid AMRMToken from appattempt_1477044851292_468337_000002\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)\n        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy19.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)\n        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:384)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid AMRMToken from appattempt_1477044851292_468337_000002\n        at org.apache.hadoop.ipc.Client.call(Client.java:1468)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy18.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n        ... 9 more\n16/11/02 03:41:17 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2\n16/11/02 03:41:17 INFO RetryInvocationHandler: Exception while invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm2 after 1 fail over attempts. Trying to fail over after sleeping for 22672ms.\njava.net.ConnectException: Call From NM-304-SA5212M4-BIGDATA-519/10.142.116.19 to NM-304-RH5885V3-BIGDATA-008:8030 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1472)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy18.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy19.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)\n        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:384)\nCaused by: java.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)\n        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)\n        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)\n        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1438)\n        ... 13 more\n16/11/02 03:41:20 INFO JobScheduler: Added jobs for time 1478029280000 ms\n```\n\n然后这些日志持续出现了十分钟左右，当然spark的stage是依旧在增加，依旧在运行\n\n``` bash\n16/11/02 03:51:27 INFO RetryInvocationHandler: Exception while invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm1 after 14 fail over attempts. Trying to fail over immediately.\norg.apache.hadoop.security.token.SecretManager$InvalidToken: Invalid AMRMToken from appattempt_1477044851292_468337_000002\n        at sun.reflect.GeneratedConstructorAccessor74.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)\n        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy19.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)\n        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:384)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid AMRMToken from appattempt_1477044851292_468337_000002\n        at org.apache.hadoop.ipc.Client.call(Client.java:1468)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy18.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n        ... 9 more\n16/11/02 03:51:27 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2\n16/11/02 03:51:27 INFO RetryInvocationHandler: Exception while invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm2 after 15 fail over attempts. Trying to fail over after sleeping for 31772ms.\njava.net.ConnectException: Call From NM-304-SA5212M4-BIGDATA-519/10.142.116.19 to NM-304-RH5885V3-BIGDATA-008:8030 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n        at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1472)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy18.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy19.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)\n        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:384)\nCaused by: java.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)\n        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)\n        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)\n        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1438)\n        ... 13 more\n```\n\n然后在最后收到这个信号就挂了：\n\n``` bash\n16/11/02 03:51:31 INFO WriteAheadLogManager  for Thread: Attempting to clear 0 old log files in hdfs://ns/user/wzfw/checkpoint/receivedBlockMetadata older than 1478029880000:\n16/11/02 03:51:31 INFO InputInfoTracker: remove old batch metadata: 1478029870000 ms\n16/11/02 03:51:32 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16\n16/11/02 03:51:32 WARN ApplicationMaster: Reporter thread fails 2 time(s) in a row.\njava.lang.reflect.UndeclaredThrowableException\n        at com.sun.proxy.$Proxy19.allocate(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)\n        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:384)\nCaused by: java.lang.InterruptedException: sleep interrupted\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:151)\n        ... 4 more\n16/11/02 03:51:32 ERROR ApplicationMaster: RECEIVED SIGNAL 15: SIGTERM\n16/11/02 03:51:33 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook\n16/11/02 03:51:33 INFO JobGenerator: Stopping JobGenerator immediately\n```\n后来又跑了一次，又因为同样的原因挂了，这次统计了时间：\nslaver：\n程序启动时间：16/11/02 22:58:27\n程序结束时间： 16/11/03 22:51:34\n\ndriver \n程序启动时间：16/11/02 22:58:18\n遇到Kerberos问题时间： 16/11/03 22:41:23\n程序结束时间：16/11/03 22:51:35\n\n上面的时间是有不正确的地方的，因为我启动的时间肯定不是16/11/02 22:58:18（那个时候我已经回家了），我是在 11/02号当天上午启动，所以一个很可能的原因是，在22:58的这个时候，因为种种原因，程序崩溃重启，换了一个driver。\n\n## 可能的原因\n\ngoogle一下，网上查到有类似的原因：\nstorm on yarn在这个问题中存在一些bug；\nhttps://community.cloudera.com/t5/Batch-Processing-and-Workflow/Long-running-yarn-app-storm-yarn-exits-with-Invalid-AMRMToken/td-p/26717\n\nhttp://www.codeflitting.com/blog/article/%E4%B8%BA%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E7%9A%84%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%85%8D%E7%BD%AE%20YARN\n\nhttps://issues.apache.org/jira/browse/YARN-3103\n\n根据日志以及网上资料，初步认为与Kerberos过期认证有关，AMRMToken无效，也就是说，在运行了一段时期（一天多）后，AM到RM的token无效了，然后无效很多次后，后续的任务分配不到container，所以程序就挂了，突然想到，出现这种错误的原因有以下几种；\n\n1. 大约12个小时以前，SparkStreaming程序因为后面的flume测试守护进程，导致换了一个driver，是否是由于driver改动后，导致token失效？\n\n2. yarn 2.6.0有一个相关的bug，https://issues.apache.org/jira/browse/YARN-3103 \nAMRMClientImpl.updateAMRMToken updates the token service before storing it to the credentials, so the token is mapped using the newly updated service rather than the empty service that was used when the RM created the original AMRM token. This leads to two AMRM tokens in the credentials and can still fail if the AMRMTokenSelector picks the wrong one.\nIn addition the AMRMClientImpl grabs the login user rather than the current user when security is enabled, so it's likely the UGI being updated is not the UGI that will be used when reconnecting to the RM.\nThe end result is that AMs can fail with invalid token errors when trying to reconnect to an RM after a new AMRM secret has been activated.\n大概意思是，更新token的时候会有两个token，如果选择了错误的token，就会导致出错\n\n3. hadoop设置仅允许 hdfs 用户的委派令牌保留最大生存期 7 （default）天，这始终是不够的\n需要修改一下yarn的配置，增加如下参数：\n\n将 ResourceManager 配置为对应 HDFS NameNode 的代理用户，以便在现有令牌超过其最大生存期时，ResourceManager 可以请求新的令牌。YARN 随后能够代表 hdfs 用户继续执行本地化和日志聚合\n\n``` xml yarn-site.xml\n<property>\n    <name>yarn.resourcemanager.proxy-user-privileges.enabled</name>\n    <value>true</value>\n</property>\n\n\n``` xml core-site.xml\n<property>\n    <name>hadoop.proxyuser.yarn.hosts</name>\n    <value>*</value>\n</property>\n\n<property>\n    <name>hadoop.proxyuser.yarn.groups</name>\n    <value>*</value>\n</property>\n```\n然后重启YARN和HDFS服务\n\n## 问题的暂时解决\n由于项目紧张，我写了一个脚本，每隔一分钟监控yarn上面运行的spark Streaming程序，如果挂了就重新启动起来，虽然也不影响后续的结果。后来观察，基本上是一天一挂或者两天一挂，不过这也太不靠谱了。\n\n## 问题解决\n后来发现都不是上述原因，同事的前爱奇艺的同事遇到过同样的问题：原因是：\n** NameNode采用了HA后，AM与Namenode通信使用的token的结构变为HA token,HA token中会有两个private token，代表两台namenode服务。 当AM更新token时，会调用hadoop客户端的addDelegationTokens更新token。但addDelegationTokens存在问题，其只会更新HA token，不会更新private token，而AM向NameNode发起请求时，会使用private token，导致出现异常。**\n\n发生这个问题需要以下几个条件：\n``` bash\n1. NameNode HA is enabled.\n2. Kerberos is enabled.\n3. HDFS Delegation Token (not Keytab or TGT) is used to communicate with NameNode.\n4. We want to update the HDFS Delegation Token for long running applicatons. HDFS Client will generate private tokens for each NameNode. When we update the HDFS Delegation Token, these private tokens will not be updated, which will cause token expired.\n```\n### hadoop修复方案\n这是hadoop的一个bug，在hadoop 2.9的新版本中修复了这个问题，所以，给hadoop打个patch就好了：\n\nHadoop: https://issues.apache.org/jira/browse/HDFS-9276 \nhadoop2.9修复了private token不更新问题\n\n### spark修复方案\n\nSpark： https://github.com/apache/spark/pull/9168/files\n主动使FileSystem对象关闭，再重新建立。FileSystem对象建立期间会重新设置private token\n\n由于hadoop变动对生产环境的影响很大，所以我们选择了spark的修复方案，修改了源码后打包编译上线，问题解决；\n\n感谢聪哥，也感谢那位在爱奇艺的发现并解决了这个问题的大神，没有你们，我要惨了。\n\n与Kerberos对干的日子很酸爽\n\n### 后记\n还有一个方案可以解决这个问题，可以不需要修改spark源码，提交的时候增加参数：\n```\n--conf spark.hadoop.fs.hdfs.impl.disable.cache=true\n```\n\n\n#### 引用\nhttp://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/","slug":"Spark踩坑之Streaming在Kerberos的hadoop中renew失败","published":1,"updated":"2017-12-15T02:43:11.144Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfw8001npsgum0jfd66h","content":"<h2 id=\"问题描述：\"><a href=\"#问题描述：\" class=\"headerlink\" title=\"问题描述：\"></a>问题描述：</h2><p>SparkStreaming任务的Kerberos环境下两天后出现 AMRMTOKEN INVALID</p>\n<p>早上回来，跑在yarn上面的Streaming程序莫名奇妙崩溃了，yarn logs 查看日志，700万行，发现在每个executor的最后报错如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/11/01 15:02:05 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers <span class=\"keyword\">for</span> [TERM, HUP, INT]</div><div class=\"line\">16/11/01 15:02:06 INFO spark.SecurityManager: Changing view acls to: wzfw</div><div class=\"line\">16/11/01 15:02:06 INFO spark.SecurityManager: Changing modify acls to: wzfw</div><div class=\"line\">16/11/01 15:02:06 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wzfw); users with modify permissions: Set(wzfw)</div><div class=\"line\">...</div><div class=\"line\">16/11/02 03:51:33 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM</div><div class=\"line\">...</div><div class=\"line\">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM</div><div class=\"line\">16/11/02 03:51:34 WARN executor.CoarseGrainedExecutorBackend: An unknown (NM-304-SA5212M4-BIGDATA-519:44457) driver disconnected.</div><div class=\"line\">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.142.116.19:44457 disassociated! Shutting down.</div></pre></td></tr></table></figure></p>\n<p>这个报错应该只是表象，原因应该出自driver，定位到driver： NM-304-SA5212M4-BIGDATA-519<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/11/02 03:51:34 WARN executor.CoarseGrainedExecutorBackend: An unknown (NM-304-SA5212M4-BIGDATA-519:44457) driver disconnected.</div><div class=\"line\">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.142.116.19:44457 disassociated! Shutting down.</div><div class=\"line\">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM</div><div class=\"line\">16/11/02 03:51:34 ERROR client.TransportClient: Failed to send RPC 5285936577870185909 to NM-304-SA5212M4-BIGDATA-519/10.142.116.19:44457: java.nio.channels.ClosedChannelException</div><div class=\"line\">java.nio.channels.ClosedChannelException</div><div class=\"line\">16/11/02 03:51:34 WARN netty.NettyRpcEndpointRef: Error sending message [message = Heartbeat(53,[Lscala.Tuple2;@519f65d9,BlockManagerId(53, NM-304-SA5212M4-BIGDATA-382, 29163))] <span class=\"keyword\">in</span> 1 attempts</div><div class=\"line\">java.io.IOException: Failed to send RPC 5285936577870185909 to NM-304-SA5212M4-BIGDATA-519/10.142.116.19:44457: java.nio.channels.ClosedChannelException</div><div class=\"line\">        at org.apache.spark.network.client.TransportClient<span class=\"variable\">$3</span>.operationComplete(TransportClient.java:239)</div><div class=\"line\">        at org.apache.spark.network.client.TransportClient<span class=\"variable\">$3</span>.operationComplete(TransportClient.java:226)</div><div class=\"line\">        at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)</div><div class=\"line\">        at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)</div><div class=\"line\">        at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)</div><div class=\"line\">        at io.netty.channel.AbstractChannel<span class=\"variable\">$AbstractUnsafe</span>.safeSetFailure(AbstractChannel.java:801)</div><div class=\"line\">        at io.netty.channel.AbstractChannel<span class=\"variable\">$AbstractUnsafe</span>.write(AbstractChannel.java:699)</div><div class=\"line\">        at io.netty.channel.DefaultChannelPipeline<span class=\"variable\">$HeadContext</span>.write(DefaultChannelPipeline.java:1122)</div><div class=\"line\">        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)</div><div class=\"line\">        at io.netty.channel.AbstractChannelHandlerContext.access<span class=\"variable\">$1900</span>(AbstractChannelHandlerContext.java:32)</div><div class=\"line\">        at io.netty.channel.AbstractChannelHandlerContext<span class=\"variable\">$AbstractWriteTask</span>.write(AbstractChannelHandlerContext.java:908)</div><div class=\"line\">        at io.netty.channel.AbstractChannelHandlerContext<span class=\"variable\">$WriteAndFlushTask</span>.write(AbstractChannelHandlerContext.java:960)</div><div class=\"line\">        at io.netty.channel.AbstractChannelHandlerContext<span class=\"variable\">$AbstractWriteTask</span>.run(AbstractChannelHandlerContext.java:893)</div><div class=\"line\">        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)</div><div class=\"line\">        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)</div><div class=\"line\">        at io.netty.util.concurrent.SingleThreadEventExecutor<span class=\"variable\">$2</span>.run(SingleThreadEventExecutor.java:111)</div><div class=\"line\">        at java.lang.Thread.run(Thread.java:745)</div><div class=\"line\">Caused by: java.nio.channels.ClosedChannelException</div></pre></td></tr></table></figure></p>\n<p>找到driver的日志：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Container: container_1477044851292_468337_02_000004 on NM-304-SA5212M4-BIGDATA-519_8041</div><div class=\"line\">=========================================================================================</div><div class=\"line\">LogType:stderr</div><div class=\"line\">Log Upload Time:星期三 十一月 02 03:51:47 +0800 2016</div><div class=\"line\">LogLength:194115722</div><div class=\"line\">Log Contents:</div></pre></td></tr></table></figure></p>\n<p>根据时间定位：（vim中在driver中搜索 16\\/11\\/02 03:51:3）</p>\n<p>发现问题出题的源头：<br>在 16/11/02 03:40 之前，driver的日志还是比较稳定的，但是在 此之后，频繁出现下述异常：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/11/01 15:11:43 INFO ApplicationMaster: Registered signal handlers <span class=\"keyword\">for</span> [TERM, HUP, INT]</div><div class=\"line\">16/11/01 15:11:43 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1477044851292_468337_000002</div><div class=\"line\">16/11/01 15:11:45 INFO SecurityManager: Changing view acls to: wzfw</div><div class=\"line\">16/11/01 15:11:45 INFO SecurityManager: Changing modify acls to: wzfw</div><div class=\"line\">16/11/01 15:11:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wzfw); users with modify permissions: Set(wzfw)</div><div class=\"line\">...</div><div class=\"line\">16/11/02 03:41:17 WARN Client: Exception encountered <span class=\"keyword\">while</span> connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager<span class=\"variable\">$InvalidToken</span>): Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class=\"line\">16/11/02 03:41:17 INFO RetryInvocationHandler: Exception <span class=\"keyword\">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm1. Trying to fail over immediately.</div><div class=\"line\">org.apache.hadoop.security.token.SecretManager<span class=\"variable\">$InvalidToken</span>: Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</div><div class=\"line\">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class=\"line\">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class=\"line\">        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)</div><div class=\"line\">        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)</div><div class=\"line\">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class=\"line\">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy19</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class=\"variable\">$anon</span><span class=\"variable\">$1</span>.run(ApplicationMaster.scala:384)</div><div class=\"line\">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager<span class=\"variable\">$InvalidToken</span>): Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1468)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class=\"line\">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class=\"variable\">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy18</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class=\"line\">        ... 9 more</div><div class=\"line\">16/11/02 03:41:17 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2</div><div class=\"line\">16/11/02 03:41:17 INFO RetryInvocationHandler: Exception <span class=\"keyword\">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm2 after 1 fail over attempts. Trying to fail over after sleeping <span class=\"keyword\">for</span> 22672ms.</div><div class=\"line\">java.net.ConnectException: Call From NM-304-SA5212M4-BIGDATA-519/10.142.116.19 to NM-304-RH5885V3-BIGDATA-008:8030 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</div><div class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</div><div class=\"line\">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class=\"line\">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1472)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class=\"line\">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class=\"variable\">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy18</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class=\"line\">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class=\"line\">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy19</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class=\"variable\">$anon</span><span class=\"variable\">$1</span>.run(ApplicationMaster.scala:384)</div><div class=\"line\">Caused by: java.net.ConnectException: Connection refused</div><div class=\"line\">        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</div><div class=\"line\">        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)</div><div class=\"line\">        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.setupConnection(Client.java:607)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.setupIOstreams(Client.java:705)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.access<span class=\"variable\">$2800</span>(Client.java:368)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1438)</div><div class=\"line\">        ... 13 more</div><div class=\"line\">16/11/02 03:41:20 INFO JobScheduler: Added <span class=\"built_in\">jobs</span> <span class=\"keyword\">for</span> time 1478029280000 ms</div></pre></td></tr></table></figure>\n<p>然后这些日志持续出现了十分钟左右，当然spark的stage是依旧在增加，依旧在运行</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/11/02 03:51:27 INFO RetryInvocationHandler: Exception <span class=\"keyword\">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm1 after 14 fail over attempts. Trying to fail over immediately.</div><div class=\"line\">org.apache.hadoop.security.token.SecretManager<span class=\"variable\">$InvalidToken</span>: Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class=\"line\">        at sun.reflect.GeneratedConstructorAccessor74.newInstance(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class=\"line\">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class=\"line\">        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)</div><div class=\"line\">        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)</div><div class=\"line\">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class=\"line\">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy19</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class=\"variable\">$anon</span><span class=\"variable\">$1</span>.run(ApplicationMaster.scala:384)</div><div class=\"line\">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager<span class=\"variable\">$InvalidToken</span>): Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1468)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class=\"line\">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class=\"variable\">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy18</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class=\"line\">        ... 9 more</div><div class=\"line\">16/11/02 03:51:27 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2</div><div class=\"line\">16/11/02 03:51:27 INFO RetryInvocationHandler: Exception <span class=\"keyword\">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm2 after 15 fail over attempts. Trying to fail over after sleeping <span class=\"keyword\">for</span> 31772ms.</div><div class=\"line\">java.net.ConnectException: Call From NM-304-SA5212M4-BIGDATA-519/10.142.116.19 to NM-304-RH5885V3-BIGDATA-008:8030 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</div><div class=\"line\">        at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class=\"line\">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1472)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class=\"line\">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class=\"variable\">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy18</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class=\"line\">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class=\"line\">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy19</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class=\"variable\">$anon</span><span class=\"variable\">$1</span>.run(ApplicationMaster.scala:384)</div><div class=\"line\">Caused by: java.net.ConnectException: Connection refused</div><div class=\"line\">        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</div><div class=\"line\">        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)</div><div class=\"line\">        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.setupConnection(Client.java:607)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.setupIOstreams(Client.java:705)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.access<span class=\"variable\">$2800</span>(Client.java:368)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1438)</div><div class=\"line\">        ... 13 more</div></pre></td></tr></table></figure>\n<p>然后在最后收到这个信号就挂了：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/11/02 03:51:31 INFO WriteAheadLogManager  <span class=\"keyword\">for</span> Thread: Attempting to clear 0 old <span class=\"built_in\">log</span> files <span class=\"keyword\">in</span> hdfs://ns/user/wzfw/checkpoint/receivedBlockMetadata older than 1478029880000:</div><div class=\"line\">16/11/02 03:51:31 INFO InputInfoTracker: remove old batch metadata: 1478029870000 ms</div><div class=\"line\">16/11/02 03:51:32 INFO ApplicationMaster: Final app status: FAILED, <span class=\"built_in\">exit</span>Code: 16</div><div class=\"line\">16/11/02 03:51:32 WARN ApplicationMaster: Reporter thread fails 2 time(s) <span class=\"keyword\">in</span> a row.</div><div class=\"line\">java.lang.reflect.UndeclaredThrowableException</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy19</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class=\"variable\">$anon</span><span class=\"variable\">$1</span>.run(ApplicationMaster.scala:384)</div><div class=\"line\">Caused by: java.lang.InterruptedException: sleep interrupted</div><div class=\"line\">        at java.lang.Thread.sleep(Native Method)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:151)</div><div class=\"line\">        ... 4 more</div><div class=\"line\">16/11/02 03:51:32 ERROR ApplicationMaster: RECEIVED SIGNAL 15: SIGTERM</div><div class=\"line\">16/11/02 03:51:33 INFO StreamingContext: Invoking stop(stopGracefully=<span class=\"literal\">false</span>) from shutdown hook</div><div class=\"line\">16/11/02 03:51:33 INFO JobGenerator: Stopping JobGenerator immediately</div></pre></td></tr></table></figure>\n<p>后来又跑了一次，又因为同样的原因挂了，这次统计了时间：<br>slaver：<br>程序启动时间：16/11/02 22:58:27<br>程序结束时间： 16/11/03 22:51:34</p>\n<p>driver<br>程序启动时间：16/11/02 22:58:18<br>遇到Kerberos问题时间： 16/11/03 22:41:23<br>程序结束时间：16/11/03 22:51:35</p>\n<p>上面的时间是有不正确的地方的，因为我启动的时间肯定不是16/11/02 22:58:18（那个时候我已经回家了），我是在 11/02号当天上午启动，所以一个很可能的原因是，在22:58的这个时候，因为种种原因，程序崩溃重启，换了一个driver。</p>\n<h2 id=\"可能的原因\"><a href=\"#可能的原因\" class=\"headerlink\" title=\"可能的原因\"></a>可能的原因</h2><p>google一下，网上查到有类似的原因：<br>storm on yarn在这个问题中存在一些bug；<br><a href=\"https://community.cloudera.com/t5/Batch-Processing-and-Workflow/Long-running-yarn-app-storm-yarn-exits-with-Invalid-AMRMToken/td-p/26717\" target=\"_blank\" rel=\"external\">https://community.cloudera.com/t5/Batch-Processing-and-Workflow/Long-running-yarn-app-storm-yarn-exits-with-Invalid-AMRMToken/td-p/26717</a></p>\n<p><a href=\"http://www.codeflitting.com/blog/article/%E4%B8%BA%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E7%9A%84%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%85%8D%E7%BD%AE%20YARN\" target=\"_blank\" rel=\"external\">http://www.codeflitting.com/blog/article/%E4%B8%BA%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E7%9A%84%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%85%8D%E7%BD%AE%20YARN</a></p>\n<p><a href=\"https://issues.apache.org/jira/browse/YARN-3103\" target=\"_blank\" rel=\"external\">https://issues.apache.org/jira/browse/YARN-3103</a></p>\n<p>根据日志以及网上资料，初步认为与Kerberos过期认证有关，AMRMToken无效，也就是说，在运行了一段时期（一天多）后，AM到RM的token无效了，然后无效很多次后，后续的任务分配不到container，所以程序就挂了，突然想到，出现这种错误的原因有以下几种；</p>\n<ol>\n<li><p>大约12个小时以前，SparkStreaming程序因为后面的flume测试守护进程，导致换了一个driver，是否是由于driver改动后，导致token失效？</p>\n</li>\n<li><p>yarn 2.6.0有一个相关的bug，<a href=\"https://issues.apache.org/jira/browse/YARN-3103\" target=\"_blank\" rel=\"external\">https://issues.apache.org/jira/browse/YARN-3103</a><br>AMRMClientImpl.updateAMRMToken updates the token service before storing it to the credentials, so the token is mapped using the newly updated service rather than the empty service that was used when the RM created the original AMRM token. This leads to two AMRM tokens in the credentials and can still fail if the AMRMTokenSelector picks the wrong one.<br>In addition the AMRMClientImpl grabs the login user rather than the current user when security is enabled, so it’s likely the UGI being updated is not the UGI that will be used when reconnecting to the RM.<br>The end result is that AMs can fail with invalid token errors when trying to reconnect to an RM after a new AMRM secret has been activated.<br>大概意思是，更新token的时候会有两个token，如果选择了错误的token，就会导致出错</p>\n</li>\n<li><p>hadoop设置仅允许 hdfs 用户的委派令牌保留最大生存期 7 （default）天，这始终是不够的<br>需要修改一下yarn的配置，增加如下参数：</p>\n</li>\n</ol>\n<p>将 ResourceManager 配置为对应 HDFS NameNode 的代理用户，以便在现有令牌超过其最大生存期时，ResourceManager 可以请求新的令牌。YARN 随后能够代表 hdfs 用户继续执行本地化和日志聚合</p>\n<figure class=\"highlight xml\"><figcaption><span>yarn-site.xml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.proxy-user-privileges.enabled<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">``` xml core-site.xml</div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.yarn.hosts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.yarn.groups<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div></pre></td></tr></table></figure>\n<p>然后重启YARN和HDFS服务</p>\n<h2 id=\"问题的暂时解决\"><a href=\"#问题的暂时解决\" class=\"headerlink\" title=\"问题的暂时解决\"></a>问题的暂时解决</h2><p>由于项目紧张，我写了一个脚本，每隔一分钟监控yarn上面运行的spark Streaming程序，如果挂了就重新启动起来，虽然也不影响后续的结果。后来观察，基本上是一天一挂或者两天一挂，不过这也太不靠谱了。</p>\n<h2 id=\"问题解决\"><a href=\"#问题解决\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h2><p>后来发现都不是上述原因，同事的前爱奇艺的同事遇到过同样的问题：原因是：<br><strong> NameNode采用了HA后，AM与Namenode通信使用的token的结构变为HA token,HA token中会有两个private token，代表两台namenode服务。 当AM更新token时，会调用hadoop客户端的addDelegationTokens更新token。但addDelegationTokens存在问题，其只会更新HA token，不会更新private token，而AM向NameNode发起请求时，会使用private token，导致出现异常。</strong></p>\n<p>发生这个问题需要以下几个条件：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. NameNode HA is enabled.</div><div class=\"line\">2. Kerberos is enabled.</div><div class=\"line\">3. HDFS Delegation Token (not Keytab or TGT) is used to communicate with NameNode.</div><div class=\"line\">4. We want to update the HDFS Delegation Token <span class=\"keyword\">for</span> long running applicatons. HDFS Client will generate private tokens <span class=\"keyword\">for</span> each NameNode. When we update the HDFS Delegation Token, these private tokens will not be updated, <span class=\"built_in\">which</span> will cause token expired.</div></pre></td></tr></table></figure></p>\n<h3 id=\"hadoop修复方案\"><a href=\"#hadoop修复方案\" class=\"headerlink\" title=\"hadoop修复方案\"></a>hadoop修复方案</h3><p>这是hadoop的一个bug，在hadoop 2.9的新版本中修复了这个问题，所以，给hadoop打个patch就好了：</p>\n<p>Hadoop: <a href=\"https://issues.apache.org/jira/browse/HDFS-9276\" target=\"_blank\" rel=\"external\">https://issues.apache.org/jira/browse/HDFS-9276</a><br>hadoop2.9修复了private token不更新问题</p>\n<h3 id=\"spark修复方案\"><a href=\"#spark修复方案\" class=\"headerlink\" title=\"spark修复方案\"></a>spark修复方案</h3><p>Spark： <a href=\"https://github.com/apache/spark/pull/9168/files\" target=\"_blank\" rel=\"external\">https://github.com/apache/spark/pull/9168/files</a><br>主动使FileSystem对象关闭，再重新建立。FileSystem对象建立期间会重新设置private token</p>\n<p>由于hadoop变动对生产环境的影响很大，所以我们选择了spark的修复方案，修改了源码后打包编译上线，问题解决；</p>\n<p>感谢聪哥，也感谢那位在爱奇艺的发现并解决了这个问题的大神，没有你们，我要惨了。</p>\n<p>与Kerberos对干的日子很酸爽</p>\n<h3 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h3><p>还有一个方案可以解决这个问题，可以不需要修改spark源码，提交的时候增加参数：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">--conf spark.hadoop.fs.hdfs.impl.disable.cache=true</div></pre></td></tr></table></figure></p>\n<h4 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h4><p><a href=\"http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/\" target=\"_blank\" rel=\"external\">http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/</a></p>\n","excerpt":"","more":"<h2 id=\"问题描述：\"><a href=\"#问题描述：\" class=\"headerlink\" title=\"问题描述：\"></a>问题描述：</h2><p>SparkStreaming任务的Kerberos环境下两天后出现 AMRMTOKEN INVALID</p>\n<p>早上回来，跑在yarn上面的Streaming程序莫名奇妙崩溃了，yarn logs 查看日志，700万行，发现在每个executor的最后报错如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/11/01 15:02:05 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers <span class=\"keyword\">for</span> [TERM, HUP, INT]</div><div class=\"line\">16/11/01 15:02:06 INFO spark.SecurityManager: Changing view acls to: wzfw</div><div class=\"line\">16/11/01 15:02:06 INFO spark.SecurityManager: Changing modify acls to: wzfw</div><div class=\"line\">16/11/01 15:02:06 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wzfw); users with modify permissions: Set(wzfw)</div><div class=\"line\">...</div><div class=\"line\">16/11/02 03:51:33 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM</div><div class=\"line\">...</div><div class=\"line\">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM</div><div class=\"line\">16/11/02 03:51:34 WARN executor.CoarseGrainedExecutorBackend: An unknown (NM-304-SA5212M4-BIGDATA-519:44457) driver disconnected.</div><div class=\"line\">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.142.116.19:44457 disassociated! Shutting down.</div></pre></td></tr></table></figure></p>\n<p>这个报错应该只是表象，原因应该出自driver，定位到driver： NM-304-SA5212M4-BIGDATA-519<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/11/02 03:51:34 WARN executor.CoarseGrainedExecutorBackend: An unknown (NM-304-SA5212M4-BIGDATA-519:44457) driver disconnected.</div><div class=\"line\">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: Driver 10.142.116.19:44457 disassociated! Shutting down.</div><div class=\"line\">16/11/02 03:51:34 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM</div><div class=\"line\">16/11/02 03:51:34 ERROR client.TransportClient: Failed to send RPC 5285936577870185909 to NM-304-SA5212M4-BIGDATA-519/10.142.116.19:44457: java.nio.channels.ClosedChannelException</div><div class=\"line\">java.nio.channels.ClosedChannelException</div><div class=\"line\">16/11/02 03:51:34 WARN netty.NettyRpcEndpointRef: Error sending message [message = Heartbeat(53,[Lscala.Tuple2;@519f65d9,BlockManagerId(53, NM-304-SA5212M4-BIGDATA-382, 29163))] <span class=\"keyword\">in</span> 1 attempts</div><div class=\"line\">java.io.IOException: Failed to send RPC 5285936577870185909 to NM-304-SA5212M4-BIGDATA-519/10.142.116.19:44457: java.nio.channels.ClosedChannelException</div><div class=\"line\">        at org.apache.spark.network.client.TransportClient<span class=\"variable\">$3</span>.operationComplete(TransportClient.java:239)</div><div class=\"line\">        at org.apache.spark.network.client.TransportClient<span class=\"variable\">$3</span>.operationComplete(TransportClient.java:226)</div><div class=\"line\">        at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)</div><div class=\"line\">        at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)</div><div class=\"line\">        at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)</div><div class=\"line\">        at io.netty.channel.AbstractChannel<span class=\"variable\">$AbstractUnsafe</span>.safeSetFailure(AbstractChannel.java:801)</div><div class=\"line\">        at io.netty.channel.AbstractChannel<span class=\"variable\">$AbstractUnsafe</span>.write(AbstractChannel.java:699)</div><div class=\"line\">        at io.netty.channel.DefaultChannelPipeline<span class=\"variable\">$HeadContext</span>.write(DefaultChannelPipeline.java:1122)</div><div class=\"line\">        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)</div><div class=\"line\">        at io.netty.channel.AbstractChannelHandlerContext.access<span class=\"variable\">$1900</span>(AbstractChannelHandlerContext.java:32)</div><div class=\"line\">        at io.netty.channel.AbstractChannelHandlerContext<span class=\"variable\">$AbstractWriteTask</span>.write(AbstractChannelHandlerContext.java:908)</div><div class=\"line\">        at io.netty.channel.AbstractChannelHandlerContext<span class=\"variable\">$WriteAndFlushTask</span>.write(AbstractChannelHandlerContext.java:960)</div><div class=\"line\">        at io.netty.channel.AbstractChannelHandlerContext<span class=\"variable\">$AbstractWriteTask</span>.run(AbstractChannelHandlerContext.java:893)</div><div class=\"line\">        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)</div><div class=\"line\">        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)</div><div class=\"line\">        at io.netty.util.concurrent.SingleThreadEventExecutor<span class=\"variable\">$2</span>.run(SingleThreadEventExecutor.java:111)</div><div class=\"line\">        at java.lang.Thread.run(Thread.java:745)</div><div class=\"line\">Caused by: java.nio.channels.ClosedChannelException</div></pre></td></tr></table></figure></p>\n<p>找到driver的日志：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Container: container_1477044851292_468337_02_000004 on NM-304-SA5212M4-BIGDATA-519_8041</div><div class=\"line\">=========================================================================================</div><div class=\"line\">LogType:stderr</div><div class=\"line\">Log Upload Time:星期三 十一月 02 03:51:47 +0800 2016</div><div class=\"line\">LogLength:194115722</div><div class=\"line\">Log Contents:</div></pre></td></tr></table></figure></p>\n<p>根据时间定位：（vim中在driver中搜索 16\\/11\\/02 03:51:3）</p>\n<p>发现问题出题的源头：<br>在 16/11/02 03:40 之前，driver的日志还是比较稳定的，但是在 此之后，频繁出现下述异常：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/11/01 15:11:43 INFO ApplicationMaster: Registered signal handlers <span class=\"keyword\">for</span> [TERM, HUP, INT]</div><div class=\"line\">16/11/01 15:11:43 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1477044851292_468337_000002</div><div class=\"line\">16/11/01 15:11:45 INFO SecurityManager: Changing view acls to: wzfw</div><div class=\"line\">16/11/01 15:11:45 INFO SecurityManager: Changing modify acls to: wzfw</div><div class=\"line\">16/11/01 15:11:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wzfw); users with modify permissions: Set(wzfw)</div><div class=\"line\">...</div><div class=\"line\">16/11/02 03:41:17 WARN Client: Exception encountered <span class=\"keyword\">while</span> connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager<span class=\"variable\">$InvalidToken</span>): Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class=\"line\">16/11/02 03:41:17 INFO RetryInvocationHandler: Exception <span class=\"keyword\">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm1. Trying to fail over immediately.</div><div class=\"line\">org.apache.hadoop.security.token.SecretManager<span class=\"variable\">$InvalidToken</span>: Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</div><div class=\"line\">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class=\"line\">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class=\"line\">        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)</div><div class=\"line\">        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)</div><div class=\"line\">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class=\"line\">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy19</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class=\"variable\">$anon</span><span class=\"variable\">$1</span>.run(ApplicationMaster.scala:384)</div><div class=\"line\">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager<span class=\"variable\">$InvalidToken</span>): Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1468)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class=\"line\">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class=\"variable\">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy18</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class=\"line\">        ... 9 more</div><div class=\"line\">16/11/02 03:41:17 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2</div><div class=\"line\">16/11/02 03:41:17 INFO RetryInvocationHandler: Exception <span class=\"keyword\">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm2 after 1 fail over attempts. Trying to fail over after sleeping <span class=\"keyword\">for</span> 22672ms.</div><div class=\"line\">java.net.ConnectException: Call From NM-304-SA5212M4-BIGDATA-519/10.142.116.19 to NM-304-RH5885V3-BIGDATA-008:8030 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</div><div class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class=\"line\">        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</div><div class=\"line\">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class=\"line\">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1472)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class=\"line\">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class=\"variable\">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy18</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class=\"line\">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class=\"line\">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy19</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class=\"variable\">$anon</span><span class=\"variable\">$1</span>.run(ApplicationMaster.scala:384)</div><div class=\"line\">Caused by: java.net.ConnectException: Connection refused</div><div class=\"line\">        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</div><div class=\"line\">        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)</div><div class=\"line\">        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.setupConnection(Client.java:607)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.setupIOstreams(Client.java:705)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.access<span class=\"variable\">$2800</span>(Client.java:368)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1438)</div><div class=\"line\">        ... 13 more</div><div class=\"line\">16/11/02 03:41:20 INFO JobScheduler: Added <span class=\"built_in\">jobs</span> <span class=\"keyword\">for</span> time 1478029280000 ms</div></pre></td></tr></table></figure>\n<p>然后这些日志持续出现了十分钟左右，当然spark的stage是依旧在增加，依旧在运行</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/11/02 03:51:27 INFO RetryInvocationHandler: Exception <span class=\"keyword\">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm1 after 14 fail over attempts. Trying to fail over immediately.</div><div class=\"line\">org.apache.hadoop.security.token.SecretManager<span class=\"variable\">$InvalidToken</span>: Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class=\"line\">        at sun.reflect.GeneratedConstructorAccessor74.newInstance(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class=\"line\">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class=\"line\">        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)</div><div class=\"line\">        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)</div><div class=\"line\">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class=\"line\">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy19</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class=\"variable\">$anon</span><span class=\"variable\">$1</span>.run(ApplicationMaster.scala:384)</div><div class=\"line\">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager<span class=\"variable\">$InvalidToken</span>): Invalid AMRMToken from appattempt_1477044851292_468337_000002</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1468)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class=\"line\">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class=\"variable\">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy18</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class=\"line\">        ... 9 more</div><div class=\"line\">16/11/02 03:51:27 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2</div><div class=\"line\">16/11/02 03:51:27 INFO RetryInvocationHandler: Exception <span class=\"keyword\">while</span> invoking allocate of class ApplicationMasterProtocolPBClientImpl over rm2 after 15 fail over attempts. Trying to fail over after sleeping <span class=\"keyword\">for</span> 31772ms.</div><div class=\"line\">java.net.ConnectException: Call From NM-304-SA5212M4-BIGDATA-519/10.142.116.19 to NM-304-RH5885V3-BIGDATA-008:8030 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</div><div class=\"line\">        at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class=\"line\">        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1472)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1399)</div><div class=\"line\">        at org.apache.hadoop.ipc.ProtobufRpcEngine<span class=\"variable\">$Invoker</span>.invoke(ProtobufRpcEngine.java:232)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy18</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)</div><div class=\"line\">        at sun.reflect.GeneratedMethodAccessor137.invoke(Unknown Source)</div><div class=\"line\">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class=\"line\">        at java.lang.reflect.Method.invoke(Method.java:606)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy19</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class=\"variable\">$anon</span><span class=\"variable\">$1</span>.run(ApplicationMaster.scala:384)</div><div class=\"line\">Caused by: java.net.ConnectException: Connection refused</div><div class=\"line\">        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</div><div class=\"line\">        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)</div><div class=\"line\">        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</div><div class=\"line\">        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.setupConnection(Client.java:607)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.setupIOstreams(Client.java:705)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client<span class=\"variable\">$Connection</span>.access<span class=\"variable\">$2800</span>(Client.java:368)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)</div><div class=\"line\">        at org.apache.hadoop.ipc.Client.call(Client.java:1438)</div><div class=\"line\">        ... 13 more</div></pre></td></tr></table></figure>\n<p>然后在最后收到这个信号就挂了：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/11/02 03:51:31 INFO WriteAheadLogManager  <span class=\"keyword\">for</span> Thread: Attempting to clear 0 old <span class=\"built_in\">log</span> files <span class=\"keyword\">in</span> hdfs://ns/user/wzfw/checkpoint/receivedBlockMetadata older than 1478029880000:</div><div class=\"line\">16/11/02 03:51:31 INFO InputInfoTracker: remove old batch metadata: 1478029870000 ms</div><div class=\"line\">16/11/02 03:51:32 INFO ApplicationMaster: Final app status: FAILED, <span class=\"built_in\">exit</span>Code: 16</div><div class=\"line\">16/11/02 03:51:32 WARN ApplicationMaster: Reporter thread fails 2 time(s) <span class=\"keyword\">in</span> a row.</div><div class=\"line\">java.lang.reflect.UndeclaredThrowableException</div><div class=\"line\">        at com.sun.proxy.<span class=\"variable\">$Proxy19</span>.allocate(Unknown Source)</div><div class=\"line\">        at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:278)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:225)</div><div class=\"line\">        at org.apache.spark.deploy.yarn.ApplicationMaster$<span class=\"variable\">$anon</span><span class=\"variable\">$1</span>.run(ApplicationMaster.scala:384)</div><div class=\"line\">Caused by: java.lang.InterruptedException: sleep interrupted</div><div class=\"line\">        at java.lang.Thread.sleep(Native Method)</div><div class=\"line\">        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:151)</div><div class=\"line\">        ... 4 more</div><div class=\"line\">16/11/02 03:51:32 ERROR ApplicationMaster: RECEIVED SIGNAL 15: SIGTERM</div><div class=\"line\">16/11/02 03:51:33 INFO StreamingContext: Invoking stop(stopGracefully=<span class=\"literal\">false</span>) from shutdown hook</div><div class=\"line\">16/11/02 03:51:33 INFO JobGenerator: Stopping JobGenerator immediately</div></pre></td></tr></table></figure>\n<p>后来又跑了一次，又因为同样的原因挂了，这次统计了时间：<br>slaver：<br>程序启动时间：16/11/02 22:58:27<br>程序结束时间： 16/11/03 22:51:34</p>\n<p>driver<br>程序启动时间：16/11/02 22:58:18<br>遇到Kerberos问题时间： 16/11/03 22:41:23<br>程序结束时间：16/11/03 22:51:35</p>\n<p>上面的时间是有不正确的地方的，因为我启动的时间肯定不是16/11/02 22:58:18（那个时候我已经回家了），我是在 11/02号当天上午启动，所以一个很可能的原因是，在22:58的这个时候，因为种种原因，程序崩溃重启，换了一个driver。</p>\n<h2 id=\"可能的原因\"><a href=\"#可能的原因\" class=\"headerlink\" title=\"可能的原因\"></a>可能的原因</h2><p>google一下，网上查到有类似的原因：<br>storm on yarn在这个问题中存在一些bug；<br><a href=\"https://community.cloudera.com/t5/Batch-Processing-and-Workflow/Long-running-yarn-app-storm-yarn-exits-with-Invalid-AMRMToken/td-p/26717\">https://community.cloudera.com/t5/Batch-Processing-and-Workflow/Long-running-yarn-app-storm-yarn-exits-with-Invalid-AMRMToken/td-p/26717</a></p>\n<p><a href=\"http://www.codeflitting.com/blog/article/%E4%B8%BA%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E7%9A%84%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%85%8D%E7%BD%AE%20YARN\">http://www.codeflitting.com/blog/article/%E4%B8%BA%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E7%9A%84%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%85%8D%E7%BD%AE%20YARN</a></p>\n<p><a href=\"https://issues.apache.org/jira/browse/YARN-3103\">https://issues.apache.org/jira/browse/YARN-3103</a></p>\n<p>根据日志以及网上资料，初步认为与Kerberos过期认证有关，AMRMToken无效，也就是说，在运行了一段时期（一天多）后，AM到RM的token无效了，然后无效很多次后，后续的任务分配不到container，所以程序就挂了，突然想到，出现这种错误的原因有以下几种；</p>\n<ol>\n<li><p>大约12个小时以前，SparkStreaming程序因为后面的flume测试守护进程，导致换了一个driver，是否是由于driver改动后，导致token失效？</p>\n</li>\n<li><p>yarn 2.6.0有一个相关的bug，<a href=\"https://issues.apache.org/jira/browse/YARN-3103\">https://issues.apache.org/jira/browse/YARN-3103</a><br>AMRMClientImpl.updateAMRMToken updates the token service before storing it to the credentials, so the token is mapped using the newly updated service rather than the empty service that was used when the RM created the original AMRM token. This leads to two AMRM tokens in the credentials and can still fail if the AMRMTokenSelector picks the wrong one.<br>In addition the AMRMClientImpl grabs the login user rather than the current user when security is enabled, so it’s likely the UGI being updated is not the UGI that will be used when reconnecting to the RM.<br>The end result is that AMs can fail with invalid token errors when trying to reconnect to an RM after a new AMRM secret has been activated.<br>大概意思是，更新token的时候会有两个token，如果选择了错误的token，就会导致出错</p>\n</li>\n<li><p>hadoop设置仅允许 hdfs 用户的委派令牌保留最大生存期 7 （default）天，这始终是不够的<br>需要修改一下yarn的配置，增加如下参数：</p>\n</li>\n</ol>\n<p>将 ResourceManager 配置为对应 HDFS NameNode 的代理用户，以便在现有令牌超过其最大生存期时，ResourceManager 可以请求新的令牌。YARN 随后能够代表 hdfs 用户继续执行本地化和日志聚合</p>\n<figure class=\"highlight xml\"><figcaption><span>yarn-site.xml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.proxy-user-privileges.enabled<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">``` xml core-site.xml</div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.yarn.hosts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.yarn.groups<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div></pre></td></tr></table></figure>\n<p>然后重启YARN和HDFS服务</p>\n<h2 id=\"问题的暂时解决\"><a href=\"#问题的暂时解决\" class=\"headerlink\" title=\"问题的暂时解决\"></a>问题的暂时解决</h2><p>由于项目紧张，我写了一个脚本，每隔一分钟监控yarn上面运行的spark Streaming程序，如果挂了就重新启动起来，虽然也不影响后续的结果。后来观察，基本上是一天一挂或者两天一挂，不过这也太不靠谱了。</p>\n<h2 id=\"问题解决\"><a href=\"#问题解决\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h2><p>后来发现都不是上述原因，同事的前爱奇艺的同事遇到过同样的问题：原因是：<br><strong> NameNode采用了HA后，AM与Namenode通信使用的token的结构变为HA token,HA token中会有两个private token，代表两台namenode服务。 当AM更新token时，会调用hadoop客户端的addDelegationTokens更新token。但addDelegationTokens存在问题，其只会更新HA token，不会更新private token，而AM向NameNode发起请求时，会使用private token，导致出现异常。</strong></p>\n<p>发生这个问题需要以下几个条件：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. NameNode HA is enabled.</div><div class=\"line\">2. Kerberos is enabled.</div><div class=\"line\">3. HDFS Delegation Token (not Keytab or TGT) is used to communicate with NameNode.</div><div class=\"line\">4. We want to update the HDFS Delegation Token <span class=\"keyword\">for</span> long running applicatons. HDFS Client will generate private tokens <span class=\"keyword\">for</span> each NameNode. When we update the HDFS Delegation Token, these private tokens will not be updated, <span class=\"built_in\">which</span> will cause token expired.</div></pre></td></tr></table></figure></p>\n<h3 id=\"hadoop修复方案\"><a href=\"#hadoop修复方案\" class=\"headerlink\" title=\"hadoop修复方案\"></a>hadoop修复方案</h3><p>这是hadoop的一个bug，在hadoop 2.9的新版本中修复了这个问题，所以，给hadoop打个patch就好了：</p>\n<p>Hadoop: <a href=\"https://issues.apache.org/jira/browse/HDFS-9276\">https://issues.apache.org/jira/browse/HDFS-9276</a><br>hadoop2.9修复了private token不更新问题</p>\n<h3 id=\"spark修复方案\"><a href=\"#spark修复方案\" class=\"headerlink\" title=\"spark修复方案\"></a>spark修复方案</h3><p>Spark： <a href=\"https://github.com/apache/spark/pull/9168/files\">https://github.com/apache/spark/pull/9168/files</a><br>主动使FileSystem对象关闭，再重新建立。FileSystem对象建立期间会重新设置private token</p>\n<p>由于hadoop变动对生产环境的影响很大，所以我们选择了spark的修复方案，修改了源码后打包编译上线，问题解决；</p>\n<p>感谢聪哥，也感谢那位在爱奇艺的发现并解决了这个问题的大神，没有你们，我要惨了。</p>\n<p>与Kerberos对干的日子很酸爽</p>\n<h3 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h3><p>还有一个方案可以解决这个问题，可以不需要修改spark源码，提交的时候增加参数：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">--conf spark.hadoop.fs.hdfs.impl.disable.cache=true</div></pre></td></tr></table></figure></p>\n<h4 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h4><p><a href=\"http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/\">http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/</a></p>\n"},{"title":"通过经纬度计算距离算法的scala实现","toc":false,"date":"2016-11-24T09:06:15.000Z","_content":"\n## 问题描述\n\n给定一个景点的经纬度，给定距离，给定形状，判断其它点是否在某个区域内：\n\n## 圆形方案\n\n使用通用的地球上两点距离函数，圆形只需要判断距离，正方形需要计算两次距离（指定经度与景点的经度一样，计算是否在范围内，然后指定纬度与景点的纬度一样，计算是否在范围内，如果都在范围内，则代表该点在景区范围内），其它形状基于这个基础类推\n\n### 距离函数\n\n``` scala\n  /**\n    * Created on 2016/10/9.\n    *\n    * 求地球上两点间的距离 返回的是 double 格式的 km\n    * 第一点经纬度为（lat1,lng1），第二点经纬度为（lat2,lng2），地球平均半径R=6378.137\n    * 按照0度经线的基准，东经取经度的正值(Longitude)，西经取经度负值(-Longitude)，北纬取90-纬度值(90- Latitude)，南纬取90+纬度值(90+Latitude)，\n    * 则经过上述处理过后的两点被计为(MLon1, MLat1)和(MLon2, MLat2)。那么根据三角推导，可以得到计算两点距离的如下公式：\n    * C = sin(MLat1)*sin(MLat2)*cos(MLon1-MLon2) + cos(MLat1)*cos(MLat2)，Distance = R*Arccos(C)*Pi/180\n    * 如果仅对经度作正负的处理，而不对纬度作90-Latitude(假设都是北半球，南半球只有澳洲具有应用意义)的处理，那么公式将是：\n    * C = sin(LatA)*sin(LatB) + cos(LatA)*cos(LatB)*cos(MLonA-MLonB)，Distance = R*Arccos(C)*Pi/180\n    * 三角函数的输入和输出都采用弧度值，那么公式还可以写作:\n    * C = sin(Lat1*Pi/180)*sin(Lat2*Pi/180) + cos(Lat1*Pi/180)*cos(Lat2*Pi/180)*cos((MLon1-MLon2)*Pi/180)，Distance = R*Arccos(C)*Pi/180\n    * rad()函数求弧度，Distance（）函数求距离\n    *\n    * 结果验证工具地址 http://www.storyday.com/wp-content/uploads/2008/09/latlung_dis.html\n    */\n  def distance(lat1: Double, lng1: Double, lat2: Double, lng2: Double): Double = {\n    val EARTH_RADIUS = 6378.137\n\n    val radLat1 = rad(lat1)\n    val radLat2 = rad(lat2)\n    val a = rad(lat1) - rad(lat2)\n    val b = rad(lng1) - rad(lng2)\n    val distance = EARTH_RADIUS * 2 * Math.asin(Math.sqrt(Math.pow(Math.sin(a / 2), 2) + Math.cos(radLat1) * Math.cos(radLat2) * Math.pow(Math.sin((b) / 2), 2)))\n    //    printLog.debug(\"lat1: \" + lat1 + \" lng1: \" + lng1 + \" lat2: \" + lat2 + \" lng2: \" + lng2)\n    printLog.debug(\"distance:\" + distance)\n    distance\n  }\n```\n\n## 不规则图形方案\n\nscala实现函数：使用现成的算法PNPoly即可实现；\n\n``` scala\n  /**\n    * 多点位置判断算法 判断一个坐标点是否在不规则多边形内部\n    * *\n    * 在 GIS（地理信息管理系统）中，判断一个坐标是否在多边形内部是个经常要遇到的问题。乍听起来还挺复杂。\n    * 根据 W. Randolph Franklin 提出的 PNPoly (http://www.ecse.rpi.edu/Homepages/wrf/Research/Short_Notes/pnpoly.html) 算法，只需区区几行代码就解决了这个问题:\n    * *\n    * 针对每一个点，算法遍历多边形相邻的每两个顶点（即一条边），假如待判断点满足以下两个条件即改变点是否在多边形内的状态标识c：\n    * 待判断点的Y坐标在点i和点j的Y坐标范围之内\n    * 待判断点的X坐标在点i和点j连线之下\n    * 遍历所有的边之后假如以上两个条件同时满足奇数次则该带判断点位于多边形之内，否则位于多边形之外。\n    * 算法复杂度为O(n)，其中n为多边形的顶点个数。\n    *\n    * @param vertexes\n    * @param testPoint\n    * @return\n    */\n  def pNPoly(vertexes: Array[LocationPoint], testPoint: LocationPoint): Boolean = {\n    var flag = false\n    var flag0 = false\n    var flag1 = false\n    var flag2 = false\n    var flag3 = false\n    var j = vertexes.length - 1\n\n    val loop = new Breaks\n    loop.breakable {\n      for (i <- 0 until vertexes.length) {\n        if (i != 0) {\n          j = i - 1\n        }\n        if ((vertexes(i).lat == testPoint.lat) && (vertexes(i).long == testPoint.long)) {\n          flag0 = true\n        }\n\n        if (((vertexes(i).lat - testPoint.lat) * (vertexes(i).long - testPoint.long) * (vertexes(j).lat - testPoint.lat) * (vertexes(j).long - testPoint.long) == 0) &&\n          (((vertexes(i).lat > testPoint.lat) != (vertexes(j).lat > testPoint.lat)) && ((vertexes(i).long > testPoint.long) != (vertexes(j).long > testPoint.long)))) {\n          flag1 = true\n        }\n\n        if (((vertexes(i).lat - testPoint.lat) * (vertexes(i).long - testPoint.long) * (vertexes(j).lat - testPoint.lat) * (vertexes(j).long - testPoint.long) != 0) &&\n          ((vertexes(i).lat - testPoint.lat) / (vertexes(i).long - testPoint.long) == (vertexes(j).lat - testPoint.lat) / (vertexes(j).long - testPoint.long))) {\n          flag2 = true\n        }\n\n        if (((vertexes(i).lat > testPoint.lat) != (vertexes(j).lat > testPoint.lat)) &&\n          (testPoint.long < (vertexes(j).long - vertexes(i).long) * (testPoint.lat - vertexes(j).lat) / (vertexes(j).lat - vertexes(i).lat) + vertexes(i).long)) {\n          flag3 = true\n        }\n\n        flag = flag0 || flag1 || flag2 || flag3\n\n        if (flag) {\n          loop.break\n        }\n      }\n    }\n    flag\n  }\n```\n\n在运用该算法之前，有个优化方案，就是先进行最小外接矩形范围判断，如果不在最小外接矩形中，则直接跳过，\n``` c\nif (p.x < minX || p.x > maxX || p.y < minY || p.y > maxY) {\n    // 点在多边形之外\n}\n```\n首先判断点是否在多边形的最小外接矩形之内，该步骤不是必须的，但是可以有效避免不必要的计算。\n\n\n## 引用\n\nhttp://riyueshi.github.io/2015/10/07/is_point_in_polygon/","source":"_posts/2016-11-24-通过经纬度计算距离算法的scala实现.md","raw":"---\ntitle: 通过经纬度计算距离算法的scala实现\ntoc: false\ndate: 2016-11-24 17:06:15\ntags: \n- 算法\n- scala\ncategories: \n- scala\n---\n\n## 问题描述\n\n给定一个景点的经纬度，给定距离，给定形状，判断其它点是否在某个区域内：\n\n## 圆形方案\n\n使用通用的地球上两点距离函数，圆形只需要判断距离，正方形需要计算两次距离（指定经度与景点的经度一样，计算是否在范围内，然后指定纬度与景点的纬度一样，计算是否在范围内，如果都在范围内，则代表该点在景区范围内），其它形状基于这个基础类推\n\n### 距离函数\n\n``` scala\n  /**\n    * Created on 2016/10/9.\n    *\n    * 求地球上两点间的距离 返回的是 double 格式的 km\n    * 第一点经纬度为（lat1,lng1），第二点经纬度为（lat2,lng2），地球平均半径R=6378.137\n    * 按照0度经线的基准，东经取经度的正值(Longitude)，西经取经度负值(-Longitude)，北纬取90-纬度值(90- Latitude)，南纬取90+纬度值(90+Latitude)，\n    * 则经过上述处理过后的两点被计为(MLon1, MLat1)和(MLon2, MLat2)。那么根据三角推导，可以得到计算两点距离的如下公式：\n    * C = sin(MLat1)*sin(MLat2)*cos(MLon1-MLon2) + cos(MLat1)*cos(MLat2)，Distance = R*Arccos(C)*Pi/180\n    * 如果仅对经度作正负的处理，而不对纬度作90-Latitude(假设都是北半球，南半球只有澳洲具有应用意义)的处理，那么公式将是：\n    * C = sin(LatA)*sin(LatB) + cos(LatA)*cos(LatB)*cos(MLonA-MLonB)，Distance = R*Arccos(C)*Pi/180\n    * 三角函数的输入和输出都采用弧度值，那么公式还可以写作:\n    * C = sin(Lat1*Pi/180)*sin(Lat2*Pi/180) + cos(Lat1*Pi/180)*cos(Lat2*Pi/180)*cos((MLon1-MLon2)*Pi/180)，Distance = R*Arccos(C)*Pi/180\n    * rad()函数求弧度，Distance（）函数求距离\n    *\n    * 结果验证工具地址 http://www.storyday.com/wp-content/uploads/2008/09/latlung_dis.html\n    */\n  def distance(lat1: Double, lng1: Double, lat2: Double, lng2: Double): Double = {\n    val EARTH_RADIUS = 6378.137\n\n    val radLat1 = rad(lat1)\n    val radLat2 = rad(lat2)\n    val a = rad(lat1) - rad(lat2)\n    val b = rad(lng1) - rad(lng2)\n    val distance = EARTH_RADIUS * 2 * Math.asin(Math.sqrt(Math.pow(Math.sin(a / 2), 2) + Math.cos(radLat1) * Math.cos(radLat2) * Math.pow(Math.sin((b) / 2), 2)))\n    //    printLog.debug(\"lat1: \" + lat1 + \" lng1: \" + lng1 + \" lat2: \" + lat2 + \" lng2: \" + lng2)\n    printLog.debug(\"distance:\" + distance)\n    distance\n  }\n```\n\n## 不规则图形方案\n\nscala实现函数：使用现成的算法PNPoly即可实现；\n\n``` scala\n  /**\n    * 多点位置判断算法 判断一个坐标点是否在不规则多边形内部\n    * *\n    * 在 GIS（地理信息管理系统）中，判断一个坐标是否在多边形内部是个经常要遇到的问题。乍听起来还挺复杂。\n    * 根据 W. Randolph Franklin 提出的 PNPoly (http://www.ecse.rpi.edu/Homepages/wrf/Research/Short_Notes/pnpoly.html) 算法，只需区区几行代码就解决了这个问题:\n    * *\n    * 针对每一个点，算法遍历多边形相邻的每两个顶点（即一条边），假如待判断点满足以下两个条件即改变点是否在多边形内的状态标识c：\n    * 待判断点的Y坐标在点i和点j的Y坐标范围之内\n    * 待判断点的X坐标在点i和点j连线之下\n    * 遍历所有的边之后假如以上两个条件同时满足奇数次则该带判断点位于多边形之内，否则位于多边形之外。\n    * 算法复杂度为O(n)，其中n为多边形的顶点个数。\n    *\n    * @param vertexes\n    * @param testPoint\n    * @return\n    */\n  def pNPoly(vertexes: Array[LocationPoint], testPoint: LocationPoint): Boolean = {\n    var flag = false\n    var flag0 = false\n    var flag1 = false\n    var flag2 = false\n    var flag3 = false\n    var j = vertexes.length - 1\n\n    val loop = new Breaks\n    loop.breakable {\n      for (i <- 0 until vertexes.length) {\n        if (i != 0) {\n          j = i - 1\n        }\n        if ((vertexes(i).lat == testPoint.lat) && (vertexes(i).long == testPoint.long)) {\n          flag0 = true\n        }\n\n        if (((vertexes(i).lat - testPoint.lat) * (vertexes(i).long - testPoint.long) * (vertexes(j).lat - testPoint.lat) * (vertexes(j).long - testPoint.long) == 0) &&\n          (((vertexes(i).lat > testPoint.lat) != (vertexes(j).lat > testPoint.lat)) && ((vertexes(i).long > testPoint.long) != (vertexes(j).long > testPoint.long)))) {\n          flag1 = true\n        }\n\n        if (((vertexes(i).lat - testPoint.lat) * (vertexes(i).long - testPoint.long) * (vertexes(j).lat - testPoint.lat) * (vertexes(j).long - testPoint.long) != 0) &&\n          ((vertexes(i).lat - testPoint.lat) / (vertexes(i).long - testPoint.long) == (vertexes(j).lat - testPoint.lat) / (vertexes(j).long - testPoint.long))) {\n          flag2 = true\n        }\n\n        if (((vertexes(i).lat > testPoint.lat) != (vertexes(j).lat > testPoint.lat)) &&\n          (testPoint.long < (vertexes(j).long - vertexes(i).long) * (testPoint.lat - vertexes(j).lat) / (vertexes(j).lat - vertexes(i).lat) + vertexes(i).long)) {\n          flag3 = true\n        }\n\n        flag = flag0 || flag1 || flag2 || flag3\n\n        if (flag) {\n          loop.break\n        }\n      }\n    }\n    flag\n  }\n```\n\n在运用该算法之前，有个优化方案，就是先进行最小外接矩形范围判断，如果不在最小外接矩形中，则直接跳过，\n``` c\nif (p.x < minX || p.x > maxX || p.y < minY || p.y > maxY) {\n    // 点在多边形之外\n}\n```\n首先判断点是否在多边形的最小外接矩形之内，该步骤不是必须的，但是可以有效避免不必要的计算。\n\n\n## 引用\n\nhttp://riyueshi.github.io/2015/10/07/is_point_in_polygon/","slug":"通过经纬度计算距离算法的scala实现","published":1,"updated":"2017-06-28T12:52:53.157Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfwb001rpsgu552fo1vr","content":"<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><p>给定一个景点的经纬度，给定距离，给定形状，判断其它点是否在某个区域内：</p>\n<h2 id=\"圆形方案\"><a href=\"#圆形方案\" class=\"headerlink\" title=\"圆形方案\"></a>圆形方案</h2><p>使用通用的地球上两点距离函数，圆形只需要判断距离，正方形需要计算两次距离（指定经度与景点的经度一样，计算是否在范围内，然后指定纬度与景点的纬度一样，计算是否在范围内，如果都在范围内，则代表该点在景区范围内），其它形状基于这个基础类推</p>\n<h3 id=\"距离函数\"><a href=\"#距离函数\" class=\"headerlink\" title=\"距离函数\"></a>距离函数</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">  * Created on 2016/10/9.</div><div class=\"line\">  *</div><div class=\"line\">  * 求地球上两点间的距离 返回的是 double 格式的 km</div><div class=\"line\">  * 第一点经纬度为（lat1,lng1），第二点经纬度为（lat2,lng2），地球平均半径R=6378.137</div><div class=\"line\">  * 按照0度经线的基准，东经取经度的正值(Longitude)，西经取经度负值(-Longitude)，北纬取90-纬度值(90- Latitude)，南纬取90+纬度值(90+Latitude)，</div><div class=\"line\">  * 则经过上述处理过后的两点被计为(MLon1, MLat1)和(MLon2, MLat2)。那么根据三角推导，可以得到计算两点距离的如下公式：</div><div class=\"line\">  * C = sin(MLat1)*sin(MLat2)*cos(MLon1-MLon2) + cos(MLat1)*cos(MLat2)，Distance = R*Arccos(C)*Pi/180</div><div class=\"line\">  * 如果仅对经度作正负的处理，而不对纬度作90-Latitude(假设都是北半球，南半球只有澳洲具有应用意义)的处理，那么公式将是：</div><div class=\"line\">  * C = sin(LatA)*sin(LatB) + cos(LatA)*cos(LatB)*cos(MLonA-MLonB)，Distance = R*Arccos(C)*Pi/180</div><div class=\"line\">  * 三角函数的输入和输出都采用弧度值，那么公式还可以写作:</div><div class=\"line\">  * C = sin(Lat1*Pi/180)*sin(Lat2*Pi/180) + cos(Lat1*Pi/180)*cos(Lat2*Pi/180)*cos((MLon1-MLon2)*Pi/180)，Distance = R*Arccos(C)*Pi/180</div><div class=\"line\">  * rad()函数求弧度，Distance（）函数求距离</div><div class=\"line\">  *</div><div class=\"line\">  * 结果验证工具地址 http://www.storyday.com/wp-content/uploads/2008/09/latlung_dis.html</div><div class=\"line\">  */</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">distance</span></span>(lat1: <span class=\"type\">Double</span>, lng1: <span class=\"type\">Double</span>, lat2: <span class=\"type\">Double</span>, lng2: <span class=\"type\">Double</span>): <span class=\"type\">Double</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">EARTH_RADIUS</span> = <span class=\"number\">6378.137</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">val</span> radLat1 = rad(lat1)</div><div class=\"line\">  <span class=\"keyword\">val</span> radLat2 = rad(lat2)</div><div class=\"line\">  <span class=\"keyword\">val</span> a = rad(lat1) - rad(lat2)</div><div class=\"line\">  <span class=\"keyword\">val</span> b = rad(lng1) - rad(lng2)</div><div class=\"line\">  <span class=\"keyword\">val</span> distance = <span class=\"type\">EARTH_RADIUS</span> * <span class=\"number\">2</span> * <span class=\"type\">Math</span>.asin(<span class=\"type\">Math</span>.sqrt(<span class=\"type\">Math</span>.pow(<span class=\"type\">Math</span>.sin(a / <span class=\"number\">2</span>), <span class=\"number\">2</span>) + <span class=\"type\">Math</span>.cos(radLat1) * <span class=\"type\">Math</span>.cos(radLat2) * <span class=\"type\">Math</span>.pow(<span class=\"type\">Math</span>.sin((b) / <span class=\"number\">2</span>), <span class=\"number\">2</span>)))</div><div class=\"line\">  <span class=\"comment\">//    printLog.debug(\"lat1: \" + lat1 + \" lng1: \" + lng1 + \" lat2: \" + lat2 + \" lng2: \" + lng2)</span></div><div class=\"line\">  printLog.debug(<span class=\"string\">\"distance:\"</span> + distance)</div><div class=\"line\">  distance</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"不规则图形方案\"><a href=\"#不规则图形方案\" class=\"headerlink\" title=\"不规则图形方案\"></a>不规则图形方案</h2><p>scala实现函数：使用现成的算法PNPoly即可实现；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">  * 多点位置判断算法 判断一个坐标点是否在不规则多边形内部</div><div class=\"line\">  * *</div><div class=\"line\">  * 在 GIS（地理信息管理系统）中，判断一个坐标是否在多边形内部是个经常要遇到的问题。乍听起来还挺复杂。</div><div class=\"line\">  * 根据 W. Randolph Franklin 提出的 PNPoly (http://www.ecse.rpi.edu/Homepages/wrf/Research/Short_Notes/pnpoly.html) 算法，只需区区几行代码就解决了这个问题:</div><div class=\"line\">  * *</div><div class=\"line\">  * 针对每一个点，算法遍历多边形相邻的每两个顶点（即一条边），假如待判断点满足以下两个条件即改变点是否在多边形内的状态标识c：</div><div class=\"line\">  * 待判断点的Y坐标在点i和点j的Y坐标范围之内</div><div class=\"line\">  * 待判断点的X坐标在点i和点j连线之下</div><div class=\"line\">  * 遍历所有的边之后假如以上两个条件同时满足奇数次则该带判断点位于多边形之内，否则位于多边形之外。</div><div class=\"line\">  * 算法复杂度为O(n)，其中n为多边形的顶点个数。</div><div class=\"line\">  *</div><div class=\"line\">  * @param vertexes</div><div class=\"line\">  * @param testPoint</div><div class=\"line\">  * @return</div><div class=\"line\">  */</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pNPoly</span></span>(vertexes: <span class=\"type\">Array</span>[<span class=\"type\">LocationPoint</span>], testPoint: <span class=\"type\">LocationPoint</span>): <span class=\"type\">Boolean</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">var</span> flag = <span class=\"literal\">false</span></div><div class=\"line\">  <span class=\"keyword\">var</span> flag0 = <span class=\"literal\">false</span></div><div class=\"line\">  <span class=\"keyword\">var</span> flag1 = <span class=\"literal\">false</span></div><div class=\"line\">  <span class=\"keyword\">var</span> flag2 = <span class=\"literal\">false</span></div><div class=\"line\">  <span class=\"keyword\">var</span> flag3 = <span class=\"literal\">false</span></div><div class=\"line\">  <span class=\"keyword\">var</span> j = vertexes.length - <span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">val</span> loop = <span class=\"keyword\">new</span> <span class=\"type\">Breaks</span></div><div class=\"line\">  loop.breakable &#123;</div><div class=\"line\">    <span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">0</span> until vertexes.length) &#123;</div><div class=\"line\">      <span class=\"keyword\">if</span> (i != <span class=\"number\">0</span>) &#123;</div><div class=\"line\">        j = i - <span class=\"number\">1</span></div><div class=\"line\">      &#125;</div><div class=\"line\">      <span class=\"keyword\">if</span> ((vertexes(i).lat == testPoint.lat) &amp;&amp; (vertexes(i).long == testPoint.long)) &#123;</div><div class=\"line\">        flag0 = <span class=\"literal\">true</span></div><div class=\"line\">      &#125;</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">if</span> (((vertexes(i).lat - testPoint.lat) * (vertexes(i).long - testPoint.long) * (vertexes(j).lat - testPoint.lat) * (vertexes(j).long - testPoint.long) == <span class=\"number\">0</span>) &amp;&amp;</div><div class=\"line\">        (((vertexes(i).lat &gt; testPoint.lat) != (vertexes(j).lat &gt; testPoint.lat)) &amp;&amp; ((vertexes(i).long &gt; testPoint.long) != (vertexes(j).long &gt; testPoint.long)))) &#123;</div><div class=\"line\">        flag1 = <span class=\"literal\">true</span></div><div class=\"line\">      &#125;</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">if</span> (((vertexes(i).lat - testPoint.lat) * (vertexes(i).long - testPoint.long) * (vertexes(j).lat - testPoint.lat) * (vertexes(j).long - testPoint.long) != <span class=\"number\">0</span>) &amp;&amp;</div><div class=\"line\">        ((vertexes(i).lat - testPoint.lat) / (vertexes(i).long - testPoint.long) == (vertexes(j).lat - testPoint.lat) / (vertexes(j).long - testPoint.long))) &#123;</div><div class=\"line\">        flag2 = <span class=\"literal\">true</span></div><div class=\"line\">      &#125;</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">if</span> (((vertexes(i).lat &gt; testPoint.lat) != (vertexes(j).lat &gt; testPoint.lat)) &amp;&amp;</div><div class=\"line\">        (testPoint.long &lt; (vertexes(j).long - vertexes(i).long) * (testPoint.lat - vertexes(j).lat) / (vertexes(j).lat - vertexes(i).lat) + vertexes(i).long)) &#123;</div><div class=\"line\">        flag3 = <span class=\"literal\">true</span></div><div class=\"line\">      &#125;</div><div class=\"line\"></div><div class=\"line\">      flag = flag0 || flag1 || flag2 || flag3</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">if</span> (flag) &#123;</div><div class=\"line\">        loop.<span class=\"keyword\">break</span></div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  flag</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>在运用该算法之前，有个优化方案，就是先进行最小外接矩形范围判断，如果不在最小外接矩形中，则直接跳过，<br><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (p.x &lt; minX || p.x &gt; maxX || p.y &lt; minY || p.y &gt; maxY) &#123;</div><div class=\"line\">    <span class=\"comment\">// 点在多边形之外</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>首先判断点是否在多边形的最小外接矩形之内，该步骤不是必须的，但是可以有效避免不必要的计算。</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"http://riyueshi.github.io/2015/10/07/is_point_in_polygon/\" target=\"_blank\" rel=\"external\">http://riyueshi.github.io/2015/10/07/is_point_in_polygon/</a></p>\n","excerpt":"","more":"<h2 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h2><p>给定一个景点的经纬度，给定距离，给定形状，判断其它点是否在某个区域内：</p>\n<h2 id=\"圆形方案\"><a href=\"#圆形方案\" class=\"headerlink\" title=\"圆形方案\"></a>圆形方案</h2><p>使用通用的地球上两点距离函数，圆形只需要判断距离，正方形需要计算两次距离（指定经度与景点的经度一样，计算是否在范围内，然后指定纬度与景点的纬度一样，计算是否在范围内，如果都在范围内，则代表该点在景区范围内），其它形状基于这个基础类推</p>\n<h3 id=\"距离函数\"><a href=\"#距离函数\" class=\"headerlink\" title=\"距离函数\"></a>距离函数</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">  * Created on 2016/10/9.</div><div class=\"line\">  *</div><div class=\"line\">  * 求地球上两点间的距离 返回的是 double 格式的 km</div><div class=\"line\">  * 第一点经纬度为（lat1,lng1），第二点经纬度为（lat2,lng2），地球平均半径R=6378.137</div><div class=\"line\">  * 按照0度经线的基准，东经取经度的正值(Longitude)，西经取经度负值(-Longitude)，北纬取90-纬度值(90- Latitude)，南纬取90+纬度值(90+Latitude)，</div><div class=\"line\">  * 则经过上述处理过后的两点被计为(MLon1, MLat1)和(MLon2, MLat2)。那么根据三角推导，可以得到计算两点距离的如下公式：</div><div class=\"line\">  * C = sin(MLat1)*sin(MLat2)*cos(MLon1-MLon2) + cos(MLat1)*cos(MLat2)，Distance = R*Arccos(C)*Pi/180</div><div class=\"line\">  * 如果仅对经度作正负的处理，而不对纬度作90-Latitude(假设都是北半球，南半球只有澳洲具有应用意义)的处理，那么公式将是：</div><div class=\"line\">  * C = sin(LatA)*sin(LatB) + cos(LatA)*cos(LatB)*cos(MLonA-MLonB)，Distance = R*Arccos(C)*Pi/180</div><div class=\"line\">  * 三角函数的输入和输出都采用弧度值，那么公式还可以写作:</div><div class=\"line\">  * C = sin(Lat1*Pi/180)*sin(Lat2*Pi/180) + cos(Lat1*Pi/180)*cos(Lat2*Pi/180)*cos((MLon1-MLon2)*Pi/180)，Distance = R*Arccos(C)*Pi/180</div><div class=\"line\">  * rad()函数求弧度，Distance（）函数求距离</div><div class=\"line\">  *</div><div class=\"line\">  * 结果验证工具地址 http://www.storyday.com/wp-content/uploads/2008/09/latlung_dis.html</div><div class=\"line\">  */</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">distance</span></span>(lat1: <span class=\"type\">Double</span>, lng1: <span class=\"type\">Double</span>, lat2: <span class=\"type\">Double</span>, lng2: <span class=\"type\">Double</span>): <span class=\"type\">Double</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">EARTH_RADIUS</span> = <span class=\"number\">6378.137</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">val</span> radLat1 = rad(lat1)</div><div class=\"line\">  <span class=\"keyword\">val</span> radLat2 = rad(lat2)</div><div class=\"line\">  <span class=\"keyword\">val</span> a = rad(lat1) - rad(lat2)</div><div class=\"line\">  <span class=\"keyword\">val</span> b = rad(lng1) - rad(lng2)</div><div class=\"line\">  <span class=\"keyword\">val</span> distance = <span class=\"type\">EARTH_RADIUS</span> * <span class=\"number\">2</span> * <span class=\"type\">Math</span>.asin(<span class=\"type\">Math</span>.sqrt(<span class=\"type\">Math</span>.pow(<span class=\"type\">Math</span>.sin(a / <span class=\"number\">2</span>), <span class=\"number\">2</span>) + <span class=\"type\">Math</span>.cos(radLat1) * <span class=\"type\">Math</span>.cos(radLat2) * <span class=\"type\">Math</span>.pow(<span class=\"type\">Math</span>.sin((b) / <span class=\"number\">2</span>), <span class=\"number\">2</span>)))</div><div class=\"line\">  <span class=\"comment\">//    printLog.debug(\"lat1: \" + lat1 + \" lng1: \" + lng1 + \" lat2: \" + lat2 + \" lng2: \" + lng2)</span></div><div class=\"line\">  printLog.debug(<span class=\"string\">\"distance:\"</span> + distance)</div><div class=\"line\">  distance</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"不规则图形方案\"><a href=\"#不规则图形方案\" class=\"headerlink\" title=\"不规则图形方案\"></a>不规则图形方案</h2><p>scala实现函数：使用现成的算法PNPoly即可实现；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">  * 多点位置判断算法 判断一个坐标点是否在不规则多边形内部</div><div class=\"line\">  * *</div><div class=\"line\">  * 在 GIS（地理信息管理系统）中，判断一个坐标是否在多边形内部是个经常要遇到的问题。乍听起来还挺复杂。</div><div class=\"line\">  * 根据 W. Randolph Franklin 提出的 PNPoly (http://www.ecse.rpi.edu/Homepages/wrf/Research/Short_Notes/pnpoly.html) 算法，只需区区几行代码就解决了这个问题:</div><div class=\"line\">  * *</div><div class=\"line\">  * 针对每一个点，算法遍历多边形相邻的每两个顶点（即一条边），假如待判断点满足以下两个条件即改变点是否在多边形内的状态标识c：</div><div class=\"line\">  * 待判断点的Y坐标在点i和点j的Y坐标范围之内</div><div class=\"line\">  * 待判断点的X坐标在点i和点j连线之下</div><div class=\"line\">  * 遍历所有的边之后假如以上两个条件同时满足奇数次则该带判断点位于多边形之内，否则位于多边形之外。</div><div class=\"line\">  * 算法复杂度为O(n)，其中n为多边形的顶点个数。</div><div class=\"line\">  *</div><div class=\"line\">  * @param vertexes</div><div class=\"line\">  * @param testPoint</div><div class=\"line\">  * @return</div><div class=\"line\">  */</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pNPoly</span></span>(vertexes: <span class=\"type\">Array</span>[<span class=\"type\">LocationPoint</span>], testPoint: <span class=\"type\">LocationPoint</span>): <span class=\"type\">Boolean</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">var</span> flag = <span class=\"literal\">false</span></div><div class=\"line\">  <span class=\"keyword\">var</span> flag0 = <span class=\"literal\">false</span></div><div class=\"line\">  <span class=\"keyword\">var</span> flag1 = <span class=\"literal\">false</span></div><div class=\"line\">  <span class=\"keyword\">var</span> flag2 = <span class=\"literal\">false</span></div><div class=\"line\">  <span class=\"keyword\">var</span> flag3 = <span class=\"literal\">false</span></div><div class=\"line\">  <span class=\"keyword\">var</span> j = vertexes.length - <span class=\"number\">1</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">val</span> loop = <span class=\"keyword\">new</span> <span class=\"type\">Breaks</span></div><div class=\"line\">  loop.breakable &#123;</div><div class=\"line\">    <span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">0</span> until vertexes.length) &#123;</div><div class=\"line\">      <span class=\"keyword\">if</span> (i != <span class=\"number\">0</span>) &#123;</div><div class=\"line\">        j = i - <span class=\"number\">1</span></div><div class=\"line\">      &#125;</div><div class=\"line\">      <span class=\"keyword\">if</span> ((vertexes(i).lat == testPoint.lat) &amp;&amp; (vertexes(i).long == testPoint.long)) &#123;</div><div class=\"line\">        flag0 = <span class=\"literal\">true</span></div><div class=\"line\">      &#125;</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">if</span> (((vertexes(i).lat - testPoint.lat) * (vertexes(i).long - testPoint.long) * (vertexes(j).lat - testPoint.lat) * (vertexes(j).long - testPoint.long) == <span class=\"number\">0</span>) &amp;&amp;</div><div class=\"line\">        (((vertexes(i).lat &gt; testPoint.lat) != (vertexes(j).lat &gt; testPoint.lat)) &amp;&amp; ((vertexes(i).long &gt; testPoint.long) != (vertexes(j).long &gt; testPoint.long)))) &#123;</div><div class=\"line\">        flag1 = <span class=\"literal\">true</span></div><div class=\"line\">      &#125;</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">if</span> (((vertexes(i).lat - testPoint.lat) * (vertexes(i).long - testPoint.long) * (vertexes(j).lat - testPoint.lat) * (vertexes(j).long - testPoint.long) != <span class=\"number\">0</span>) &amp;&amp;</div><div class=\"line\">        ((vertexes(i).lat - testPoint.lat) / (vertexes(i).long - testPoint.long) == (vertexes(j).lat - testPoint.lat) / (vertexes(j).long - testPoint.long))) &#123;</div><div class=\"line\">        flag2 = <span class=\"literal\">true</span></div><div class=\"line\">      &#125;</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">if</span> (((vertexes(i).lat &gt; testPoint.lat) != (vertexes(j).lat &gt; testPoint.lat)) &amp;&amp;</div><div class=\"line\">        (testPoint.long &lt; (vertexes(j).long - vertexes(i).long) * (testPoint.lat - vertexes(j).lat) / (vertexes(j).lat - vertexes(i).lat) + vertexes(i).long)) &#123;</div><div class=\"line\">        flag3 = <span class=\"literal\">true</span></div><div class=\"line\">      &#125;</div><div class=\"line\"></div><div class=\"line\">      flag = flag0 || flag1 || flag2 || flag3</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">if</span> (flag) &#123;</div><div class=\"line\">        loop.<span class=\"keyword\">break</span></div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  flag</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>在运用该算法之前，有个优化方案，就是先进行最小外接矩形范围判断，如果不在最小外接矩形中，则直接跳过，<br><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (p.x &lt; minX || p.x &gt; maxX || p.y &lt; minY || p.y &gt; maxY) &#123;</div><div class=\"line\">    <span class=\"comment\">// 点在多边形之外</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>首先判断点是否在多边形的最小外接矩形之内，该步骤不是必须的，但是可以有效避免不必要的计算。</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"http://riyueshi.github.io/2015/10/07/is_point_in_polygon/\">http://riyueshi.github.io/2015/10/07/is_point_in_polygon/</a></p>\n"},{"title":"spark中读取hdfs文件简记","toc":true,"date":"2016-12-19T02:46:18.000Z","_content":"\n使用spark的API读取hdfs的方法是：\n``` scala\nval lines: RDD[String] = sc.textFile(filePath)\n```\n如果该文件不存在，就会报错，报错多次，就会奔溃\n\n``` bash\n16/12/16 16:53:12 ERROR JobScheduler: Error running job streaming job 1481878392000 ms.0\norg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data/.../4811_000.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\n\t...\n\n16/12/16 16:53:12 ERROR ApplicationMaster: User class threw exception: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data...00.txt\norg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data/hjpt/...000.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n```\n\n由于spark中hdfs读是lazy的，所以无法使用try-catch把它装住，即使用try-catch将其包住也会报错。\n所以目前使用的解决方法是需要在读取该文件之前检验该文件是否存在。\n在spark中的实现不需要再次指定hadoopConf，只要从sc中拿就可以了：\n\n``` scala\nval conf = sc.hadoopConfiguration\nval fs = org.apache.hadoop.fs.FileSystem.get(conf)\nval exists = fs.exists(new org.apache.hadoop.fs.Path(\"/path/on/hdfs/to/SUCCESS.txt\"))\n```\n\n实际实现如下：\n``` scala\n...\nval sc: SparkContext = eachRdd.sparkContext\nval hadoopConf: Configuration = sc.hadoopConfiguration\nval fs: FileSystem = org.apache.hadoop.fs.FileSystem.get(hadoopConf)\n\n// 这里是否不需要collect？\nval lines: Array[(String, FtpMap)] = oiddRdd.collect()\n// 文件名流转化为文件数据流\nlines.foreach {\neachFileJson: (String, FtpMap) => {\n  val topic: String = eachFileJson._1\n  printLog.info(\"topic: \" + topic)\n  val fileJson = eachFileJson._2\n  val filePath = fileJson.file_path\n  val fileExists: Boolean = try {\n    fs.exists(new org.apache.hadoop.fs.Path(filePath))\n  } catch {\n    case e: Exception => {\n      printLog.error(\"Exception: filePath:\" + filePath + \" e:\" + e)\n      false\n    }\n  }\n  if (fileExists) {\n    val lines: RDD[String] = sc.textFile(filePath)\n    ...\n  }\n}\n```\n\n值得注意的是：\n\n如果读取一个hdfs目录下的所有文件，当文件的数目非常多，比如说有一亿个文件，由于spark读hdfs文件是lazy的，它在读取hdfs下该文件的列表的时候，会先将这个列表保存到内存中，但是如果在这期间hdfs文件被删除了，则还是会发生文件不存在的错误，所以以后遇到这类问题的时候要注意。","source":"_posts/2016-12-19-spark中读取hdfs文件简记.md","raw":"---\ntitle: spark中读取hdfs文件简记\ntoc: true\ndate: 2016-12-19 10:46:18\ntags:\n- spark开发\ncategories: spark开发\n---\n\n使用spark的API读取hdfs的方法是：\n``` scala\nval lines: RDD[String] = sc.textFile(filePath)\n```\n如果该文件不存在，就会报错，报错多次，就会奔溃\n\n``` bash\n16/12/16 16:53:12 ERROR JobScheduler: Error running job streaming job 1481878392000 ms.0\norg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data/.../4811_000.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\n\t...\n\n16/12/16 16:53:12 ERROR ApplicationMaster: User class threw exception: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data...00.txt\norg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data/hjpt/...000.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n```\n\n由于spark中hdfs读是lazy的，所以无法使用try-catch把它装住，即使用try-catch将其包住也会报错。\n所以目前使用的解决方法是需要在读取该文件之前检验该文件是否存在。\n在spark中的实现不需要再次指定hadoopConf，只要从sc中拿就可以了：\n\n``` scala\nval conf = sc.hadoopConfiguration\nval fs = org.apache.hadoop.fs.FileSystem.get(conf)\nval exists = fs.exists(new org.apache.hadoop.fs.Path(\"/path/on/hdfs/to/SUCCESS.txt\"))\n```\n\n实际实现如下：\n``` scala\n...\nval sc: SparkContext = eachRdd.sparkContext\nval hadoopConf: Configuration = sc.hadoopConfiguration\nval fs: FileSystem = org.apache.hadoop.fs.FileSystem.get(hadoopConf)\n\n// 这里是否不需要collect？\nval lines: Array[(String, FtpMap)] = oiddRdd.collect()\n// 文件名流转化为文件数据流\nlines.foreach {\neachFileJson: (String, FtpMap) => {\n  val topic: String = eachFileJson._1\n  printLog.info(\"topic: \" + topic)\n  val fileJson = eachFileJson._2\n  val filePath = fileJson.file_path\n  val fileExists: Boolean = try {\n    fs.exists(new org.apache.hadoop.fs.Path(filePath))\n  } catch {\n    case e: Exception => {\n      printLog.error(\"Exception: filePath:\" + filePath + \" e:\" + e)\n      false\n    }\n  }\n  if (fileExists) {\n    val lines: RDD[String] = sc.textFile(filePath)\n    ...\n  }\n}\n```\n\n值得注意的是：\n\n如果读取一个hdfs目录下的所有文件，当文件的数目非常多，比如说有一亿个文件，由于spark读hdfs文件是lazy的，它在读取hdfs下该文件的列表的时候，会先将这个列表保存到内存中，但是如果在这期间hdfs文件被删除了，则还是会发生文件不存在的错误，所以以后遇到这类问题的时候要注意。","slug":"spark中读取hdfs文件简记","published":1,"updated":"2016-12-19T03:07:31.009Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfwf001vpsguh54wfh4h","content":"<p>使用spark的API读取hdfs的方法是：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> lines: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.textFile(filePath)</div></pre></td></tr></table></figure></p>\n<p>如果该文件不存在，就会报错，报错多次，就会奔溃</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/12/16 16:53:12 ERROR JobScheduler: Error running job streaming job 1481878392000 ms.0</div><div class=\"line\">org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data/.../4811_000.txt</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)</div><div class=\"line\">\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)</div><div class=\"line\">\tat org.apache.spark.rdd.RDD$<span class=\"variable\">$anonfun</span><span class=\"variable\">$partitions</span><span class=\"variable\">$2</span>.apply(RDD.scala:239)</div><div class=\"line\">\tat org.apache.spark.rdd.RDD$<span class=\"variable\">$anonfun</span><span class=\"variable\">$partitions</span><span class=\"variable\">$2</span>.apply(RDD.scala:237)</div><div class=\"line\">\tat scala.Option.getOrElse(Option.scala:120)</div><div class=\"line\"></div><div class=\"line\">\t...</div><div class=\"line\"></div><div class=\"line\">16/12/16 16:53:12 ERROR ApplicationMaster: User class threw exception: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data...00.txt</div><div class=\"line\">org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data/hjpt/...000.txt</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)</div><div class=\"line\">\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)</div><div class=\"line\">\tat org.apache.spark.rdd.RDD$<span class=\"variable\">$anonfun</span><span class=\"variable\">$partitions</span><span class=\"variable\">$2</span>.apply(RDD.scala:239)</div><div class=\"line\">\tat org.apache.spark.rdd.RDD$<span class=\"variable\">$anonfun</span><span class=\"variable\">$partitions</span><span class=\"variable\">$2</span>.apply(RDD.scala:237)</div><div class=\"line\">\tat scala.Option.getOrElse(Option.scala:120)</div><div class=\"line\">\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)</div></pre></td></tr></table></figure>\n<p>由于spark中hdfs读是lazy的，所以无法使用try-catch把它装住，即使用try-catch将其包住也会报错。<br>所以目前使用的解决方法是需要在读取该文件之前检验该文件是否存在。<br>在spark中的实现不需要再次指定hadoopConf，只要从sc中拿就可以了：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> conf = sc.hadoopConfiguration</div><div class=\"line\"><span class=\"keyword\">val</span> fs = org.apache.hadoop.fs.<span class=\"type\">FileSystem</span>.get(conf)</div><div class=\"line\"><span class=\"keyword\">val</span> exists = fs.exists(<span class=\"keyword\">new</span> org.apache.hadoop.fs.<span class=\"type\">Path</span>(<span class=\"string\">\"/path/on/hdfs/to/SUCCESS.txt\"</span>))</div></pre></td></tr></table></figure>\n<p>实际实现如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">...</div><div class=\"line\"><span class=\"keyword\">val</span> sc: <span class=\"type\">SparkContext</span> = eachRdd.sparkContext</div><div class=\"line\"><span class=\"keyword\">val</span> hadoopConf: <span class=\"type\">Configuration</span> = sc.hadoopConfiguration</div><div class=\"line\"><span class=\"keyword\">val</span> fs: <span class=\"type\">FileSystem</span> = org.apache.hadoop.fs.<span class=\"type\">FileSystem</span>.get(hadoopConf)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 这里是否不需要collect？</span></div><div class=\"line\"><span class=\"keyword\">val</span> lines: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">FtpMap</span>)] = oiddRdd.collect()</div><div class=\"line\"><span class=\"comment\">// 文件名流转化为文件数据流</span></div><div class=\"line\">lines.foreach &#123;</div><div class=\"line\">eachFileJson: (<span class=\"type\">String</span>, <span class=\"type\">FtpMap</span>) =&gt; &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> topic: <span class=\"type\">String</span> = eachFileJson._1</div><div class=\"line\">  printLog.info(<span class=\"string\">\"topic: \"</span> + topic)</div><div class=\"line\">  <span class=\"keyword\">val</span> fileJson = eachFileJson._2</div><div class=\"line\">  <span class=\"keyword\">val</span> filePath = fileJson.file_path</div><div class=\"line\">  <span class=\"keyword\">val</span> fileExists: <span class=\"type\">Boolean</span> = <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">    fs.exists(<span class=\"keyword\">new</span> org.apache.hadoop.fs.<span class=\"type\">Path</span>(filePath))</div><div class=\"line\">  &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">    <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt; &#123;</div><div class=\"line\">      printLog.error(<span class=\"string\">\"Exception: filePath:\"</span> + filePath + <span class=\"string\">\" e:\"</span> + e)</div><div class=\"line\">      <span class=\"literal\">false</span></div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">if</span> (fileExists) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> lines: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.textFile(filePath)</div><div class=\"line\">    ...</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>值得注意的是：</p>\n<p>如果读取一个hdfs目录下的所有文件，当文件的数目非常多，比如说有一亿个文件，由于spark读hdfs文件是lazy的，它在读取hdfs下该文件的列表的时候，会先将这个列表保存到内存中，但是如果在这期间hdfs文件被删除了，则还是会发生文件不存在的错误，所以以后遇到这类问题的时候要注意。</p>\n","excerpt":"","more":"<p>使用spark的API读取hdfs的方法是：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> lines: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.textFile(filePath)</div></pre></td></tr></table></figure></p>\n<p>如果该文件不存在，就会报错，报错多次，就会奔溃</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">16/12/16 16:53:12 ERROR JobScheduler: Error running job streaming job 1481878392000 ms.0</div><div class=\"line\">org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data/.../4811_000.txt</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)</div><div class=\"line\">\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)</div><div class=\"line\">\tat org.apache.spark.rdd.RDD$<span class=\"variable\">$anonfun</span><span class=\"variable\">$partitions</span><span class=\"variable\">$2</span>.apply(RDD.scala:239)</div><div class=\"line\">\tat org.apache.spark.rdd.RDD$<span class=\"variable\">$anonfun</span><span class=\"variable\">$partitions</span><span class=\"variable\">$2</span>.apply(RDD.scala:237)</div><div class=\"line\">\tat scala.Option.getOrElse(Option.scala:120)</div><div class=\"line\"></div><div class=\"line\">\t...</div><div class=\"line\"></div><div class=\"line\">16/12/16 16:53:12 ERROR ApplicationMaster: User class threw exception: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data...00.txt</div><div class=\"line\">org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ns/data/hjpt/...000.txt</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)</div><div class=\"line\">\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)</div><div class=\"line\">\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)</div><div class=\"line\">\tat org.apache.spark.rdd.RDD$<span class=\"variable\">$anonfun</span><span class=\"variable\">$partitions</span><span class=\"variable\">$2</span>.apply(RDD.scala:239)</div><div class=\"line\">\tat org.apache.spark.rdd.RDD$<span class=\"variable\">$anonfun</span><span class=\"variable\">$partitions</span><span class=\"variable\">$2</span>.apply(RDD.scala:237)</div><div class=\"line\">\tat scala.Option.getOrElse(Option.scala:120)</div><div class=\"line\">\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)</div></pre></td></tr></table></figure>\n<p>由于spark中hdfs读是lazy的，所以无法使用try-catch把它装住，即使用try-catch将其包住也会报错。<br>所以目前使用的解决方法是需要在读取该文件之前检验该文件是否存在。<br>在spark中的实现不需要再次指定hadoopConf，只要从sc中拿就可以了：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> conf = sc.hadoopConfiguration</div><div class=\"line\"><span class=\"keyword\">val</span> fs = org.apache.hadoop.fs.<span class=\"type\">FileSystem</span>.get(conf)</div><div class=\"line\"><span class=\"keyword\">val</span> exists = fs.exists(<span class=\"keyword\">new</span> org.apache.hadoop.fs.<span class=\"type\">Path</span>(<span class=\"string\">\"/path/on/hdfs/to/SUCCESS.txt\"</span>))</div></pre></td></tr></table></figure>\n<p>实际实现如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">...</div><div class=\"line\"><span class=\"keyword\">val</span> sc: <span class=\"type\">SparkContext</span> = eachRdd.sparkContext</div><div class=\"line\"><span class=\"keyword\">val</span> hadoopConf: <span class=\"type\">Configuration</span> = sc.hadoopConfiguration</div><div class=\"line\"><span class=\"keyword\">val</span> fs: <span class=\"type\">FileSystem</span> = org.apache.hadoop.fs.<span class=\"type\">FileSystem</span>.get(hadoopConf)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 这里是否不需要collect？</span></div><div class=\"line\"><span class=\"keyword\">val</span> lines: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">FtpMap</span>)] = oiddRdd.collect()</div><div class=\"line\"><span class=\"comment\">// 文件名流转化为文件数据流</span></div><div class=\"line\">lines.foreach &#123;</div><div class=\"line\">eachFileJson: (<span class=\"type\">String</span>, <span class=\"type\">FtpMap</span>) =&gt; &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> topic: <span class=\"type\">String</span> = eachFileJson._1</div><div class=\"line\">  printLog.info(<span class=\"string\">\"topic: \"</span> + topic)</div><div class=\"line\">  <span class=\"keyword\">val</span> fileJson = eachFileJson._2</div><div class=\"line\">  <span class=\"keyword\">val</span> filePath = fileJson.file_path</div><div class=\"line\">  <span class=\"keyword\">val</span> fileExists: <span class=\"type\">Boolean</span> = <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">    fs.exists(<span class=\"keyword\">new</span> org.apache.hadoop.fs.<span class=\"type\">Path</span>(filePath))</div><div class=\"line\">  &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">    <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt; &#123;</div><div class=\"line\">      printLog.error(<span class=\"string\">\"Exception: filePath:\"</span> + filePath + <span class=\"string\">\" e:\"</span> + e)</div><div class=\"line\">      <span class=\"literal\">false</span></div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"keyword\">if</span> (fileExists) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> lines: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.textFile(filePath)</div><div class=\"line\">    ...</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>值得注意的是：</p>\n<p>如果读取一个hdfs目录下的所有文件，当文件的数目非常多，比如说有一亿个文件，由于spark读hdfs文件是lazy的，它在读取hdfs下该文件的列表的时候，会先将这个列表保存到内存中，但是如果在这期间hdfs文件被删除了，则还是会发生文件不存在的错误，所以以后遇到这类问题的时候要注意。</p>\n"},{"title":"桂花真香","toc":true,"date":"2009-09-29T11:34:50.000Z","_content":"\n夏已消逝，秋意更浓，人们穿上了久违的长袖，大雁不时成群在天上飞过，桂花香也飘满了校园。\n\n没来过南方的人永远想像不到桂花的香味，家在南方的人也不会感觉到桂花香的震撼，他们已经习惯了，因为很多东西当人们拥有时都不会去珍惜它。从北方来求学的我，站在秋天桂花香的风中，当多愁善感的人碰上多愁善感的事，不免得要多愁善感一番。\n\n虽然家乡那边也有花，我也故作浪漫地闻过，但说实话我真没闻到过花香。当我来到南方，闻到桂花香时，才真正明白了花香的含义。桂花在暮夏时节就有了，那时只是一抹抹，一缕缕的，花香像调皮的小天使，随风飘荡，不经意间飞进鼻孔，当你使劲闻的时候，它却调皮地飞走了。而现在，大批的桂花跟随着秋姑娘的脚步轻轻走来，微风轻轻一吹，就会闻到浓浓的清香，\"满城尽飘桂花香\"，使劲闻吧！不必害怕把它惊走，多浪漫啊！我的心都醉了。\n\n在桂花香的风中奔跑，路人用奇怪的眼光看我，我不在乎，应为我已沉浸在桂花香风中。真想化成风，一股微蓝的风，让别人看不见我，自由自在地在空中飞翔，空中陶醉，然后睡在桂花中。看！多小的桂花呀，一个个黄色的小花，像一个个黄色的小铃铛，在风的吹佛下，奏出悦耳的浪漫诗!\n\n真想化成风，载着桂花香，飞到天上，飞到桂花树下私语的情侣旁，多浪漫的爱情啊!我为他们送去香甜；\n\n真想化成风，飞到通渭，将桂花香带到家里，我轻轻触摸爸爸的脸，闻着妈妈洗完衣服的手上的肥皂香味，看着可爱的弟弟学习时的傻样,轻轻抚摸他柔软的头发；\n\n真想化成风，飞到通渭一中，飞到教室里,为那些埋头苦读的兄弟们送去桂花香，真心祝福你们；\n\n真想化成风，载着桂花香，飞到北京，西安，长春......飞到大学里和我一样离家的朋友旁，让他们也闻到桂花香，为他们驱散孤独。\n\n如果可以，我会化成风，飞到一中，飞到200天以前的高三（九）班，看着那个坐在第二排戴眼镜的可爱男生，飞到他耳边，轻轻告诉他：\n\n\t\"勇敢点，不要让美好的高中留下遗憾。\"\n\t\"乖点，再不要让父母生气了。\"\n\t\"大度点，不要再与弟弟争了\"\n\t......\n    \n真想化成风，一股微蓝的风，是秋天里浪漫的风，畅游在花香的海洋，载着花香，含着泪，在无人的时候，一个人独自飘。\n\n\n——2009年秋\n\n——安大新区\n","source":"_posts/2016-11-29-桂花真香.md","raw":"---\ntitle: 桂花真香\ntoc: true\ndate: 2009-09-29 19:34:50\ntags: \n- 散文\ncategories: \n- 单车岁月\n---\n\n夏已消逝，秋意更浓，人们穿上了久违的长袖，大雁不时成群在天上飞过，桂花香也飘满了校园。\n\n没来过南方的人永远想像不到桂花的香味，家在南方的人也不会感觉到桂花香的震撼，他们已经习惯了，因为很多东西当人们拥有时都不会去珍惜它。从北方来求学的我，站在秋天桂花香的风中，当多愁善感的人碰上多愁善感的事，不免得要多愁善感一番。\n\n虽然家乡那边也有花，我也故作浪漫地闻过，但说实话我真没闻到过花香。当我来到南方，闻到桂花香时，才真正明白了花香的含义。桂花在暮夏时节就有了，那时只是一抹抹，一缕缕的，花香像调皮的小天使，随风飘荡，不经意间飞进鼻孔，当你使劲闻的时候，它却调皮地飞走了。而现在，大批的桂花跟随着秋姑娘的脚步轻轻走来，微风轻轻一吹，就会闻到浓浓的清香，\"满城尽飘桂花香\"，使劲闻吧！不必害怕把它惊走，多浪漫啊！我的心都醉了。\n\n在桂花香的风中奔跑，路人用奇怪的眼光看我，我不在乎，应为我已沉浸在桂花香风中。真想化成风，一股微蓝的风，让别人看不见我，自由自在地在空中飞翔，空中陶醉，然后睡在桂花中。看！多小的桂花呀，一个个黄色的小花，像一个个黄色的小铃铛，在风的吹佛下，奏出悦耳的浪漫诗!\n\n真想化成风，载着桂花香，飞到天上，飞到桂花树下私语的情侣旁，多浪漫的爱情啊!我为他们送去香甜；\n\n真想化成风，飞到通渭，将桂花香带到家里，我轻轻触摸爸爸的脸，闻着妈妈洗完衣服的手上的肥皂香味，看着可爱的弟弟学习时的傻样,轻轻抚摸他柔软的头发；\n\n真想化成风，飞到通渭一中，飞到教室里,为那些埋头苦读的兄弟们送去桂花香，真心祝福你们；\n\n真想化成风，载着桂花香，飞到北京，西安，长春......飞到大学里和我一样离家的朋友旁，让他们也闻到桂花香，为他们驱散孤独。\n\n如果可以，我会化成风，飞到一中，飞到200天以前的高三（九）班，看着那个坐在第二排戴眼镜的可爱男生，飞到他耳边，轻轻告诉他：\n\n\t\"勇敢点，不要让美好的高中留下遗憾。\"\n\t\"乖点，再不要让父母生气了。\"\n\t\"大度点，不要再与弟弟争了\"\n\t......\n    \n真想化成风，一股微蓝的风，是秋天里浪漫的风，畅游在花香的海洋，载着花香，含着泪，在无人的时候，一个人独自飘。\n\n\n——2009年秋\n\n——安大新区\n","slug":"桂花真香","published":1,"updated":"2017-01-11T11:27:57.685Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfwh001zpsgut1btaa52","content":"<p>夏已消逝，秋意更浓，人们穿上了久违的长袖，大雁不时成群在天上飞过，桂花香也飘满了校园。</p>\n<p>没来过南方的人永远想像不到桂花的香味，家在南方的人也不会感觉到桂花香的震撼，他们已经习惯了，因为很多东西当人们拥有时都不会去珍惜它。从北方来求学的我，站在秋天桂花香的风中，当多愁善感的人碰上多愁善感的事，不免得要多愁善感一番。</p>\n<p>虽然家乡那边也有花，我也故作浪漫地闻过，但说实话我真没闻到过花香。当我来到南方，闻到桂花香时，才真正明白了花香的含义。桂花在暮夏时节就有了，那时只是一抹抹，一缕缕的，花香像调皮的小天使，随风飘荡，不经意间飞进鼻孔，当你使劲闻的时候，它却调皮地飞走了。而现在，大批的桂花跟随着秋姑娘的脚步轻轻走来，微风轻轻一吹，就会闻到浓浓的清香，”满城尽飘桂花香”，使劲闻吧！不必害怕把它惊走，多浪漫啊！我的心都醉了。</p>\n<p>在桂花香的风中奔跑，路人用奇怪的眼光看我，我不在乎，应为我已沉浸在桂花香风中。真想化成风，一股微蓝的风，让别人看不见我，自由自在地在空中飞翔，空中陶醉，然后睡在桂花中。看！多小的桂花呀，一个个黄色的小花，像一个个黄色的小铃铛，在风的吹佛下，奏出悦耳的浪漫诗!</p>\n<p>真想化成风，载着桂花香，飞到天上，飞到桂花树下私语的情侣旁，多浪漫的爱情啊!我为他们送去香甜；</p>\n<p>真想化成风，飞到通渭，将桂花香带到家里，我轻轻触摸爸爸的脸，闻着妈妈洗完衣服的手上的肥皂香味，看着可爱的弟弟学习时的傻样,轻轻抚摸他柔软的头发；</p>\n<p>真想化成风，飞到通渭一中，飞到教室里,为那些埋头苦读的兄弟们送去桂花香，真心祝福你们；</p>\n<p>真想化成风，载着桂花香，飞到北京，西安，长春……飞到大学里和我一样离家的朋友旁，让他们也闻到桂花香，为他们驱散孤独。</p>\n<p>如果可以，我会化成风，飞到一中，飞到200天以前的高三（九）班，看着那个坐在第二排戴眼镜的可爱男生，飞到他耳边，轻轻告诉他：</p>\n<pre><code>&quot;勇敢点，不要让美好的高中留下遗憾。&quot;\n&quot;乖点，再不要让父母生气了。&quot;\n&quot;大度点，不要再与弟弟争了&quot;\n......\n</code></pre><p>真想化成风，一股微蓝的风，是秋天里浪漫的风，畅游在花香的海洋，载着花香，含着泪，在无人的时候，一个人独自飘。</p>\n<p>——2009年秋</p>\n<p>——安大新区</p>\n","excerpt":"","more":"<p>夏已消逝，秋意更浓，人们穿上了久违的长袖，大雁不时成群在天上飞过，桂花香也飘满了校园。</p>\n<p>没来过南方的人永远想像不到桂花的香味，家在南方的人也不会感觉到桂花香的震撼，他们已经习惯了，因为很多东西当人们拥有时都不会去珍惜它。从北方来求学的我，站在秋天桂花香的风中，当多愁善感的人碰上多愁善感的事，不免得要多愁善感一番。</p>\n<p>虽然家乡那边也有花，我也故作浪漫地闻过，但说实话我真没闻到过花香。当我来到南方，闻到桂花香时，才真正明白了花香的含义。桂花在暮夏时节就有了，那时只是一抹抹，一缕缕的，花香像调皮的小天使，随风飘荡，不经意间飞进鼻孔，当你使劲闻的时候，它却调皮地飞走了。而现在，大批的桂花跟随着秋姑娘的脚步轻轻走来，微风轻轻一吹，就会闻到浓浓的清香，”满城尽飘桂花香”，使劲闻吧！不必害怕把它惊走，多浪漫啊！我的心都醉了。</p>\n<p>在桂花香的风中奔跑，路人用奇怪的眼光看我，我不在乎，应为我已沉浸在桂花香风中。真想化成风，一股微蓝的风，让别人看不见我，自由自在地在空中飞翔，空中陶醉，然后睡在桂花中。看！多小的桂花呀，一个个黄色的小花，像一个个黄色的小铃铛，在风的吹佛下，奏出悦耳的浪漫诗!</p>\n<p>真想化成风，载着桂花香，飞到天上，飞到桂花树下私语的情侣旁，多浪漫的爱情啊!我为他们送去香甜；</p>\n<p>真想化成风，飞到通渭，将桂花香带到家里，我轻轻触摸爸爸的脸，闻着妈妈洗完衣服的手上的肥皂香味，看着可爱的弟弟学习时的傻样,轻轻抚摸他柔软的头发；</p>\n<p>真想化成风，飞到通渭一中，飞到教室里,为那些埋头苦读的兄弟们送去桂花香，真心祝福你们；</p>\n<p>真想化成风，载着桂花香，飞到北京，西安，长春……飞到大学里和我一样离家的朋友旁，让他们也闻到桂花香，为他们驱散孤独。</p>\n<p>如果可以，我会化成风，飞到一中，飞到200天以前的高三（九）班，看着那个坐在第二排戴眼镜的可爱男生，飞到他耳边，轻轻告诉他：</p>\n<pre><code>&quot;勇敢点，不要让美好的高中留下遗憾。&quot;\n&quot;乖点，再不要让父母生气了。&quot;\n&quot;大度点，不要再与弟弟争了&quot;\n......\n</code></pre><p>真想化成风，一股微蓝的风，是秋天里浪漫的风，畅游在花香的海洋，载着花香，含着泪，在无人的时候，一个人独自飘。</p>\n<p>——2009年秋</p>\n<p>——安大新区</p>\n"},{"title":"Spark源码阅读之——StreamingContext详解","toc":true,"date":"2017-01-03T06:23:04.000Z","_content":"\n[Spark Streaming 源码解析系列](https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97)很好地解析了Spark Streaming框架的源码，遗留了一点关于StreamingContext的解析，我基于自己的理解，简要阐述如下：\n\n\n```\n本系列内容适用范围：\n* 2016.12.28 update, Spark 2.1 全系列 √ (2.1.0)\n* 2016.11.14 update, Spark 2.0 全系列 √ (2.0.0, 2.0.1, 2.0.2)\n* 2016.11.07 update, Spark 1.6 全系列 √ (1.6.0, 1.6.1, 1.6.2, 1.6.3)\n```\n\n阅读本文前，请一定先阅读 [Spark Streaming 实现思路与模块概述](0.1 Spark Streaming 实现思路与模块概述.md) 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 `StreamingContext` 细节的解释。\n\n## 引言\n\n![image](040.png)\n\n如各个模块的架构图所示，`StreamingContext` 是 Spark Streaming 提供给用户 code 的、与前述 4 个模块交互的一个简单和统一的入口，是Spark Streaming程序与Spark Core的连接器，下面我们用这段11行的完整 [quick example](http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example)，来说明用户 code 是怎么通过 `StreamingContext` 与前面几个模块进行交互的：\n\n```scala\nimport org.apache.spark._\nimport org.apache.spark.streaming._\n\n// 首先配置一下本 quick example 将跑在本机，app name 是 NetworkWordCount\nval conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\")\n// batchDuration 设置为 1 秒，然后创建一个 streaming 入口\nval ssc = new StreamingContext(conf, Seconds(1))\n\n// ssc.socketTextStream() 将创建一个 SocketInputDStream；这个 InputDStream 的 SocketReceiver 将监听本机 9999 端口\nval lines = ssc.socketTextStream(\"localhost\", 9999)\n\nval words = lines.flatMap(_.split(\" \"))      // DStream transformation\nval pairs = words.map(word => (word, 1))     // DStream transformation\nval wordCounts = pairs.reduceByKey(_ + _)    // DStream transformation\nwordCounts.print()                           // DStream output\n// 上面 4 行利用 DStream transformation 构造出了 lines -> words -> pairs -> wordCounts -> .print() 这样一个 DStreamGraph\n// 但注意，到目前是定义好了产生数据的 SocketReceiver，以及一个 DStreamGraph，这些都是静态的\n\n// 下面这行 start() 将在幕后启动 JobScheduler, 进而启动 JobGenerator 和 ReceiverTracker\n// ssc.start()\n//    -> JobScheduler.start()\n//        -> JobGenerator.start();    开始不断生成一个一个 batch\n//        -> ReceiverTracker.start(); 开始往 executor 上分布 ReceiverSupervisor 了，也会进一步创建和启动 Receiver\nssc.start()\n\n// 然后用户 code 主线程就 block 在下面这行代码了\n// block 的后果就是，后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息\n// 也就是在这里，我们前面静态定义的 DStreamGraph 的 print()，才一次一次被在 RDD 实例上调用，一次一次打印出当前 batch 的结果\nssc.awaitTermination()\n```\n\n从上述样例程序可知，程序的前两行创建了一个新的 `StreamingContext`，第三行通过 `ssc.socketTextStream`通过ssc暴露的方法创建了一个`ReceiverInputDStream`，接着基于DStream的各种方法对数据进行了操作，最后通过 `ssc.start` 方法启动了Spark Streaming 程序，最后一句`ssc.awaitTermination()`将用户 code 主线程 block 住，由后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息地处理，除非发生异常。\n\n我们可以发现，StreamingContext主要包含以下内容：\n\n- StreamingContext的创建(构造函数)\n- StreamingContext的初始化(成员)\n- StreamingContext的状态控制(函数)\n\n\n### StreamingContext的创建\n\n首先我们来看`StreamingContext`的构造函数，主要由三个参数，分别是：\n- SparkContext：SparkStreaming的最终处理是交给SparkContext的；\n- Checkpoint：检查点，用于错误恢复；\n- Duration：设定Streaming每个批次的积累时间。\n\n``` scala\nclass StreamingContext private[streaming] (\n    _sc: SparkContext,\n    _cp: Checkpoint,\n    _batchDur: Duration\n  ) extends Logging {\n```\n\n`StreamingContext` 有以下几种不同的创建方式：\n\n``` scala\n\n  // 1. 通过已经存在的SparkContext创建.\n  def this(sparkContext: SparkContext, batchDuration: Duration) = {\n    this(sparkContext, null, batchDuration)\n  }\n\n  // 2. 通过SparkConf中的配置信息来来创建.\n  def this(conf: SparkConf, batchDuration: Duration) = {\n    this(StreamingContext.createNewSparkContext(conf), null, batchDuration)\n  }\n\n  // 3. 通过一些必要参数来创建\n  def this(\n      master: String,\n      appName: String,\n      batchDuration: Duration,\n      sparkHome: String = null,\n      jars: Seq[String] = Nil,\n      environment: Map[String, String] = Map()) = {\n    this(StreamingContext.createNewSparkContext(master, appName, sparkHome, jars, environment),\n         null, batchDuration)\n  }\n\n  // 4. 从checkpoint文件中读取来重新创建\n  def this(path: String, hadoopConf: Configuration) =\n    this(null, CheckpointReader.read(path, new SparkConf(), hadoopConf).orNull, null)\n\n  // ...\n  // 5. 已经存在的SparkContext，通过读取checkpoint文件重新创建\n  def this(path: String, sparkContext: SparkContext) = {\n    this(\n      sparkContext,\n      CheckpointReader.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull,\n      null)\n  }\n  // ...\n  // 6. 值得一提的是，StreamingContext对象提供了一个构造方法，如果存在Checkpoint就通过Checkpoint创建，否则新建一个StreamingContext\n    def getOrCreate(\n      checkpointPath: String,\n      creatingFunc: () => StreamingContext,\n      hadoopConf: Configuration = SparkHadoopUtil.get.conf,\n      createOnError: Boolean = false\n    ): StreamingContext = {\n    val checkpointOption = CheckpointReader.read(\n      checkpointPath, new SparkConf(), hadoopConf, createOnError)\n    checkpointOption.map(new StreamingContext(null, _, null)).getOrElse(creatingFunc())\n  }\n\n```\n\n整理上述的文件创建过程，可以看出，StreamingContext的创建是一定要包含SparkContext的，同理也可以推出，Spark Streaming最终实际是交给SparkContext来处理的，Spark Streaming更像是Spark Core的一个应用程序。\n`StreamingContext`的创建主要分为两类：\n\n- 1: 通过SparkContext建立新的StreamingContext，需要指定`batchDuration`时间；\n- 2: 从checkpoint文件中读取的Checkpoint对象中创建StreamingContext，用于异常情况下的恢复，`batchDuration`在Checkpoint中已经保存，所以可以不用显示指定。\n\n所以说，要构建StreamingContext，就必须要以上两者至少选一，下面的代码也说明了这点：\n\n``` scala\n  require(_sc != null || _cp != null,\n    \"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null\")\n```\n注意：由于SparkStreaming至少需要一个线程来接收数据，所以local与local[1]模式下是不可以启动的。\n\n### StreamingContext的初始化\n\nStreamingContext在创建后会进行一些初始化（静态定义）的工作，定义一些静态的数据结构，由图可知，StreamingContext主要持有 DStreamGraph 与 JobScheduler 的对象：\n\n如下是graph的初始化定义代码，如果之前存在 Checkpoint ，则graph从 Checkpoint 得到，否则创建一个新的graph\n``` scala\n  private[streaming] val graph: DStreamGraph = {\n    if (isCheckpointPresent) {\n      _cp.graph.setContext(this)\n      // 遍历从Checkpoint数据中恢复出RDD\n      _cp.graph.restoreCheckpointData()\n      _cp.graph\n    } else {\n      require(_batchDur != null, \"Batch duration for StreamingContext cannot be null\")\n      val newGraph = new DStreamGraph()\n      newGraph.setBatchDuration(_batchDur)\n      newGraph\n    }\n  }\n```\n如下是初始化jobScheduler的代码：\n``` scala\n  private[streaming] val scheduler = new JobScheduler(this)\n```\n\n除了上述两个主要成员，StreamingContext还包含以下成员：\n- ContextWaiter：用于等待任务执行结束；\n- StreamingJobProgressListener：用于监听StreamingJob，用以更新StreamingTab的显示；\n- StreamingTab：用于生成SparkUI中Streaming那一页标签；\n- StreamingSource： 流式计算的测量数据源metrics。\n\n除了定义上述成员，StreamingContext还进行了Checkpoint,创建了Checkpoint目录：\n\n``` scala\nconf.getOption(\"spark.streaming.checkpoint.directory\").foreach(checkpoint)\n```\ncheckpoint方法如下：\n``` scala\n  // 创建Checkpoint目录\n  def checkpoint(directory: String) {\n    if (directory != null) {\n      val path = new Path(directory)\n      val fs = path.getFileSystem(sparkContext.hadoopConfiguration)\n      fs.mkdirs(path)\n      val fullPath = fs.getFileStatus(path).getPath().toString\n      sc.setCheckpointDir(fullPath)\n      checkpointDir = fullPath\n    } else {\n      checkpointDir = null\n    }\n  }\n```\n\n### StreamingContext的控制\n\nStreamingContext作为控制面板，给用户提供了许多控制方法，就像控制面板上的按钮，让我们来开发spark Streaming程序，主要有以下方法：\n\n- sparkContext： 获得ssc所属的SparkContext\n- remember(duration: Duration)：通过设置 `graph.remember(duration)` 来设置`rememberDuration`\n\n简单解释一下`rememberDuration`，Spark Streaming 会在每个Batch任务结束时进行一次清理动作`clearMetadata`，每个DStream 都会被扫描，先清理输出Dstream，接着清理输入DStream，清理的时候，根据`rememberDuration`来计算出oldRDD然后清理。`rememberDuration` 有默认值，大体是`slideDuration`，也就是DStream生成RDD的时间间隔，如果设置了checkpointDuration 则是2*checkpointDuration，手动指定的值要大于默认值才会生效。\n\n接下来是定义一些定义输入流的方法，主要有：\n\n- receiverStream[T: ClassTag](receiver: Receiver[T])：创建一个用户自定义的Receiver；\n- socketTextStream：创建TCP socketReceiver，默认是utf-8的文本格式，以'\\n'分隔；\n- socketStream：创建TCP socketReceiver，用户提供自己的转化函数；\n- rawSocketStream：创建SocketReceiver，相比上着，没有中间的解码转化所以比较高效；\n- fileStream：创建监控HDFS目录的InputDStream，通过检测文件的修改时间来判断是否是新文件；\n- binaryRecordsStream：创建二进制文件的监听InputDStream，使用了fileStream方法；\n- queueStream：创建一个RDD队列流，底层调用了UnionRDD的方法将这些RDD转化为一个RDD，开启oneAtATime参数则每个RDD只取一个值，可以用于调试和测试；\n\n在InputDStream的构造过程中，会将此输入流InputDStream添加到DStreamGraph的inputStreams数据结构中，\n``` scala InputDStream.scala\nssc.graph.addInputStream(this)\n```\n\n还定义了一些DStream的其它方法：\n\n- union： 多个DStreams合成一个DStream，底层调用了ssc.sc.union(rdds)；\n- transform： 根据自定义的transformFunc生成新的DStream；\n- addStreamingListener： 在listenerBus上增加一个StreamingListener对象，供JobScheduler的StreamingListenerBus对象监听输入流的ReceiverRateController对象；\n\n还定义了一些控制启动与关闭的方法：\n\n- start：启动StreamingContext。\n\nStreamingContext的start方法启动过程中，会判断StreamingContext的状态，它有三个状态INITIALIZED、ACTIVE、STOP。只有状态为INITAILIZED才允许启动，主要有以下步骤：\n1. 验证graph是否有效；\n2. 设置Checkpoint；\n3. 使用新的线程异步启动 JobScheduler ，启动后将状态由初始化状态INITIALIZED改为ACTIVE状态，JobScheduler的启动请见[JobGenerator 详解](https://github.com/lw-lin/CoolplaySpark/blob/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/2.2%20JobGenerator%20%E8%AF%A6%E8%A7%A3.md)；\n5. 同时添加Streaming的shutdownHookRef，用于程序的异常终止，StreamingContext的shutdownHook优先级比SparkContext的值大1；\n6. 往metricsSystem中注册streamingSource测量数据源；\n7. 添加生成SparkUI中Streaming相关标签\n\n- awaitTermination：等待Streaming程序的停止；\n- stop：停止SparkStreaming程序，其中可以传入参数以表示是否同时停止相关的SparkContext，默认为true(这个参数在文件名流转化为数据流的时候应该设置为false，通过spark.streaming.stopSparkContextByDefault来设置);还有一个参数是是否优雅地停止(等待其它已经接收到的数据处理完毕再停止)；\n\n\n### 引用\n\nhttp://lqding.blog.51cto.com/9123978/1771017\nhttps://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97\nhttp://lqding.blog.51cto.com/9123978/1773912","source":"_posts/2017-01-03-Spark源码阅读之——StreamingContext详解.md","raw":"---\ntitle: Spark源码阅读之——StreamingContext详解\ntoc: true\ndate: 2017-01-03 14:23:04\ntags: spark streaming\ncategories: spark\n---\n\n[Spark Streaming 源码解析系列](https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97)很好地解析了Spark Streaming框架的源码，遗留了一点关于StreamingContext的解析，我基于自己的理解，简要阐述如下：\n\n\n```\n本系列内容适用范围：\n* 2016.12.28 update, Spark 2.1 全系列 √ (2.1.0)\n* 2016.11.14 update, Spark 2.0 全系列 √ (2.0.0, 2.0.1, 2.0.2)\n* 2016.11.07 update, Spark 1.6 全系列 √ (1.6.0, 1.6.1, 1.6.2, 1.6.3)\n```\n\n阅读本文前，请一定先阅读 [Spark Streaming 实现思路与模块概述](0.1 Spark Streaming 实现思路与模块概述.md) 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 `StreamingContext` 细节的解释。\n\n## 引言\n\n![image](040.png)\n\n如各个模块的架构图所示，`StreamingContext` 是 Spark Streaming 提供给用户 code 的、与前述 4 个模块交互的一个简单和统一的入口，是Spark Streaming程序与Spark Core的连接器，下面我们用这段11行的完整 [quick example](http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example)，来说明用户 code 是怎么通过 `StreamingContext` 与前面几个模块进行交互的：\n\n```scala\nimport org.apache.spark._\nimport org.apache.spark.streaming._\n\n// 首先配置一下本 quick example 将跑在本机，app name 是 NetworkWordCount\nval conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\")\n// batchDuration 设置为 1 秒，然后创建一个 streaming 入口\nval ssc = new StreamingContext(conf, Seconds(1))\n\n// ssc.socketTextStream() 将创建一个 SocketInputDStream；这个 InputDStream 的 SocketReceiver 将监听本机 9999 端口\nval lines = ssc.socketTextStream(\"localhost\", 9999)\n\nval words = lines.flatMap(_.split(\" \"))      // DStream transformation\nval pairs = words.map(word => (word, 1))     // DStream transformation\nval wordCounts = pairs.reduceByKey(_ + _)    // DStream transformation\nwordCounts.print()                           // DStream output\n// 上面 4 行利用 DStream transformation 构造出了 lines -> words -> pairs -> wordCounts -> .print() 这样一个 DStreamGraph\n// 但注意，到目前是定义好了产生数据的 SocketReceiver，以及一个 DStreamGraph，这些都是静态的\n\n// 下面这行 start() 将在幕后启动 JobScheduler, 进而启动 JobGenerator 和 ReceiverTracker\n// ssc.start()\n//    -> JobScheduler.start()\n//        -> JobGenerator.start();    开始不断生成一个一个 batch\n//        -> ReceiverTracker.start(); 开始往 executor 上分布 ReceiverSupervisor 了，也会进一步创建和启动 Receiver\nssc.start()\n\n// 然后用户 code 主线程就 block 在下面这行代码了\n// block 的后果就是，后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息\n// 也就是在这里，我们前面静态定义的 DStreamGraph 的 print()，才一次一次被在 RDD 实例上调用，一次一次打印出当前 batch 的结果\nssc.awaitTermination()\n```\n\n从上述样例程序可知，程序的前两行创建了一个新的 `StreamingContext`，第三行通过 `ssc.socketTextStream`通过ssc暴露的方法创建了一个`ReceiverInputDStream`，接着基于DStream的各种方法对数据进行了操作，最后通过 `ssc.start` 方法启动了Spark Streaming 程序，最后一句`ssc.awaitTermination()`将用户 code 主线程 block 住，由后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息地处理，除非发生异常。\n\n我们可以发现，StreamingContext主要包含以下内容：\n\n- StreamingContext的创建(构造函数)\n- StreamingContext的初始化(成员)\n- StreamingContext的状态控制(函数)\n\n\n### StreamingContext的创建\n\n首先我们来看`StreamingContext`的构造函数，主要由三个参数，分别是：\n- SparkContext：SparkStreaming的最终处理是交给SparkContext的；\n- Checkpoint：检查点，用于错误恢复；\n- Duration：设定Streaming每个批次的积累时间。\n\n``` scala\nclass StreamingContext private[streaming] (\n    _sc: SparkContext,\n    _cp: Checkpoint,\n    _batchDur: Duration\n  ) extends Logging {\n```\n\n`StreamingContext` 有以下几种不同的创建方式：\n\n``` scala\n\n  // 1. 通过已经存在的SparkContext创建.\n  def this(sparkContext: SparkContext, batchDuration: Duration) = {\n    this(sparkContext, null, batchDuration)\n  }\n\n  // 2. 通过SparkConf中的配置信息来来创建.\n  def this(conf: SparkConf, batchDuration: Duration) = {\n    this(StreamingContext.createNewSparkContext(conf), null, batchDuration)\n  }\n\n  // 3. 通过一些必要参数来创建\n  def this(\n      master: String,\n      appName: String,\n      batchDuration: Duration,\n      sparkHome: String = null,\n      jars: Seq[String] = Nil,\n      environment: Map[String, String] = Map()) = {\n    this(StreamingContext.createNewSparkContext(master, appName, sparkHome, jars, environment),\n         null, batchDuration)\n  }\n\n  // 4. 从checkpoint文件中读取来重新创建\n  def this(path: String, hadoopConf: Configuration) =\n    this(null, CheckpointReader.read(path, new SparkConf(), hadoopConf).orNull, null)\n\n  // ...\n  // 5. 已经存在的SparkContext，通过读取checkpoint文件重新创建\n  def this(path: String, sparkContext: SparkContext) = {\n    this(\n      sparkContext,\n      CheckpointReader.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull,\n      null)\n  }\n  // ...\n  // 6. 值得一提的是，StreamingContext对象提供了一个构造方法，如果存在Checkpoint就通过Checkpoint创建，否则新建一个StreamingContext\n    def getOrCreate(\n      checkpointPath: String,\n      creatingFunc: () => StreamingContext,\n      hadoopConf: Configuration = SparkHadoopUtil.get.conf,\n      createOnError: Boolean = false\n    ): StreamingContext = {\n    val checkpointOption = CheckpointReader.read(\n      checkpointPath, new SparkConf(), hadoopConf, createOnError)\n    checkpointOption.map(new StreamingContext(null, _, null)).getOrElse(creatingFunc())\n  }\n\n```\n\n整理上述的文件创建过程，可以看出，StreamingContext的创建是一定要包含SparkContext的，同理也可以推出，Spark Streaming最终实际是交给SparkContext来处理的，Spark Streaming更像是Spark Core的一个应用程序。\n`StreamingContext`的创建主要分为两类：\n\n- 1: 通过SparkContext建立新的StreamingContext，需要指定`batchDuration`时间；\n- 2: 从checkpoint文件中读取的Checkpoint对象中创建StreamingContext，用于异常情况下的恢复，`batchDuration`在Checkpoint中已经保存，所以可以不用显示指定。\n\n所以说，要构建StreamingContext，就必须要以上两者至少选一，下面的代码也说明了这点：\n\n``` scala\n  require(_sc != null || _cp != null,\n    \"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null\")\n```\n注意：由于SparkStreaming至少需要一个线程来接收数据，所以local与local[1]模式下是不可以启动的。\n\n### StreamingContext的初始化\n\nStreamingContext在创建后会进行一些初始化（静态定义）的工作，定义一些静态的数据结构，由图可知，StreamingContext主要持有 DStreamGraph 与 JobScheduler 的对象：\n\n如下是graph的初始化定义代码，如果之前存在 Checkpoint ，则graph从 Checkpoint 得到，否则创建一个新的graph\n``` scala\n  private[streaming] val graph: DStreamGraph = {\n    if (isCheckpointPresent) {\n      _cp.graph.setContext(this)\n      // 遍历从Checkpoint数据中恢复出RDD\n      _cp.graph.restoreCheckpointData()\n      _cp.graph\n    } else {\n      require(_batchDur != null, \"Batch duration for StreamingContext cannot be null\")\n      val newGraph = new DStreamGraph()\n      newGraph.setBatchDuration(_batchDur)\n      newGraph\n    }\n  }\n```\n如下是初始化jobScheduler的代码：\n``` scala\n  private[streaming] val scheduler = new JobScheduler(this)\n```\n\n除了上述两个主要成员，StreamingContext还包含以下成员：\n- ContextWaiter：用于等待任务执行结束；\n- StreamingJobProgressListener：用于监听StreamingJob，用以更新StreamingTab的显示；\n- StreamingTab：用于生成SparkUI中Streaming那一页标签；\n- StreamingSource： 流式计算的测量数据源metrics。\n\n除了定义上述成员，StreamingContext还进行了Checkpoint,创建了Checkpoint目录：\n\n``` scala\nconf.getOption(\"spark.streaming.checkpoint.directory\").foreach(checkpoint)\n```\ncheckpoint方法如下：\n``` scala\n  // 创建Checkpoint目录\n  def checkpoint(directory: String) {\n    if (directory != null) {\n      val path = new Path(directory)\n      val fs = path.getFileSystem(sparkContext.hadoopConfiguration)\n      fs.mkdirs(path)\n      val fullPath = fs.getFileStatus(path).getPath().toString\n      sc.setCheckpointDir(fullPath)\n      checkpointDir = fullPath\n    } else {\n      checkpointDir = null\n    }\n  }\n```\n\n### StreamingContext的控制\n\nStreamingContext作为控制面板，给用户提供了许多控制方法，就像控制面板上的按钮，让我们来开发spark Streaming程序，主要有以下方法：\n\n- sparkContext： 获得ssc所属的SparkContext\n- remember(duration: Duration)：通过设置 `graph.remember(duration)` 来设置`rememberDuration`\n\n简单解释一下`rememberDuration`，Spark Streaming 会在每个Batch任务结束时进行一次清理动作`clearMetadata`，每个DStream 都会被扫描，先清理输出Dstream，接着清理输入DStream，清理的时候，根据`rememberDuration`来计算出oldRDD然后清理。`rememberDuration` 有默认值，大体是`slideDuration`，也就是DStream生成RDD的时间间隔，如果设置了checkpointDuration 则是2*checkpointDuration，手动指定的值要大于默认值才会生效。\n\n接下来是定义一些定义输入流的方法，主要有：\n\n- receiverStream[T: ClassTag](receiver: Receiver[T])：创建一个用户自定义的Receiver；\n- socketTextStream：创建TCP socketReceiver，默认是utf-8的文本格式，以'\\n'分隔；\n- socketStream：创建TCP socketReceiver，用户提供自己的转化函数；\n- rawSocketStream：创建SocketReceiver，相比上着，没有中间的解码转化所以比较高效；\n- fileStream：创建监控HDFS目录的InputDStream，通过检测文件的修改时间来判断是否是新文件；\n- binaryRecordsStream：创建二进制文件的监听InputDStream，使用了fileStream方法；\n- queueStream：创建一个RDD队列流，底层调用了UnionRDD的方法将这些RDD转化为一个RDD，开启oneAtATime参数则每个RDD只取一个值，可以用于调试和测试；\n\n在InputDStream的构造过程中，会将此输入流InputDStream添加到DStreamGraph的inputStreams数据结构中，\n``` scala InputDStream.scala\nssc.graph.addInputStream(this)\n```\n\n还定义了一些DStream的其它方法：\n\n- union： 多个DStreams合成一个DStream，底层调用了ssc.sc.union(rdds)；\n- transform： 根据自定义的transformFunc生成新的DStream；\n- addStreamingListener： 在listenerBus上增加一个StreamingListener对象，供JobScheduler的StreamingListenerBus对象监听输入流的ReceiverRateController对象；\n\n还定义了一些控制启动与关闭的方法：\n\n- start：启动StreamingContext。\n\nStreamingContext的start方法启动过程中，会判断StreamingContext的状态，它有三个状态INITIALIZED、ACTIVE、STOP。只有状态为INITAILIZED才允许启动，主要有以下步骤：\n1. 验证graph是否有效；\n2. 设置Checkpoint；\n3. 使用新的线程异步启动 JobScheduler ，启动后将状态由初始化状态INITIALIZED改为ACTIVE状态，JobScheduler的启动请见[JobGenerator 详解](https://github.com/lw-lin/CoolplaySpark/blob/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/2.2%20JobGenerator%20%E8%AF%A6%E8%A7%A3.md)；\n5. 同时添加Streaming的shutdownHookRef，用于程序的异常终止，StreamingContext的shutdownHook优先级比SparkContext的值大1；\n6. 往metricsSystem中注册streamingSource测量数据源；\n7. 添加生成SparkUI中Streaming相关标签\n\n- awaitTermination：等待Streaming程序的停止；\n- stop：停止SparkStreaming程序，其中可以传入参数以表示是否同时停止相关的SparkContext，默认为true(这个参数在文件名流转化为数据流的时候应该设置为false，通过spark.streaming.stopSparkContextByDefault来设置);还有一个参数是是否优雅地停止(等待其它已经接收到的数据处理完毕再停止)；\n\n\n### 引用\n\nhttp://lqding.blog.51cto.com/9123978/1771017\nhttps://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97\nhttp://lqding.blog.51cto.com/9123978/1773912","slug":"Spark源码阅读之——StreamingContext详解","published":1,"updated":"2017-01-06T09:07:38.608Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfwl0023psgu04jybaj6","content":"<p><a href=\"https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97\" target=\"_blank\" rel=\"external\">Spark Streaming 源码解析系列</a>很好地解析了Spark Streaming框架的源码，遗留了一点关于StreamingContext的解析，我基于自己的理解，简要阐述如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">本系列内容适用范围：</div><div class=\"line\">* 2016.12.28 update, Spark 2.1 全系列 √ (2.1.0)</div><div class=\"line\">* 2016.11.14 update, Spark 2.0 全系列 √ (2.0.0, 2.0.1, 2.0.2)</div><div class=\"line\">* 2016.11.07 update, Spark 1.6 全系列 √ (1.6.0, 1.6.1, 1.6.2, 1.6.3)</div></pre></td></tr></table></figure>\n<p>阅读本文前，请一定先阅读 <a href=\"0.1 Spark Streaming 实现思路与模块概述.md\">Spark Streaming 实现思路与模块概述</a> 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 <code>StreamingContext</code> 细节的解释。</p>\n<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p><img src=\"040.png\" alt=\"image\"></p>\n<p>如各个模块的架构图所示，<code>StreamingContext</code> 是 Spark Streaming 提供给用户 code 的、与前述 4 个模块交互的一个简单和统一的入口，是Spark Streaming程序与Spark Core的连接器，下面我们用这段11行的完整 <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example\" target=\"_blank\" rel=\"external\">quick example</a>，来说明用户 code 是怎么通过 <code>StreamingContext</code> 与前面几个模块进行交互的：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark._</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming._</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 首先配置一下本 quick example 将跑在本机，app name 是 NetworkWordCount</span></div><div class=\"line\"><span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setMaster(<span class=\"string\">\"local[2]\"</span>).setAppName(<span class=\"string\">\"NetworkWordCount\"</span>)</div><div class=\"line\"><span class=\"comment\">// batchDuration 设置为 1 秒，然后创建一个 streaming 入口</span></div><div class=\"line\"><span class=\"keyword\">val</span> ssc = <span class=\"keyword\">new</span> <span class=\"type\">StreamingContext</span>(conf, <span class=\"type\">Seconds</span>(<span class=\"number\">1</span>))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// ssc.socketTextStream() 将创建一个 SocketInputDStream；这个 InputDStream 的 SocketReceiver 将监听本机 9999 端口</span></div><div class=\"line\"><span class=\"keyword\">val</span> lines = ssc.socketTextStream(<span class=\"string\">\"localhost\"</span>, <span class=\"number\">9999</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> words = lines.flatMap(_.split(<span class=\"string\">\" \"</span>))      <span class=\"comment\">// DStream transformation</span></div><div class=\"line\"><span class=\"keyword\">val</span> pairs = words.map(word =&gt; (word, <span class=\"number\">1</span>))     <span class=\"comment\">// DStream transformation</span></div><div class=\"line\"><span class=\"keyword\">val</span> wordCounts = pairs.reduceByKey(_ + _)    <span class=\"comment\">// DStream transformation</span></div><div class=\"line\">wordCounts.print()                           <span class=\"comment\">// DStream output</span></div><div class=\"line\"><span class=\"comment\">// 上面 4 行利用 DStream transformation 构造出了 lines -&gt; words -&gt; pairs -&gt; wordCounts -&gt; .print() 这样一个 DStreamGraph</span></div><div class=\"line\"><span class=\"comment\">// 但注意，到目前是定义好了产生数据的 SocketReceiver，以及一个 DStreamGraph，这些都是静态的</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 下面这行 start() 将在幕后启动 JobScheduler, 进而启动 JobGenerator 和 ReceiverTracker</span></div><div class=\"line\"><span class=\"comment\">// ssc.start()</span></div><div class=\"line\"><span class=\"comment\">//    -&gt; JobScheduler.start()</span></div><div class=\"line\"><span class=\"comment\">//        -&gt; JobGenerator.start();    开始不断生成一个一个 batch</span></div><div class=\"line\"><span class=\"comment\">//        -&gt; ReceiverTracker.start(); 开始往 executor 上分布 ReceiverSupervisor 了，也会进一步创建和启动 Receiver</span></div><div class=\"line\">ssc.start()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 然后用户 code 主线程就 block 在下面这行代码了</span></div><div class=\"line\"><span class=\"comment\">// block 的后果就是，后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息</span></div><div class=\"line\"><span class=\"comment\">// 也就是在这里，我们前面静态定义的 DStreamGraph 的 print()，才一次一次被在 RDD 实例上调用，一次一次打印出当前 batch 的结果</span></div><div class=\"line\">ssc.awaitTermination()</div></pre></td></tr></table></figure>\n<p>从上述样例程序可知，程序的前两行创建了一个新的 <code>StreamingContext</code>，第三行通过 <code>ssc.socketTextStream</code>通过ssc暴露的方法创建了一个<code>ReceiverInputDStream</code>，接着基于DStream的各种方法对数据进行了操作，最后通过 <code>ssc.start</code> 方法启动了Spark Streaming 程序，最后一句<code>ssc.awaitTermination()</code>将用户 code 主线程 block 住，由后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息地处理，除非发生异常。</p>\n<p>我们可以发现，StreamingContext主要包含以下内容：</p>\n<ul>\n<li>StreamingContext的创建(构造函数)</li>\n<li>StreamingContext的初始化(成员)</li>\n<li>StreamingContext的状态控制(函数)</li>\n</ul>\n<h3 id=\"StreamingContext的创建\"><a href=\"#StreamingContext的创建\" class=\"headerlink\" title=\"StreamingContext的创建\"></a>StreamingContext的创建</h3><p>首先我们来看<code>StreamingContext</code>的构造函数，主要由三个参数，分别是：</p>\n<ul>\n<li>SparkContext：SparkStreaming的最终处理是交给SparkContext的；</li>\n<li>Checkpoint：检查点，用于错误恢复；</li>\n<li>Duration：设定Streaming每个批次的积累时间。</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StreamingContext</span> <span class=\"title\">private</span>[streaming] (<span class=\"params\"></span></span></div><div class=\"line\">    _sc: <span class=\"type\">SparkContext</span>,</div><div class=\"line\">    _cp: <span class=\"type\">Checkpoint</span>,</div><div class=\"line\">    _batchDur: <span class=\"type\">Duration</span></div><div class=\"line\">  ) <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> &#123;</div></pre></td></tr></table></figure>\n<p><code>StreamingContext</code> 有以下几种不同的创建方式：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 1. 通过已经存在的SparkContext创建.</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(sparkContext: <span class=\"type\">SparkContext</span>, batchDuration: <span class=\"type\">Duration</span>) = &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>(sparkContext, <span class=\"literal\">null</span>, batchDuration)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 2. 通过SparkConf中的配置信息来来创建.</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(conf: <span class=\"type\">SparkConf</span>, batchDuration: <span class=\"type\">Duration</span>) = &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>(<span class=\"type\">StreamingContext</span>.createNewSparkContext(conf), <span class=\"literal\">null</span>, batchDuration)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 3. 通过一些必要参数来创建</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(</div><div class=\"line\">    master: <span class=\"type\">String</span>,</div><div class=\"line\">    appName: <span class=\"type\">String</span>,</div><div class=\"line\">    batchDuration: <span class=\"type\">Duration</span>,</div><div class=\"line\">    sparkHome: <span class=\"type\">String</span> = <span class=\"literal\">null</span>,</div><div class=\"line\">    jars: <span class=\"type\">Seq</span>[<span class=\"type\">String</span>] = <span class=\"type\">Nil</span>,</div><div class=\"line\">    environment: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"type\">Map</span>()) = &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>(<span class=\"type\">StreamingContext</span>.createNewSparkContext(master, appName, sparkHome, jars, environment),</div><div class=\"line\">       <span class=\"literal\">null</span>, batchDuration)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 4. 从checkpoint文件中读取来重新创建</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(path: <span class=\"type\">String</span>, hadoopConf: <span class=\"type\">Configuration</span>) =</div><div class=\"line\">  <span class=\"keyword\">this</span>(<span class=\"literal\">null</span>, <span class=\"type\">CheckpointReader</span>.read(path, <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>(), hadoopConf).orNull, <span class=\"literal\">null</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\"><span class=\"comment\">// 5. 已经存在的SparkContext，通过读取checkpoint文件重新创建</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(path: <span class=\"type\">String</span>, sparkContext: <span class=\"type\">SparkContext</span>) = &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>(</div><div class=\"line\">    sparkContext,</div><div class=\"line\">    <span class=\"type\">CheckpointReader</span>.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull,</div><div class=\"line\">    <span class=\"literal\">null</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\"><span class=\"comment\">// 6. 值得一提的是，StreamingContext对象提供了一个构造方法，如果存在Checkpoint就通过Checkpoint创建，否则新建一个StreamingContext</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getOrCreate</span></span>(</div><div class=\"line\">    checkpointPath: <span class=\"type\">String</span>,</div><div class=\"line\">    creatingFunc: () =&gt; <span class=\"type\">StreamingContext</span>,</div><div class=\"line\">    hadoopConf: <span class=\"type\">Configuration</span> = <span class=\"type\">SparkHadoopUtil</span>.get.conf,</div><div class=\"line\">    createOnError: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span></div><div class=\"line\">  ): <span class=\"type\">StreamingContext</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> checkpointOption = <span class=\"type\">CheckpointReader</span>.read(</div><div class=\"line\">    checkpointPath, <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>(), hadoopConf, createOnError)</div><div class=\"line\">  checkpointOption.map(<span class=\"keyword\">new</span> <span class=\"type\">StreamingContext</span>(<span class=\"literal\">null</span>, _, <span class=\"literal\">null</span>)).getOrElse(creatingFunc())</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>整理上述的文件创建过程，可以看出，StreamingContext的创建是一定要包含SparkContext的，同理也可以推出，Spark Streaming最终实际是交给SparkContext来处理的，Spark Streaming更像是Spark Core的一个应用程序。<br><code>StreamingContext</code>的创建主要分为两类：</p>\n<ul>\n<li>1: 通过SparkContext建立新的StreamingContext，需要指定<code>batchDuration</code>时间；</li>\n<li>2: 从checkpoint文件中读取的Checkpoint对象中创建StreamingContext，用于异常情况下的恢复，<code>batchDuration</code>在Checkpoint中已经保存，所以可以不用显示指定。</li>\n</ul>\n<p>所以说，要构建StreamingContext，就必须要以上两者至少选一，下面的代码也说明了这点：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">require(_sc != <span class=\"literal\">null</span> || _cp != <span class=\"literal\">null</span>,</div><div class=\"line\">  <span class=\"string\">\"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null\"</span>)</div></pre></td></tr></table></figure>\n<p>注意：由于SparkStreaming至少需要一个线程来接收数据，所以local与local[1]模式下是不可以启动的。</p>\n<h3 id=\"StreamingContext的初始化\"><a href=\"#StreamingContext的初始化\" class=\"headerlink\" title=\"StreamingContext的初始化\"></a>StreamingContext的初始化</h3><p>StreamingContext在创建后会进行一些初始化（静态定义）的工作，定义一些静态的数据结构，由图可知，StreamingContext主要持有 DStreamGraph 与 JobScheduler 的对象：</p>\n<p>如下是graph的初始化定义代码，如果之前存在 Checkpoint ，则graph从 Checkpoint 得到，否则创建一个新的graph<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span>[streaming] <span class=\"keyword\">val</span> graph: <span class=\"type\">DStreamGraph</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (isCheckpointPresent) &#123;</div><div class=\"line\">    _cp.graph.setContext(<span class=\"keyword\">this</span>)</div><div class=\"line\">    <span class=\"comment\">// 遍历从Checkpoint数据中恢复出RDD</span></div><div class=\"line\">    _cp.graph.restoreCheckpointData()</div><div class=\"line\">    _cp.graph</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">    require(_batchDur != <span class=\"literal\">null</span>, <span class=\"string\">\"Batch duration for StreamingContext cannot be null\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> newGraph = <span class=\"keyword\">new</span> <span class=\"type\">DStreamGraph</span>()</div><div class=\"line\">    newGraph.setBatchDuration(_batchDur)</div><div class=\"line\">    newGraph</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>如下是初始化jobScheduler的代码：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span>[streaming] <span class=\"keyword\">val</span> scheduler = <span class=\"keyword\">new</span> <span class=\"type\">JobScheduler</span>(<span class=\"keyword\">this</span>)</div></pre></td></tr></table></figure></p>\n<p>除了上述两个主要成员，StreamingContext还包含以下成员：</p>\n<ul>\n<li>ContextWaiter：用于等待任务执行结束；</li>\n<li>StreamingJobProgressListener：用于监听StreamingJob，用以更新StreamingTab的显示；</li>\n<li>StreamingTab：用于生成SparkUI中Streaming那一页标签；</li>\n<li>StreamingSource： 流式计算的测量数据源metrics。</li>\n</ul>\n<p>除了定义上述成员，StreamingContext还进行了Checkpoint,创建了Checkpoint目录：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">conf.getOption(<span class=\"string\">\"spark.streaming.checkpoint.directory\"</span>).foreach(checkpoint)</div></pre></td></tr></table></figure>\n<p>checkpoint方法如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 创建Checkpoint目录</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">checkpoint</span></span>(directory: <span class=\"type\">String</span>) &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (directory != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> path = <span class=\"keyword\">new</span> <span class=\"type\">Path</span>(directory)</div><div class=\"line\">    <span class=\"keyword\">val</span> fs = path.getFileSystem(sparkContext.hadoopConfiguration)</div><div class=\"line\">    fs.mkdirs(path)</div><div class=\"line\">    <span class=\"keyword\">val</span> fullPath = fs.getFileStatus(path).getPath().toString</div><div class=\"line\">    sc.setCheckpointDir(fullPath)</div><div class=\"line\">    checkpointDir = fullPath</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">    checkpointDir = <span class=\"literal\">null</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"StreamingContext的控制\"><a href=\"#StreamingContext的控制\" class=\"headerlink\" title=\"StreamingContext的控制\"></a>StreamingContext的控制</h3><p>StreamingContext作为控制面板，给用户提供了许多控制方法，就像控制面板上的按钮，让我们来开发spark Streaming程序，主要有以下方法：</p>\n<ul>\n<li>sparkContext： 获得ssc所属的SparkContext</li>\n<li>remember(duration: Duration)：通过设置 <code>graph.remember(duration)</code> 来设置<code>rememberDuration</code></li>\n</ul>\n<p>简单解释一下<code>rememberDuration</code>，Spark Streaming 会在每个Batch任务结束时进行一次清理动作<code>clearMetadata</code>，每个DStream 都会被扫描，先清理输出Dstream，接着清理输入DStream，清理的时候，根据<code>rememberDuration</code>来计算出oldRDD然后清理。<code>rememberDuration</code> 有默认值，大体是<code>slideDuration</code>，也就是DStream生成RDD的时间间隔，如果设置了checkpointDuration 则是2*checkpointDuration，手动指定的值要大于默认值才会生效。</p>\n<p>接下来是定义一些定义输入流的方法，主要有：</p>\n<ul>\n<li>receiverStream<a href=\"receiver: Receiver[T]\" target=\"_blank\" rel=\"external\">T: ClassTag</a>：创建一个用户自定义的Receiver；</li>\n<li>socketTextStream：创建TCP socketReceiver，默认是utf-8的文本格式，以’\\n’分隔；</li>\n<li>socketStream：创建TCP socketReceiver，用户提供自己的转化函数；</li>\n<li>rawSocketStream：创建SocketReceiver，相比上着，没有中间的解码转化所以比较高效；</li>\n<li>fileStream：创建监控HDFS目录的InputDStream，通过检测文件的修改时间来判断是否是新文件；</li>\n<li>binaryRecordsStream：创建二进制文件的监听InputDStream，使用了fileStream方法；</li>\n<li>queueStream：创建一个RDD队列流，底层调用了UnionRDD的方法将这些RDD转化为一个RDD，开启oneAtATime参数则每个RDD只取一个值，可以用于调试和测试；</li>\n</ul>\n<p>在InputDStream的构造过程中，会将此输入流InputDStream添加到DStreamGraph的inputStreams数据结构中，<br><figure class=\"highlight scala\"><figcaption><span>InputDStream.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ssc.graph.addInputStream(<span class=\"keyword\">this</span>)</div></pre></td></tr></table></figure></p>\n<p>还定义了一些DStream的其它方法：</p>\n<ul>\n<li>union： 多个DStreams合成一个DStream，底层调用了ssc.sc.union(rdds)；</li>\n<li>transform： 根据自定义的transformFunc生成新的DStream；</li>\n<li>addStreamingListener： 在listenerBus上增加一个StreamingListener对象，供JobScheduler的StreamingListenerBus对象监听输入流的ReceiverRateController对象；</li>\n</ul>\n<p>还定义了一些控制启动与关闭的方法：</p>\n<ul>\n<li>start：启动StreamingContext。</li>\n</ul>\n<p>StreamingContext的start方法启动过程中，会判断StreamingContext的状态，它有三个状态INITIALIZED、ACTIVE、STOP。只有状态为INITAILIZED才允许启动，主要有以下步骤：</p>\n<ol>\n<li>验证graph是否有效；</li>\n<li>设置Checkpoint；</li>\n<li>使用新的线程异步启动 JobScheduler ，启动后将状态由初始化状态INITIALIZED改为ACTIVE状态，JobScheduler的启动请见<a href=\"https://github.com/lw-lin/CoolplaySpark/blob/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/2.2%20JobGenerator%20%E8%AF%A6%E8%A7%A3.md\" target=\"_blank\" rel=\"external\">JobGenerator 详解</a>；</li>\n<li>同时添加Streaming的shutdownHookRef，用于程序的异常终止，StreamingContext的shutdownHook优先级比SparkContext的值大1；</li>\n<li>往metricsSystem中注册streamingSource测量数据源；</li>\n<li>添加生成SparkUI中Streaming相关标签</li>\n</ol>\n<ul>\n<li>awaitTermination：等待Streaming程序的停止；</li>\n<li>stop：停止SparkStreaming程序，其中可以传入参数以表示是否同时停止相关的SparkContext，默认为true(这个参数在文件名流转化为数据流的时候应该设置为false，通过spark.streaming.stopSparkContextByDefault来设置);还有一个参数是是否优雅地停止(等待其它已经接收到的数据处理完毕再停止)；</li>\n</ul>\n<h3 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h3><p><a href=\"http://lqding.blog.51cto.com/9123978/1771017\" target=\"_blank\" rel=\"external\">http://lqding.blog.51cto.com/9123978/1771017</a><br><a href=\"https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97\" target=\"_blank\" rel=\"external\">https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97</a><br><a href=\"http://lqding.blog.51cto.com/9123978/1773912\" target=\"_blank\" rel=\"external\">http://lqding.blog.51cto.com/9123978/1773912</a></p>\n","excerpt":"","more":"<p><a href=\"https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97\">Spark Streaming 源码解析系列</a>很好地解析了Spark Streaming框架的源码，遗留了一点关于StreamingContext的解析，我基于自己的理解，简要阐述如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">本系列内容适用范围：</div><div class=\"line\">* 2016.12.28 update, Spark 2.1 全系列 √ (2.1.0)</div><div class=\"line\">* 2016.11.14 update, Spark 2.0 全系列 √ (2.0.0, 2.0.1, 2.0.2)</div><div class=\"line\">* 2016.11.07 update, Spark 1.6 全系列 √ (1.6.0, 1.6.1, 1.6.2, 1.6.3)</div></pre></td></tr></table></figure>\n<p>阅读本文前，请一定先阅读 <a href=\"0.1 Spark Streaming 实现思路与模块概述.md\">Spark Streaming 实现思路与模块概述</a> 一文，其中概述了 Spark Streaming 的 4 大模块的基本作用，有了全局概念后再看本文对 <code>StreamingContext</code> 细节的解释。</p>\n<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p><img src=\"040.png\" alt=\"image\"></p>\n<p>如各个模块的架构图所示，<code>StreamingContext</code> 是 Spark Streaming 提供给用户 code 的、与前述 4 个模块交互的一个简单和统一的入口，是Spark Streaming程序与Spark Core的连接器，下面我们用这段11行的完整 <a href=\"http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example\">quick example</a>，来说明用户 code 是怎么通过 <code>StreamingContext</code> 与前面几个模块进行交互的：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark._</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.streaming._</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 首先配置一下本 quick example 将跑在本机，app name 是 NetworkWordCount</span></div><div class=\"line\"><span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setMaster(<span class=\"string\">\"local[2]\"</span>).setAppName(<span class=\"string\">\"NetworkWordCount\"</span>)</div><div class=\"line\"><span class=\"comment\">// batchDuration 设置为 1 秒，然后创建一个 streaming 入口</span></div><div class=\"line\"><span class=\"keyword\">val</span> ssc = <span class=\"keyword\">new</span> <span class=\"type\">StreamingContext</span>(conf, <span class=\"type\">Seconds</span>(<span class=\"number\">1</span>))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// ssc.socketTextStream() 将创建一个 SocketInputDStream；这个 InputDStream 的 SocketReceiver 将监听本机 9999 端口</span></div><div class=\"line\"><span class=\"keyword\">val</span> lines = ssc.socketTextStream(<span class=\"string\">\"localhost\"</span>, <span class=\"number\">9999</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> words = lines.flatMap(_.split(<span class=\"string\">\" \"</span>))      <span class=\"comment\">// DStream transformation</span></div><div class=\"line\"><span class=\"keyword\">val</span> pairs = words.map(word =&gt; (word, <span class=\"number\">1</span>))     <span class=\"comment\">// DStream transformation</span></div><div class=\"line\"><span class=\"keyword\">val</span> wordCounts = pairs.reduceByKey(_ + _)    <span class=\"comment\">// DStream transformation</span></div><div class=\"line\">wordCounts.print()                           <span class=\"comment\">// DStream output</span></div><div class=\"line\"><span class=\"comment\">// 上面 4 行利用 DStream transformation 构造出了 lines -&gt; words -&gt; pairs -&gt; wordCounts -&gt; .print() 这样一个 DStreamGraph</span></div><div class=\"line\"><span class=\"comment\">// 但注意，到目前是定义好了产生数据的 SocketReceiver，以及一个 DStreamGraph，这些都是静态的</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 下面这行 start() 将在幕后启动 JobScheduler, 进而启动 JobGenerator 和 ReceiverTracker</span></div><div class=\"line\"><span class=\"comment\">// ssc.start()</span></div><div class=\"line\"><span class=\"comment\">//    -&gt; JobScheduler.start()</span></div><div class=\"line\"><span class=\"comment\">//        -&gt; JobGenerator.start();    开始不断生成一个一个 batch</span></div><div class=\"line\"><span class=\"comment\">//        -&gt; ReceiverTracker.start(); 开始往 executor 上分布 ReceiverSupervisor 了，也会进一步创建和启动 Receiver</span></div><div class=\"line\">ssc.start()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 然后用户 code 主线程就 block 在下面这行代码了</span></div><div class=\"line\"><span class=\"comment\">// block 的后果就是，后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息</span></div><div class=\"line\"><span class=\"comment\">// 也就是在这里，我们前面静态定义的 DStreamGraph 的 print()，才一次一次被在 RDD 实例上调用，一次一次打印出当前 batch 的结果</span></div><div class=\"line\">ssc.awaitTermination()</div></pre></td></tr></table></figure>\n<p>从上述样例程序可知，程序的前两行创建了一个新的 <code>StreamingContext</code>，第三行通过 <code>ssc.socketTextStream</code>通过ssc暴露的方法创建了一个<code>ReceiverInputDStream</code>，接着基于DStream的各种方法对数据进行了操作，最后通过 <code>ssc.start</code> 方法启动了Spark Streaming 程序，最后一句<code>ssc.awaitTermination()</code>将用户 code 主线程 block 住，由后台的 JobScheduler 线程周而复始的产生一个一个 batch 而不停息地处理，除非发生异常。</p>\n<p>我们可以发现，StreamingContext主要包含以下内容：</p>\n<ul>\n<li>StreamingContext的创建(构造函数)</li>\n<li>StreamingContext的初始化(成员)</li>\n<li>StreamingContext的状态控制(函数)</li>\n</ul>\n<h3 id=\"StreamingContext的创建\"><a href=\"#StreamingContext的创建\" class=\"headerlink\" title=\"StreamingContext的创建\"></a>StreamingContext的创建</h3><p>首先我们来看<code>StreamingContext</code>的构造函数，主要由三个参数，分别是：</p>\n<ul>\n<li>SparkContext：SparkStreaming的最终处理是交给SparkContext的；</li>\n<li>Checkpoint：检查点，用于错误恢复；</li>\n<li>Duration：设定Streaming每个批次的积累时间。</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StreamingContext</span> <span class=\"title\">private</span>[streaming] (<span class=\"params\"></div><div class=\"line\">    _sc: <span class=\"type\">SparkContext</span>,</div><div class=\"line\">    _cp: <span class=\"type\">Checkpoint</span>,</div><div class=\"line\">    _batchDur: <span class=\"type\">Duration</span></div><div class=\"line\">  </span>) <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</div></pre></td></tr></table></figure>\n<p><code>StreamingContext</code> 有以下几种不同的创建方式：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 1. 通过已经存在的SparkContext创建.</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(sparkContext: <span class=\"type\">SparkContext</span>, batchDuration: <span class=\"type\">Duration</span>) = &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>(sparkContext, <span class=\"literal\">null</span>, batchDuration)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 2. 通过SparkConf中的配置信息来来创建.</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(conf: <span class=\"type\">SparkConf</span>, batchDuration: <span class=\"type\">Duration</span>) = &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>(<span class=\"type\">StreamingContext</span>.createNewSparkContext(conf), <span class=\"literal\">null</span>, batchDuration)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 3. 通过一些必要参数来创建</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(</div><div class=\"line\">    master: <span class=\"type\">String</span>,</div><div class=\"line\">    appName: <span class=\"type\">String</span>,</div><div class=\"line\">    batchDuration: <span class=\"type\">Duration</span>,</div><div class=\"line\">    sparkHome: <span class=\"type\">String</span> = <span class=\"literal\">null</span>,</div><div class=\"line\">    jars: <span class=\"type\">Seq</span>[<span class=\"type\">String</span>] = <span class=\"type\">Nil</span>,</div><div class=\"line\">    environment: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"type\">Map</span>()) = &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>(<span class=\"type\">StreamingContext</span>.createNewSparkContext(master, appName, sparkHome, jars, environment),</div><div class=\"line\">       <span class=\"literal\">null</span>, batchDuration)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 4. 从checkpoint文件中读取来重新创建</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(path: <span class=\"type\">String</span>, hadoopConf: <span class=\"type\">Configuration</span>) =</div><div class=\"line\">  <span class=\"keyword\">this</span>(<span class=\"literal\">null</span>, <span class=\"type\">CheckpointReader</span>.read(path, <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>(), hadoopConf).orNull, <span class=\"literal\">null</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\"><span class=\"comment\">// 5. 已经存在的SparkContext，通过读取checkpoint文件重新创建</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">this</span></span>(path: <span class=\"type\">String</span>, sparkContext: <span class=\"type\">SparkContext</span>) = &#123;</div><div class=\"line\">  <span class=\"keyword\">this</span>(</div><div class=\"line\">    sparkContext,</div><div class=\"line\">    <span class=\"type\">CheckpointReader</span>.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull,</div><div class=\"line\">    <span class=\"literal\">null</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"comment\">// ...</span></div><div class=\"line\"><span class=\"comment\">// 6. 值得一提的是，StreamingContext对象提供了一个构造方法，如果存在Checkpoint就通过Checkpoint创建，否则新建一个StreamingContext</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getOrCreate</span></span>(</div><div class=\"line\">    checkpointPath: <span class=\"type\">String</span>,</div><div class=\"line\">    creatingFunc: () =&gt; <span class=\"type\">StreamingContext</span>,</div><div class=\"line\">    hadoopConf: <span class=\"type\">Configuration</span> = <span class=\"type\">SparkHadoopUtil</span>.get.conf,</div><div class=\"line\">    createOnError: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span></div><div class=\"line\">  ): <span class=\"type\">StreamingContext</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> checkpointOption = <span class=\"type\">CheckpointReader</span>.read(</div><div class=\"line\">    checkpointPath, <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>(), hadoopConf, createOnError)</div><div class=\"line\">  checkpointOption.map(<span class=\"keyword\">new</span> <span class=\"type\">StreamingContext</span>(<span class=\"literal\">null</span>, _, <span class=\"literal\">null</span>)).getOrElse(creatingFunc())</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>整理上述的文件创建过程，可以看出，StreamingContext的创建是一定要包含SparkContext的，同理也可以推出，Spark Streaming最终实际是交给SparkContext来处理的，Spark Streaming更像是Spark Core的一个应用程序。<br><code>StreamingContext</code>的创建主要分为两类：</p>\n<ul>\n<li>1: 通过SparkContext建立新的StreamingContext，需要指定<code>batchDuration</code>时间；</li>\n<li>2: 从checkpoint文件中读取的Checkpoint对象中创建StreamingContext，用于异常情况下的恢复，<code>batchDuration</code>在Checkpoint中已经保存，所以可以不用显示指定。</li>\n</ul>\n<p>所以说，要构建StreamingContext，就必须要以上两者至少选一，下面的代码也说明了这点：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">require(_sc != <span class=\"literal\">null</span> || _cp != <span class=\"literal\">null</span>,</div><div class=\"line\">  <span class=\"string\">\"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null\"</span>)</div></pre></td></tr></table></figure>\n<p>注意：由于SparkStreaming至少需要一个线程来接收数据，所以local与local[1]模式下是不可以启动的。</p>\n<h3 id=\"StreamingContext的初始化\"><a href=\"#StreamingContext的初始化\" class=\"headerlink\" title=\"StreamingContext的初始化\"></a>StreamingContext的初始化</h3><p>StreamingContext在创建后会进行一些初始化（静态定义）的工作，定义一些静态的数据结构，由图可知，StreamingContext主要持有 DStreamGraph 与 JobScheduler 的对象：</p>\n<p>如下是graph的初始化定义代码，如果之前存在 Checkpoint ，则graph从 Checkpoint 得到，否则创建一个新的graph<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span>[streaming] <span class=\"keyword\">val</span> graph: <span class=\"type\">DStreamGraph</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (isCheckpointPresent) &#123;</div><div class=\"line\">    _cp.graph.setContext(<span class=\"keyword\">this</span>)</div><div class=\"line\">    <span class=\"comment\">// 遍历从Checkpoint数据中恢复出RDD</span></div><div class=\"line\">    _cp.graph.restoreCheckpointData()</div><div class=\"line\">    _cp.graph</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">    require(_batchDur != <span class=\"literal\">null</span>, <span class=\"string\">\"Batch duration for StreamingContext cannot be null\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> newGraph = <span class=\"keyword\">new</span> <span class=\"type\">DStreamGraph</span>()</div><div class=\"line\">    newGraph.setBatchDuration(_batchDur)</div><div class=\"line\">    newGraph</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>如下是初始化jobScheduler的代码：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span>[streaming] <span class=\"keyword\">val</span> scheduler = <span class=\"keyword\">new</span> <span class=\"type\">JobScheduler</span>(<span class=\"keyword\">this</span>)</div></pre></td></tr></table></figure></p>\n<p>除了上述两个主要成员，StreamingContext还包含以下成员：</p>\n<ul>\n<li>ContextWaiter：用于等待任务执行结束；</li>\n<li>StreamingJobProgressListener：用于监听StreamingJob，用以更新StreamingTab的显示；</li>\n<li>StreamingTab：用于生成SparkUI中Streaming那一页标签；</li>\n<li>StreamingSource： 流式计算的测量数据源metrics。</li>\n</ul>\n<p>除了定义上述成员，StreamingContext还进行了Checkpoint,创建了Checkpoint目录：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">conf.getOption(<span class=\"string\">\"spark.streaming.checkpoint.directory\"</span>).foreach(checkpoint)</div></pre></td></tr></table></figure>\n<p>checkpoint方法如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 创建Checkpoint目录</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">checkpoint</span></span>(directory: <span class=\"type\">String</span>) &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (directory != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> path = <span class=\"keyword\">new</span> <span class=\"type\">Path</span>(directory)</div><div class=\"line\">    <span class=\"keyword\">val</span> fs = path.getFileSystem(sparkContext.hadoopConfiguration)</div><div class=\"line\">    fs.mkdirs(path)</div><div class=\"line\">    <span class=\"keyword\">val</span> fullPath = fs.getFileStatus(path).getPath().toString</div><div class=\"line\">    sc.setCheckpointDir(fullPath)</div><div class=\"line\">    checkpointDir = fullPath</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">    checkpointDir = <span class=\"literal\">null</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"StreamingContext的控制\"><a href=\"#StreamingContext的控制\" class=\"headerlink\" title=\"StreamingContext的控制\"></a>StreamingContext的控制</h3><p>StreamingContext作为控制面板，给用户提供了许多控制方法，就像控制面板上的按钮，让我们来开发spark Streaming程序，主要有以下方法：</p>\n<ul>\n<li>sparkContext： 获得ssc所属的SparkContext</li>\n<li>remember(duration: Duration)：通过设置 <code>graph.remember(duration)</code> 来设置<code>rememberDuration</code></li>\n</ul>\n<p>简单解释一下<code>rememberDuration</code>，Spark Streaming 会在每个Batch任务结束时进行一次清理动作<code>clearMetadata</code>，每个DStream 都会被扫描，先清理输出Dstream，接着清理输入DStream，清理的时候，根据<code>rememberDuration</code>来计算出oldRDD然后清理。<code>rememberDuration</code> 有默认值，大体是<code>slideDuration</code>，也就是DStream生成RDD的时间间隔，如果设置了checkpointDuration 则是2*checkpointDuration，手动指定的值要大于默认值才会生效。</p>\n<p>接下来是定义一些定义输入流的方法，主要有：</p>\n<ul>\n<li>receiverStream<a href=\"receiver: Receiver[T]\">T: ClassTag</a>：创建一个用户自定义的Receiver；</li>\n<li>socketTextStream：创建TCP socketReceiver，默认是utf-8的文本格式，以’\\n’分隔；</li>\n<li>socketStream：创建TCP socketReceiver，用户提供自己的转化函数；</li>\n<li>rawSocketStream：创建SocketReceiver，相比上着，没有中间的解码转化所以比较高效；</li>\n<li>fileStream：创建监控HDFS目录的InputDStream，通过检测文件的修改时间来判断是否是新文件；</li>\n<li>binaryRecordsStream：创建二进制文件的监听InputDStream，使用了fileStream方法；</li>\n<li>queueStream：创建一个RDD队列流，底层调用了UnionRDD的方法将这些RDD转化为一个RDD，开启oneAtATime参数则每个RDD只取一个值，可以用于调试和测试；</li>\n</ul>\n<p>在InputDStream的构造过程中，会将此输入流InputDStream添加到DStreamGraph的inputStreams数据结构中，<br><figure class=\"highlight scala\"><figcaption><span>InputDStream.scala</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ssc.graph.addInputStream(<span class=\"keyword\">this</span>)</div></pre></td></tr></table></figure></p>\n<p>还定义了一些DStream的其它方法：</p>\n<ul>\n<li>union： 多个DStreams合成一个DStream，底层调用了ssc.sc.union(rdds)；</li>\n<li>transform： 根据自定义的transformFunc生成新的DStream；</li>\n<li>addStreamingListener： 在listenerBus上增加一个StreamingListener对象，供JobScheduler的StreamingListenerBus对象监听输入流的ReceiverRateController对象；</li>\n</ul>\n<p>还定义了一些控制启动与关闭的方法：</p>\n<ul>\n<li>start：启动StreamingContext。</li>\n</ul>\n<p>StreamingContext的start方法启动过程中，会判断StreamingContext的状态，它有三个状态INITIALIZED、ACTIVE、STOP。只有状态为INITAILIZED才允许启动，主要有以下步骤：</p>\n<ol>\n<li>验证graph是否有效；</li>\n<li>设置Checkpoint；</li>\n<li>使用新的线程异步启动 JobScheduler ，启动后将状态由初始化状态INITIALIZED改为ACTIVE状态，JobScheduler的启动请见<a href=\"https://github.com/lw-lin/CoolplaySpark/blob/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/2.2%20JobGenerator%20%E8%AF%A6%E8%A7%A3.md\">JobGenerator 详解</a>；</li>\n<li>同时添加Streaming的shutdownHookRef，用于程序的异常终止，StreamingContext的shutdownHook优先级比SparkContext的值大1；</li>\n<li>往metricsSystem中注册streamingSource测量数据源；</li>\n<li>添加生成SparkUI中Streaming相关标签</li>\n</ol>\n<ul>\n<li>awaitTermination：等待Streaming程序的停止；</li>\n<li>stop：停止SparkStreaming程序，其中可以传入参数以表示是否同时停止相关的SparkContext，默认为true(这个参数在文件名流转化为数据流的时候应该设置为false，通过spark.streaming.stopSparkContextByDefault来设置);还有一个参数是是否优雅地停止(等待其它已经接收到的数据处理完毕再停止)；</li>\n</ul>\n<h3 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h3><p><a href=\"http://lqding.blog.51cto.com/9123978/1771017\">http://lqding.blog.51cto.com/9123978/1771017</a><br><a href=\"https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97\">https://github.com/lw-lin/CoolplaySpark/tree/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97</a><br><a href=\"http://lqding.blog.51cto.com/9123978/1773912\">http://lqding.blog.51cto.com/9123978/1773912</a></p>\n"},{"title":"给未来的自己","toc":false,"date":"2012-11-11T11:16:06.000Z","_content":"                                                       —— 2012年秋于安大新区\n<!-- <iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=0 height=0 src=\"http://qqma.tingge123.com:823/mp3/2016-04-06/1459941003.mp3\"></iframe> -->\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=0 height=0 src=\"http://qqma.tingge123.com:823/mp3/2016-04-06/1459941083.mp3\"></iframe>\n--------------\n突然无名的落寞，对未来的迷茫，学业的压力，不自信的自信，也稍微有一点点陷入了比B哥还低级的纠结死循环，各种心绪缓缓积蓄，使人沉默，不过天天一起的基友室友一定不会这样认为，因为每天都会发明各种搞笑，笑得很猥琐……当然这个世界根本不可能有人能看出来……\n\n过去的半年，最大的愿望就是自己能变得天真，单纯，幼稚，想了各种办法，看了些道家的调息养生法，有些效果，但是还是没有坚持下去，天真真的很难，否则不会有那么多人为了一丝内心的宁静归隐山林。当然我是极其世俗的，天天因为湖人输球而着急，因为皇马穆帅不用卡卡而愤恨，吃饭打水的路上还要计算最短的路线\n\n好室友回来一天，真的很好。我从来实质上是不合群的，孤傲的本性也很愧对一些人，其实在意我对我好真心认为我好的人我怎么可能不知道，我身如浮萍，随风流落，一无所有，也无法给予什么，只能把这些感动记在心里。一年前这个时候天天思考什么是幸福，有了一点自己的答案：\n\n“何谓幸福？每个人每时每刻都是幸福的，所谓幸福就是能意识到自己正沉浸在幸福之中并且加倍珍惜今天！”\n\n深秋总是天蝎最喜欢的季节，其中的小波折，太跳跃性的感触以致于不想用文字来描述，不过天蝎总是带刺的，太容易最怕伤人伤己，当想玩真的时候，其实已经输了，更何况是这么貌似乐观开朗直接奇葩喜欢破釜沉舟的。我希望我的世界只有纯粹与简单，拒绝低俗凑合，我相信自己的主观能动性，再大的痛苦我也会笑着承受。\n\n总是高估自己，过于乐观。也许什么都不是，一个晚上单曲循环听歌，《给未来的自己》，涛哥的AKG K7确实音质不错。放低姿态，相信所有已经做过的选择永远都是对的，快意回首，拂心莫停。\n\t\n        \n        《给未来的自己》\n\n        站在狂风的天台一望无际\n        这一座孤独的城市\n        在天空与高楼交界的尽头\n        谁追寻空旷的自由\n\n        阳光覆满这一刻宁静的我\n        隔绝了喧嚣和冷漠\n        川流不息的人游荡在街头\n        谁能听见谁的寂寞\n\n        找一个人惺惺相惜\n        找一颗心心心相印\n        在这个宇宙我是独一无二\n        没人能取代\n        不管怎样 怎样都会受伤\n        伤了又怎样\n        至少我很坚强\n        我很坦荡\n\n        夜幕笼罩灿烂的一片灯海\n        多少人多少种无奈\n        在星光里遗忘昨天的伤害\n        一觉醒来还有期待\n\n        我不放弃爱的勇气\n        我不怀疑会有真心\n        我要握住一个最美的梦\n        给未来的自己\n\n        一天一天 \n\t\t 一天推翻一天\n        坚持的信仰\n        我会记住自己今天的模样\n\n        有一个人惺惺相惜\n        有一颗心心心相印\n        抛开过去我想认真去追寻\n        未来的自己\n\n        不管怎样 怎样都会受伤\n        伤了又怎样\n        至少我很坚强\n        我很坦荡\n\n        我不放弃爱的勇气\n        我不怀疑会有真心\n        我要握住一个最美的梦\n        给未来的自己\n\n        不管怎样 怎样都会受伤\n        伤了又怎样\n        至少我很坚强\n        我很坦荡\n\n        未来的你会懂我的疯狂\n \n\n","source":"_posts/2017-01-11-给未来的自己.md","raw":"---\ntitle: 给未来的自己\ntoc: false\ndate: 2012-11-11 19:16:06\ntags: 散文\ncategories: 单车岁月\n---\n                                                       —— 2012年秋于安大新区\n<!-- <iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=0 height=0 src=\"http://qqma.tingge123.com:823/mp3/2016-04-06/1459941003.mp3\"></iframe> -->\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=0 height=0 src=\"http://qqma.tingge123.com:823/mp3/2016-04-06/1459941083.mp3\"></iframe>\n--------------\n突然无名的落寞，对未来的迷茫，学业的压力，不自信的自信，也稍微有一点点陷入了比B哥还低级的纠结死循环，各种心绪缓缓积蓄，使人沉默，不过天天一起的基友室友一定不会这样认为，因为每天都会发明各种搞笑，笑得很猥琐……当然这个世界根本不可能有人能看出来……\n\n过去的半年，最大的愿望就是自己能变得天真，单纯，幼稚，想了各种办法，看了些道家的调息养生法，有些效果，但是还是没有坚持下去，天真真的很难，否则不会有那么多人为了一丝内心的宁静归隐山林。当然我是极其世俗的，天天因为湖人输球而着急，因为皇马穆帅不用卡卡而愤恨，吃饭打水的路上还要计算最短的路线\n\n好室友回来一天，真的很好。我从来实质上是不合群的，孤傲的本性也很愧对一些人，其实在意我对我好真心认为我好的人我怎么可能不知道，我身如浮萍，随风流落，一无所有，也无法给予什么，只能把这些感动记在心里。一年前这个时候天天思考什么是幸福，有了一点自己的答案：\n\n“何谓幸福？每个人每时每刻都是幸福的，所谓幸福就是能意识到自己正沉浸在幸福之中并且加倍珍惜今天！”\n\n深秋总是天蝎最喜欢的季节，其中的小波折，太跳跃性的感触以致于不想用文字来描述，不过天蝎总是带刺的，太容易最怕伤人伤己，当想玩真的时候，其实已经输了，更何况是这么貌似乐观开朗直接奇葩喜欢破釜沉舟的。我希望我的世界只有纯粹与简单，拒绝低俗凑合，我相信自己的主观能动性，再大的痛苦我也会笑着承受。\n\n总是高估自己，过于乐观。也许什么都不是，一个晚上单曲循环听歌，《给未来的自己》，涛哥的AKG K7确实音质不错。放低姿态，相信所有已经做过的选择永远都是对的，快意回首，拂心莫停。\n\t\n        \n        《给未来的自己》\n\n        站在狂风的天台一望无际\n        这一座孤独的城市\n        在天空与高楼交界的尽头\n        谁追寻空旷的自由\n\n        阳光覆满这一刻宁静的我\n        隔绝了喧嚣和冷漠\n        川流不息的人游荡在街头\n        谁能听见谁的寂寞\n\n        找一个人惺惺相惜\n        找一颗心心心相印\n        在这个宇宙我是独一无二\n        没人能取代\n        不管怎样 怎样都会受伤\n        伤了又怎样\n        至少我很坚强\n        我很坦荡\n\n        夜幕笼罩灿烂的一片灯海\n        多少人多少种无奈\n        在星光里遗忘昨天的伤害\n        一觉醒来还有期待\n\n        我不放弃爱的勇气\n        我不怀疑会有真心\n        我要握住一个最美的梦\n        给未来的自己\n\n        一天一天 \n\t\t 一天推翻一天\n        坚持的信仰\n        我会记住自己今天的模样\n\n        有一个人惺惺相惜\n        有一颗心心心相印\n        抛开过去我想认真去追寻\n        未来的自己\n\n        不管怎样 怎样都会受伤\n        伤了又怎样\n        至少我很坚强\n        我很坦荡\n\n        我不放弃爱的勇气\n        我不怀疑会有真心\n        我要握住一个最美的梦\n        给未来的自己\n\n        不管怎样 怎样都会受伤\n        伤了又怎样\n        至少我很坚强\n        我很坦荡\n\n        未来的你会懂我的疯狂\n \n\n","slug":"给未来的自己","published":1,"updated":"2017-01-11T12:00:00.467Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfwo0027psgu07m1p8co","content":"<pre><code>—— 2012年秋于安大新区\n</code></pre><!-- <iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=0 height=0 src=\"http://qqma.tingge123.com:823/mp3/2016-04-06/1459941003.mp3\"></iframe> -->\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"0\" height=\"0\" src=\"http://qqma.tingge123.com:823/mp3/2016-04-06/1459941083.mp3\"></iframe></h2><p>突然无名的落寞，对未来的迷茫，学业的压力，不自信的自信，也稍微有一点点陷入了比B哥还低级的纠结死循环，各种心绪缓缓积蓄，使人沉默，不过天天一起的基友室友一定不会这样认为，因为每天都会发明各种搞笑，笑得很猥琐……当然这个世界根本不可能有人能看出来……</p>\n<p>过去的半年，最大的愿望就是自己能变得天真，单纯，幼稚，想了各种办法，看了些道家的调息养生法，有些效果，但是还是没有坚持下去，天真真的很难，否则不会有那么多人为了一丝内心的宁静归隐山林。当然我是极其世俗的，天天因为湖人输球而着急，因为皇马穆帅不用卡卡而愤恨，吃饭打水的路上还要计算最短的路线</p>\n<p>好室友回来一天，真的很好。我从来实质上是不合群的，孤傲的本性也很愧对一些人，其实在意我对我好真心认为我好的人我怎么可能不知道，我身如浮萍，随风流落，一无所有，也无法给予什么，只能把这些感动记在心里。一年前这个时候天天思考什么是幸福，有了一点自己的答案：</p>\n<p>“何谓幸福？每个人每时每刻都是幸福的，所谓幸福就是能意识到自己正沉浸在幸福之中并且加倍珍惜今天！”</p>\n<p>深秋总是天蝎最喜欢的季节，其中的小波折，太跳跃性的感触以致于不想用文字来描述，不过天蝎总是带刺的，太容易最怕伤人伤己，当想玩真的时候，其实已经输了，更何况是这么貌似乐观开朗直接奇葩喜欢破釜沉舟的。我希望我的世界只有纯粹与简单，拒绝低俗凑合，我相信自己的主观能动性，再大的痛苦我也会笑着承受。</p>\n<p>总是高估自己，过于乐观。也许什么都不是，一个晚上单曲循环听歌，《给未来的自己》，涛哥的AKG K7确实音质不错。放低姿态，相信所有已经做过的选择永远都是对的，快意回首，拂心莫停。</p>\n<pre><code>《给未来的自己》\n\n站在狂风的天台一望无际\n这一座孤独的城市\n在天空与高楼交界的尽头\n谁追寻空旷的自由\n\n阳光覆满这一刻宁静的我\n隔绝了喧嚣和冷漠\n川流不息的人游荡在街头\n谁能听见谁的寂寞\n\n找一个人惺惺相惜\n找一颗心心心相印\n在这个宇宙我是独一无二\n没人能取代\n不管怎样 怎样都会受伤\n伤了又怎样\n至少我很坚强\n我很坦荡\n\n夜幕笼罩灿烂的一片灯海\n多少人多少种无奈\n在星光里遗忘昨天的伤害\n一觉醒来还有期待\n\n我不放弃爱的勇气\n我不怀疑会有真心\n我要握住一个最美的梦\n给未来的自己\n\n一天一天 \n 一天推翻一天\n坚持的信仰\n我会记住自己今天的模样\n\n有一个人惺惺相惜\n有一颗心心心相印\n抛开过去我想认真去追寻\n未来的自己\n\n不管怎样 怎样都会受伤\n伤了又怎样\n至少我很坚强\n我很坦荡\n\n我不放弃爱的勇气\n我不怀疑会有真心\n我要握住一个最美的梦\n给未来的自己\n\n不管怎样 怎样都会受伤\n伤了又怎样\n至少我很坚强\n我很坦荡\n\n未来的你会懂我的疯狂\n</code></pre>","excerpt":"","more":"<pre><code>—— 2012年秋于安大新区\n</code></pre><!-- <iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=0 height=0 src=\"http://qqma.tingge123.com:823/mp3/2016-04-06/1459941003.mp3\"></iframe> -->\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=0 height=0 src=\"http://qqma.tingge123.com:823/mp3/2016-04-06/1459941083.mp3\"></iframe></h2><p>突然无名的落寞，对未来的迷茫，学业的压力，不自信的自信，也稍微有一点点陷入了比B哥还低级的纠结死循环，各种心绪缓缓积蓄，使人沉默，不过天天一起的基友室友一定不会这样认为，因为每天都会发明各种搞笑，笑得很猥琐……当然这个世界根本不可能有人能看出来……</p>\n<p>过去的半年，最大的愿望就是自己能变得天真，单纯，幼稚，想了各种办法，看了些道家的调息养生法，有些效果，但是还是没有坚持下去，天真真的很难，否则不会有那么多人为了一丝内心的宁静归隐山林。当然我是极其世俗的，天天因为湖人输球而着急，因为皇马穆帅不用卡卡而愤恨，吃饭打水的路上还要计算最短的路线</p>\n<p>好室友回来一天，真的很好。我从来实质上是不合群的，孤傲的本性也很愧对一些人，其实在意我对我好真心认为我好的人我怎么可能不知道，我身如浮萍，随风流落，一无所有，也无法给予什么，只能把这些感动记在心里。一年前这个时候天天思考什么是幸福，有了一点自己的答案：</p>\n<p>“何谓幸福？每个人每时每刻都是幸福的，所谓幸福就是能意识到自己正沉浸在幸福之中并且加倍珍惜今天！”</p>\n<p>深秋总是天蝎最喜欢的季节，其中的小波折，太跳跃性的感触以致于不想用文字来描述，不过天蝎总是带刺的，太容易最怕伤人伤己，当想玩真的时候，其实已经输了，更何况是这么貌似乐观开朗直接奇葩喜欢破釜沉舟的。我希望我的世界只有纯粹与简单，拒绝低俗凑合，我相信自己的主观能动性，再大的痛苦我也会笑着承受。</p>\n<p>总是高估自己，过于乐观。也许什么都不是，一个晚上单曲循环听歌，《给未来的自己》，涛哥的AKG K7确实音质不错。放低姿态，相信所有已经做过的选择永远都是对的，快意回首，拂心莫停。</p>\n<pre><code>《给未来的自己》\n\n站在狂风的天台一望无际\n这一座孤独的城市\n在天空与高楼交界的尽头\n谁追寻空旷的自由\n\n阳光覆满这一刻宁静的我\n隔绝了喧嚣和冷漠\n川流不息的人游荡在街头\n谁能听见谁的寂寞\n\n找一个人惺惺相惜\n找一颗心心心相印\n在这个宇宙我是独一无二\n没人能取代\n不管怎样 怎样都会受伤\n伤了又怎样\n至少我很坚强\n我很坦荡\n\n夜幕笼罩灿烂的一片灯海\n多少人多少种无奈\n在星光里遗忘昨天的伤害\n一觉醒来还有期待\n\n我不放弃爱的勇气\n我不怀疑会有真心\n我要握住一个最美的梦\n给未来的自己\n\n一天一天 \n 一天推翻一天\n坚持的信仰\n我会记住自己今天的模样\n\n有一个人惺惺相惜\n有一颗心心心相印\n抛开过去我想认真去追寻\n未来的自己\n\n不管怎样 怎样都会受伤\n伤了又怎样\n至少我很坚强\n我很坦荡\n\n我不放弃爱的勇气\n我不怀疑会有真心\n我要握住一个最美的梦\n给未来的自己\n\n不管怎样 怎样都会受伤\n伤了又怎样\n至少我很坚强\n我很坦荡\n\n未来的你会懂我的疯狂\n</code></pre>"},{"title":"天鹅湖之暮","toc":true,"date":"2010-01-06T11:24:45.000Z","_content":"\n#### 注： 原名：《日记》\n\n                                            ——2010.1.6日于安大新区\n\n--------------\n\n本来想去翡翠湖的，但已经是晚上11点了，太晚了，就只好去天鹅湖转转。\n\n不知为什么，今天又有了一个人转的欲望，这两天感觉总是不太好，不知道是作息不好还是昨天去市中心晕车的缘故。\n\n今晚一下上了三个多小时的自习，虽然中途开了一些小差，但总体来说效果不错，相比之下，前几天太颓废了。\n\n因为昨天刚买了mp3，所以今天是插了耳机的，显得格外安静，决定抛开所有的纠结，远离所有的虚伪，放下所有突兀的事，什么也不想，就在天鹅胡边轻轻地转转。\n\n也许是因为我太安静了吧，今晚那对黑天鹅也没有出来，它们正在小房子里温馨地睡觉呢！决定今天不去打扰它俩了。湖面很平静，静得像一潭死水，本来就是一潭死水呵！总是沉默着，但不一定会永远沉默下去，也许可以忍受得了天鹅的叽叽喳喳，但很难忍受别人在后面人垃圾，谁这么没有道德，把垃圾扔进去的，是谁？湖面除了垃圾外其它什么都没有，但绝不显得空荡，湖面的草早已枯黄了，踩在上面软软的。记得夏天的时候，这儿还是有很多人的啊，现在只有我一个了，一个人更好，两个人在一起有肯能分开，一个人在一起就不会分开了，一切是这么得安静，让人流连。\n\n转到一半的时候，几棵树突兀地垂在我面前，树枝像网一样，交结在一起，中间还有结，又让我想起了恶心的东西，现在一想，原来是垂柳，呵呵！原来夏天枝叶繁茂，相互拥簇，亲密相间的它们，当脱去繁华炫目的外衣后，也是一张张复杂的网呵！\n\n湖边的路灯，闪着诱人的黄光，在湖面的倒影，一下伸得很长很长，难道幻想化成利剑，戳破着无尽的黑暗吗？路灯下有两个人正站着接吻，他们应该是情侣吧,好像不欢迎我这个不速之客，我可不想打扰他们，戴上帽子独自走开，说真的，我不喜欢接吻什么的（至少现在不喜欢），但我真的想要有一个，一个这样的朋友（女朋友），平时在一起学习啊，玩啊，总之在一起很愉快的，然后在这个时候，和我一起在湖边转，很少说话，心里各自想着各自的事情，但一点也不觉得尴尬，最好能拉着她软软的手，或一起靠在软软的草地上，或轻轻地抱着她……\n\n就这样幻想着，这样轻轻地飘着，上完晚自习回来的人好奇地偷偷看着我，不知他们是怎样想的，我把帽子拉紧，让他们把我当怪物算了，反正他们又不认识我，反之我还有点沾沾自喜。唉，可怜的浸在现实的人啊，又怎么知道我的世界的美妙。这个感觉真好，虽然我又饿又困，但我还是不由自主地转下去，真不想回到那个地方，也许现在那对面正在“Fireinthehole”吧，也许现在他们三个又在说些无聊的废话吧，呵呵，还有人在看小说吧！\n\n不小心在湖面上看到了我的倒影，哈哈！可真滑稽，像个巫师，真像“冰封王座”里面的不死族寺僧，寺僧最后是可以变隐形的，如果我可以隐形就好了，就没人看见我了，如果给我学两样技能，我是先学“白眼”呢还是先学“隐身术”呢……\n\n啊！一天过得可真快，早上的我是寝室颓废、上课冷陌的我；下午的我是在对面寝室蹦来跳去、活泼但不可爱的我；晚上的我是认真上自习的我；而现在，又是沉浸在幻想中的我。连我都糊涂了，到底哪一个才是真正的我？\n\n路上一直听着歌，其实也只听了四五首，除了西城的《my love》和肖邦的《什么什么大调交响曲》外，今天又发现了一首很好听的歌：仙剑的《一直很安静》，感觉和这首歌的调子有种共鸣。周杰伦的歌我这个时候是不听的，太扎耳，此情此景此歌，让我不由自主放慢了脚步。\n\n不得不回去了，快要熄灯了，我饿了，但食堂的门早关了，超市也正好关门了，一切都安静了下来，只有路灯依然亮着，路灯是看到得最多的，也是最沉默的，当它看到我静静的身影时，会想些什么呢？\n\n……\n\n心情好多了，寝室门虚掩着，灯熄了，他们应该睡了吧，我调整了一下脸部肌肉，走进了现实……好久没写日记了……\n","source":"_posts/2017-01-11-天鹅湖之暮.md","raw":"---\ntitle: 天鹅湖之暮\ntoc: true\ndate: 2010-01-06 19:24:45\ntags: 散文\ncategories: 单车岁月\n---\n\n#### 注： 原名：《日记》\n\n                                            ——2010.1.6日于安大新区\n\n--------------\n\n本来想去翡翠湖的，但已经是晚上11点了，太晚了，就只好去天鹅湖转转。\n\n不知为什么，今天又有了一个人转的欲望，这两天感觉总是不太好，不知道是作息不好还是昨天去市中心晕车的缘故。\n\n今晚一下上了三个多小时的自习，虽然中途开了一些小差，但总体来说效果不错，相比之下，前几天太颓废了。\n\n因为昨天刚买了mp3，所以今天是插了耳机的，显得格外安静，决定抛开所有的纠结，远离所有的虚伪，放下所有突兀的事，什么也不想，就在天鹅胡边轻轻地转转。\n\n也许是因为我太安静了吧，今晚那对黑天鹅也没有出来，它们正在小房子里温馨地睡觉呢！决定今天不去打扰它俩了。湖面很平静，静得像一潭死水，本来就是一潭死水呵！总是沉默着，但不一定会永远沉默下去，也许可以忍受得了天鹅的叽叽喳喳，但很难忍受别人在后面人垃圾，谁这么没有道德，把垃圾扔进去的，是谁？湖面除了垃圾外其它什么都没有，但绝不显得空荡，湖面的草早已枯黄了，踩在上面软软的。记得夏天的时候，这儿还是有很多人的啊，现在只有我一个了，一个人更好，两个人在一起有肯能分开，一个人在一起就不会分开了，一切是这么得安静，让人流连。\n\n转到一半的时候，几棵树突兀地垂在我面前，树枝像网一样，交结在一起，中间还有结，又让我想起了恶心的东西，现在一想，原来是垂柳，呵呵！原来夏天枝叶繁茂，相互拥簇，亲密相间的它们，当脱去繁华炫目的外衣后，也是一张张复杂的网呵！\n\n湖边的路灯，闪着诱人的黄光，在湖面的倒影，一下伸得很长很长，难道幻想化成利剑，戳破着无尽的黑暗吗？路灯下有两个人正站着接吻，他们应该是情侣吧,好像不欢迎我这个不速之客，我可不想打扰他们，戴上帽子独自走开，说真的，我不喜欢接吻什么的（至少现在不喜欢），但我真的想要有一个，一个这样的朋友（女朋友），平时在一起学习啊，玩啊，总之在一起很愉快的，然后在这个时候，和我一起在湖边转，很少说话，心里各自想着各自的事情，但一点也不觉得尴尬，最好能拉着她软软的手，或一起靠在软软的草地上，或轻轻地抱着她……\n\n就这样幻想着，这样轻轻地飘着，上完晚自习回来的人好奇地偷偷看着我，不知他们是怎样想的，我把帽子拉紧，让他们把我当怪物算了，反正他们又不认识我，反之我还有点沾沾自喜。唉，可怜的浸在现实的人啊，又怎么知道我的世界的美妙。这个感觉真好，虽然我又饿又困，但我还是不由自主地转下去，真不想回到那个地方，也许现在那对面正在“Fireinthehole”吧，也许现在他们三个又在说些无聊的废话吧，呵呵，还有人在看小说吧！\n\n不小心在湖面上看到了我的倒影，哈哈！可真滑稽，像个巫师，真像“冰封王座”里面的不死族寺僧，寺僧最后是可以变隐形的，如果我可以隐形就好了，就没人看见我了，如果给我学两样技能，我是先学“白眼”呢还是先学“隐身术”呢……\n\n啊！一天过得可真快，早上的我是寝室颓废、上课冷陌的我；下午的我是在对面寝室蹦来跳去、活泼但不可爱的我；晚上的我是认真上自习的我；而现在，又是沉浸在幻想中的我。连我都糊涂了，到底哪一个才是真正的我？\n\n路上一直听着歌，其实也只听了四五首，除了西城的《my love》和肖邦的《什么什么大调交响曲》外，今天又发现了一首很好听的歌：仙剑的《一直很安静》，感觉和这首歌的调子有种共鸣。周杰伦的歌我这个时候是不听的，太扎耳，此情此景此歌，让我不由自主放慢了脚步。\n\n不得不回去了，快要熄灯了，我饿了，但食堂的门早关了，超市也正好关门了，一切都安静了下来，只有路灯依然亮着，路灯是看到得最多的，也是最沉默的，当它看到我静静的身影时，会想些什么呢？\n\n……\n\n心情好多了，寝室门虚掩着，灯熄了，他们应该睡了吧，我调整了一下脸部肌肉，走进了现实……好久没写日记了……\n","slug":"天鹅湖之暮","published":1,"updated":"2017-01-11T11:27:03.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfwr002bpsgud51k3xe4","content":"<h4 id=\"注：-原名：《日记》\"><a href=\"#注：-原名：《日记》\" class=\"headerlink\" title=\"注： 原名：《日记》\"></a>注： 原名：《日记》</h4><pre><code>——2010.1.6日于安大新区\n</code></pre><hr>\n<p>本来想去翡翠湖的，但已经是晚上11点了，太晚了，就只好去天鹅湖转转。</p>\n<p>不知为什么，今天又有了一个人转的欲望，这两天感觉总是不太好，不知道是作息不好还是昨天去市中心晕车的缘故。</p>\n<p>今晚一下上了三个多小时的自习，虽然中途开了一些小差，但总体来说效果不错，相比之下，前几天太颓废了。</p>\n<p>因为昨天刚买了mp3，所以今天是插了耳机的，显得格外安静，决定抛开所有的纠结，远离所有的虚伪，放下所有突兀的事，什么也不想，就在天鹅胡边轻轻地转转。</p>\n<p>也许是因为我太安静了吧，今晚那对黑天鹅也没有出来，它们正在小房子里温馨地睡觉呢！决定今天不去打扰它俩了。湖面很平静，静得像一潭死水，本来就是一潭死水呵！总是沉默着，但不一定会永远沉默下去，也许可以忍受得了天鹅的叽叽喳喳，但很难忍受别人在后面人垃圾，谁这么没有道德，把垃圾扔进去的，是谁？湖面除了垃圾外其它什么都没有，但绝不显得空荡，湖面的草早已枯黄了，踩在上面软软的。记得夏天的时候，这儿还是有很多人的啊，现在只有我一个了，一个人更好，两个人在一起有肯能分开，一个人在一起就不会分开了，一切是这么得安静，让人流连。</p>\n<p>转到一半的时候，几棵树突兀地垂在我面前，树枝像网一样，交结在一起，中间还有结，又让我想起了恶心的东西，现在一想，原来是垂柳，呵呵！原来夏天枝叶繁茂，相互拥簇，亲密相间的它们，当脱去繁华炫目的外衣后，也是一张张复杂的网呵！</p>\n<p>湖边的路灯，闪着诱人的黄光，在湖面的倒影，一下伸得很长很长，难道幻想化成利剑，戳破着无尽的黑暗吗？路灯下有两个人正站着接吻，他们应该是情侣吧,好像不欢迎我这个不速之客，我可不想打扰他们，戴上帽子独自走开，说真的，我不喜欢接吻什么的（至少现在不喜欢），但我真的想要有一个，一个这样的朋友（女朋友），平时在一起学习啊，玩啊，总之在一起很愉快的，然后在这个时候，和我一起在湖边转，很少说话，心里各自想着各自的事情，但一点也不觉得尴尬，最好能拉着她软软的手，或一起靠在软软的草地上，或轻轻地抱着她……</p>\n<p>就这样幻想着，这样轻轻地飘着，上完晚自习回来的人好奇地偷偷看着我，不知他们是怎样想的，我把帽子拉紧，让他们把我当怪物算了，反正他们又不认识我，反之我还有点沾沾自喜。唉，可怜的浸在现实的人啊，又怎么知道我的世界的美妙。这个感觉真好，虽然我又饿又困，但我还是不由自主地转下去，真不想回到那个地方，也许现在那对面正在“Fireinthehole”吧，也许现在他们三个又在说些无聊的废话吧，呵呵，还有人在看小说吧！</p>\n<p>不小心在湖面上看到了我的倒影，哈哈！可真滑稽，像个巫师，真像“冰封王座”里面的不死族寺僧，寺僧最后是可以变隐形的，如果我可以隐形就好了，就没人看见我了，如果给我学两样技能，我是先学“白眼”呢还是先学“隐身术”呢……</p>\n<p>啊！一天过得可真快，早上的我是寝室颓废、上课冷陌的我；下午的我是在对面寝室蹦来跳去、活泼但不可爱的我；晚上的我是认真上自习的我；而现在，又是沉浸在幻想中的我。连我都糊涂了，到底哪一个才是真正的我？</p>\n<p>路上一直听着歌，其实也只听了四五首，除了西城的《my love》和肖邦的《什么什么大调交响曲》外，今天又发现了一首很好听的歌：仙剑的《一直很安静》，感觉和这首歌的调子有种共鸣。周杰伦的歌我这个时候是不听的，太扎耳，此情此景此歌，让我不由自主放慢了脚步。</p>\n<p>不得不回去了，快要熄灯了，我饿了，但食堂的门早关了，超市也正好关门了，一切都安静了下来，只有路灯依然亮着，路灯是看到得最多的，也是最沉默的，当它看到我静静的身影时，会想些什么呢？</p>\n<p>……</p>\n<p>心情好多了，寝室门虚掩着，灯熄了，他们应该睡了吧，我调整了一下脸部肌肉，走进了现实……好久没写日记了……</p>\n","excerpt":"","more":"<h4 id=\"注：-原名：《日记》\"><a href=\"#注：-原名：《日记》\" class=\"headerlink\" title=\"注： 原名：《日记》\"></a>注： 原名：《日记》</h4><pre><code>——2010.1.6日于安大新区\n</code></pre><hr>\n<p>本来想去翡翠湖的，但已经是晚上11点了，太晚了，就只好去天鹅湖转转。</p>\n<p>不知为什么，今天又有了一个人转的欲望，这两天感觉总是不太好，不知道是作息不好还是昨天去市中心晕车的缘故。</p>\n<p>今晚一下上了三个多小时的自习，虽然中途开了一些小差，但总体来说效果不错，相比之下，前几天太颓废了。</p>\n<p>因为昨天刚买了mp3，所以今天是插了耳机的，显得格外安静，决定抛开所有的纠结，远离所有的虚伪，放下所有突兀的事，什么也不想，就在天鹅胡边轻轻地转转。</p>\n<p>也许是因为我太安静了吧，今晚那对黑天鹅也没有出来，它们正在小房子里温馨地睡觉呢！决定今天不去打扰它俩了。湖面很平静，静得像一潭死水，本来就是一潭死水呵！总是沉默着，但不一定会永远沉默下去，也许可以忍受得了天鹅的叽叽喳喳，但很难忍受别人在后面人垃圾，谁这么没有道德，把垃圾扔进去的，是谁？湖面除了垃圾外其它什么都没有，但绝不显得空荡，湖面的草早已枯黄了，踩在上面软软的。记得夏天的时候，这儿还是有很多人的啊，现在只有我一个了，一个人更好，两个人在一起有肯能分开，一个人在一起就不会分开了，一切是这么得安静，让人流连。</p>\n<p>转到一半的时候，几棵树突兀地垂在我面前，树枝像网一样，交结在一起，中间还有结，又让我想起了恶心的东西，现在一想，原来是垂柳，呵呵！原来夏天枝叶繁茂，相互拥簇，亲密相间的它们，当脱去繁华炫目的外衣后，也是一张张复杂的网呵！</p>\n<p>湖边的路灯，闪着诱人的黄光，在湖面的倒影，一下伸得很长很长，难道幻想化成利剑，戳破着无尽的黑暗吗？路灯下有两个人正站着接吻，他们应该是情侣吧,好像不欢迎我这个不速之客，我可不想打扰他们，戴上帽子独自走开，说真的，我不喜欢接吻什么的（至少现在不喜欢），但我真的想要有一个，一个这样的朋友（女朋友），平时在一起学习啊，玩啊，总之在一起很愉快的，然后在这个时候，和我一起在湖边转，很少说话，心里各自想着各自的事情，但一点也不觉得尴尬，最好能拉着她软软的手，或一起靠在软软的草地上，或轻轻地抱着她……</p>\n<p>就这样幻想着，这样轻轻地飘着，上完晚自习回来的人好奇地偷偷看着我，不知他们是怎样想的，我把帽子拉紧，让他们把我当怪物算了，反正他们又不认识我，反之我还有点沾沾自喜。唉，可怜的浸在现实的人啊，又怎么知道我的世界的美妙。这个感觉真好，虽然我又饿又困，但我还是不由自主地转下去，真不想回到那个地方，也许现在那对面正在“Fireinthehole”吧，也许现在他们三个又在说些无聊的废话吧，呵呵，还有人在看小说吧！</p>\n<p>不小心在湖面上看到了我的倒影，哈哈！可真滑稽，像个巫师，真像“冰封王座”里面的不死族寺僧，寺僧最后是可以变隐形的，如果我可以隐形就好了，就没人看见我了，如果给我学两样技能，我是先学“白眼”呢还是先学“隐身术”呢……</p>\n<p>啊！一天过得可真快，早上的我是寝室颓废、上课冷陌的我；下午的我是在对面寝室蹦来跳去、活泼但不可爱的我；晚上的我是认真上自习的我；而现在，又是沉浸在幻想中的我。连我都糊涂了，到底哪一个才是真正的我？</p>\n<p>路上一直听着歌，其实也只听了四五首，除了西城的《my love》和肖邦的《什么什么大调交响曲》外，今天又发现了一首很好听的歌：仙剑的《一直很安静》，感觉和这首歌的调子有种共鸣。周杰伦的歌我这个时候是不听的，太扎耳，此情此景此歌，让我不由自主放慢了脚步。</p>\n<p>不得不回去了，快要熄灯了，我饿了，但食堂的门早关了，超市也正好关门了，一切都安静了下来，只有路灯依然亮着，路灯是看到得最多的，也是最沉默的，当它看到我静静的身影时，会想些什么呢？</p>\n<p>……</p>\n<p>心情好多了，寝室门虚掩着，灯熄了，他们应该睡了吧，我调整了一下脸部肌肉，走进了现实……好久没写日记了……</p>\n"},{"title":"大数据团队scala代码规范","toc":false,"date":"2017-01-09T08:02:19.000Z","_content":"\n## 前言\n\n### 一、 scala代码规范\n\nScala 是一种强大到令人难以置信的多范式编程语言。目前我们团队有很多项目需要使用scala语言进行编程，尤其是Spark相关开发项目，为了能够统一scala相关开发，本人基于Spark 贡献者及 [Databricks](http://databricks.com/) 工程团队总结出了以下指南，本人增加了单元测试的规范，希望对大家的开发有帮助。当然，这个指南并非绝对，根据我们团队需求与实际经验，持续更新。\n\n### 二、 选择此规范的理由\n很早就有大家统一编程规范的想法，网上也有一些关于编程规范的文档供参考。最终选择了以 Databricks 公司的编程规范为模板制作。理由如下：\n\n- 目前公司使用scala语言主要用在Spark的开发，而这份指南是Spark 贡献者及 Databricks 工程团队一起工作时总结出来的；\n- 跟我们之前写得代码，以及IDEA等编译器自动格式化相差不大；\n- 这份指南经过多次的修改和总结，经过了实践的检验；\n- 这份指南相比较而言简明概要，易于理解，遵循 Java 的地方就没有赘述，比较解耦，适用于根据公司代码体系来修改；\n\n## <a name='TOC'>目录</a>\n\n  0. [文档历史](#history)\n  1. [语法风格](#syntactic)\n    - [命名约定](#naming)\n    - [变量命名约定](#variable-naming)\n    - [一行长度](#linelength)\n    - [30 法则](#rule_of_30)\n    - [空格与缩进](#indent)\n    - [空行](#blanklines)\n    - [括号](#parentheses)\n    - [大括号](#curly)\n    - [长整型字面量](#long_literal)\n    - [文档风格](#doc)\n    - [类内秩序](#ordering_class)\n    - [Imports](#imports)\n    - [模式匹配](#pattern-matching)\n    - [中缀方法](#infix)\n    - [匿名方法](#anonymous)\n  2. [Scala 语言特性](#lang)\n    - [样例类与不可变性](#case_class_immutability)\n    - [apply 方法](#apply_method)\n    - [override 修饰符](#override_modifier)\n    - [解构绑定](#destruct_bind)\n    - [按名称传参](#call_by_name)\n    - [多参数列表](#multi-param-list)\n    - [符号方法 (运算符重载)](#symbolic_methods)\n    - [类型推导](#type_inference)\n    - [Return 语句](#return)\n    - [递归及尾递归](#recursion)\n    - [Implicits](#implicits)\n    - [异常处理 (Try 还是 try)](#exception)\n    - [Options](#option)\n    - [单子链接](#chaining)\n  3. [并发](#concurrency)\n    - [Scala concurrent.Map](#concurrency-scala-collection)\n    - [显式同步 vs 并发集合](#concurrency-sync-vs-map)\n    - [显式同步 vs 原子变量 vs @volatile](#concurrency-sync-vs-atomic)\n    - [私有字段](#concurrency-private-this)\n    - [隔离](#concurrency-isolation)\n  4. [性能](#perf)\n    - [Microbenchmarks](#perf-microbenchmarks)\n    - [Traversal 与 zipWithIndex](#perf-whileloops)\n    - [Option 与 null](#perf-option)\n    - [Scala 集合库](#perf-collection)\n    - [private[this]](#perf-private)\n  5. [与 Java 的互操作性](#java)\n    - [Scala 中缺失的 Java 特性](#java-missing-features)\n    - [Traits 与抽象类](#java-traits)\n    - [类型别名](#java-type-alias)\n    - [默认参数值](#java-default-param-values)\n    - [多参数列表](#java-multi-param-list)\n    - [可变参数](#java-varargs)\n    - [Implicits](#java-implicits)\n    - [伴生对象, 静态方法与字段](#java-companion-object)\n  6. [其它](#misc)\n    - [优先使用 nanoTime 而非 currentTimeMillis](#misc_currentTimeMillis_vs_nanoTime)\n    - [优先使用 URI 而非 URL](#misc_uri_url)\n  7. [单元测试](#unit-test)\n\t- [单元测试框架](#unit-test-framework)\n\t- [单元测试风格](#unit-test-style)\n\n## <a name='history'>文档历史</a>\n\n- 2017-01-10: 最初版本。[Databricks版本](https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md)\n- 2017-01-11: 增加 [单元测试](#unit-test) 一节。\n\n## <a name='syntactic'>语法风格</a>\n\n### <a name='naming'>命名约定</a>\n\n我们主要遵循 Java 和 Scala 的标准命名约定。\n\n- 类，trait, 对象应该遵循 Java 中类的命名约定，即 PascalCase 风格。\n\n  ```scala\n  class ClusterManager\n\n  trait Expression\n  ```\n\n- 包名应该遵循 Java 中包名的命名约定，即使用全小写的 ASCII 字母。\n\n  ```scala\n  package com.databricks.resourcemanager\n  ```\n\n- 方法/函数应当使用驼峰式风格命名。\n\n- 常量命名使用全大写字母，并将它们放在伴生对象中。\n\n  ```scala\n  object Configuration {\n    val DEFAULT_PORT = 10000\n  }\n  ```\n\n- 枚举命名与类命名一致，使用 PascalCase 风格。\n\n- 注解也应遵循 Java 中的约定，即使用 PascalCase 风格。注意，这一点与 Scala 的官方指南不同。\n\n  ```scala\n  final class MyAnnotation extends StaticAnnotation\n  ```\n\n### <a name='variable-naming'>变量命名约定</a>\n\n- 变量命名应当遵循驼峰式命名方法，并且变量名应当是不言而喻的，即变量名可以直观地反应它的涵义。\n  ```scala\n  val serverPort = 1000\n  val clientPort = 2000\n  ```\n\n- 可以在小段的局部代码中使用单字符的变量名，比如在小段的循环体中（例如 10 行以内的代码），“i” 常常被用作循环索引。然而，即使在小段的代码中，也不要使用 “l” （Larry 中的 l）作为标识符，因为它看起来和 “1”，“|”，“I” 很像，难以区分，容易搞错。\n\n\n### <a name='linelength'>一行长度</a>\n\n- 一行长度的上限是 100 个字符。\n- 唯一的例外是 import 语句和 URL (即便如此，也尽量将它们保持在 100 个字符以下)。\n\n\n### <a name='rule_of_30'>30 法则</a>\n\n「如果一个元素包含的子元素超过 30 个，那么极有可能出现了严重的问题」 - [Refactoring in Large Software Projects](http://www.amazon.com/Refactoring-Large-Software-Projects-Restructurings/dp/0470858923)。\n\n一般来说:\n\n- 一个方法包含的代码行数不宜超过 30 行。\n- 一个类包含的方法数量不宜超过 30 个。\n\n\n### <a name='indent'>空格与缩进</a>\n\n- 一般情况下，使用两个空格的缩进。\n\n  ```scala\n  if (true) {\n    println(\"Wow!\")\n  }\n  ```\n\n- 对于方法声明，如果一行无法容纳下所有的参数，那么使用 4 个空格来缩进它们。返回类型可以与最后一个参数在同一行，也可以放在下一行，使用两个空格缩进。\n\n  ```scala\n  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]](\n      path: String,\n      fClass: Class[F],\n      kClass: Class[K],\n      vClass: Class[V],\n      conf: Configuration = hadoopConfiguration): RDD[(K, V)] = {\n    // method body\n  }\n\n  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]](\n      path: String,\n      fClass: Class[F],\n      kClass: Class[K],\n      vClass: Class[V],\n      conf: Configuration = hadoopConfiguration)\n    : RDD[(K, V)] = {\n    // method body\n  }\n  ```\n\n- 如果一行无法容纳下类头（即 extends 后面那部分），则把它们放到新的一行，用两个空格缩进，然后在类内空一行再开始函数或字段的定义（或是包的导入）。\n\n  ```scala\n  class Foo(\n      val param1: String,  // 4 space indent for parameters\n      val param2: String,\n      val param3: Array[Byte])\n    extends FooInterface  // 2 space here\n    with Logging {\n\n    def firstMethod(): Unit = { ... }  // blank line above\n  }\n  ```\n\n- 不要使用垂直对齐。它使你的注意力放在代码的错误部分并增大了后人修改代码的难度。\n\n  ```scala\n  // Don't align vertically\n  val plus     = \"+\"\n  val minus    = \"-\"\n  val multiply = \"*\"\n\n  // Do the following\n  val plus = \"+\"\n  val minus = \"-\"\n  val multiply = \"*\"\n  ```\n\n\n### <a name='blanklines'>空行</a>\n\n- 一个空行可以出现在：\n  - 连续的类成员或初始化器（initializers）之间：字段，构造函数，方法，嵌套类，静态初始化器及实例初始化器。\n    - 例外：连续的两个字段之间的空行是可选的（前提是它们之间没有其它代码），这一类空行主要为这些字段做逻辑上的分组。\n  - 在方法体内，根据需要，使用空行来为语句创建逻辑上的分组。\n  - 在类的第一个成员之前或最后一个成员之后，空行都是可选的（既不鼓励也不阻止）。\n- 使用一个或两个空行来分隔不同类的定义。\n- 不鼓励使用过多的空行。\n\n\n### <a name='parentheses'>括号</a>\n\n- 方法声明应该加括号（即使没有参数列表），除非它们是没有副作用（状态改变，IO 操作都认为是有副作用的）的访问器（accessor）。\n\n  ```scala\n  class Job {\n    // Wrong: killJob changes state. Should have ().\n    def killJob: Unit\n\n    // Correct:\n    def killJob(): Unit\n  }\n  ```\n\n- 函数调用应该与函数声明在形式上保持一致，也就是说，如果一个方法声明时带了括号，那调用时也要把括号带上。注意这不仅仅是语法层面的人为约定，当返回对象中定义了 `apply` 方法时，这一点还会影响正确性。\n\n  ```scala\n  class Foo {\n    def apply(args: String*): Int\n  }\n\n  class Bar {\n    def foo: Foo\n  }\n\n  new Bar().foo  // This returns a Foo\n  new Bar().foo()  // This returns an Int!\n  ```\n\n\n### <a name='curly'>大括号</a>\n\n即使条件语句或循环语句只有一行时，也请使用大括号。唯一的例外是，当你把 if/else 作为一个单行的三元操作符来使用并且没有副作用时，这时你可以不加大括号。\n\n```scala\n// Correct:\nif (true) {\n  println(\"Wow!\")\n}\n\n// Correct:\nif (true) statement1 else statement2\n\n// Correct:\ntry {\n  foo()\n} catch {\n  ...\n}\n\n// Wrong:\nif (true)\n  println(\"Wow!\")\n\n// Wrong:\ntry foo() catch {\n  ...\n}\n```\n\n\n### <a name='long_literal'>长整型字面量</a>\n\n长整型字面量使用大写的 `L` 作为后缀，不要使用小写，因为它和数字 `1` 长得很像，常常难以区分。\n\n```scala\nval longValue = 5432L  // Do this\n\nval longValue = 5432l  // Do NOT do this\n```\n\n\n### <a name='doc'>文档风格</a>\n\n使用 Java Doc 风格，而非 Scala Doc 风格。\n\n```scala\n/** This is a correct one-liner, short description. */\n\n/**\n * This is correct multi-line JavaDoc comment. And\n * this is my second line, and if I keep typing, this would be\n * my third line.\n */\n\n/** In Spark, we don't use the ScalaDoc style so this\n  * is not correct.\n  */\n```\n\n\n### <a name='ordering_class'>类内秩序</a>\n\n如果一个类很长，包含许多的方法，那么在逻辑上把它们分成不同的部分并加上注释头，以此组织它们。\n\n```scala\nclass DataFrame {\n\n  ///////////////////////////////////////////////////////////////////////////\n  // DataFrame operations\n  ///////////////////////////////////////////////////////////////////////////\n\n  ...\n\n  ///////////////////////////////////////////////////////////////////////////\n  // RDD operations\n  ///////////////////////////////////////////////////////////////////////////\n\n  ...\n}\n```\n\n当然，强烈不建议把一个类写得这么长，一般只有在构建某些公共 API 时才允许这么做。\n\n\n### <a name='imports'>Imports</a>\n\n- __导入时避免使用通配符__, 除非你需要导入超过 6 个实体或者隐式方法。通配符导入会使代码在面对外部变化时不够健壮。\n- 始终使用绝对路径来导入包 (如：`scala.util.Random`) ，而不是相对路径 (如：`util.Random`)。\n- 此外，导入语句按照以下顺序排序：\n  * `java.*` 和 `javax.*`\n  * `scala.*`\n  * 第三方库 (`org.*`, `com.*`, 等)\n  * 项目中的类 (对于 Spark 项目，即 `com.databricks.*` 或 `org.apache.spark`)\n- 在每一组导入语句内，按照字母序进行排序。\n- 你可以使用 IntelliJ 的「import organizer」来自动处理，请使用以下配置：\n\n  ```\n  java\n  javax\n  _______ blank line _______\n  scala\n  _______ blank line _______\n  all other imports\n  _______ blank line _______\n  com.databricks  // or org.apache.spark if you are working on spark\n  ```\n\n\n### <a name='pattern-matching'>模式匹配</a>\n\n- 如果整个方法就是一个模式匹配表达式，可能的话，可以把 match 关键词与方法声明放在同一行，以此减少一级缩进。\n\n  ```scala\n  def test(msg: Message): Unit = msg match {\n    case ...\n  }\n  ```\n\n- 当以闭包形式调用一个函数时，如果只有一个 case 语句，那么把 case 语句与函数调用放在同一行。\n\n  ```scala\n  list.zipWithIndex.map { case (elem, i) =>\n    // ...\n  }\n  ```\n  如果有多个 case 语句，把它们缩进并且包起来。\n\n  ```scala\n  list.map {\n    case a: Foo =>  ...\n    case b: Bar =>  ...\n  }\n  ```\n\n- 如果唯一的目的就是想匹配某个对象的类型，那么不要展开所有的参数来做模式匹配，这样会使得重构变得更加困难，代码更容易出错。\n\n```scala\ncase class Pokemon(name: String, weight: Int, hp: Int, attack: Int, defense: Int)\ncase class Human(name: String, hp: Int)\n\n// 不要像下面那样做，因为\n// 1. 当 pokemon 加入一个新的字段，我们需要改变下面的模式匹配代码\n// 2. 非常容易发生误匹配，尤其是当所有字段的类型都一样的时候\ntargets.foreach {\n  case target @ Pokemon(_, _, hp, _, defense) =>\n    val loss = sys.min(0, myAttack - defense)\n    target.copy(hp = hp - loss)\n  case target @ Human(_, hp) =>\n    target.copy(hp = hp - myAttack)\n}\n\n// 像下面这样做就好多了:\ntargets.foreach {\n  case target: Pokemon =>\n    val loss = sys.min(0, myAttack - target.defense)\n    target.copy(hp = target.hp - loss)\n  case target: Human =>\n    target.copy(hp = target.hp - myAttack)\n}\n```\n\n\n### <a name='infix'>中缀方法</a>\n\n__避免中缀表示法__，除非是符号方法（即运算符重载）。\n\n```scala\n// Correct\nlist.map(func)\nstring.contains(\"foo\")\n\n// Wrong\nlist map (func)\nstring contains \"foo\"\n\n// 重载的运算符应该以中缀形式调用\narrayBuffer += elem\n```\n\n### <a name='anonymous'>匿名方法</a>\n\n对于匿名方法，__避免使用过多的小括号和花括号__。\n\n```scala\n// Correct\nlist.map { item =>\n  ...\n}\n\n// Correct\nlist.map(item => ...)\n\n// Wrong\nlist.map(item => {\n  ...\n})\n\n// Wrong\nlist.map { item => {\n  ...\n}}\n\n// Wrong\nlist.map({ item => ... })\n```\n\n\n## <a name='lang'>Scala 语言特性</a>\n\n### <a name='case_class_immutability'>样例类与不可变性</a>\n\n样例类（case class）本质也是普通的类，编译器会自动地为它加上以下支持：\n- 构造器参数的公有 getter 方法\n- 拷贝构造函数\n- 构造器参数的模式匹配\n- 默认的 toString/hash/equals 实现\n\n对于样例类来说，构造器参数不应设为可变的，可以使用拷贝构造函数达到同样的效果。使用可变的样例类容易出错，例如，哈希表中，对象根据旧的哈希值被放在错误的位置上。\n\n```scala\n// This is OK\ncase class Person(name: String, age: Int)\n\n// This is NOT OK\ncase class Person(name: String, var age: Int)\n\n// 通过拷贝构造函数创建一个新的实例来改变其中的值\nval p1 = Person(\"Peter\", 15)\nval p2 = p2.copy(age = 16)\n```\n\n\n### <a name='apply_method'>apply 方法</a>\n\n避免在类里定义 apply 方法。这些方法往往会使代码的可读性变差，尤其是对于不熟悉 Scala 的人。它也难以被 IDE（或 grep）所跟踪。在最坏的情况下，它还可能影响代码的正确性，正如你在[括号](#parentheses)一节中看到的。\n\n然而，将 apply 方法作为工厂方法定义在伴生对象中是可以接受的。在这种情况下，apply 方法应该返回其伴生类的类型。\n\n```scala\nobject TreeNode {\n  // 下面这种定义是 OK 的\n  def apply(name: String): TreeNode = ...\n\n  // 不要像下面那样定义，因为它没有返回其伴生类的类型：TreeNode\n  def apply(name: String): String = ...\n}\n```\n\n\n### <A name='override_modifier'>override 修饰符</a>\n\n无论是覆盖具体的方法还是实现抽象的方法，始终都为方法加上 override 修饰符。实现抽象方法时，不加 override 修饰符，Scala 编译器也不会报错。即便如此，我们也应该始终把 override 修饰符加上，以此显式地表示覆盖行为。以此避免由于方法签名不同（而你也难以发现）而导致没有覆盖到本应覆盖的方法。\n\n```scala\ntrait Parent {\n  def hello(data: Map[String, String]): Unit = {\n    print(data)\n  }\n}\n\nclass Child extends Parent {\n  import scala.collection.Map\n\n  // 下面的方法没有覆盖 Parent.hello,\n  // 因为两个 Map 的类型是不同的。\n  // 如果我们加上 override 修饰符，编译器就会帮你找出问题并报错。\n  def hello(data: Map[String, String]): Unit = {\n    print(\"This is supposed to override the parent method, but it is actually not!\")\n  }\n}\n```\n\n\n\n### <a name='destruct_bind'>解构绑定</a>\n\n解构绑定（有时也叫元组提取）是一种在一个表达式中为两个变量赋值的便捷方式。\n\n```scala\nval (a, b) = (1, 2)\n```\n\n然而，请不要在构造函数中使用它们，尤其是当 `a` 和 `b` 需要被标记为 `transient` 的时候。Scala 编译器会产生一个额外的 Tuple2 字段，而它并不是暂态的（transient）。\n\n```scala\nclass MyClass {\n  // 以下代码无法 work，因为编译器会产生一个非暂态的 Tuple2 指向 a 和 b\n  @transient private val (a, b) = someFuncThatReturnsTuple2()\n}\n```\n\n\n### <a name='call_by_name'>按名称传参</a>\n\n__避免使用按名传参__. 显式地使用 `() => T` 。\n\n背景：Scala 允许按名称来定义方法参数，例如：以下例子是可以成功执行的：\n\n```scala\ndef print(value: => Int): Unit = {\n  println(value)\n  println(value + 1)\n}\n\nvar a = 0\ndef inc(): Int = {\n  a += 1\n  a\n}\n\nprint(inc())\n```\n\n在上面的代码中，`inc()` 以闭包的形式传递给 `print` 函数，并且在 `print` 函数中被执行了两次，而不是以数值 `1` 传入。按名传参的一个主要问题是在方法调用处，我们无法区分是按名传参还是按值传参。因此无法确切地知道这个表达式是否会被执行（更糟糕的是它可能会被执行多次）。对于带有副作用的表达式来说，这一点是非常危险的。\n\n\n### <A name='multi-param-list'>多参数列表</a>\n\n__避免使用多参数列表__。它们使运算符重载变得复杂，并且会使不熟悉 Scala 的程序员感到困惑。例如：\n\n```scala\n// Avoid this!\ncase class Person(name: String, age: Int)(secret: String)\n```\n\n一个值得注意的例外是，当在定义底层库时，可以使用第二个参数列表来存放隐式（implicit）参数。尽管如此，[我们应该避免使用 implicits](#implicits)！\n\n\n### <a name='symbolic_methods'>符号方法（运算符重载）</a>\n\n__不要使用符号作为方法名__，除非你是在定义算术运算的方法（如：`+`, `-`, `*`, `/`），否则在任何其它情况下，都不要使用。符号化的方法名让人难以理解方法的意图是什么，来看下面两个例子：\n\n```scala\n// 符号化的方法名难以理解\nchannel ! msg\nstream1 >>= stream2\n\n// 下面的方法意图则不言而喻\nchannel.send(msg)\nstream1.join(stream2)\n```\n\n\n### <a name='type_inference'>类型推导</a>\n\nScala 的类型推导，尤其是左侧类型推导以及闭包推导，可以使代码变得更加简洁。尽管如此，也有一些情况我们是需要显式地声明类型的：\n\n- __公有方法应该显式地声明类型__，编译器推导出来的类型往往会使你大吃一惊。\n- __隐式方法应该显式地声明类型__，否则在增量编译时，它会使 Scala 编译器崩溃。\n- __如果变量或闭包的类型并非显而易见，请显式声明类型__。一个不错的判断准则是，如果评审代码的人无法在 3 秒内确定相应实体的类型，那么你就应该显式地声明类型。\n\n\n### <a name='return'>Return 语句</a>\n\n__闭包中避免使用 return__。`return` 会被编译器转成 ``scala.runtime.NonLocalReturnControl`` 异常的 ``try/catch`` 语句，这可能会导致意外行为。请看下面的例子：\n\n  ```scala\n  def receive(rpc: WebSocketRPC): Option[Response] = {\n    tableFut.onComplete { table =>\n      if (table.isFailure) {\n        return None // Do not do that!\n      } else { ... }\n    }\n  }\n  ```\n\n`.onComplete` 方法接收一个匿名闭包并把它传递到一个不同的线程中。这个闭包最终会抛出一个 `NonLocalReturnControl` 异常，并在 __一个不同的线程中__被捕获，而这里执行的方法却没有任何影响。\n\n然而，也有少数情况我们是推荐使用 `return` 的。\n\n- 使用 `return` 来简化控制流，避免增加一级缩进。\n\n  ```scala\n  def doSomething(obj: Any): Any = {\n    if (obj eq null) {\n      return null\n    }\n    // do something ...\n  }\n  ```\n\n- 使用 `return` 来提前终止循环，这样就不用额外构造状态标志。\n\n  ```scala\n  while (true) {\n    if (cond) {\n      return\n    }\n  }\n  ```\n\n### <a name='recursion'>递归及尾递归</a>\n\n__避免使用递归__，除非问题可以非常自然地用递归来描述（比如，图和树的遍历）。\n\n对于那些你意欲使之成为尾递归的方法，请加上 `@tailrec` 注解以确保编译器去检查它是否真的是尾递归（你会非常惊讶地看到，由于使用了闭包和函数变换，许多看似尾递归的代码事实并非尾递归）。\n\n大多数的代码使用简单的循环和状态机会更容易推理，使用尾递归反而可能会使它更加繁琐且难以理解。例如，下面的例子中，命令式的代码比尾递归版本的代码要更加易读：\n\n```scala\n// Tail recursive version.\ndef max(data: Array[Int]): Int = {\n  @tailrec\n  def max0(data: Array[Int], pos: Int, max: Int): Int = {\n    if (pos == data.length) {\n      max\n    } else {\n      max0(data, pos + 1, if (data(pos) > max) data(pos) else max)\n    }\n  }\n  max0(data, 0, Int.MinValue)\n}\n\n// Explicit loop version\ndef max(data: Array[Int]): Int = {\n  var max = Int.MinValue\n  for (v <- data) {\n    if (v > max) {\n      max = v\n    }\n  }\n  max\n}\n```\n\n\n### <a name='implicits'>Implicits</a>\n\n__避免使用 implicit__，除非：\n\n- 你在构建领域特定的语言（DSL）\n- 你在隐式类型参数中使用它（如：`ClassTag`，`TypeTag`）\n- 你在你自己的类中使用它（意指不要污染外部空间），以此减少类型转换的冗余度（如：Scala 闭包到 Java 闭包的转换）。\n\n当使用 implicit 时，我们应该确保另一个工程师可以直接理解使用语义，而无需去阅读隐式定义本身。Implicit 有着非常复杂的解析规则，这会使代码变得极其难以理解。Twitter 的 Effective Scala 指南中写道：「如果你发现你在使用 implicit，始终停下来问一下你自己，是否可以在不使用 implicit 的条件下达到相同的效果」。\n\n如果你必需使用它们（比如：丰富 DSL），那么不要重载隐式方法，即确保每个隐式方法有着不同的名字，这样使用者就可以选择性地导入它们。\n\n```scala\n// 别这么做，这样使用者无法选择性地只导入其中一个方法。\nobject ImplicitHolder {\n  def toRdd(seq: Seq[Int]): RDD[Int] = ...\n  def toRdd(seq: Seq[Long]): RDD[Long] = ...\n}\n\n// 应该将它们定义为不同的名字：\nobject ImplicitHolder {\n  def intSeqToRdd(seq: Seq[Int]): RDD[Int] = ...\n  def longSeqToRdd(seq: Seq[Long]): RDD[Long] = ...\n}\n```\n\n\n## <a name='exception'>异常处理 (Try 还是 try)</a>\n\n- 不要捕获 Throwable 或 Exception 类型的异常。请使用 `scala.util.control.NonFatal`：\n\n  ```scala\n  try {\n    ...\n  } catch {\n    case NonFatal(e) =>\n      // 异常处理；注意 NonFatal 无法匹配 InterruptedException 类型的异常\n    case e: InterruptedException =>\n      // 处理 InterruptedException\n  }\n  ```\n  这能保证我们不会去捕获 `NonLocalReturnControl` 异常（正如在[Return 语句](#return)中所解释的）。\n\n- 不要在 API 中使用 `Try`，即，不要在任何方法中返回 Try。对于异常执行，请显式地抛出异常，并使用 Java 风格的 try/catch 做异常处理。\n\n  背景资料：Scala 提供了单子（monadic）错误处理（通过 `Try`，`Success` 和 `Failure`），这样便于做链式处理。然而，根据我们的经验，发现使用它通常会带来更多的嵌套层级，使得代码难以阅读。此外，对于预期错误还是异常，在语义上常常是不明晰的。因此，我们不鼓励使用 `Try` 来做错误处理，尤其是以下情况：\n\n  一个人为的例子：\n\n  ```scala\n  class UserService {\n    /** Look up a user's profile in the user database. */\n    def get(userId: Int): Try[User]\n  }\n  ```\n  以下的写法会更好：\n\n  ```scala\n  class UserService {\n    /**\n     * Look up a user's profile in the user database.\n     * @return None if the user is not found.\n     * @throws DatabaseConnectionException when we have trouble connecting to the database/\n     */\n    @throws(DatabaseConnectionException)\n    def get(userId: Int): Option[User]\n  }\n  ```\n  第二种写法非常明显地能让调用者知道需要处理哪些错误情况。\n\n\n### <a name='option'>Options</a>\n\n- 如果一个值可能为空，那么请使用 `Option`。相对于 `null`，`Option` 显式地表明了一个 API 的返回值可能为空。\n- 构造 `Option` 值时，请使用 `Option` 而非 `Some`，以防那个值为 `null`。\n\n  ```scala\n  def myMethod1(input: String): Option[String] = Option(transform(input))\n\n  // This is not as robust because transform can return null, and then\n  // myMethod2 will return Some(null).\n  def myMethod2(input: String): Option[String] = Some(transform(input))\n  ```\n- 不要使用 None 来表示异常，有异常时请显式抛出。\n- 不要在一个 `Option` 值上直接调用 `get` 方法，除非你百分百确定那个 `Option` 值不是 `None`。\n\n\n### <a name='chaining'>单子链接</a>\n\n单子链接是 Scala 的一个强大特性。Scala 中几乎一切都是单子（如：集合，Option，Future，Try 等），对它们的操作可以链接在一起。这是一个非常强大的概念，但你应该谨慎使用，尤其是：\n\n- 避免链接（或嵌套）超过 3 个操作。\n- 如果需要花超过 5 秒钟来理解其中的逻辑，那么你应该尽量去想想有没什么办法在不使用单子链接的条件下来达到相同的效果。一般来说，你需要注意的是：不要滥用 `flatMap` 和 `fold`。\n- 链接应该在 flatMap 之后断开（因为类型发生了变化）。\n\n通过给中间结果显式地赋予一个变量名，将链接断开变成一种更加过程化的风格，能让单子链接更加易于理解。来看下面的例子：\n\n```scala\nclass Person(val data: Map[String, String])\nval database = Map[String, Person]\n// Sometimes the client can store \"null\" value in the  store \"address\"\n\n// A monadic chaining approach\ndef getAddress(name: String): Option[String] = {\n  database.get(name).flatMap { elem =>\n    elem.data.get(\"address\")\n      .flatMap(Option.apply)  // handle null value\n  }\n}\n\n// 尽管代码会长一些，但以下方法可读性更高\ndef getAddress(name: String): Option[String] = {\n  if (!database.contains(name)) {\n    return None\n  }\n\n  database(name).data.get(\"address\") match {\n    case Some(null) => None  // handle null value\n    case Some(addr) => Option(addr)\n    case None => None\n  }\n}\n\n```\n\n\n## <a name='concurrency'>并发</a>\n\n### <a name='concurrency-scala-collection'>Scala concurrent.Map</a>\n\n__优先考虑使用 `java.util.concurrent.ConcurrentHashMap` 而非 `scala.collection.concurrent.Map`__。尤其是 `scala.collection.concurrent.Map` 中的 `getOrElseUpdate` 方法要慎用，它并非原子操作（这个问题在 Scala 2.11.16 中 fix 了：[SI-7943](https://issues.scala-lang.org/browse/SI-7943)）。由于我们做的所有项目都需要在 Scala 2.10 和 Scala 2.11 上使用，因此要避免使用 `scala.collection.concurrent.Map`。\n\n\n### <a name='concurrency-sync-vs-map'>显式同步 vs 并发集合</a>\n\n有 3 种推荐的方法来安全地并发访问共享状态。__不要混用它们__，因为这会使程序变得难以推理，并且可能导致死锁。\n\n- `java.util.concurrent.ConcurrentHashMap`：当所有的状态都存储在一个 map 中，并且有高程度的竞争时使用。\n\n  ```scala\n  private[this] val map = new java.util.concurrent.ConcurrentHashMap[String, String]\n  ```\n\n- `java.util.Collections.synchronizedMap`：使用情景：当所有状态都存储在一个 map 中，并且预期不存在竞争情况，但你仍想确保代码在并发下是安全的。如果没有竞争出现，JVM 的 JIT 编译器能够通过偏置锁（biased locking）移除同步开销。\n\n  ```scala\n  private[this] val map = java.util.Collections.synchronizedMap(new java.util.HashMap[String, String])\n  ```\n\n- 通过同步所有临界区进行显式同步，可用于监视多个变量。与 2 相似，JVM 的 JIT 编译器能够通过偏置锁（biased locking）移除同步开销。\n\n  ```scala\n  class Manager {\n    private[this] var count = 0\n    private[this] val map = new java.util.HashMap[String, String]\n    def update(key: String, value: String): Unit = synchronized {\n      map.put(key, value)\n      count += 1\n    }\n    def getCount: Int = synchronized { count }\n  }\n  ```\n\n注意，对于 case 1 和 case 2，不要让集合的视图或迭代器从保护区域逃逸。这可能会以一种不明显的方式发生，比如：返回了 `Map.keySet` 或 `Map.values`。如果需要传递集合的视图或值，生成一份数据拷贝再传递。\n\n  ```scala\n  val map = java.util.Collections.synchronizedMap(new java.util.HashMap[String, String])\n\n  // This is broken!\n  def values: Iterable[String] = map.values\n\n  // Instead, copy the elements\n  def values: Iterable[String] = map.synchronized { Seq(map.values: _*) }\n  ```\n\n### <a name='concurrency-sync-vs-atomic'>显式同步 vs 原子变量 vs @volatile</a>\n\n`java.util.concurrent.atomic` 包提供了对基本类型的无锁访问，比如：`AtomicBoolean`, `AtomicInteger` 和 `AtomicReference`。\n\n始终优先考虑使用原子变量而非 `@volatile`，它们是相关功能的严格超集并且从代码上看更加明显。原子变量的底层实现使用了 `@volatile`。\n\n优先考虑使用原子变量而非显式同步的情况：（1）一个对象的所有临界区更新都被限制在单个变量里并且预期会有竞争情况出现。原子变量是无锁的并且允许更为有效的竞争。（2）同步被明确地表示为 `getAndSet` 操作。例如：\n\n  ```scala\n  // good: 明确又有效地表达了下面的并发代码只执行一次\n  val initialized = new AtomicBoolean(false)\n  ...\n  if (!initialized.getAndSet(true)) {\n    ...\n  }\n\n  // poor: 下面的同步就没那么明晰，而且会出现不必要的同步\n  val initialized = false\n  ...\n  var wasInitialized = false\n  synchronized {\n    wasInitialized = initialized\n    initialized = true\n  }\n  if (!wasInitialized) {\n    ...\n  }\n  ```\n\n### <a name='concurrency-private-this'>私有字段</a>\n\n注意，`private` 字段仍然可以被相同类的其它实例所访问，所以仅仅通过 `this.synchronized`（或 `synchronized`）来保护它从技术上来说是不够的，不过你可以通过 `private[this]` 修饰私有字段来达到目的。\n\n```scala\n// 以下代码仍然是不安全的。\nclass Foo {\n  private var count: Int = 0\n  def inc(): Unit = synchronized { count += 1 }\n}\n\n// 以下代码是安全的。\nclass Foo {\n  private[this] var count: Int = 0\n  def inc(): Unit = synchronized { count += 1 }\n}\n```\n\n\n### <a name='concurrency-isolation'>隔离</a>\n\n一般来说，并发和同步逻辑应该尽可能地被隔离和包含起来。这实际上意味着：\n\n- 避免在 API 层面、面向用户的方法以及回调中暴露同步原语。\n- 对于复杂模块，创建一个小的内部模块来包含并发原语。\n\n\n## <a name='perf'>性能</a>\n\n对于你写的绝大多数代码，性能都不应该成为一个问题。然而，对于一些性能敏感的代码，以下有一些小建议：\n\n### <a name='perf-microbenchmarks'>Microbenchmarks</a>\n\n由于 Scala 编译器和 JVM JIT 编译器会对你的代码做许多神奇的事情，因此要写出一个好的微基准程序（microbenchmark）是极其困难的。更多的情况往往是你的微基准程序并没有测量你想要测量的东西。\n\n如果你要写一个微基准程序，请使用 [jmh](http://openjdk.java.net/projects/code-tools/jmh/)。请确保你阅读了[所有的样例](http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/)，这样你才理解微基准程序中「死代码」移除、常量折叠以及循环展开的效果。\n\n\n### <a name='perf-whileloops'>Traversal 与 zipWithIndex</a>\n\n使用 `while` 循环而非 `for` 循环或函数变换（如：`map`、`foreach`），for 循环和函数变换非常慢（由于虚函数调用和装箱的缘故）。\n\n```scala\n\nval arr = // array of ints\n// 偶数位置的数置零\nval newArr = list.zipWithIndex.map { case (elem, i) =>\n  if (i % 2 == 0) 0 else elem\n}\n\n// 这是上面代码的高性能版本\nval newArr = new Array[Int](arr.length)\nvar i = 0\nval len = newArr.length\nwhile (i < len) {\n  newArr(i) = if (i % 2 == 0) 0 else arr(i)\n  i += 1\n}\n```\n\n### <a name='perf-option'>Option 与 null</a>\n\n对于性能有要求的代码，优先考虑使用 `null` 而不是 `Option`，以此避免虚函数调用以及装箱操作。用 Nullable 注解明确标示出可能为 `null` 的值。\n\n```scala\nclass Foo {\n  @javax.annotation.Nullable\n  private[this] var nullableField: Bar = _\n}\n```\n\n### <a name='perf-collection'>Scala 集合库</a>\n\n对于性能有要求的代码，优先考虑使用 Java 集合库而非 Scala 集合库，因为一般来说，Scala 集合库要比 Java 的集合库慢。\n\n### <a name='perf-private'>private[this]</a>\n\n对于性能有要求的代码，优先考虑使用 `private[this]` 而非 `private`。`private[this]` 生成一个字段而非生成一个访问方法。根据我们的经验，JVM JIT 编译器并不总是会内联 `private` 字段的访问方法，因此通过使用\n`private[this]` 来确保没有虚函数调用会更保险。\n\n```scala\nclass MyClass {\n  private val field1 = ...\n  private[this] val field2 = ...\n\n  def perfSensitiveMethod(): Unit = {\n    var i = 0\n    while (i < 1000000) {\n      field1  // This might invoke a virtual method call\n      field2  // This is just a field access\n      i += 1\n    }\n  }\n}\n```\n\n\n## <a name='java'>与 Java 的互操作性</a>\n\n本节内容介绍的是构建 Java 兼容 API 的准则。如果你构建的组件并不需要与 Java 有交互，那么请无视这一节。这一节的内容主要是从我们开发 Spark 的 Java API 的经历中得出的。\n\n\n### <a name='java-missing-features'>Scala 中缺失的 Java 特性</a>\n\n以下的 Java 特性在 Scala 中是没有的，如果你需要使用以下特性，请在 Java 中定义它们。然而，需要提醒一点的是，你无法为 Java 源文件生成 ScalaDoc。\n\n- 静态字段\n- 静态内部类\n- Java 枚举\n- 注解\n\n\n### <a name='java-traits'>Traits 与抽象类</a>\n\n对于允许从外部实现的接口，请记住以下几点：\n\n- 包含了默认方法实现的 trait 是无法在 Java 中使用的，请使用抽象类来代替。\n- 一般情况下，请避免使用 trait，除非你百分百确定这个接口即使在未来也不会有默认的方法实现。\n\n```scala\n// 以下默认实现无法在 Java 中使用\ntrait Listener {\n  def onTermination(): Unit = { ... }\n}\n\n// 可以在 Java 中使用\nabstract class Listener {\n  def onTermination(): Unit = { ... }\n}\n```\n\n\n### <a name='java-type-alias'>类型别名</a>\n\n不要使用类型别名，它们在字节码和 Java 中是不可见的。\n\n\n### <a name='java-default-param-values'>默认参数值</a>\n\n不要使用默认参数值，通过重载方法来代替。\n\n```scala\n// 打破了与 Java 的互操作性\ndef sample(ratio: Double, withReplacement: Boolean = false): RDD[T] = { ... }\n\n// 以下方法是 work 的\ndef sample(ratio: Double, withReplacement: Boolean): RDD[T] = { ... }\ndef sample(ratio: Double): RDD[T] = sample(ratio, withReplacement = false)\n```\n\n### <a name='java-multi-param-list'>多参数列表</a>\n\n不要使用多参数列表。\n\n### <a name='java-varargs'>可变参数</a>\n\n- 为可变参数方法添加 `@scala.annotation.varargs` 注解，以确保它能在 Java 中使用。Scala 编译器会生成两个方法，一个给 Scala 使用（字节码参数是一个 Seq），另一个给 Java 使用（字节码参数是一个数组）。\n\n  ```scala\n  @scala.annotation.varargs\n  def select(exprs: Expression*): DataFrame = { ... }\n  ```\n\n- 需要注意的一点是，由于 Scala 编译器的一个 bug（[SI-1459](https://issues.scala-lang.org/browse/SI-1459)，[SI-9013](https://issues.scala-lang.org/browse/SI-9013)），抽象的变参方法是无法在 Java 中使用的。\n\n- 重载变参方法时要小心，用另一个类型去重载变参方法会破坏源码的兼容性。\n\n  ```scala\n  class Database {\n    @scala.annotation.varargs\n    def remove(elems: String*): Unit = ...\n\n    // 当调用无参的 remove 方法时会出问题。\n    @scala.annotation.varargs\n    def remove(elems: People*): Unit = ...\n  }\n\n  // remove 方法有歧义，因此编译不过。\n  new Database().remove()\n  ```\n  一种解决方法是，在可变参数前显式地定义第一个参数：\n\n  ```scala\n  class Database {\n    @scala.annotation.varargs\n    def remove(elems: String*): Unit = ...\n\n    // 以下重载是 OK 的。\n    @scala.annotation.varargs\n    def remove(elem: People, elems: People*): Unit = ...\n  }\n  ```\n\n\n### <a name='java-implicits'>Implicits</a>\n\n不要为类或方法使用 implicit，包括了不要使用 `ClassTag` 和 `TypeTag`。\n\n```scala\nclass JavaFriendlyAPI {\n  // 以下定义对 Java 是不友好的，因为方法中包含了一个隐式参数（ClassTag）。\n  def convertTo[T: ClassTag](): T\n}\n```\n\n### <a name='java-companion-object'>伴生对象，静态方法与字段</a>\n\n当涉及到伴生对象和静态方法/字段时，有几件事情是需要注意的：\n\n- 伴生对象在 Java 中的使用是非常别扭的（伴生对象 `Foo` 会被定义为 `Foo$` 类内的一个类型为 `Foo$` 的静态字段 `MODULE$`）。\n\n  ```scala\n  object Foo\n\n  // 等价于以下的 Java 代码\n  public class Foo$ {\n    Foo$ MODULE$ = // 对象的实例化\n  }\n  ```\n  如果非要使用伴生对象，可以在一个单独的类中创建一个 Java 静态字段。\n\n- 不幸的是，没有办法在 Scala 中定义一个 JVM 静态字段。请创建一个 Java 文件来定义它。\n- 伴生对象里的方法会被自动转成伴生类里的静态方法，除非方法名有冲突。确保静态方法正确生成的最好方式是用 Java 写一个测试文件，然后调用生成的静态方法。\n\n  ```scala\n  class Foo {\n    def method2(): Unit = { ... }\n  }\n\n  object Foo {\n    def method1(): Unit = { ... }  // 静态方法 Foo.method1 会被创建（字节码）\n    def method2(): Unit = { ... }  // 静态方法 Foo.method2 不会被创建\n  }\n\n  // FooJavaTest.java (in test/scala/com/databricks/...)\n  public class FooJavaTest {\n    public static void compileTest() {\n      Foo.method1();  // 正常编译\n      Foo.method2();  // 编译失败，因为 method2 并没有生成\n    }\n  }\n  ```\n\n- 样例对象（case object） MyClass 的类型并不是 MyClass。\n\n  ```scala\n  case object MyClass\n\n  // Test.java\n  if (MyClass$.MODULE instanceof MyClass) {\n    // 上述条件始终为 false\n  }\n  ```\n  要实现正确的类型层级结构，请定义一个伴生类，然后用一个样例对象去继承它：\n\n  ```scala\n  class MyClass\n  case object MyClass extends MyClass\n  ```\n\n\n## <a name='misc'>其它</a>\n\n### <a name='misc_currentTimeMillis_vs_nanoTime'>优先使用 nanoTime 而非 currentTimeMillis</a>\n\n当要计算*持续时间*或者检查*超时*的时候，避免使用 `System.currentTimeMillis()`。请使用 `System.nanoTime()`，即使你对亚毫秒级的精度并不感兴趣。\n\n`System.currentTimeMillis()` 返回的是当前的时钟时间，并且会跟进系统时钟的改变。因此，负的时钟调整可能会导致超时而挂起很长一段时间（直到时钟时间赶上先前的值）。这种情况可能发生在网络已经中断一段时间，ntpd 走过了一步之后。最典型的例子是，在系统启动的过程中，DHCP 花费的时间要比平常的长。这可能会导致非常难以理解且难以重现的问题。而 `System.nanoTime()` 则可以保证是单调递增的，与时钟变化无关。\n\n注意事项：\n\n- 永远不要序列化一个绝对的 `nanoTime()` 值或是把它传递给另一个系统。绝对的 `nanoTime()` 值是无意义的、与系统相关的，并且在系统重启时会重置。\n- 绝对的 `nanoTime()` 值并不保证总是正数（但 `t2 - t1` 能确保总是产生正确的值）。\n- `nanoTime()` 每 292 年就会重新计算起。所以，如果你的 Spark 任务需要花非常非常非常长的时间，你可能需要别的东西来处理了：）\n\n\n### <a name='misc_uri_url'>优先使用 URI 而非 URL</a>\n\n当存储服务的 URL 时，你应当使用 `URI` 来表示。\n\n`URL` 的[相等性检查](http://docs.oracle.com/javase/7/docs/api/java/net/URL.html#equals(java.lang.Object))实际上执行了一次网络调用（这是阻塞的）来解析 IP 地址。`URI` 类在表示能力上是 `URL` 的超集，并且它执行的是字段的相等性检查。\n\n## <a name='unit-test'>单元测试</a>\n\n### <a name='unit-test-framework'>单元测试框架</a>\nScalaTest几乎已经成为Scala语言默认的测试框架，这主要源于它提供了多种表达力超强的测试风格，能够满足各种层次的需求包括单元测试、BDD、验收测试、数据驱动测试。我们也使用[ScalaTest](http://www.scalatest.org/)测试框架。使用的时候在`pom.xml`中添加如下类似引用：\n\n``` xml\n<!-- test: https://mvnrepository.com/artifact/org.scalatest/scalatest_2.10 -->\n        <dependency>\n            <groupId>org.scalatest</groupId>\n            <artifactId>scalatest_2.10</artifactId>\n            <version>3.0.0</version>\n            <scope>test</scope>\n        </dependency>\n```\n测试我们遵循以下几点规则：\n- 测试类应该与被测试类处于同一包下，测试类的命名为：被测试类名 + Test\n- 测试含有具体实现的trait时，可以让被测试类直接继承Trait。例如：\n\n``` scala\ntrait RecordsGenerator {\n     def generateRecords(table: List[List[String]]): List[Record] {\n          //...\n     }\n}\n\nclass RecordsGeneratorSpec extends FlatSpec with ShouldMatcher with RecordGenerator {\n     val table = List(List(\"abc\", \"def\"), List(\"aaa\", \"bbb\"))\n     it should \"generate records\" in {\n          val records = generateRecords(table)\n          records.size should be(2)\n     }\n}\n```\n\n- 若要对文件进行测试，可以用字符串假装文件：\n\n``` scala\ntype CsvLine = String\ndef formatCsv(source: Source): List[CsvLine] = {\n     source.getLines(_.replace(\", \", \"|\"))\n}\n```\nformatCsv需要接受一个文件源，例如Source.fromFile(\"testdata.txt\")。但在测试时，可以通过Source.fromString方法来生成formatCsv需要接收的Source对象：\n``` scala\ntest(\"format csv lines\") {\n     val lines = Source.fromString(\"abc, def, hgi\\n1, 2, 3\\none, two, three\")\n     val result = formatCsv(lines)\n     assert(result.mkString(\"\\n\").equles(\"abc|def|hgi\\n1|2|3\\none|two|three\"))\n}\n```\n\n### <a name='unit-test-style'>测试风格的选择</a>\n\nScalaTest一共提供了七种测试风格，分别为：FunSuite，FlatSpec，FunSpec，WordSpec，FreeSpec，PropSpec和FeatureSpec。这就好像使用相同的原料做成不同美味乃至不同菜系的佳肴，你可以根据自己的口味进行选择。我们统一推荐使用FunSuite的方式，因为它更灵活，而且更符合传统测试方法的风格，区别仅在于test()方法可以接受一个闭包:\n\n``` scala\nimport org.scalatest.FunSuite\n\nclass SetSuite extends FunSuite {\n\n  test(\"An empty Set should have size 0\") {\n    assert(Set.empty.size == 0)\n  }\n\n  test(\"Invoking head on an empty Set should produce NoSuchElementException\") {\n    assertThrows[NoSuchElementException] {\n      Set.empty.head\n    }\n  }\n}\n```\n当然，如果你有必须的理由选择其它测试风格的话，本规则并不强制。\n\n# 三、 引用\n\n[Databricks Scala 编程风格指南](https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md)(团队最终选择的模板)\n[Effective Scala](http://twitter.github.io/effectivescala/index-cn.html)(Twitter Scala资料，值得参考)\n[Thinking in Scala--Scala编程规范](https://zhangyi.gitbooks.io/thinking-in-scala/content/scala-convention.html)(个人整理，可以参考)\n[scala-lang--Scala语言规范.pdf](http://www.scala-lang.org/docu/files/Scala%E8%AF%AD%E8%A8%80%E8%A7%84%E8%8C%83.pdf)(官方原版的中文文档，共127页，过于复杂琐碎，可以参考)\n[Scala官网Style Guide](http://docs.scala-lang.org/style/)(官方原版，其它版本基本上都是基于此版进行的改进)\n[分析GitHub上托管的scala开源代码统计](https://segmentfault.com/a/1190000000420018)","source":"_posts/2017-01-09-大数据团队scala代码规范.md","raw":"---\ntitle: 大数据团队scala代码规范\ntoc: false\ndate: 2017-01-09 16:02:19\ntags: scala\ncategories: scala\n---\n\n## 前言\n\n### 一、 scala代码规范\n\nScala 是一种强大到令人难以置信的多范式编程语言。目前我们团队有很多项目需要使用scala语言进行编程，尤其是Spark相关开发项目，为了能够统一scala相关开发，本人基于Spark 贡献者及 [Databricks](http://databricks.com/) 工程团队总结出了以下指南，本人增加了单元测试的规范，希望对大家的开发有帮助。当然，这个指南并非绝对，根据我们团队需求与实际经验，持续更新。\n\n### 二、 选择此规范的理由\n很早就有大家统一编程规范的想法，网上也有一些关于编程规范的文档供参考。最终选择了以 Databricks 公司的编程规范为模板制作。理由如下：\n\n- 目前公司使用scala语言主要用在Spark的开发，而这份指南是Spark 贡献者及 Databricks 工程团队一起工作时总结出来的；\n- 跟我们之前写得代码，以及IDEA等编译器自动格式化相差不大；\n- 这份指南经过多次的修改和总结，经过了实践的检验；\n- 这份指南相比较而言简明概要，易于理解，遵循 Java 的地方就没有赘述，比较解耦，适用于根据公司代码体系来修改；\n\n## <a name='TOC'>目录</a>\n\n  0. [文档历史](#history)\n  1. [语法风格](#syntactic)\n    - [命名约定](#naming)\n    - [变量命名约定](#variable-naming)\n    - [一行长度](#linelength)\n    - [30 法则](#rule_of_30)\n    - [空格与缩进](#indent)\n    - [空行](#blanklines)\n    - [括号](#parentheses)\n    - [大括号](#curly)\n    - [长整型字面量](#long_literal)\n    - [文档风格](#doc)\n    - [类内秩序](#ordering_class)\n    - [Imports](#imports)\n    - [模式匹配](#pattern-matching)\n    - [中缀方法](#infix)\n    - [匿名方法](#anonymous)\n  2. [Scala 语言特性](#lang)\n    - [样例类与不可变性](#case_class_immutability)\n    - [apply 方法](#apply_method)\n    - [override 修饰符](#override_modifier)\n    - [解构绑定](#destruct_bind)\n    - [按名称传参](#call_by_name)\n    - [多参数列表](#multi-param-list)\n    - [符号方法 (运算符重载)](#symbolic_methods)\n    - [类型推导](#type_inference)\n    - [Return 语句](#return)\n    - [递归及尾递归](#recursion)\n    - [Implicits](#implicits)\n    - [异常处理 (Try 还是 try)](#exception)\n    - [Options](#option)\n    - [单子链接](#chaining)\n  3. [并发](#concurrency)\n    - [Scala concurrent.Map](#concurrency-scala-collection)\n    - [显式同步 vs 并发集合](#concurrency-sync-vs-map)\n    - [显式同步 vs 原子变量 vs @volatile](#concurrency-sync-vs-atomic)\n    - [私有字段](#concurrency-private-this)\n    - [隔离](#concurrency-isolation)\n  4. [性能](#perf)\n    - [Microbenchmarks](#perf-microbenchmarks)\n    - [Traversal 与 zipWithIndex](#perf-whileloops)\n    - [Option 与 null](#perf-option)\n    - [Scala 集合库](#perf-collection)\n    - [private[this]](#perf-private)\n  5. [与 Java 的互操作性](#java)\n    - [Scala 中缺失的 Java 特性](#java-missing-features)\n    - [Traits 与抽象类](#java-traits)\n    - [类型别名](#java-type-alias)\n    - [默认参数值](#java-default-param-values)\n    - [多参数列表](#java-multi-param-list)\n    - [可变参数](#java-varargs)\n    - [Implicits](#java-implicits)\n    - [伴生对象, 静态方法与字段](#java-companion-object)\n  6. [其它](#misc)\n    - [优先使用 nanoTime 而非 currentTimeMillis](#misc_currentTimeMillis_vs_nanoTime)\n    - [优先使用 URI 而非 URL](#misc_uri_url)\n  7. [单元测试](#unit-test)\n\t- [单元测试框架](#unit-test-framework)\n\t- [单元测试风格](#unit-test-style)\n\n## <a name='history'>文档历史</a>\n\n- 2017-01-10: 最初版本。[Databricks版本](https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md)\n- 2017-01-11: 增加 [单元测试](#unit-test) 一节。\n\n## <a name='syntactic'>语法风格</a>\n\n### <a name='naming'>命名约定</a>\n\n我们主要遵循 Java 和 Scala 的标准命名约定。\n\n- 类，trait, 对象应该遵循 Java 中类的命名约定，即 PascalCase 风格。\n\n  ```scala\n  class ClusterManager\n\n  trait Expression\n  ```\n\n- 包名应该遵循 Java 中包名的命名约定，即使用全小写的 ASCII 字母。\n\n  ```scala\n  package com.databricks.resourcemanager\n  ```\n\n- 方法/函数应当使用驼峰式风格命名。\n\n- 常量命名使用全大写字母，并将它们放在伴生对象中。\n\n  ```scala\n  object Configuration {\n    val DEFAULT_PORT = 10000\n  }\n  ```\n\n- 枚举命名与类命名一致，使用 PascalCase 风格。\n\n- 注解也应遵循 Java 中的约定，即使用 PascalCase 风格。注意，这一点与 Scala 的官方指南不同。\n\n  ```scala\n  final class MyAnnotation extends StaticAnnotation\n  ```\n\n### <a name='variable-naming'>变量命名约定</a>\n\n- 变量命名应当遵循驼峰式命名方法，并且变量名应当是不言而喻的，即变量名可以直观地反应它的涵义。\n  ```scala\n  val serverPort = 1000\n  val clientPort = 2000\n  ```\n\n- 可以在小段的局部代码中使用单字符的变量名，比如在小段的循环体中（例如 10 行以内的代码），“i” 常常被用作循环索引。然而，即使在小段的代码中，也不要使用 “l” （Larry 中的 l）作为标识符，因为它看起来和 “1”，“|”，“I” 很像，难以区分，容易搞错。\n\n\n### <a name='linelength'>一行长度</a>\n\n- 一行长度的上限是 100 个字符。\n- 唯一的例外是 import 语句和 URL (即便如此，也尽量将它们保持在 100 个字符以下)。\n\n\n### <a name='rule_of_30'>30 法则</a>\n\n「如果一个元素包含的子元素超过 30 个，那么极有可能出现了严重的问题」 - [Refactoring in Large Software Projects](http://www.amazon.com/Refactoring-Large-Software-Projects-Restructurings/dp/0470858923)。\n\n一般来说:\n\n- 一个方法包含的代码行数不宜超过 30 行。\n- 一个类包含的方法数量不宜超过 30 个。\n\n\n### <a name='indent'>空格与缩进</a>\n\n- 一般情况下，使用两个空格的缩进。\n\n  ```scala\n  if (true) {\n    println(\"Wow!\")\n  }\n  ```\n\n- 对于方法声明，如果一行无法容纳下所有的参数，那么使用 4 个空格来缩进它们。返回类型可以与最后一个参数在同一行，也可以放在下一行，使用两个空格缩进。\n\n  ```scala\n  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]](\n      path: String,\n      fClass: Class[F],\n      kClass: Class[K],\n      vClass: Class[V],\n      conf: Configuration = hadoopConfiguration): RDD[(K, V)] = {\n    // method body\n  }\n\n  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]](\n      path: String,\n      fClass: Class[F],\n      kClass: Class[K],\n      vClass: Class[V],\n      conf: Configuration = hadoopConfiguration)\n    : RDD[(K, V)] = {\n    // method body\n  }\n  ```\n\n- 如果一行无法容纳下类头（即 extends 后面那部分），则把它们放到新的一行，用两个空格缩进，然后在类内空一行再开始函数或字段的定义（或是包的导入）。\n\n  ```scala\n  class Foo(\n      val param1: String,  // 4 space indent for parameters\n      val param2: String,\n      val param3: Array[Byte])\n    extends FooInterface  // 2 space here\n    with Logging {\n\n    def firstMethod(): Unit = { ... }  // blank line above\n  }\n  ```\n\n- 不要使用垂直对齐。它使你的注意力放在代码的错误部分并增大了后人修改代码的难度。\n\n  ```scala\n  // Don't align vertically\n  val plus     = \"+\"\n  val minus    = \"-\"\n  val multiply = \"*\"\n\n  // Do the following\n  val plus = \"+\"\n  val minus = \"-\"\n  val multiply = \"*\"\n  ```\n\n\n### <a name='blanklines'>空行</a>\n\n- 一个空行可以出现在：\n  - 连续的类成员或初始化器（initializers）之间：字段，构造函数，方法，嵌套类，静态初始化器及实例初始化器。\n    - 例外：连续的两个字段之间的空行是可选的（前提是它们之间没有其它代码），这一类空行主要为这些字段做逻辑上的分组。\n  - 在方法体内，根据需要，使用空行来为语句创建逻辑上的分组。\n  - 在类的第一个成员之前或最后一个成员之后，空行都是可选的（既不鼓励也不阻止）。\n- 使用一个或两个空行来分隔不同类的定义。\n- 不鼓励使用过多的空行。\n\n\n### <a name='parentheses'>括号</a>\n\n- 方法声明应该加括号（即使没有参数列表），除非它们是没有副作用（状态改变，IO 操作都认为是有副作用的）的访问器（accessor）。\n\n  ```scala\n  class Job {\n    // Wrong: killJob changes state. Should have ().\n    def killJob: Unit\n\n    // Correct:\n    def killJob(): Unit\n  }\n  ```\n\n- 函数调用应该与函数声明在形式上保持一致，也就是说，如果一个方法声明时带了括号，那调用时也要把括号带上。注意这不仅仅是语法层面的人为约定，当返回对象中定义了 `apply` 方法时，这一点还会影响正确性。\n\n  ```scala\n  class Foo {\n    def apply(args: String*): Int\n  }\n\n  class Bar {\n    def foo: Foo\n  }\n\n  new Bar().foo  // This returns a Foo\n  new Bar().foo()  // This returns an Int!\n  ```\n\n\n### <a name='curly'>大括号</a>\n\n即使条件语句或循环语句只有一行时，也请使用大括号。唯一的例外是，当你把 if/else 作为一个单行的三元操作符来使用并且没有副作用时，这时你可以不加大括号。\n\n```scala\n// Correct:\nif (true) {\n  println(\"Wow!\")\n}\n\n// Correct:\nif (true) statement1 else statement2\n\n// Correct:\ntry {\n  foo()\n} catch {\n  ...\n}\n\n// Wrong:\nif (true)\n  println(\"Wow!\")\n\n// Wrong:\ntry foo() catch {\n  ...\n}\n```\n\n\n### <a name='long_literal'>长整型字面量</a>\n\n长整型字面量使用大写的 `L` 作为后缀，不要使用小写，因为它和数字 `1` 长得很像，常常难以区分。\n\n```scala\nval longValue = 5432L  // Do this\n\nval longValue = 5432l  // Do NOT do this\n```\n\n\n### <a name='doc'>文档风格</a>\n\n使用 Java Doc 风格，而非 Scala Doc 风格。\n\n```scala\n/** This is a correct one-liner, short description. */\n\n/**\n * This is correct multi-line JavaDoc comment. And\n * this is my second line, and if I keep typing, this would be\n * my third line.\n */\n\n/** In Spark, we don't use the ScalaDoc style so this\n  * is not correct.\n  */\n```\n\n\n### <a name='ordering_class'>类内秩序</a>\n\n如果一个类很长，包含许多的方法，那么在逻辑上把它们分成不同的部分并加上注释头，以此组织它们。\n\n```scala\nclass DataFrame {\n\n  ///////////////////////////////////////////////////////////////////////////\n  // DataFrame operations\n  ///////////////////////////////////////////////////////////////////////////\n\n  ...\n\n  ///////////////////////////////////////////////////////////////////////////\n  // RDD operations\n  ///////////////////////////////////////////////////////////////////////////\n\n  ...\n}\n```\n\n当然，强烈不建议把一个类写得这么长，一般只有在构建某些公共 API 时才允许这么做。\n\n\n### <a name='imports'>Imports</a>\n\n- __导入时避免使用通配符__, 除非你需要导入超过 6 个实体或者隐式方法。通配符导入会使代码在面对外部变化时不够健壮。\n- 始终使用绝对路径来导入包 (如：`scala.util.Random`) ，而不是相对路径 (如：`util.Random`)。\n- 此外，导入语句按照以下顺序排序：\n  * `java.*` 和 `javax.*`\n  * `scala.*`\n  * 第三方库 (`org.*`, `com.*`, 等)\n  * 项目中的类 (对于 Spark 项目，即 `com.databricks.*` 或 `org.apache.spark`)\n- 在每一组导入语句内，按照字母序进行排序。\n- 你可以使用 IntelliJ 的「import organizer」来自动处理，请使用以下配置：\n\n  ```\n  java\n  javax\n  _______ blank line _______\n  scala\n  _______ blank line _______\n  all other imports\n  _______ blank line _______\n  com.databricks  // or org.apache.spark if you are working on spark\n  ```\n\n\n### <a name='pattern-matching'>模式匹配</a>\n\n- 如果整个方法就是一个模式匹配表达式，可能的话，可以把 match 关键词与方法声明放在同一行，以此减少一级缩进。\n\n  ```scala\n  def test(msg: Message): Unit = msg match {\n    case ...\n  }\n  ```\n\n- 当以闭包形式调用一个函数时，如果只有一个 case 语句，那么把 case 语句与函数调用放在同一行。\n\n  ```scala\n  list.zipWithIndex.map { case (elem, i) =>\n    // ...\n  }\n  ```\n  如果有多个 case 语句，把它们缩进并且包起来。\n\n  ```scala\n  list.map {\n    case a: Foo =>  ...\n    case b: Bar =>  ...\n  }\n  ```\n\n- 如果唯一的目的就是想匹配某个对象的类型，那么不要展开所有的参数来做模式匹配，这样会使得重构变得更加困难，代码更容易出错。\n\n```scala\ncase class Pokemon(name: String, weight: Int, hp: Int, attack: Int, defense: Int)\ncase class Human(name: String, hp: Int)\n\n// 不要像下面那样做，因为\n// 1. 当 pokemon 加入一个新的字段，我们需要改变下面的模式匹配代码\n// 2. 非常容易发生误匹配，尤其是当所有字段的类型都一样的时候\ntargets.foreach {\n  case target @ Pokemon(_, _, hp, _, defense) =>\n    val loss = sys.min(0, myAttack - defense)\n    target.copy(hp = hp - loss)\n  case target @ Human(_, hp) =>\n    target.copy(hp = hp - myAttack)\n}\n\n// 像下面这样做就好多了:\ntargets.foreach {\n  case target: Pokemon =>\n    val loss = sys.min(0, myAttack - target.defense)\n    target.copy(hp = target.hp - loss)\n  case target: Human =>\n    target.copy(hp = target.hp - myAttack)\n}\n```\n\n\n### <a name='infix'>中缀方法</a>\n\n__避免中缀表示法__，除非是符号方法（即运算符重载）。\n\n```scala\n// Correct\nlist.map(func)\nstring.contains(\"foo\")\n\n// Wrong\nlist map (func)\nstring contains \"foo\"\n\n// 重载的运算符应该以中缀形式调用\narrayBuffer += elem\n```\n\n### <a name='anonymous'>匿名方法</a>\n\n对于匿名方法，__避免使用过多的小括号和花括号__。\n\n```scala\n// Correct\nlist.map { item =>\n  ...\n}\n\n// Correct\nlist.map(item => ...)\n\n// Wrong\nlist.map(item => {\n  ...\n})\n\n// Wrong\nlist.map { item => {\n  ...\n}}\n\n// Wrong\nlist.map({ item => ... })\n```\n\n\n## <a name='lang'>Scala 语言特性</a>\n\n### <a name='case_class_immutability'>样例类与不可变性</a>\n\n样例类（case class）本质也是普通的类，编译器会自动地为它加上以下支持：\n- 构造器参数的公有 getter 方法\n- 拷贝构造函数\n- 构造器参数的模式匹配\n- 默认的 toString/hash/equals 实现\n\n对于样例类来说，构造器参数不应设为可变的，可以使用拷贝构造函数达到同样的效果。使用可变的样例类容易出错，例如，哈希表中，对象根据旧的哈希值被放在错误的位置上。\n\n```scala\n// This is OK\ncase class Person(name: String, age: Int)\n\n// This is NOT OK\ncase class Person(name: String, var age: Int)\n\n// 通过拷贝构造函数创建一个新的实例来改变其中的值\nval p1 = Person(\"Peter\", 15)\nval p2 = p2.copy(age = 16)\n```\n\n\n### <a name='apply_method'>apply 方法</a>\n\n避免在类里定义 apply 方法。这些方法往往会使代码的可读性变差，尤其是对于不熟悉 Scala 的人。它也难以被 IDE（或 grep）所跟踪。在最坏的情况下，它还可能影响代码的正确性，正如你在[括号](#parentheses)一节中看到的。\n\n然而，将 apply 方法作为工厂方法定义在伴生对象中是可以接受的。在这种情况下，apply 方法应该返回其伴生类的类型。\n\n```scala\nobject TreeNode {\n  // 下面这种定义是 OK 的\n  def apply(name: String): TreeNode = ...\n\n  // 不要像下面那样定义，因为它没有返回其伴生类的类型：TreeNode\n  def apply(name: String): String = ...\n}\n```\n\n\n### <A name='override_modifier'>override 修饰符</a>\n\n无论是覆盖具体的方法还是实现抽象的方法，始终都为方法加上 override 修饰符。实现抽象方法时，不加 override 修饰符，Scala 编译器也不会报错。即便如此，我们也应该始终把 override 修饰符加上，以此显式地表示覆盖行为。以此避免由于方法签名不同（而你也难以发现）而导致没有覆盖到本应覆盖的方法。\n\n```scala\ntrait Parent {\n  def hello(data: Map[String, String]): Unit = {\n    print(data)\n  }\n}\n\nclass Child extends Parent {\n  import scala.collection.Map\n\n  // 下面的方法没有覆盖 Parent.hello,\n  // 因为两个 Map 的类型是不同的。\n  // 如果我们加上 override 修饰符，编译器就会帮你找出问题并报错。\n  def hello(data: Map[String, String]): Unit = {\n    print(\"This is supposed to override the parent method, but it is actually not!\")\n  }\n}\n```\n\n\n\n### <a name='destruct_bind'>解构绑定</a>\n\n解构绑定（有时也叫元组提取）是一种在一个表达式中为两个变量赋值的便捷方式。\n\n```scala\nval (a, b) = (1, 2)\n```\n\n然而，请不要在构造函数中使用它们，尤其是当 `a` 和 `b` 需要被标记为 `transient` 的时候。Scala 编译器会产生一个额外的 Tuple2 字段，而它并不是暂态的（transient）。\n\n```scala\nclass MyClass {\n  // 以下代码无法 work，因为编译器会产生一个非暂态的 Tuple2 指向 a 和 b\n  @transient private val (a, b) = someFuncThatReturnsTuple2()\n}\n```\n\n\n### <a name='call_by_name'>按名称传参</a>\n\n__避免使用按名传参__. 显式地使用 `() => T` 。\n\n背景：Scala 允许按名称来定义方法参数，例如：以下例子是可以成功执行的：\n\n```scala\ndef print(value: => Int): Unit = {\n  println(value)\n  println(value + 1)\n}\n\nvar a = 0\ndef inc(): Int = {\n  a += 1\n  a\n}\n\nprint(inc())\n```\n\n在上面的代码中，`inc()` 以闭包的形式传递给 `print` 函数，并且在 `print` 函数中被执行了两次，而不是以数值 `1` 传入。按名传参的一个主要问题是在方法调用处，我们无法区分是按名传参还是按值传参。因此无法确切地知道这个表达式是否会被执行（更糟糕的是它可能会被执行多次）。对于带有副作用的表达式来说，这一点是非常危险的。\n\n\n### <A name='multi-param-list'>多参数列表</a>\n\n__避免使用多参数列表__。它们使运算符重载变得复杂，并且会使不熟悉 Scala 的程序员感到困惑。例如：\n\n```scala\n// Avoid this!\ncase class Person(name: String, age: Int)(secret: String)\n```\n\n一个值得注意的例外是，当在定义底层库时，可以使用第二个参数列表来存放隐式（implicit）参数。尽管如此，[我们应该避免使用 implicits](#implicits)！\n\n\n### <a name='symbolic_methods'>符号方法（运算符重载）</a>\n\n__不要使用符号作为方法名__，除非你是在定义算术运算的方法（如：`+`, `-`, `*`, `/`），否则在任何其它情况下，都不要使用。符号化的方法名让人难以理解方法的意图是什么，来看下面两个例子：\n\n```scala\n// 符号化的方法名难以理解\nchannel ! msg\nstream1 >>= stream2\n\n// 下面的方法意图则不言而喻\nchannel.send(msg)\nstream1.join(stream2)\n```\n\n\n### <a name='type_inference'>类型推导</a>\n\nScala 的类型推导，尤其是左侧类型推导以及闭包推导，可以使代码变得更加简洁。尽管如此，也有一些情况我们是需要显式地声明类型的：\n\n- __公有方法应该显式地声明类型__，编译器推导出来的类型往往会使你大吃一惊。\n- __隐式方法应该显式地声明类型__，否则在增量编译时，它会使 Scala 编译器崩溃。\n- __如果变量或闭包的类型并非显而易见，请显式声明类型__。一个不错的判断准则是，如果评审代码的人无法在 3 秒内确定相应实体的类型，那么你就应该显式地声明类型。\n\n\n### <a name='return'>Return 语句</a>\n\n__闭包中避免使用 return__。`return` 会被编译器转成 ``scala.runtime.NonLocalReturnControl`` 异常的 ``try/catch`` 语句，这可能会导致意外行为。请看下面的例子：\n\n  ```scala\n  def receive(rpc: WebSocketRPC): Option[Response] = {\n    tableFut.onComplete { table =>\n      if (table.isFailure) {\n        return None // Do not do that!\n      } else { ... }\n    }\n  }\n  ```\n\n`.onComplete` 方法接收一个匿名闭包并把它传递到一个不同的线程中。这个闭包最终会抛出一个 `NonLocalReturnControl` 异常，并在 __一个不同的线程中__被捕获，而这里执行的方法却没有任何影响。\n\n然而，也有少数情况我们是推荐使用 `return` 的。\n\n- 使用 `return` 来简化控制流，避免增加一级缩进。\n\n  ```scala\n  def doSomething(obj: Any): Any = {\n    if (obj eq null) {\n      return null\n    }\n    // do something ...\n  }\n  ```\n\n- 使用 `return` 来提前终止循环，这样就不用额外构造状态标志。\n\n  ```scala\n  while (true) {\n    if (cond) {\n      return\n    }\n  }\n  ```\n\n### <a name='recursion'>递归及尾递归</a>\n\n__避免使用递归__，除非问题可以非常自然地用递归来描述（比如，图和树的遍历）。\n\n对于那些你意欲使之成为尾递归的方法，请加上 `@tailrec` 注解以确保编译器去检查它是否真的是尾递归（你会非常惊讶地看到，由于使用了闭包和函数变换，许多看似尾递归的代码事实并非尾递归）。\n\n大多数的代码使用简单的循环和状态机会更容易推理，使用尾递归反而可能会使它更加繁琐且难以理解。例如，下面的例子中，命令式的代码比尾递归版本的代码要更加易读：\n\n```scala\n// Tail recursive version.\ndef max(data: Array[Int]): Int = {\n  @tailrec\n  def max0(data: Array[Int], pos: Int, max: Int): Int = {\n    if (pos == data.length) {\n      max\n    } else {\n      max0(data, pos + 1, if (data(pos) > max) data(pos) else max)\n    }\n  }\n  max0(data, 0, Int.MinValue)\n}\n\n// Explicit loop version\ndef max(data: Array[Int]): Int = {\n  var max = Int.MinValue\n  for (v <- data) {\n    if (v > max) {\n      max = v\n    }\n  }\n  max\n}\n```\n\n\n### <a name='implicits'>Implicits</a>\n\n__避免使用 implicit__，除非：\n\n- 你在构建领域特定的语言（DSL）\n- 你在隐式类型参数中使用它（如：`ClassTag`，`TypeTag`）\n- 你在你自己的类中使用它（意指不要污染外部空间），以此减少类型转换的冗余度（如：Scala 闭包到 Java 闭包的转换）。\n\n当使用 implicit 时，我们应该确保另一个工程师可以直接理解使用语义，而无需去阅读隐式定义本身。Implicit 有着非常复杂的解析规则，这会使代码变得极其难以理解。Twitter 的 Effective Scala 指南中写道：「如果你发现你在使用 implicit，始终停下来问一下你自己，是否可以在不使用 implicit 的条件下达到相同的效果」。\n\n如果你必需使用它们（比如：丰富 DSL），那么不要重载隐式方法，即确保每个隐式方法有着不同的名字，这样使用者就可以选择性地导入它们。\n\n```scala\n// 别这么做，这样使用者无法选择性地只导入其中一个方法。\nobject ImplicitHolder {\n  def toRdd(seq: Seq[Int]): RDD[Int] = ...\n  def toRdd(seq: Seq[Long]): RDD[Long] = ...\n}\n\n// 应该将它们定义为不同的名字：\nobject ImplicitHolder {\n  def intSeqToRdd(seq: Seq[Int]): RDD[Int] = ...\n  def longSeqToRdd(seq: Seq[Long]): RDD[Long] = ...\n}\n```\n\n\n## <a name='exception'>异常处理 (Try 还是 try)</a>\n\n- 不要捕获 Throwable 或 Exception 类型的异常。请使用 `scala.util.control.NonFatal`：\n\n  ```scala\n  try {\n    ...\n  } catch {\n    case NonFatal(e) =>\n      // 异常处理；注意 NonFatal 无法匹配 InterruptedException 类型的异常\n    case e: InterruptedException =>\n      // 处理 InterruptedException\n  }\n  ```\n  这能保证我们不会去捕获 `NonLocalReturnControl` 异常（正如在[Return 语句](#return)中所解释的）。\n\n- 不要在 API 中使用 `Try`，即，不要在任何方法中返回 Try。对于异常执行，请显式地抛出异常，并使用 Java 风格的 try/catch 做异常处理。\n\n  背景资料：Scala 提供了单子（monadic）错误处理（通过 `Try`，`Success` 和 `Failure`），这样便于做链式处理。然而，根据我们的经验，发现使用它通常会带来更多的嵌套层级，使得代码难以阅读。此外，对于预期错误还是异常，在语义上常常是不明晰的。因此，我们不鼓励使用 `Try` 来做错误处理，尤其是以下情况：\n\n  一个人为的例子：\n\n  ```scala\n  class UserService {\n    /** Look up a user's profile in the user database. */\n    def get(userId: Int): Try[User]\n  }\n  ```\n  以下的写法会更好：\n\n  ```scala\n  class UserService {\n    /**\n     * Look up a user's profile in the user database.\n     * @return None if the user is not found.\n     * @throws DatabaseConnectionException when we have trouble connecting to the database/\n     */\n    @throws(DatabaseConnectionException)\n    def get(userId: Int): Option[User]\n  }\n  ```\n  第二种写法非常明显地能让调用者知道需要处理哪些错误情况。\n\n\n### <a name='option'>Options</a>\n\n- 如果一个值可能为空，那么请使用 `Option`。相对于 `null`，`Option` 显式地表明了一个 API 的返回值可能为空。\n- 构造 `Option` 值时，请使用 `Option` 而非 `Some`，以防那个值为 `null`。\n\n  ```scala\n  def myMethod1(input: String): Option[String] = Option(transform(input))\n\n  // This is not as robust because transform can return null, and then\n  // myMethod2 will return Some(null).\n  def myMethod2(input: String): Option[String] = Some(transform(input))\n  ```\n- 不要使用 None 来表示异常，有异常时请显式抛出。\n- 不要在一个 `Option` 值上直接调用 `get` 方法，除非你百分百确定那个 `Option` 值不是 `None`。\n\n\n### <a name='chaining'>单子链接</a>\n\n单子链接是 Scala 的一个强大特性。Scala 中几乎一切都是单子（如：集合，Option，Future，Try 等），对它们的操作可以链接在一起。这是一个非常强大的概念，但你应该谨慎使用，尤其是：\n\n- 避免链接（或嵌套）超过 3 个操作。\n- 如果需要花超过 5 秒钟来理解其中的逻辑，那么你应该尽量去想想有没什么办法在不使用单子链接的条件下来达到相同的效果。一般来说，你需要注意的是：不要滥用 `flatMap` 和 `fold`。\n- 链接应该在 flatMap 之后断开（因为类型发生了变化）。\n\n通过给中间结果显式地赋予一个变量名，将链接断开变成一种更加过程化的风格，能让单子链接更加易于理解。来看下面的例子：\n\n```scala\nclass Person(val data: Map[String, String])\nval database = Map[String, Person]\n// Sometimes the client can store \"null\" value in the  store \"address\"\n\n// A monadic chaining approach\ndef getAddress(name: String): Option[String] = {\n  database.get(name).flatMap { elem =>\n    elem.data.get(\"address\")\n      .flatMap(Option.apply)  // handle null value\n  }\n}\n\n// 尽管代码会长一些，但以下方法可读性更高\ndef getAddress(name: String): Option[String] = {\n  if (!database.contains(name)) {\n    return None\n  }\n\n  database(name).data.get(\"address\") match {\n    case Some(null) => None  // handle null value\n    case Some(addr) => Option(addr)\n    case None => None\n  }\n}\n\n```\n\n\n## <a name='concurrency'>并发</a>\n\n### <a name='concurrency-scala-collection'>Scala concurrent.Map</a>\n\n__优先考虑使用 `java.util.concurrent.ConcurrentHashMap` 而非 `scala.collection.concurrent.Map`__。尤其是 `scala.collection.concurrent.Map` 中的 `getOrElseUpdate` 方法要慎用，它并非原子操作（这个问题在 Scala 2.11.16 中 fix 了：[SI-7943](https://issues.scala-lang.org/browse/SI-7943)）。由于我们做的所有项目都需要在 Scala 2.10 和 Scala 2.11 上使用，因此要避免使用 `scala.collection.concurrent.Map`。\n\n\n### <a name='concurrency-sync-vs-map'>显式同步 vs 并发集合</a>\n\n有 3 种推荐的方法来安全地并发访问共享状态。__不要混用它们__，因为这会使程序变得难以推理，并且可能导致死锁。\n\n- `java.util.concurrent.ConcurrentHashMap`：当所有的状态都存储在一个 map 中，并且有高程度的竞争时使用。\n\n  ```scala\n  private[this] val map = new java.util.concurrent.ConcurrentHashMap[String, String]\n  ```\n\n- `java.util.Collections.synchronizedMap`：使用情景：当所有状态都存储在一个 map 中，并且预期不存在竞争情况，但你仍想确保代码在并发下是安全的。如果没有竞争出现，JVM 的 JIT 编译器能够通过偏置锁（biased locking）移除同步开销。\n\n  ```scala\n  private[this] val map = java.util.Collections.synchronizedMap(new java.util.HashMap[String, String])\n  ```\n\n- 通过同步所有临界区进行显式同步，可用于监视多个变量。与 2 相似，JVM 的 JIT 编译器能够通过偏置锁（biased locking）移除同步开销。\n\n  ```scala\n  class Manager {\n    private[this] var count = 0\n    private[this] val map = new java.util.HashMap[String, String]\n    def update(key: String, value: String): Unit = synchronized {\n      map.put(key, value)\n      count += 1\n    }\n    def getCount: Int = synchronized { count }\n  }\n  ```\n\n注意，对于 case 1 和 case 2，不要让集合的视图或迭代器从保护区域逃逸。这可能会以一种不明显的方式发生，比如：返回了 `Map.keySet` 或 `Map.values`。如果需要传递集合的视图或值，生成一份数据拷贝再传递。\n\n  ```scala\n  val map = java.util.Collections.synchronizedMap(new java.util.HashMap[String, String])\n\n  // This is broken!\n  def values: Iterable[String] = map.values\n\n  // Instead, copy the elements\n  def values: Iterable[String] = map.synchronized { Seq(map.values: _*) }\n  ```\n\n### <a name='concurrency-sync-vs-atomic'>显式同步 vs 原子变量 vs @volatile</a>\n\n`java.util.concurrent.atomic` 包提供了对基本类型的无锁访问，比如：`AtomicBoolean`, `AtomicInteger` 和 `AtomicReference`。\n\n始终优先考虑使用原子变量而非 `@volatile`，它们是相关功能的严格超集并且从代码上看更加明显。原子变量的底层实现使用了 `@volatile`。\n\n优先考虑使用原子变量而非显式同步的情况：（1）一个对象的所有临界区更新都被限制在单个变量里并且预期会有竞争情况出现。原子变量是无锁的并且允许更为有效的竞争。（2）同步被明确地表示为 `getAndSet` 操作。例如：\n\n  ```scala\n  // good: 明确又有效地表达了下面的并发代码只执行一次\n  val initialized = new AtomicBoolean(false)\n  ...\n  if (!initialized.getAndSet(true)) {\n    ...\n  }\n\n  // poor: 下面的同步就没那么明晰，而且会出现不必要的同步\n  val initialized = false\n  ...\n  var wasInitialized = false\n  synchronized {\n    wasInitialized = initialized\n    initialized = true\n  }\n  if (!wasInitialized) {\n    ...\n  }\n  ```\n\n### <a name='concurrency-private-this'>私有字段</a>\n\n注意，`private` 字段仍然可以被相同类的其它实例所访问，所以仅仅通过 `this.synchronized`（或 `synchronized`）来保护它从技术上来说是不够的，不过你可以通过 `private[this]` 修饰私有字段来达到目的。\n\n```scala\n// 以下代码仍然是不安全的。\nclass Foo {\n  private var count: Int = 0\n  def inc(): Unit = synchronized { count += 1 }\n}\n\n// 以下代码是安全的。\nclass Foo {\n  private[this] var count: Int = 0\n  def inc(): Unit = synchronized { count += 1 }\n}\n```\n\n\n### <a name='concurrency-isolation'>隔离</a>\n\n一般来说，并发和同步逻辑应该尽可能地被隔离和包含起来。这实际上意味着：\n\n- 避免在 API 层面、面向用户的方法以及回调中暴露同步原语。\n- 对于复杂模块，创建一个小的内部模块来包含并发原语。\n\n\n## <a name='perf'>性能</a>\n\n对于你写的绝大多数代码，性能都不应该成为一个问题。然而，对于一些性能敏感的代码，以下有一些小建议：\n\n### <a name='perf-microbenchmarks'>Microbenchmarks</a>\n\n由于 Scala 编译器和 JVM JIT 编译器会对你的代码做许多神奇的事情，因此要写出一个好的微基准程序（microbenchmark）是极其困难的。更多的情况往往是你的微基准程序并没有测量你想要测量的东西。\n\n如果你要写一个微基准程序，请使用 [jmh](http://openjdk.java.net/projects/code-tools/jmh/)。请确保你阅读了[所有的样例](http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/)，这样你才理解微基准程序中「死代码」移除、常量折叠以及循环展开的效果。\n\n\n### <a name='perf-whileloops'>Traversal 与 zipWithIndex</a>\n\n使用 `while` 循环而非 `for` 循环或函数变换（如：`map`、`foreach`），for 循环和函数变换非常慢（由于虚函数调用和装箱的缘故）。\n\n```scala\n\nval arr = // array of ints\n// 偶数位置的数置零\nval newArr = list.zipWithIndex.map { case (elem, i) =>\n  if (i % 2 == 0) 0 else elem\n}\n\n// 这是上面代码的高性能版本\nval newArr = new Array[Int](arr.length)\nvar i = 0\nval len = newArr.length\nwhile (i < len) {\n  newArr(i) = if (i % 2 == 0) 0 else arr(i)\n  i += 1\n}\n```\n\n### <a name='perf-option'>Option 与 null</a>\n\n对于性能有要求的代码，优先考虑使用 `null` 而不是 `Option`，以此避免虚函数调用以及装箱操作。用 Nullable 注解明确标示出可能为 `null` 的值。\n\n```scala\nclass Foo {\n  @javax.annotation.Nullable\n  private[this] var nullableField: Bar = _\n}\n```\n\n### <a name='perf-collection'>Scala 集合库</a>\n\n对于性能有要求的代码，优先考虑使用 Java 集合库而非 Scala 集合库，因为一般来说，Scala 集合库要比 Java 的集合库慢。\n\n### <a name='perf-private'>private[this]</a>\n\n对于性能有要求的代码，优先考虑使用 `private[this]` 而非 `private`。`private[this]` 生成一个字段而非生成一个访问方法。根据我们的经验，JVM JIT 编译器并不总是会内联 `private` 字段的访问方法，因此通过使用\n`private[this]` 来确保没有虚函数调用会更保险。\n\n```scala\nclass MyClass {\n  private val field1 = ...\n  private[this] val field2 = ...\n\n  def perfSensitiveMethod(): Unit = {\n    var i = 0\n    while (i < 1000000) {\n      field1  // This might invoke a virtual method call\n      field2  // This is just a field access\n      i += 1\n    }\n  }\n}\n```\n\n\n## <a name='java'>与 Java 的互操作性</a>\n\n本节内容介绍的是构建 Java 兼容 API 的准则。如果你构建的组件并不需要与 Java 有交互，那么请无视这一节。这一节的内容主要是从我们开发 Spark 的 Java API 的经历中得出的。\n\n\n### <a name='java-missing-features'>Scala 中缺失的 Java 特性</a>\n\n以下的 Java 特性在 Scala 中是没有的，如果你需要使用以下特性，请在 Java 中定义它们。然而，需要提醒一点的是，你无法为 Java 源文件生成 ScalaDoc。\n\n- 静态字段\n- 静态内部类\n- Java 枚举\n- 注解\n\n\n### <a name='java-traits'>Traits 与抽象类</a>\n\n对于允许从外部实现的接口，请记住以下几点：\n\n- 包含了默认方法实现的 trait 是无法在 Java 中使用的，请使用抽象类来代替。\n- 一般情况下，请避免使用 trait，除非你百分百确定这个接口即使在未来也不会有默认的方法实现。\n\n```scala\n// 以下默认实现无法在 Java 中使用\ntrait Listener {\n  def onTermination(): Unit = { ... }\n}\n\n// 可以在 Java 中使用\nabstract class Listener {\n  def onTermination(): Unit = { ... }\n}\n```\n\n\n### <a name='java-type-alias'>类型别名</a>\n\n不要使用类型别名，它们在字节码和 Java 中是不可见的。\n\n\n### <a name='java-default-param-values'>默认参数值</a>\n\n不要使用默认参数值，通过重载方法来代替。\n\n```scala\n// 打破了与 Java 的互操作性\ndef sample(ratio: Double, withReplacement: Boolean = false): RDD[T] = { ... }\n\n// 以下方法是 work 的\ndef sample(ratio: Double, withReplacement: Boolean): RDD[T] = { ... }\ndef sample(ratio: Double): RDD[T] = sample(ratio, withReplacement = false)\n```\n\n### <a name='java-multi-param-list'>多参数列表</a>\n\n不要使用多参数列表。\n\n### <a name='java-varargs'>可变参数</a>\n\n- 为可变参数方法添加 `@scala.annotation.varargs` 注解，以确保它能在 Java 中使用。Scala 编译器会生成两个方法，一个给 Scala 使用（字节码参数是一个 Seq），另一个给 Java 使用（字节码参数是一个数组）。\n\n  ```scala\n  @scala.annotation.varargs\n  def select(exprs: Expression*): DataFrame = { ... }\n  ```\n\n- 需要注意的一点是，由于 Scala 编译器的一个 bug（[SI-1459](https://issues.scala-lang.org/browse/SI-1459)，[SI-9013](https://issues.scala-lang.org/browse/SI-9013)），抽象的变参方法是无法在 Java 中使用的。\n\n- 重载变参方法时要小心，用另一个类型去重载变参方法会破坏源码的兼容性。\n\n  ```scala\n  class Database {\n    @scala.annotation.varargs\n    def remove(elems: String*): Unit = ...\n\n    // 当调用无参的 remove 方法时会出问题。\n    @scala.annotation.varargs\n    def remove(elems: People*): Unit = ...\n  }\n\n  // remove 方法有歧义，因此编译不过。\n  new Database().remove()\n  ```\n  一种解决方法是，在可变参数前显式地定义第一个参数：\n\n  ```scala\n  class Database {\n    @scala.annotation.varargs\n    def remove(elems: String*): Unit = ...\n\n    // 以下重载是 OK 的。\n    @scala.annotation.varargs\n    def remove(elem: People, elems: People*): Unit = ...\n  }\n  ```\n\n\n### <a name='java-implicits'>Implicits</a>\n\n不要为类或方法使用 implicit，包括了不要使用 `ClassTag` 和 `TypeTag`。\n\n```scala\nclass JavaFriendlyAPI {\n  // 以下定义对 Java 是不友好的，因为方法中包含了一个隐式参数（ClassTag）。\n  def convertTo[T: ClassTag](): T\n}\n```\n\n### <a name='java-companion-object'>伴生对象，静态方法与字段</a>\n\n当涉及到伴生对象和静态方法/字段时，有几件事情是需要注意的：\n\n- 伴生对象在 Java 中的使用是非常别扭的（伴生对象 `Foo` 会被定义为 `Foo$` 类内的一个类型为 `Foo$` 的静态字段 `MODULE$`）。\n\n  ```scala\n  object Foo\n\n  // 等价于以下的 Java 代码\n  public class Foo$ {\n    Foo$ MODULE$ = // 对象的实例化\n  }\n  ```\n  如果非要使用伴生对象，可以在一个单独的类中创建一个 Java 静态字段。\n\n- 不幸的是，没有办法在 Scala 中定义一个 JVM 静态字段。请创建一个 Java 文件来定义它。\n- 伴生对象里的方法会被自动转成伴生类里的静态方法，除非方法名有冲突。确保静态方法正确生成的最好方式是用 Java 写一个测试文件，然后调用生成的静态方法。\n\n  ```scala\n  class Foo {\n    def method2(): Unit = { ... }\n  }\n\n  object Foo {\n    def method1(): Unit = { ... }  // 静态方法 Foo.method1 会被创建（字节码）\n    def method2(): Unit = { ... }  // 静态方法 Foo.method2 不会被创建\n  }\n\n  // FooJavaTest.java (in test/scala/com/databricks/...)\n  public class FooJavaTest {\n    public static void compileTest() {\n      Foo.method1();  // 正常编译\n      Foo.method2();  // 编译失败，因为 method2 并没有生成\n    }\n  }\n  ```\n\n- 样例对象（case object） MyClass 的类型并不是 MyClass。\n\n  ```scala\n  case object MyClass\n\n  // Test.java\n  if (MyClass$.MODULE instanceof MyClass) {\n    // 上述条件始终为 false\n  }\n  ```\n  要实现正确的类型层级结构，请定义一个伴生类，然后用一个样例对象去继承它：\n\n  ```scala\n  class MyClass\n  case object MyClass extends MyClass\n  ```\n\n\n## <a name='misc'>其它</a>\n\n### <a name='misc_currentTimeMillis_vs_nanoTime'>优先使用 nanoTime 而非 currentTimeMillis</a>\n\n当要计算*持续时间*或者检查*超时*的时候，避免使用 `System.currentTimeMillis()`。请使用 `System.nanoTime()`，即使你对亚毫秒级的精度并不感兴趣。\n\n`System.currentTimeMillis()` 返回的是当前的时钟时间，并且会跟进系统时钟的改变。因此，负的时钟调整可能会导致超时而挂起很长一段时间（直到时钟时间赶上先前的值）。这种情况可能发生在网络已经中断一段时间，ntpd 走过了一步之后。最典型的例子是，在系统启动的过程中，DHCP 花费的时间要比平常的长。这可能会导致非常难以理解且难以重现的问题。而 `System.nanoTime()` 则可以保证是单调递增的，与时钟变化无关。\n\n注意事项：\n\n- 永远不要序列化一个绝对的 `nanoTime()` 值或是把它传递给另一个系统。绝对的 `nanoTime()` 值是无意义的、与系统相关的，并且在系统重启时会重置。\n- 绝对的 `nanoTime()` 值并不保证总是正数（但 `t2 - t1` 能确保总是产生正确的值）。\n- `nanoTime()` 每 292 年就会重新计算起。所以，如果你的 Spark 任务需要花非常非常非常长的时间，你可能需要别的东西来处理了：）\n\n\n### <a name='misc_uri_url'>优先使用 URI 而非 URL</a>\n\n当存储服务的 URL 时，你应当使用 `URI` 来表示。\n\n`URL` 的[相等性检查](http://docs.oracle.com/javase/7/docs/api/java/net/URL.html#equals(java.lang.Object))实际上执行了一次网络调用（这是阻塞的）来解析 IP 地址。`URI` 类在表示能力上是 `URL` 的超集，并且它执行的是字段的相等性检查。\n\n## <a name='unit-test'>单元测试</a>\n\n### <a name='unit-test-framework'>单元测试框架</a>\nScalaTest几乎已经成为Scala语言默认的测试框架，这主要源于它提供了多种表达力超强的测试风格，能够满足各种层次的需求包括单元测试、BDD、验收测试、数据驱动测试。我们也使用[ScalaTest](http://www.scalatest.org/)测试框架。使用的时候在`pom.xml`中添加如下类似引用：\n\n``` xml\n<!-- test: https://mvnrepository.com/artifact/org.scalatest/scalatest_2.10 -->\n        <dependency>\n            <groupId>org.scalatest</groupId>\n            <artifactId>scalatest_2.10</artifactId>\n            <version>3.0.0</version>\n            <scope>test</scope>\n        </dependency>\n```\n测试我们遵循以下几点规则：\n- 测试类应该与被测试类处于同一包下，测试类的命名为：被测试类名 + Test\n- 测试含有具体实现的trait时，可以让被测试类直接继承Trait。例如：\n\n``` scala\ntrait RecordsGenerator {\n     def generateRecords(table: List[List[String]]): List[Record] {\n          //...\n     }\n}\n\nclass RecordsGeneratorSpec extends FlatSpec with ShouldMatcher with RecordGenerator {\n     val table = List(List(\"abc\", \"def\"), List(\"aaa\", \"bbb\"))\n     it should \"generate records\" in {\n          val records = generateRecords(table)\n          records.size should be(2)\n     }\n}\n```\n\n- 若要对文件进行测试，可以用字符串假装文件：\n\n``` scala\ntype CsvLine = String\ndef formatCsv(source: Source): List[CsvLine] = {\n     source.getLines(_.replace(\", \", \"|\"))\n}\n```\nformatCsv需要接受一个文件源，例如Source.fromFile(\"testdata.txt\")。但在测试时，可以通过Source.fromString方法来生成formatCsv需要接收的Source对象：\n``` scala\ntest(\"format csv lines\") {\n     val lines = Source.fromString(\"abc, def, hgi\\n1, 2, 3\\none, two, three\")\n     val result = formatCsv(lines)\n     assert(result.mkString(\"\\n\").equles(\"abc|def|hgi\\n1|2|3\\none|two|three\"))\n}\n```\n\n### <a name='unit-test-style'>测试风格的选择</a>\n\nScalaTest一共提供了七种测试风格，分别为：FunSuite，FlatSpec，FunSpec，WordSpec，FreeSpec，PropSpec和FeatureSpec。这就好像使用相同的原料做成不同美味乃至不同菜系的佳肴，你可以根据自己的口味进行选择。我们统一推荐使用FunSuite的方式，因为它更灵活，而且更符合传统测试方法的风格，区别仅在于test()方法可以接受一个闭包:\n\n``` scala\nimport org.scalatest.FunSuite\n\nclass SetSuite extends FunSuite {\n\n  test(\"An empty Set should have size 0\") {\n    assert(Set.empty.size == 0)\n  }\n\n  test(\"Invoking head on an empty Set should produce NoSuchElementException\") {\n    assertThrows[NoSuchElementException] {\n      Set.empty.head\n    }\n  }\n}\n```\n当然，如果你有必须的理由选择其它测试风格的话，本规则并不强制。\n\n# 三、 引用\n\n[Databricks Scala 编程风格指南](https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md)(团队最终选择的模板)\n[Effective Scala](http://twitter.github.io/effectivescala/index-cn.html)(Twitter Scala资料，值得参考)\n[Thinking in Scala--Scala编程规范](https://zhangyi.gitbooks.io/thinking-in-scala/content/scala-convention.html)(个人整理，可以参考)\n[scala-lang--Scala语言规范.pdf](http://www.scala-lang.org/docu/files/Scala%E8%AF%AD%E8%A8%80%E8%A7%84%E8%8C%83.pdf)(官方原版的中文文档，共127页，过于复杂琐碎，可以参考)\n[Scala官网Style Guide](http://docs.scala-lang.org/style/)(官方原版，其它版本基本上都是基于此版进行的改进)\n[分析GitHub上托管的scala开源代码统计](https://segmentfault.com/a/1190000000420018)","slug":"大数据团队scala代码规范","published":1,"updated":"2017-01-09T10:10:35.793Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfwt002fpsgu9umi85gb","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><h3 id=\"一、-scala代码规范\"><a href=\"#一、-scala代码规范\" class=\"headerlink\" title=\"一、 scala代码规范\"></a>一、 scala代码规范</h3><p>Scala 是一种强大到令人难以置信的多范式编程语言。目前我们团队有很多项目需要使用scala语言进行编程，尤其是Spark相关开发项目，为了能够统一scala相关开发，本人基于Spark 贡献者及 <a href=\"http://databricks.com/\" target=\"_blank\" rel=\"external\">Databricks</a> 工程团队总结出了以下指南，本人增加了单元测试的规范，希望对大家的开发有帮助。当然，这个指南并非绝对，根据我们团队需求与实际经验，持续更新。</p>\n<h3 id=\"二、-选择此规范的理由\"><a href=\"#二、-选择此规范的理由\" class=\"headerlink\" title=\"二、 选择此规范的理由\"></a>二、 选择此规范的理由</h3><p>很早就有大家统一编程规范的想法，网上也有一些关于编程规范的文档供参考。最终选择了以 Databricks 公司的编程规范为模板制作。理由如下：</p>\n<ul>\n<li>目前公司使用scala语言主要用在Spark的开发，而这份指南是Spark 贡献者及 Databricks 工程团队一起工作时总结出来的；</li>\n<li>跟我们之前写得代码，以及IDEA等编译器自动格式化相差不大；</li>\n<li>这份指南经过多次的修改和总结，经过了实践的检验；</li>\n<li>这份指南相比较而言简明概要，易于理解，遵循 Java 的地方就没有赘述，比较解耦，适用于根据公司代码体系来修改；</li>\n</ul>\n<h2 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a><a name=\"TOC\">目录</a></h2><ol>\n<li><a href=\"#history\">文档历史</a></li>\n<li><a href=\"#syntactic\">语法风格</a><ul>\n<li><a href=\"#naming\">命名约定</a></li>\n<li><a href=\"#variable-naming\">变量命名约定</a></li>\n<li><a href=\"#linelength\">一行长度</a></li>\n<li><a href=\"#rule_of_30\">30 法则</a></li>\n<li><a href=\"#indent\">空格与缩进</a></li>\n<li><a href=\"#blanklines\">空行</a></li>\n<li><a href=\"#parentheses\">括号</a></li>\n<li><a href=\"#curly\">大括号</a></li>\n<li><a href=\"#long_literal\">长整型字面量</a></li>\n<li><a href=\"#doc\">文档风格</a></li>\n<li><a href=\"#ordering_class\">类内秩序</a></li>\n<li><a href=\"#imports\">Imports</a></li>\n<li><a href=\"#pattern-matching\">模式匹配</a></li>\n<li><a href=\"#infix\">中缀方法</a></li>\n<li><a href=\"#anonymous\">匿名方法</a></li>\n</ul>\n</li>\n<li><a href=\"#lang\">Scala 语言特性</a><ul>\n<li><a href=\"#case_class_immutability\">样例类与不可变性</a></li>\n<li><a href=\"#apply_method\">apply 方法</a></li>\n<li><a href=\"#override_modifier\">override 修饰符</a></li>\n<li><a href=\"#destruct_bind\">解构绑定</a></li>\n<li><a href=\"#call_by_name\">按名称传参</a></li>\n<li><a href=\"#multi-param-list\">多参数列表</a></li>\n<li><a href=\"#symbolic_methods\">符号方法 (运算符重载)</a></li>\n<li><a href=\"#type_inference\">类型推导</a></li>\n<li><a href=\"#return\">Return 语句</a></li>\n<li><a href=\"#recursion\">递归及尾递归</a></li>\n<li><a href=\"#implicits\">Implicits</a></li>\n<li><a href=\"#exception\">异常处理 (Try 还是 try)</a></li>\n<li><a href=\"#option\">Options</a></li>\n<li><a href=\"#chaining\">单子链接</a></li>\n</ul>\n</li>\n<li><a href=\"#concurrency\">并发</a><ul>\n<li><a href=\"#concurrency-scala-collection\">Scala concurrent.Map</a></li>\n<li><a href=\"#concurrency-sync-vs-map\">显式同步 vs 并发集合</a></li>\n<li><a href=\"#concurrency-sync-vs-atomic\">显式同步 vs 原子变量 vs @volatile</a></li>\n<li><a href=\"#concurrency-private-this\">私有字段</a></li>\n<li><a href=\"#concurrency-isolation\">隔离</a></li>\n</ul>\n</li>\n<li><a href=\"#perf\">性能</a><ul>\n<li><a href=\"#perf-microbenchmarks\">Microbenchmarks</a></li>\n<li><a href=\"#perf-whileloops\">Traversal 与 zipWithIndex</a></li>\n<li><a href=\"#perf-option\">Option 与 null</a></li>\n<li><a href=\"#perf-collection\">Scala 集合库</a></li>\n<li><a href=\"#perf-private\">private[this]</a></li>\n</ul>\n</li>\n<li><a href=\"#java\">与 Java 的互操作性</a><ul>\n<li><a href=\"#java-missing-features\">Scala 中缺失的 Java 特性</a></li>\n<li><a href=\"#java-traits\">Traits 与抽象类</a></li>\n<li><a href=\"#java-type-alias\">类型别名</a></li>\n<li><a href=\"#java-default-param-values\">默认参数值</a></li>\n<li><a href=\"#java-multi-param-list\">多参数列表</a></li>\n<li><a href=\"#java-varargs\">可变参数</a></li>\n<li><a href=\"#java-implicits\">Implicits</a></li>\n<li><a href=\"#java-companion-object\">伴生对象, 静态方法与字段</a></li>\n</ul>\n</li>\n<li><a href=\"#misc\">其它</a><ul>\n<li><a href=\"#misc_currentTimeMillis_vs_nanoTime\">优先使用 nanoTime 而非 currentTimeMillis</a></li>\n<li><a href=\"#misc_uri_url\">优先使用 URI 而非 URL</a></li>\n</ul>\n</li>\n<li><a href=\"#unit-test\">单元测试</a><ul>\n<li><a href=\"#unit-test-framework\">单元测试框架</a></li>\n<li><a href=\"#unit-test-style\">单元测试风格</a></li>\n</ul>\n</li>\n</ol>\n<h2 id=\"文档历史\"><a href=\"#文档历史\" class=\"headerlink\" title=\"文档历史\"></a><a name=\"history\">文档历史</a></h2><ul>\n<li>2017-01-10: 最初版本。<a href=\"https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md\" target=\"_blank\" rel=\"external\">Databricks版本</a></li>\n<li>2017-01-11: 增加 <a href=\"#unit-test\">单元测试</a> 一节。</li>\n</ul>\n<h2 id=\"语法风格\"><a href=\"#语法风格\" class=\"headerlink\" title=\"语法风格\"></a><a name=\"syntactic\">语法风格</a></h2><h3 id=\"命名约定\"><a href=\"#命名约定\" class=\"headerlink\" title=\"命名约定\"></a><a name=\"naming\">命名约定</a></h3><p>我们主要遵循 Java 和 Scala 的标准命名约定。</p>\n<ul>\n<li><p>类，trait, 对象应该遵循 Java 中类的命名约定，即 PascalCase 风格。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ClusterManager</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Expression</span></span></div></pre></td></tr></table></figure>\n</li>\n<li><p>包名应该遵循 Java 中包名的命名约定，即使用全小写的 ASCII 字母。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.databricks.resourcemanager</div></pre></td></tr></table></figure>\n</li>\n<li><p>方法/函数应当使用驼峰式风格命名。</p>\n</li>\n<li><p>常量命名使用全大写字母，并将它们放在伴生对象中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Configuration</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">DEFAULT_PORT</span> = <span class=\"number\">10000</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>枚举命名与类命名一致，使用 PascalCase 风格。</p>\n</li>\n<li><p>注解也应遵循 Java 中的约定，即使用 PascalCase 风格。注意，这一点与 Scala 的官方指南不同。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyAnnotation</span> <span class=\"keyword\">extends</span> <span class=\"title\">StaticAnnotation</span></span></div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"变量命名约定\"><a href=\"#变量命名约定\" class=\"headerlink\" title=\"变量命名约定\"></a><a name=\"variable-naming\">变量命名约定</a></h3><ul>\n<li><p>变量命名应当遵循驼峰式命名方法，并且变量名应当是不言而喻的，即变量名可以直观地反应它的涵义。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> serverPort = <span class=\"number\">1000</span></div><div class=\"line\"><span class=\"keyword\">val</span> clientPort = <span class=\"number\">2000</span></div></pre></td></tr></table></figure>\n</li>\n<li><p>可以在小段的局部代码中使用单字符的变量名，比如在小段的循环体中（例如 10 行以内的代码），“i” 常常被用作循环索引。然而，即使在小段的代码中，也不要使用 “l” （Larry 中的 l）作为标识符，因为它看起来和 “1”，“|”，“I” 很像，难以区分，容易搞错。</p>\n</li>\n</ul>\n<h3 id=\"一行长度\"><a href=\"#一行长度\" class=\"headerlink\" title=\"一行长度\"></a><a name=\"linelength\">一行长度</a></h3><ul>\n<li>一行长度的上限是 100 个字符。</li>\n<li>唯一的例外是 import 语句和 URL (即便如此，也尽量将它们保持在 100 个字符以下)。</li>\n</ul>\n<h3 id=\"30-法则\"><a href=\"#30-法则\" class=\"headerlink\" title=\"30 法则\"></a><a name=\"rule_of_30\">30 法则</a></h3><p>「如果一个元素包含的子元素超过 30 个，那么极有可能出现了严重的问题」 - <a href=\"http://www.amazon.com/Refactoring-Large-Software-Projects-Restructurings/dp/0470858923\" target=\"_blank\" rel=\"external\">Refactoring in Large Software Projects</a>。</p>\n<p>一般来说:</p>\n<ul>\n<li>一个方法包含的代码行数不宜超过 30 行。</li>\n<li>一个类包含的方法数量不宜超过 30 个。</li>\n</ul>\n<h3 id=\"空格与缩进\"><a href=\"#空格与缩进\" class=\"headerlink\" title=\"空格与缩进\"></a><a name=\"indent\">空格与缩进</a></h3><ul>\n<li><p>一般情况下，使用两个空格的缩进。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"literal\">true</span>) &#123;</div><div class=\"line\">  println(<span class=\"string\">\"Wow!\"</span>)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>对于方法声明，如果一行无法容纳下所有的参数，那么使用 4 个空格来缩进它们。返回类型可以与最后一个参数在同一行，也可以放在下一行，使用两个空格缩进。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">newAPIHadoopFile</span></span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">F</span> &lt;: <span class=\"type\">NewInputFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]](</div><div class=\"line\">    path: <span class=\"type\">String</span>,</div><div class=\"line\">    fClass: <span class=\"type\">Class</span>[<span class=\"type\">F</span>],</div><div class=\"line\">    kClass: <span class=\"type\">Class</span>[<span class=\"type\">K</span>],</div><div class=\"line\">    vClass: <span class=\"type\">Class</span>[<span class=\"type\">V</span>],</div><div class=\"line\">    conf: <span class=\"type\">Configuration</span> = hadoopConfiguration): <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</div><div class=\"line\">  <span class=\"comment\">// method body</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">newAPIHadoopFile</span></span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">F</span> &lt;: <span class=\"type\">NewInputFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]](</div><div class=\"line\">    path: <span class=\"type\">String</span>,</div><div class=\"line\">    fClass: <span class=\"type\">Class</span>[<span class=\"type\">F</span>],</div><div class=\"line\">    kClass: <span class=\"type\">Class</span>[<span class=\"type\">K</span>],</div><div class=\"line\">    vClass: <span class=\"type\">Class</span>[<span class=\"type\">V</span>],</div><div class=\"line\">    conf: <span class=\"type\">Configuration</span> = hadoopConfiguration)</div><div class=\"line\">  : <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</div><div class=\"line\">  <span class=\"comment\">// method body</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>如果一行无法容纳下类头（即 extends 后面那部分），则把它们放到新的一行，用两个空格缩进，然后在类内空一行再开始函数或字段的定义（或是包的导入）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span>(<span class=\"params\"></span></span></div><div class=\"line\">    val param1: <span class=\"type\">String</span>,  // 4 space indent for parameters</div><div class=\"line\">    val param2: <span class=\"type\">String</span>,</div><div class=\"line\">    val param3: <span class=\"type\">Array</span>[<span class=\"type\">Byte</span>])</div><div class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">FooInterface</span>  <span class=\"comment\">// 2 space here</span></div><div class=\"line\">  <span class=\"keyword\">with</span> <span class=\"type\">Logging</span> &#123;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">firstMethod</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;  <span class=\"comment\">// blank line above</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>不要使用垂直对齐。它使你的注意力放在代码的错误部分并增大了后人修改代码的难度。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Don't align vertically</span></div><div class=\"line\"><span class=\"keyword\">val</span> plus     = <span class=\"string\">\"+\"</span></div><div class=\"line\"><span class=\"keyword\">val</span> minus    = <span class=\"string\">\"-\"</span></div><div class=\"line\"><span class=\"keyword\">val</span> multiply = <span class=\"string\">\"*\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Do the following</span></div><div class=\"line\"><span class=\"keyword\">val</span> plus = <span class=\"string\">\"+\"</span></div><div class=\"line\"><span class=\"keyword\">val</span> minus = <span class=\"string\">\"-\"</span></div><div class=\"line\"><span class=\"keyword\">val</span> multiply = <span class=\"string\">\"*\"</span></div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"空行\"><a href=\"#空行\" class=\"headerlink\" title=\"空行\"></a><a name=\"blanklines\">空行</a></h3><ul>\n<li>一个空行可以出现在：<ul>\n<li>连续的类成员或初始化器（initializers）之间：字段，构造函数，方法，嵌套类，静态初始化器及实例初始化器。<ul>\n<li>例外：连续的两个字段之间的空行是可选的（前提是它们之间没有其它代码），这一类空行主要为这些字段做逻辑上的分组。</li>\n</ul>\n</li>\n<li>在方法体内，根据需要，使用空行来为语句创建逻辑上的分组。</li>\n<li>在类的第一个成员之前或最后一个成员之后，空行都是可选的（既不鼓励也不阻止）。</li>\n</ul>\n</li>\n<li>使用一个或两个空行来分隔不同类的定义。</li>\n<li>不鼓励使用过多的空行。</li>\n</ul>\n<h3 id=\"括号\"><a href=\"#括号\" class=\"headerlink\" title=\"括号\"></a><a name=\"parentheses\">括号</a></h3><ul>\n<li><p>方法声明应该加括号（即使没有参数列表），除非它们是没有副作用（状态改变，IO 操作都认为是有副作用的）的访问器（accessor）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Job</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">// Wrong: killJob changes state. Should have ().</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">killJob</span></span>: <span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// Correct:</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">killJob</span></span>(): <span class=\"type\">Unit</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>函数调用应该与函数声明在形式上保持一致，也就是说，如果一个方法声明时带了括号，那调用时也要把括号带上。注意这不仅仅是语法层面的人为约定，当返回对象中定义了 <code>apply</code> 方法时，这一点还会影响正确性。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(args: <span class=\"type\">String</span>*): <span class=\"type\">Int</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Bar</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foo</span></span>: <span class=\"type\">Foo</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">new</span> <span class=\"type\">Bar</span>().foo  <span class=\"comment\">// This returns a Foo</span></div><div class=\"line\"><span class=\"keyword\">new</span> <span class=\"type\">Bar</span>().foo()  <span class=\"comment\">// This returns an Int!</span></div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"大括号\"><a href=\"#大括号\" class=\"headerlink\" title=\"大括号\"></a><a name=\"curly\">大括号</a></h3><p>即使条件语句或循环语句只有一行时，也请使用大括号。唯一的例外是，当你把 if/else 作为一个单行的三元操作符来使用并且没有副作用时，这时你可以不加大括号。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Correct:</span></div><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"literal\">true</span>) &#123;</div><div class=\"line\">  println(<span class=\"string\">\"Wow!\"</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Correct:</span></div><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"literal\">true</span>) statement1 <span class=\"keyword\">else</span> statement2</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Correct:</span></div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  foo()</div><div class=\"line\">&#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong:</span></div><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"literal\">true</span>)</div><div class=\"line\">  println(<span class=\"string\">\"Wow!\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong:</span></div><div class=\"line\"><span class=\"keyword\">try</span> foo() <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"长整型字面量\"><a href=\"#长整型字面量\" class=\"headerlink\" title=\"长整型字面量\"></a><a name=\"long_literal\">长整型字面量</a></h3><p>长整型字面量使用大写的 <code>L</code> 作为后缀，不要使用小写，因为它和数字 <code>1</code> 长得很像，常常难以区分。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> longValue = <span class=\"number\">5432</span>L  <span class=\"comment\">// Do this</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> longValue = <span class=\"number\">5432</span>l  <span class=\"comment\">// Do NOT do this</span></div></pre></td></tr></table></figure>\n<h3 id=\"文档风格\"><a href=\"#文档风格\" class=\"headerlink\" title=\"文档风格\"></a><a name=\"doc\">文档风格</a></h3><p>使用 Java Doc 风格，而非 Scala Doc 风格。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/** This is a correct one-liner, short description. */</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"> * This is correct multi-line JavaDoc comment. And</div><div class=\"line\"> * this is my second line, and if I keep typing, this would be</div><div class=\"line\"> * my third line.</div><div class=\"line\"> */</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/** In Spark, we don't use the ScalaDoc style so this</span></div><div class=\"line\">  * is not correct.</div><div class=\"line\">  */</div></pre></td></tr></table></figure>\n<h3 id=\"类内秩序\"><a href=\"#类内秩序\" class=\"headerlink\" title=\"类内秩序\"></a><a name=\"ordering_class\">类内秩序</a></h3><p>如果一个类很长，包含许多的方法，那么在逻辑上把它们分成不同的部分并加上注释头，以此组织它们。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DataFrame</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">///////////////////////////////////////////////////////////////////////////</span></div><div class=\"line\">  <span class=\"comment\">// DataFrame operations</span></div><div class=\"line\">  <span class=\"comment\">///////////////////////////////////////////////////////////////////////////</span></div><div class=\"line\"></div><div class=\"line\">  ...</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">///////////////////////////////////////////////////////////////////////////</span></div><div class=\"line\">  <span class=\"comment\">// RDD operations</span></div><div class=\"line\">  <span class=\"comment\">///////////////////////////////////////////////////////////////////////////</span></div><div class=\"line\"></div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>当然，强烈不建议把一个类写得这么长，一般只有在构建某些公共 API 时才允许这么做。</p>\n<h3 id=\"Imports\"><a href=\"#Imports\" class=\"headerlink\" title=\"Imports\"></a><a name=\"imports\">Imports</a></h3><ul>\n<li><strong>导入时避免使用通配符</strong>, 除非你需要导入超过 6 个实体或者隐式方法。通配符导入会使代码在面对外部变化时不够健壮。</li>\n<li>始终使用绝对路径来导入包 (如：<code>scala.util.Random</code>) ，而不是相对路径 (如：<code>util.Random</code>)。</li>\n<li>此外，导入语句按照以下顺序排序：<ul>\n<li><code>java.*</code> 和 <code>javax.*</code></li>\n<li><code>scala.*</code></li>\n<li>第三方库 (<code>org.*</code>, <code>com.*</code>, 等)</li>\n<li>项目中的类 (对于 Spark 项目，即 <code>com.databricks.*</code> 或 <code>org.apache.spark</code>)</li>\n</ul>\n</li>\n<li>在每一组导入语句内，按照字母序进行排序。</li>\n<li><p>你可以使用 IntelliJ 的「import organizer」来自动处理，请使用以下配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">java</div><div class=\"line\">javax</div><div class=\"line\">_______ blank line _______</div><div class=\"line\">scala</div><div class=\"line\">_______ blank line _______</div><div class=\"line\">all other imports</div><div class=\"line\">_______ blank line _______</div><div class=\"line\">com.databricks  // or org.apache.spark if you are working on spark</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"模式匹配\"><a href=\"#模式匹配\" class=\"headerlink\" title=\"模式匹配\"></a><a name=\"pattern-matching\">模式匹配</a></h3><ul>\n<li><p>如果整个方法就是一个模式匹配表达式，可能的话，可以把 match 关键词与方法声明放在同一行，以此减少一级缩进。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test</span></span>(msg: <span class=\"type\">Message</span>): <span class=\"type\">Unit</span> = msg <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>当以闭包形式调用一个函数时，如果只有一个 case 语句，那么把 case 语句与函数调用放在同一行。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">list.zipWithIndex.map &#123; <span class=\"keyword\">case</span> (elem, i) =&gt;</div><div class=\"line\">  <span class=\"comment\">// ...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>如果有多个 case 语句，把它们缩进并且包起来。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">list.map &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> a: <span class=\"type\">Foo</span> =&gt;  ...</div><div class=\"line\">  <span class=\"keyword\">case</span> b: <span class=\"type\">Bar</span> =&gt;  ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>如果唯一的目的就是想匹配某个对象的类型，那么不要展开所有的参数来做模式匹配，这样会使得重构变得更加困难，代码更容易出错。</p>\n</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Pokemon</span>(<span class=\"params\">name: <span class=\"type\">String</span>, weight: <span class=\"type\">Int</span>, hp: <span class=\"type\">Int</span>, attack: <span class=\"type\">Int</span>, defense: <span class=\"type\">Int</span></span>)</span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Human</span>(<span class=\"params\">name: <span class=\"type\">String</span>, hp: <span class=\"type\">Int</span></span>)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 不要像下面那样做，因为</span></div><div class=\"line\"><span class=\"comment\">// 1. 当 pokemon 加入一个新的字段，我们需要改变下面的模式匹配代码</span></div><div class=\"line\"><span class=\"comment\">// 2. 非常容易发生误匹配，尤其是当所有字段的类型都一样的时候</span></div><div class=\"line\">targets.foreach &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> target @ <span class=\"type\">Pokemon</span>(_, _, hp, _, defense) =&gt;</div><div class=\"line\">    <span class=\"keyword\">val</span> loss = sys.min(<span class=\"number\">0</span>, myAttack - defense)</div><div class=\"line\">    target.copy(hp = hp - loss)</div><div class=\"line\">  <span class=\"keyword\">case</span> target @ <span class=\"type\">Human</span>(_, hp) =&gt;</div><div class=\"line\">    target.copy(hp = hp - myAttack)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 像下面这样做就好多了:</span></div><div class=\"line\">targets.foreach &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> target: <span class=\"type\">Pokemon</span> =&gt;</div><div class=\"line\">    <span class=\"keyword\">val</span> loss = sys.min(<span class=\"number\">0</span>, myAttack - target.defense)</div><div class=\"line\">    target.copy(hp = target.hp - loss)</div><div class=\"line\">  <span class=\"keyword\">case</span> target: <span class=\"type\">Human</span> =&gt;</div><div class=\"line\">    target.copy(hp = target.hp - myAttack)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"中缀方法\"><a href=\"#中缀方法\" class=\"headerlink\" title=\"中缀方法\"></a><a name=\"infix\">中缀方法</a></h3><p><strong>避免中缀表示法</strong>，除非是符号方法（即运算符重载）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Correct</span></div><div class=\"line\">list.map(func)</div><div class=\"line\">string.contains(<span class=\"string\">\"foo\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong</span></div><div class=\"line\">list map (func)</div><div class=\"line\">string contains <span class=\"string\">\"foo\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 重载的运算符应该以中缀形式调用</span></div><div class=\"line\">arrayBuffer += elem</div></pre></td></tr></table></figure>\n<h3 id=\"匿名方法\"><a href=\"#匿名方法\" class=\"headerlink\" title=\"匿名方法\"></a><a name=\"anonymous\">匿名方法</a></h3><p>对于匿名方法，<strong>避免使用过多的小括号和花括号</strong>。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Correct</span></div><div class=\"line\">list.map &#123; item =&gt;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Correct</span></div><div class=\"line\">list.map(item =&gt; ...)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong</span></div><div class=\"line\">list.map(item =&gt; &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong</span></div><div class=\"line\">list.map &#123; item =&gt; &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong</span></div><div class=\"line\">list.map(&#123; item =&gt; ... &#125;)</div></pre></td></tr></table></figure>\n<h2 id=\"Scala-语言特性\"><a href=\"#Scala-语言特性\" class=\"headerlink\" title=\"Scala 语言特性\"></a><a name=\"lang\">Scala 语言特性</a></h2><h3 id=\"样例类与不可变性\"><a href=\"#样例类与不可变性\" class=\"headerlink\" title=\"样例类与不可变性\"></a><a name=\"case_class_immutability\">样例类与不可变性</a></h3><p>样例类（case class）本质也是普通的类，编译器会自动地为它加上以下支持：</p>\n<ul>\n<li>构造器参数的公有 getter 方法</li>\n<li>拷贝构造函数</li>\n<li>构造器参数的模式匹配</li>\n<li>默认的 toString/hash/equals 实现</li>\n</ul>\n<p>对于样例类来说，构造器参数不应设为可变的，可以使用拷贝构造函数达到同样的效果。使用可变的样例类容易出错，例如，哈希表中，对象根据旧的哈希值被放在错误的位置上。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// This is OK</span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">name: <span class=\"type\">String</span>, age: <span class=\"type\">Int</span></span>)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// This is NOT OK</span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">name: <span class=\"type\">String</span>, var age: <span class=\"type\">Int</span></span>)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 通过拷贝构造函数创建一个新的实例来改变其中的值</span></div><div class=\"line\"><span class=\"keyword\">val</span> p1 = <span class=\"type\">Person</span>(<span class=\"string\">\"Peter\"</span>, <span class=\"number\">15</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> p2 = p2.copy(age = <span class=\"number\">16</span>)</div></pre></td></tr></table></figure>\n<h3 id=\"apply-方法\"><a href=\"#apply-方法\" class=\"headerlink\" title=\"apply 方法\"></a><a name=\"apply_method\">apply 方法</a></h3><p>避免在类里定义 apply 方法。这些方法往往会使代码的可读性变差，尤其是对于不熟悉 Scala 的人。它也难以被 IDE（或 grep）所跟踪。在最坏的情况下，它还可能影响代码的正确性，正如你在<a href=\"#parentheses\">括号</a>一节中看到的。</p>\n<p>然而，将 apply 方法作为工厂方法定义在伴生对象中是可以接受的。在这种情况下，apply 方法应该返回其伴生类的类型。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">TreeNode</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">// 下面这种定义是 OK 的</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(name: <span class=\"type\">String</span>): <span class=\"type\">TreeNode</span> = ...</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 不要像下面那样定义，因为它没有返回其伴生类的类型：TreeNode</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(name: <span class=\"type\">String</span>): <span class=\"type\">String</span> = ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"override-修饰符\"><a href=\"#override-修饰符\" class=\"headerlink\" title=\"override 修饰符\"></a><a name=\"override_modifier\">override 修饰符</a></h3><p>无论是覆盖具体的方法还是实现抽象的方法，始终都为方法加上 override 修饰符。实现抽象方法时，不加 override 修饰符，Scala 编译器也不会报错。即便如此，我们也应该始终把 override 修饰符加上，以此显式地表示覆盖行为。以此避免由于方法签名不同（而你也难以发现）而导致没有覆盖到本应覆盖的方法。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Parent</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hello</span></span>(data: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    print(data)</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Child</span> <span class=\"keyword\">extends</span> <span class=\"title\">Parent</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">import</span> scala.collection.<span class=\"type\">Map</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 下面的方法没有覆盖 Parent.hello,</span></div><div class=\"line\">  <span class=\"comment\">// 因为两个 Map 的类型是不同的。</span></div><div class=\"line\">  <span class=\"comment\">// 如果我们加上 override 修饰符，编译器就会帮你找出问题并报错。</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hello</span></span>(data: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    print(<span class=\"string\">\"This is supposed to override the parent method, but it is actually not!\"</span>)</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"解构绑定\"><a href=\"#解构绑定\" class=\"headerlink\" title=\"解构绑定\"></a><a name=\"destruct_bind\">解构绑定</a></h3><p>解构绑定（有时也叫元组提取）是一种在一个表达式中为两个变量赋值的便捷方式。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> (a, b) = (<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div></pre></td></tr></table></figure>\n<p>然而，请不要在构造函数中使用它们，尤其是当 <code>a</code> 和 <code>b</code> 需要被标记为 <code>transient</code> 的时候。Scala 编译器会产生一个额外的 Tuple2 字段，而它并不是暂态的（transient）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyClass</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">// 以下代码无法 work，因为编译器会产生一个非暂态的 Tuple2 指向 a 和 b</span></div><div class=\"line\">  <span class=\"meta\">@transient</span> <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> (a, b) = someFuncThatReturnsTuple2()</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"按名称传参\"><a href=\"#按名称传参\" class=\"headerlink\" title=\"按名称传参\"></a><a name=\"call_by_name\">按名称传参</a></h3><p><strong>避免使用按名传参</strong>. 显式地使用 <code>() =&gt; T</code> 。</p>\n<p>背景：Scala 允许按名称来定义方法参数，例如：以下例子是可以成功执行的：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">print</span></span>(value: =&gt; <span class=\"type\">Int</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">  println(value)</div><div class=\"line\">  println(value + <span class=\"number\">1</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">var</span> a = <span class=\"number\">0</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">inc</span></span>(): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">  a += <span class=\"number\">1</span></div><div class=\"line\">  a</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">print(inc())</div></pre></td></tr></table></figure>\n<p>在上面的代码中，<code>inc()</code> 以闭包的形式传递给 <code>print</code> 函数，并且在 <code>print</code> 函数中被执行了两次，而不是以数值 <code>1</code> 传入。按名传参的一个主要问题是在方法调用处，我们无法区分是按名传参还是按值传参。因此无法确切地知道这个表达式是否会被执行（更糟糕的是它可能会被执行多次）。对于带有副作用的表达式来说，这一点是非常危险的。</p>\n<h3 id=\"多参数列表\"><a href=\"#多参数列表\" class=\"headerlink\" title=\"多参数列表\"></a><a name=\"multi-param-list\">多参数列表</a></h3><p><strong>避免使用多参数列表</strong>。它们使运算符重载变得复杂，并且会使不熟悉 Scala 的程序员感到困惑。例如：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Avoid this!</span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">name: <span class=\"type\">String</span>, age: <span class=\"type\">Int</span></span>)(<span class=\"params\">secret: <span class=\"type\">String</span></span>)</span></div></pre></td></tr></table></figure>\n<p>一个值得注意的例外是，当在定义底层库时，可以使用第二个参数列表来存放隐式（implicit）参数。尽管如此，<a href=\"#implicits\">我们应该避免使用 implicits</a>！</p>\n<h3 id=\"符号方法（运算符重载）\"><a href=\"#符号方法（运算符重载）\" class=\"headerlink\" title=\"符号方法（运算符重载）\"></a><a name=\"symbolic_methods\">符号方法（运算符重载）</a></h3><p><strong>不要使用符号作为方法名</strong>，除非你是在定义算术运算的方法（如：<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>），否则在任何其它情况下，都不要使用。符号化的方法名让人难以理解方法的意图是什么，来看下面两个例子：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 符号化的方法名难以理解</span></div><div class=\"line\">channel ! msg</div><div class=\"line\">stream1 &gt;&gt;= stream2</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 下面的方法意图则不言而喻</span></div><div class=\"line\">channel.send(msg)</div><div class=\"line\">stream1.join(stream2)</div></pre></td></tr></table></figure>\n<h3 id=\"类型推导\"><a href=\"#类型推导\" class=\"headerlink\" title=\"类型推导\"></a><a name=\"type_inference\">类型推导</a></h3><p>Scala 的类型推导，尤其是左侧类型推导以及闭包推导，可以使代码变得更加简洁。尽管如此，也有一些情况我们是需要显式地声明类型的：</p>\n<ul>\n<li><strong>公有方法应该显式地声明类型</strong>，编译器推导出来的类型往往会使你大吃一惊。</li>\n<li><strong>隐式方法应该显式地声明类型</strong>，否则在增量编译时，它会使 Scala 编译器崩溃。</li>\n<li><strong>如果变量或闭包的类型并非显而易见，请显式声明类型</strong>。一个不错的判断准则是，如果评审代码的人无法在 3 秒内确定相应实体的类型，那么你就应该显式地声明类型。</li>\n</ul>\n<h3 id=\"Return-语句\"><a href=\"#Return-语句\" class=\"headerlink\" title=\"Return 语句\"></a><a name=\"return\">Return 语句</a></h3><p><strong>闭包中避免使用 return</strong>。<code>return</code> 会被编译器转成 <code>scala.runtime.NonLocalReturnControl</code> 异常的 <code>try/catch</code> 语句，这可能会导致意外行为。请看下面的例子：</p>\n  <figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>(rpc: <span class=\"type\">WebSocketRPC</span>): <span class=\"type\">Option</span>[<span class=\"type\">Response</span>] = &#123;</div><div class=\"line\">  tableFut.onComplete &#123; table =&gt;</div><div class=\"line\">    <span class=\"keyword\">if</span> (table.isFailure) &#123;</div><div class=\"line\">      <span class=\"keyword\">return</span> <span class=\"type\">None</span> <span class=\"comment\">// Do not do that!</span></div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; ... &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p><code>.onComplete</code> 方法接收一个匿名闭包并把它传递到一个不同的线程中。这个闭包最终会抛出一个 <code>NonLocalReturnControl</code> 异常，并在 <strong>一个不同的线程中</strong>被捕获，而这里执行的方法却没有任何影响。</p>\n<p>然而，也有少数情况我们是推荐使用 <code>return</code> 的。</p>\n<ul>\n<li><p>使用 <code>return</code> 来简化控制流，避免增加一级缩进。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">doSomething</span></span>(obj: <span class=\"type\">Any</span>): <span class=\"type\">Any</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (obj eq <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">null</span></div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">// do something ...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>使用 <code>return</code> 来提前终止循环，这样就不用额外构造状态标志。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (cond) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"递归及尾递归\"><a href=\"#递归及尾递归\" class=\"headerlink\" title=\"递归及尾递归\"></a><a name=\"recursion\">递归及尾递归</a></h3><p><strong>避免使用递归</strong>，除非问题可以非常自然地用递归来描述（比如，图和树的遍历）。</p>\n<p>对于那些你意欲使之成为尾递归的方法，请加上 <code>@tailrec</code> 注解以确保编译器去检查它是否真的是尾递归（你会非常惊讶地看到，由于使用了闭包和函数变换，许多看似尾递归的代码事实并非尾递归）。</p>\n<p>大多数的代码使用简单的循环和状态机会更容易推理，使用尾递归反而可能会使它更加繁琐且难以理解。例如，下面的例子中，命令式的代码比尾递归版本的代码要更加易读：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Tail recursive version.</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">max</span></span>(data: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>]): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">  <span class=\"meta\">@tailrec</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">max0</span></span>(data: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>], pos: <span class=\"type\">Int</span>, max: <span class=\"type\">Int</span>): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (pos == data.length) &#123;</div><div class=\"line\">      max</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">      max0(data, pos + <span class=\"number\">1</span>, <span class=\"keyword\">if</span> (data(pos) &gt; max) data(pos) <span class=\"keyword\">else</span> max)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  max0(data, <span class=\"number\">0</span>, <span class=\"type\">Int</span>.<span class=\"type\">MinValue</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Explicit loop version</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">max</span></span>(data: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>]): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">var</span> max = <span class=\"type\">Int</span>.<span class=\"type\">MinValue</span></div><div class=\"line\">  <span class=\"keyword\">for</span> (v &lt;- data) &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (v &gt; max) &#123;</div><div class=\"line\">      max = v</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  max</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"Implicits\"><a href=\"#Implicits\" class=\"headerlink\" title=\"Implicits\"></a><a name=\"implicits\">Implicits</a></h3><p><strong>避免使用 implicit</strong>，除非：</p>\n<ul>\n<li>你在构建领域特定的语言（DSL）</li>\n<li>你在隐式类型参数中使用它（如：<code>ClassTag</code>，<code>TypeTag</code>）</li>\n<li>你在你自己的类中使用它（意指不要污染外部空间），以此减少类型转换的冗余度（如：Scala 闭包到 Java 闭包的转换）。</li>\n</ul>\n<p>当使用 implicit 时，我们应该确保另一个工程师可以直接理解使用语义，而无需去阅读隐式定义本身。Implicit 有着非常复杂的解析规则，这会使代码变得极其难以理解。Twitter 的 Effective Scala 指南中写道：「如果你发现你在使用 implicit，始终停下来问一下你自己，是否可以在不使用 implicit 的条件下达到相同的效果」。</p>\n<p>如果你必需使用它们（比如：丰富 DSL），那么不要重载隐式方法，即确保每个隐式方法有着不同的名字，这样使用者就可以选择性地导入它们。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 别这么做，这样使用者无法选择性地只导入其中一个方法。</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">ImplicitHolder</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">toRdd</span></span>(seq: <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>]): <span class=\"type\">RDD</span>[<span class=\"type\">Int</span>] = ...</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">toRdd</span></span>(seq: <span class=\"type\">Seq</span>[<span class=\"type\">Long</span>]): <span class=\"type\">RDD</span>[<span class=\"type\">Long</span>] = ...</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 应该将它们定义为不同的名字：</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">ImplicitHolder</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">intSeqToRdd</span></span>(seq: <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>]): <span class=\"type\">RDD</span>[<span class=\"type\">Int</span>] = ...</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">longSeqToRdd</span></span>(seq: <span class=\"type\">Seq</span>[<span class=\"type\">Long</span>]): <span class=\"type\">RDD</span>[<span class=\"type\">Long</span>] = ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"异常处理-Try-还是-try\"><a href=\"#异常处理-Try-还是-try\" class=\"headerlink\" title=\"异常处理 (Try 还是 try)\"></a><a name=\"exception\">异常处理 (Try 还是 try)</a></h2><ul>\n<li><p>不要捕获 Throwable 或 Exception 类型的异常。请使用 <code>scala.util.control.NonFatal</code>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">NonFatal</span>(e) =&gt;</div><div class=\"line\">    <span class=\"comment\">// 异常处理；注意 NonFatal 无法匹配 InterruptedException 类型的异常</span></div><div class=\"line\">  <span class=\"keyword\">case</span> e: <span class=\"type\">InterruptedException</span> =&gt;</div><div class=\"line\">    <span class=\"comment\">// 处理 InterruptedException</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>这能保证我们不会去捕获 <code>NonLocalReturnControl</code> 异常（正如在<a href=\"#return\">Return 语句</a>中所解释的）。</p>\n</li>\n<li><p>不要在 API 中使用 <code>Try</code>，即，不要在任何方法中返回 Try。对于异常执行，请显式地抛出异常，并使用 Java 风格的 try/catch 做异常处理。</p>\n<p>背景资料：Scala 提供了单子（monadic）错误处理（通过 <code>Try</code>，<code>Success</code> 和 <code>Failure</code>），这样便于做链式处理。然而，根据我们的经验，发现使用它通常会带来更多的嵌套层级，使得代码难以阅读。此外，对于预期错误还是异常，在语义上常常是不明晰的。因此，我们不鼓励使用 <code>Try</code> 来做错误处理，尤其是以下情况：</p>\n<p>一个人为的例子：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">UserService</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">/** Look up a user's profile in the user database. */</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get</span></span>(userId: <span class=\"type\">Int</span>): <span class=\"type\">Try</span>[<span class=\"type\">User</span>]</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>以下的写法会更好：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">UserService</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">/**</span></div><div class=\"line\">   * Look up a user's profile in the user database.</div><div class=\"line\">   * @return None if the user is not found.</div><div class=\"line\">   * @throws DatabaseConnectionException when we have trouble connecting to the database/</div><div class=\"line\">   */</div><div class=\"line\">  <span class=\"meta\">@throws</span>(<span class=\"type\">DatabaseConnectionException</span>)</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get</span></span>(userId: <span class=\"type\">Int</span>): <span class=\"type\">Option</span>[<span class=\"type\">User</span>]</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>第二种写法非常明显地能让调用者知道需要处理哪些错误情况。</p>\n</li>\n</ul>\n<h3 id=\"Options\"><a href=\"#Options\" class=\"headerlink\" title=\"Options\"></a><a name=\"option\">Options</a></h3><ul>\n<li>如果一个值可能为空，那么请使用 <code>Option</code>。相对于 <code>null</code>，<code>Option</code> 显式地表明了一个 API 的返回值可能为空。</li>\n<li><p>构造 <code>Option</code> 值时，请使用 <code>Option</code> 而非 <code>Some</code>，以防那个值为 <code>null</code>。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">myMethod1</span></span>(input: <span class=\"type\">String</span>): <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">Option</span>(transform(input))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// This is not as robust because transform can return null, and then</span></div><div class=\"line\"><span class=\"comment\">// myMethod2 will return Some(null).</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">myMethod2</span></span>(input: <span class=\"type\">String</span>): <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">Some</span>(transform(input))</div></pre></td></tr></table></figure>\n</li>\n<li><p>不要使用 None 来表示异常，有异常时请显式抛出。</p>\n</li>\n<li>不要在一个 <code>Option</code> 值上直接调用 <code>get</code> 方法，除非你百分百确定那个 <code>Option</code> 值不是 <code>None</code>。</li>\n</ul>\n<h3 id=\"单子链接\"><a href=\"#单子链接\" class=\"headerlink\" title=\"单子链接\"></a><a name=\"chaining\">单子链接</a></h3><p>单子链接是 Scala 的一个强大特性。Scala 中几乎一切都是单子（如：集合，Option，Future，Try 等），对它们的操作可以链接在一起。这是一个非常强大的概念，但你应该谨慎使用，尤其是：</p>\n<ul>\n<li>避免链接（或嵌套）超过 3 个操作。</li>\n<li>如果需要花超过 5 秒钟来理解其中的逻辑，那么你应该尽量去想想有没什么办法在不使用单子链接的条件下来达到相同的效果。一般来说，你需要注意的是：不要滥用 <code>flatMap</code> 和 <code>fold</code>。</li>\n<li>链接应该在 flatMap 之后断开（因为类型发生了变化）。</li>\n</ul>\n<p>通过给中间结果显式地赋予一个变量名，将链接断开变成一种更加过程化的风格，能让单子链接更加易于理解。来看下面的例子：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">val data: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]</span>)</span></div><div class=\"line\"><span class=\"keyword\">val</span> database = <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">Person</span>]</div><div class=\"line\"><span class=\"comment\">// Sometimes the client can store \"null\" value in the  store \"address\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// A monadic chaining approach</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getAddress</span></span>(name: <span class=\"type\">String</span>): <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = &#123;</div><div class=\"line\">  database.get(name).flatMap &#123; elem =&gt;</div><div class=\"line\">    elem.data.get(<span class=\"string\">\"address\"</span>)</div><div class=\"line\">      .flatMap(<span class=\"type\">Option</span>.apply)  <span class=\"comment\">// handle null value</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 尽管代码会长一些，但以下方法可读性更高</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getAddress</span></span>(name: <span class=\"type\">String</span>): <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (!database.contains(name)) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"type\">None</span></div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  database(name).data.get(<span class=\"string\">\"address\"</span>) <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(<span class=\"literal\">null</span>) =&gt; <span class=\"type\">None</span>  <span class=\"comment\">// handle null value</span></div><div class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(addr) =&gt; <span class=\"type\">Option</span>(addr)</div><div class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt; <span class=\"type\">None</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"并发\"><a href=\"#并发\" class=\"headerlink\" title=\"并发\"></a><a name=\"concurrency\">并发</a></h2><h3 id=\"Scala-concurrent-Map\"><a href=\"#Scala-concurrent-Map\" class=\"headerlink\" title=\"Scala concurrent.Map\"></a><a name=\"concurrency-scala-collection\">Scala concurrent.Map</a></h3><p><strong>优先考虑使用 <code>java.util.concurrent.ConcurrentHashMap</code> 而非 <code>scala.collection.concurrent.Map</code></strong>。尤其是 <code>scala.collection.concurrent.Map</code> 中的 <code>getOrElseUpdate</code> 方法要慎用，它并非原子操作（这个问题在 Scala 2.11.16 中 fix 了：<a href=\"https://issues.scala-lang.org/browse/SI-7943\" target=\"_blank\" rel=\"external\">SI-7943</a>）。由于我们做的所有项目都需要在 Scala 2.10 和 Scala 2.11 上使用，因此要避免使用 <code>scala.collection.concurrent.Map</code>。</p>\n<h3 id=\"显式同步-vs-并发集合\"><a href=\"#显式同步-vs-并发集合\" class=\"headerlink\" title=\"显式同步 vs 并发集合\"></a><a name=\"concurrency-sync-vs-map\">显式同步 vs 并发集合</a></h3><p>有 3 种推荐的方法来安全地并发访问共享状态。<strong>不要混用它们</strong>，因为这会使程序变得难以推理，并且可能导致死锁。</p>\n<ul>\n<li><p><code>java.util.concurrent.ConcurrentHashMap</code>：当所有的状态都存储在一个 map 中，并且有高程度的竞争时使用。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">val</span> map = <span class=\"keyword\">new</span> java.util.concurrent.<span class=\"type\">ConcurrentHashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]</div></pre></td></tr></table></figure>\n</li>\n<li><p><code>java.util.Collections.synchronizedMap</code>：使用情景：当所有状态都存储在一个 map 中，并且预期不存在竞争情况，但你仍想确保代码在并发下是安全的。如果没有竞争出现，JVM 的 JIT 编译器能够通过偏置锁（biased locking）移除同步开销。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">val</span> map = java.util.<span class=\"type\">Collections</span>.synchronizedMap(<span class=\"keyword\">new</span> java.util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>])</div></pre></td></tr></table></figure>\n</li>\n<li><p>通过同步所有临界区进行显式同步，可用于监视多个变量。与 2 相似，JVM 的 JIT 编译器能够通过偏置锁（biased locking）移除同步开销。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Manager</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> count = <span class=\"number\">0</span></div><div class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">val</span> map = <span class=\"keyword\">new</span> java.util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update</span></span>(key: <span class=\"type\">String</span>, value: <span class=\"type\">String</span>): <span class=\"type\">Unit</span> = synchronized &#123;</div><div class=\"line\">    map.put(key, value)</div><div class=\"line\">    count += <span class=\"number\">1</span></div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getCount</span></span>: <span class=\"type\">Int</span> = synchronized &#123; count &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>注意，对于 case 1 和 case 2，不要让集合的视图或迭代器从保护区域逃逸。这可能会以一种不明显的方式发生，比如：返回了 <code>Map.keySet</code> 或 <code>Map.values</code>。如果需要传递集合的视图或值，生成一份数据拷贝再传递。</p>\n  <figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> map = java.util.<span class=\"type\">Collections</span>.synchronizedMap(<span class=\"keyword\">new</span> java.util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>])</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// This is broken!</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">values</span></span>: <span class=\"type\">Iterable</span>[<span class=\"type\">String</span>] = map.values</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Instead, copy the elements</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">values</span></span>: <span class=\"type\">Iterable</span>[<span class=\"type\">String</span>] = map.synchronized &#123; <span class=\"type\">Seq</span>(map.values: _*) &#125;</div></pre></td></tr></table></figure>\n<h3 id=\"显式同步-vs-原子变量-vs-volatile\"><a href=\"#显式同步-vs-原子变量-vs-volatile\" class=\"headerlink\" title=\"显式同步 vs 原子变量 vs @volatile\"></a><a name=\"concurrency-sync-vs-atomic\">显式同步 vs 原子变量 vs @volatile</a></h3><p><code>java.util.concurrent.atomic</code> 包提供了对基本类型的无锁访问，比如：<code>AtomicBoolean</code>, <code>AtomicInteger</code> 和 <code>AtomicReference</code>。</p>\n<p>始终优先考虑使用原子变量而非 <code>@volatile</code>，它们是相关功能的严格超集并且从代码上看更加明显。原子变量的底层实现使用了 <code>@volatile</code>。</p>\n<p>优先考虑使用原子变量而非显式同步的情况：（1）一个对象的所有临界区更新都被限制在单个变量里并且预期会有竞争情况出现。原子变量是无锁的并且允许更为有效的竞争。（2）同步被明确地表示为 <code>getAndSet</code> 操作。例如：</p>\n  <figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// good: 明确又有效地表达了下面的并发代码只执行一次</span></div><div class=\"line\"><span class=\"keyword\">val</span> initialized = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</div><div class=\"line\">...</div><div class=\"line\"><span class=\"keyword\">if</span> (!initialized.getAndSet(<span class=\"literal\">true</span>)) &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// poor: 下面的同步就没那么明晰，而且会出现不必要的同步</span></div><div class=\"line\"><span class=\"keyword\">val</span> initialized = <span class=\"literal\">false</span></div><div class=\"line\">...</div><div class=\"line\"><span class=\"keyword\">var</span> wasInitialized = <span class=\"literal\">false</span></div><div class=\"line\">synchronized &#123;</div><div class=\"line\">  wasInitialized = initialized</div><div class=\"line\">  initialized = <span class=\"literal\">true</span></div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"keyword\">if</span> (!wasInitialized) &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"私有字段\"><a href=\"#私有字段\" class=\"headerlink\" title=\"私有字段\"></a><a name=\"concurrency-private-this\">私有字段</a></h3><p>注意，<code>private</code> 字段仍然可以被相同类的其它实例所访问，所以仅仅通过 <code>this.synchronized</code>（或 <code>synchronized</code>）来保护它从技术上来说是不够的，不过你可以通过 <code>private[this]</code> 修饰私有字段来达到目的。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 以下代码仍然是不安全的。</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> count: <span class=\"type\">Int</span> = <span class=\"number\">0</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">inc</span></span>(): <span class=\"type\">Unit</span> = synchronized &#123; count += <span class=\"number\">1</span> &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 以下代码是安全的。</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> count: <span class=\"type\">Int</span> = <span class=\"number\">0</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">inc</span></span>(): <span class=\"type\">Unit</span> = synchronized &#123; count += <span class=\"number\">1</span> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"隔离\"><a href=\"#隔离\" class=\"headerlink\" title=\"隔离\"></a><a name=\"concurrency-isolation\">隔离</a></h3><p>一般来说，并发和同步逻辑应该尽可能地被隔离和包含起来。这实际上意味着：</p>\n<ul>\n<li>避免在 API 层面、面向用户的方法以及回调中暴露同步原语。</li>\n<li>对于复杂模块，创建一个小的内部模块来包含并发原语。</li>\n</ul>\n<h2 id=\"性能\"><a href=\"#性能\" class=\"headerlink\" title=\"性能\"></a><a name=\"perf\">性能</a></h2><p>对于你写的绝大多数代码，性能都不应该成为一个问题。然而，对于一些性能敏感的代码，以下有一些小建议：</p>\n<h3 id=\"Microbenchmarks\"><a href=\"#Microbenchmarks\" class=\"headerlink\" title=\"Microbenchmarks\"></a><a name=\"perf-microbenchmarks\">Microbenchmarks</a></h3><p>由于 Scala 编译器和 JVM JIT 编译器会对你的代码做许多神奇的事情，因此要写出一个好的微基准程序（microbenchmark）是极其困难的。更多的情况往往是你的微基准程序并没有测量你想要测量的东西。</p>\n<p>如果你要写一个微基准程序，请使用 <a href=\"http://openjdk.java.net/projects/code-tools/jmh/\" target=\"_blank\" rel=\"external\">jmh</a>。请确保你阅读了<a href=\"http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/\" target=\"_blank\" rel=\"external\">所有的样例</a>，这样你才理解微基准程序中「死代码」移除、常量折叠以及循环展开的效果。</p>\n<h3 id=\"Traversal-与-zipWithIndex\"><a href=\"#Traversal-与-zipWithIndex\" class=\"headerlink\" title=\"Traversal 与 zipWithIndex\"></a><a name=\"perf-whileloops\">Traversal 与 zipWithIndex</a></h3><p>使用 <code>while</code> 循环而非 <code>for</code> 循环或函数变换（如：<code>map</code>、<code>foreach</code>），for 循环和函数变换非常慢（由于虚函数调用和装箱的缘故）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> arr = <span class=\"comment\">// array of ints</span></div><div class=\"line\"><span class=\"comment\">// 偶数位置的数置零</span></div><div class=\"line\"><span class=\"keyword\">val</span> newArr = list.zipWithIndex.map &#123; <span class=\"keyword\">case</span> (elem, i) =&gt;</div><div class=\"line\">  <span class=\"keyword\">if</span> (i % <span class=\"number\">2</span> == <span class=\"number\">0</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> elem</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 这是上面代码的高性能版本</span></div><div class=\"line\"><span class=\"keyword\">val</span> newArr = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Int</span>](arr.length)</div><div class=\"line\"><span class=\"keyword\">var</span> i = <span class=\"number\">0</span></div><div class=\"line\"><span class=\"keyword\">val</span> len = newArr.length</div><div class=\"line\"><span class=\"keyword\">while</span> (i &lt; len) &#123;</div><div class=\"line\">  newArr(i) = <span class=\"keyword\">if</span> (i % <span class=\"number\">2</span> == <span class=\"number\">0</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> arr(i)</div><div class=\"line\">  i += <span class=\"number\">1</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"Option-与-null\"><a href=\"#Option-与-null\" class=\"headerlink\" title=\"Option 与 null\"></a><a name=\"perf-option\">Option 与 null</a></h3><p>对于性能有要求的代码，优先考虑使用 <code>null</code> 而不是 <code>Option</code>，以此避免虚函数调用以及装箱操作。用 Nullable 注解明确标示出可能为 <code>null</code> 的值。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"meta\">@javax</span>.annotation.<span class=\"type\">Nullable</span></div><div class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> nullableField: <span class=\"type\">Bar</span> = _</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"Scala-集合库\"><a href=\"#Scala-集合库\" class=\"headerlink\" title=\"Scala 集合库\"></a><a name=\"perf-collection\">Scala 集合库</a></h3><p>对于性能有要求的代码，优先考虑使用 Java 集合库而非 Scala 集合库，因为一般来说，Scala 集合库要比 Java 的集合库慢。</p>\n<h3 id=\"private-this\"><a href=\"#private-this\" class=\"headerlink\" title=\"private[this]\"></a><a name=\"perf-private\">private[this]</a></h3><p>对于性能有要求的代码，优先考虑使用 <code>private[this]</code> 而非 <code>private</code>。<code>private[this]</code> 生成一个字段而非生成一个访问方法。根据我们的经验，JVM JIT 编译器并不总是会内联 <code>private</code> 字段的访问方法，因此通过使用<br><code>private[this]</code> 来确保没有虚函数调用会更保险。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyClass</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> field1 = ...</div><div class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">val</span> field2 = ...</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">perfSensitiveMethod</span></span>(): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">while</span> (i &lt; <span class=\"number\">1000000</span>) &#123;</div><div class=\"line\">      field1  <span class=\"comment\">// This might invoke a virtual method call</span></div><div class=\"line\">      field2  <span class=\"comment\">// This is just a field access</span></div><div class=\"line\">      i += <span class=\"number\">1</span></div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"与-Java-的互操作性\"><a href=\"#与-Java-的互操作性\" class=\"headerlink\" title=\"与 Java 的互操作性\"></a><a name=\"java\">与 Java 的互操作性</a></h2><p>本节内容介绍的是构建 Java 兼容 API 的准则。如果你构建的组件并不需要与 Java 有交互，那么请无视这一节。这一节的内容主要是从我们开发 Spark 的 Java API 的经历中得出的。</p>\n<h3 id=\"Scala-中缺失的-Java-特性\"><a href=\"#Scala-中缺失的-Java-特性\" class=\"headerlink\" title=\"Scala 中缺失的 Java 特性\"></a><a name=\"java-missing-features\">Scala 中缺失的 Java 特性</a></h3><p>以下的 Java 特性在 Scala 中是没有的，如果你需要使用以下特性，请在 Java 中定义它们。然而，需要提醒一点的是，你无法为 Java 源文件生成 ScalaDoc。</p>\n<ul>\n<li>静态字段</li>\n<li>静态内部类</li>\n<li>Java 枚举</li>\n<li>注解</li>\n</ul>\n<h3 id=\"Traits-与抽象类\"><a href=\"#Traits-与抽象类\" class=\"headerlink\" title=\"Traits 与抽象类\"></a><a name=\"java-traits\">Traits 与抽象类</a></h3><p>对于允许从外部实现的接口，请记住以下几点：</p>\n<ul>\n<li>包含了默认方法实现的 trait 是无法在 Java 中使用的，请使用抽象类来代替。</li>\n<li>一般情况下，请避免使用 trait，除非你百分百确定这个接口即使在未来也不会有默认的方法实现。</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 以下默认实现无法在 Java 中使用</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Listener</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onTermination</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 可以在 Java 中使用</span></div><div class=\"line\"><span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Listener</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onTermination</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"类型别名\"><a href=\"#类型别名\" class=\"headerlink\" title=\"类型别名\"></a><a name=\"java-type-alias\">类型别名</a></h3><p>不要使用类型别名，它们在字节码和 Java 中是不可见的。</p>\n<h3 id=\"默认参数值\"><a href=\"#默认参数值\" class=\"headerlink\" title=\"默认参数值\"></a><a name=\"java-default-param-values\">默认参数值</a></h3><p>不要使用默认参数值，通过重载方法来代替。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 打破了与 Java 的互操作性</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample</span></span>(ratio: <span class=\"type\">Double</span>, withReplacement: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = &#123; ... &#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 以下方法是 work 的</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample</span></span>(ratio: <span class=\"type\">Double</span>, withReplacement: <span class=\"type\">Boolean</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = &#123; ... &#125;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample</span></span>(ratio: <span class=\"type\">Double</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = sample(ratio, withReplacement = <span class=\"literal\">false</span>)</div></pre></td></tr></table></figure>\n<h3 id=\"多参数列表-1\"><a href=\"#多参数列表-1\" class=\"headerlink\" title=\"多参数列表\"></a><a name=\"java-multi-param-list\">多参数列表</a></h3><p>不要使用多参数列表。</p>\n<h3 id=\"可变参数\"><a href=\"#可变参数\" class=\"headerlink\" title=\"可变参数\"></a><a name=\"java-varargs\">可变参数</a></h3><ul>\n<li><p>为可变参数方法添加 <code>@scala.annotation.varargs</code> 注解，以确保它能在 Java 中使用。Scala 编译器会生成两个方法，一个给 Scala 使用（字节码参数是一个 Seq），另一个给 Java 使用（字节码参数是一个数组）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@scala</span>.annotation.varargs</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">select</span></span>(exprs: <span class=\"type\">Expression</span>*): <span class=\"type\">DataFrame</span> = &#123; ... &#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>需要注意的一点是，由于 Scala 编译器的一个 bug（<a href=\"https://issues.scala-lang.org/browse/SI-1459\" target=\"_blank\" rel=\"external\">SI-1459</a>，<a href=\"https://issues.scala-lang.org/browse/SI-9013\" target=\"_blank\" rel=\"external\">SI-9013</a>），抽象的变参方法是无法在 Java 中使用的。</p>\n</li>\n<li><p>重载变参方法时要小心，用另一个类型去重载变参方法会破坏源码的兼容性。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Database</span> </span>&#123;</div><div class=\"line\">  <span class=\"meta\">@scala</span>.annotation.varargs</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">remove</span></span>(elems: <span class=\"type\">String</span>*): <span class=\"type\">Unit</span> = ...</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 当调用无参的 remove 方法时会出问题。</span></div><div class=\"line\">  <span class=\"meta\">@scala</span>.annotation.varargs</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">remove</span></span>(elems: <span class=\"type\">People</span>*): <span class=\"type\">Unit</span> = ...</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// remove 方法有歧义，因此编译不过。</span></div><div class=\"line\"><span class=\"keyword\">new</span> <span class=\"type\">Database</span>().remove()</div></pre></td></tr></table></figure>\n<p>一种解决方法是，在可变参数前显式地定义第一个参数：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Database</span> </span>&#123;</div><div class=\"line\">  <span class=\"meta\">@scala</span>.annotation.varargs</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">remove</span></span>(elems: <span class=\"type\">String</span>*): <span class=\"type\">Unit</span> = ...</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 以下重载是 OK 的。</span></div><div class=\"line\">  <span class=\"meta\">@scala</span>.annotation.varargs</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">remove</span></span>(elem: <span class=\"type\">People</span>, elems: <span class=\"type\">People</span>*): <span class=\"type\">Unit</span> = ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"Implicits-1\"><a href=\"#Implicits-1\" class=\"headerlink\" title=\"Implicits\"></a><a name=\"java-implicits\">Implicits</a></h3><p>不要为类或方法使用 implicit，包括了不要使用 <code>ClassTag</code> 和 <code>TypeTag</code>。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JavaFriendlyAPI</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">// 以下定义对 Java 是不友好的，因为方法中包含了一个隐式参数（ClassTag）。</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convertTo</span></span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](): <span class=\"type\">T</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"伴生对象，静态方法与字段\"><a href=\"#伴生对象，静态方法与字段\" class=\"headerlink\" title=\"伴生对象，静态方法与字段\"></a><a name=\"java-companion-object\">伴生对象，静态方法与字段</a></h3><p>当涉及到伴生对象和静态方法/字段时，有几件事情是需要注意的：</p>\n<ul>\n<li><p>伴生对象在 Java 中的使用是非常别扭的（伴生对象 <code>Foo</code> 会被定义为 <code>Foo$</code> 类内的一个类型为 <code>Foo$</code> 的静态字段 <code>MODULE$</code>）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Foo</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 等价于以下的 Java 代码</span></div><div class=\"line\">public <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo$</span> </span>&#123;</div><div class=\"line\">  <span class=\"type\">Foo</span>$ <span class=\"type\">MODULE</span>$ = <span class=\"comment\">// 对象的实例化</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>如果非要使用伴生对象，可以在一个单独的类中创建一个 Java 静态字段。</p>\n</li>\n<li><p>不幸的是，没有办法在 Scala 中定义一个 JVM 静态字段。请创建一个 Java 文件来定义它。</p>\n</li>\n<li><p>伴生对象里的方法会被自动转成伴生类里的静态方法，除非方法名有冲突。确保静态方法正确生成的最好方式是用 Java 写一个测试文件，然后调用生成的静态方法。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">method2</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">method1</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;  <span class=\"comment\">// 静态方法 Foo.method1 会被创建（字节码）</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">method2</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;  <span class=\"comment\">// 静态方法 Foo.method2 不会被创建</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// FooJavaTest.java (in test/scala/com/databricks/...)</span></div><div class=\"line\">public <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FooJavaTest</span> </span>&#123;</div><div class=\"line\">  public static void compileTest() &#123;</div><div class=\"line\">    <span class=\"type\">Foo</span>.method1();  <span class=\"comment\">// 正常编译</span></div><div class=\"line\">    <span class=\"type\">Foo</span>.method2();  <span class=\"comment\">// 编译失败，因为 method2 并没有生成</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>样例对象（case object） MyClass 的类型并不是 MyClass。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">MyClass</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Test.java</span></div><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"type\">MyClass</span>$.<span class=\"type\">MODULE</span> instanceof <span class=\"type\">MyClass</span>) &#123;</div><div class=\"line\">  <span class=\"comment\">// 上述条件始终为 false</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>要实现正确的类型层级结构，请定义一个伴生类，然后用一个样例对象去继承它：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyClass</span></span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">MyClass</span> <span class=\"keyword\">extends</span> <span class=\"title\">MyClass</span></span></div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"其它\"><a href=\"#其它\" class=\"headerlink\" title=\"其它\"></a><a name=\"misc\">其它</a></h2><h3 id=\"优先使用-nanoTime-而非-currentTimeMillis\"><a href=\"#优先使用-nanoTime-而非-currentTimeMillis\" class=\"headerlink\" title=\"优先使用 nanoTime 而非 currentTimeMillis\"></a><a name=\"misc_currentTimeMillis_vs_nanoTime\">优先使用 nanoTime 而非 currentTimeMillis</a></h3><p>当要计算<em>持续时间</em>或者检查<em>超时</em>的时候，避免使用 <code>System.currentTimeMillis()</code>。请使用 <code>System.nanoTime()</code>，即使你对亚毫秒级的精度并不感兴趣。</p>\n<p><code>System.currentTimeMillis()</code> 返回的是当前的时钟时间，并且会跟进系统时钟的改变。因此，负的时钟调整可能会导致超时而挂起很长一段时间（直到时钟时间赶上先前的值）。这种情况可能发生在网络已经中断一段时间，ntpd 走过了一步之后。最典型的例子是，在系统启动的过程中，DHCP 花费的时间要比平常的长。这可能会导致非常难以理解且难以重现的问题。而 <code>System.nanoTime()</code> 则可以保证是单调递增的，与时钟变化无关。</p>\n<p>注意事项：</p>\n<ul>\n<li>永远不要序列化一个绝对的 <code>nanoTime()</code> 值或是把它传递给另一个系统。绝对的 <code>nanoTime()</code> 值是无意义的、与系统相关的，并且在系统重启时会重置。</li>\n<li>绝对的 <code>nanoTime()</code> 值并不保证总是正数（但 <code>t2 - t1</code> 能确保总是产生正确的值）。</li>\n<li><code>nanoTime()</code> 每 292 年就会重新计算起。所以，如果你的 Spark 任务需要花非常非常非常长的时间，你可能需要别的东西来处理了：）</li>\n</ul>\n<h3 id=\"优先使用-URI-而非-URL\"><a href=\"#优先使用-URI-而非-URL\" class=\"headerlink\" title=\"优先使用 URI 而非 URL\"></a><a name=\"misc_uri_url\">优先使用 URI 而非 URL</a></h3><p>当存储服务的 URL 时，你应当使用 <code>URI</code> 来表示。</p>\n<p><code>URL</code> 的<a href=\"http://docs.oracle.com/javase/7/docs/api/java/net/URL.html#equals(java.lang.Object\" target=\"_blank\" rel=\"external\">相等性检查</a>)实际上执行了一次网络调用（这是阻塞的）来解析 IP 地址。<code>URI</code> 类在表示能力上是 <code>URL</code> 的超集，并且它执行的是字段的相等性检查。</p>\n<h2 id=\"单元测试\"><a href=\"#单元测试\" class=\"headerlink\" title=\"单元测试\"></a><a name=\"unit-test\">单元测试</a></h2><h3 id=\"单元测试框架\"><a href=\"#单元测试框架\" class=\"headerlink\" title=\"单元测试框架\"></a><a name=\"unit-test-framework\">单元测试框架</a></h3><p>ScalaTest几乎已经成为Scala语言默认的测试框架，这主要源于它提供了多种表达力超强的测试风格，能够满足各种层次的需求包括单元测试、BDD、验收测试、数据驱动测试。我们也使用<a href=\"http://www.scalatest.org/\" target=\"_blank\" rel=\"external\">ScalaTest</a>测试框架。使用的时候在<code>pom.xml</code>中添加如下类似引用：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">&lt;!-- test: https://mvnrepository.com/artifact/org.scalatest/scalatest_2.10 --&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.scalatest<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>scalatest_2.10<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>3.0.0<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>test<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></div></pre></td></tr></table></figure>\n<p>测试我们遵循以下几点规则：</p>\n<ul>\n<li>测试类应该与被测试类处于同一包下，测试类的命名为：被测试类名 + Test</li>\n<li>测试含有具体实现的trait时，可以让被测试类直接继承Trait。例如：</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">RecordsGenerator</span> </span>&#123;</div><div class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generateRecords</span></span>(table: <span class=\"type\">List</span>[<span class=\"type\">List</span>[<span class=\"type\">String</span>]]): <span class=\"type\">List</span>[<span class=\"type\">Record</span>] &#123;</div><div class=\"line\">          <span class=\"comment\">//...</span></div><div class=\"line\">     &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RecordsGeneratorSpec</span> <span class=\"keyword\">extends</span> <span class=\"title\">FlatSpec</span> <span class=\"keyword\">with</span> <span class=\"title\">ShouldMatcher</span> <span class=\"keyword\">with</span> <span class=\"title\">RecordGenerator</span> </span>&#123;</div><div class=\"line\">     <span class=\"keyword\">val</span> table = <span class=\"type\">List</span>(<span class=\"type\">List</span>(<span class=\"string\">\"abc\"</span>, <span class=\"string\">\"def\"</span>), <span class=\"type\">List</span>(<span class=\"string\">\"aaa\"</span>, <span class=\"string\">\"bbb\"</span>))</div><div class=\"line\">     it should <span class=\"string\">\"generate records\"</span> in &#123;</div><div class=\"line\">          <span class=\"keyword\">val</span> records = generateRecords(table)</div><div class=\"line\">          records.size should be(<span class=\"number\">2</span>)</div><div class=\"line\">     &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>若要对文件进行测试，可以用字符串假装文件：</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">type</span> <span class=\"title\">CsvLine</span> </span>= <span class=\"type\">String</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">formatCsv</span></span>(source: <span class=\"type\">Source</span>): <span class=\"type\">List</span>[<span class=\"type\">CsvLine</span>] = &#123;</div><div class=\"line\">     source.getLines(_.replace(<span class=\"string\">\", \"</span>, <span class=\"string\">\"|\"</span>))</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>formatCsv需要接受一个文件源，例如Source.fromFile(“testdata.txt”)。但在测试时，可以通过Source.fromString方法来生成formatCsv需要接收的Source对象：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">test(<span class=\"string\">\"format csv lines\"</span>) &#123;</div><div class=\"line\">     <span class=\"keyword\">val</span> lines = <span class=\"type\">Source</span>.fromString(<span class=\"string\">\"abc, def, hgi\\n1, 2, 3\\none, two, three\"</span>)</div><div class=\"line\">     <span class=\"keyword\">val</span> result = formatCsv(lines)</div><div class=\"line\">     assert(result.mkString(<span class=\"string\">\"\\n\"</span>).equles(<span class=\"string\">\"abc|def|hgi\\n1|2|3\\none|two|three\"</span>))</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"测试风格的选择\"><a href=\"#测试风格的选择\" class=\"headerlink\" title=\"测试风格的选择\"></a><a name=\"unit-test-style\">测试风格的选择</a></h3><p>ScalaTest一共提供了七种测试风格，分别为：FunSuite，FlatSpec，FunSpec，WordSpec，FreeSpec，PropSpec和FeatureSpec。这就好像使用相同的原料做成不同美味乃至不同菜系的佳肴，你可以根据自己的口味进行选择。我们统一推荐使用FunSuite的方式，因为它更灵活，而且更符合传统测试方法的风格，区别仅在于test()方法可以接受一个闭包:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> org.scalatest.<span class=\"type\">FunSuite</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SetSuite</span> <span class=\"keyword\">extends</span> <span class=\"title\">FunSuite</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">  test(<span class=\"string\">\"An empty Set should have size 0\"</span>) &#123;</div><div class=\"line\">    assert(<span class=\"type\">Set</span>.empty.size == <span class=\"number\">0</span>)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  test(<span class=\"string\">\"Invoking head on an empty Set should produce NoSuchElementException\"</span>) &#123;</div><div class=\"line\">    assertThrows[<span class=\"type\">NoSuchElementException</span>] &#123;</div><div class=\"line\">      <span class=\"type\">Set</span>.empty.head</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>当然，如果你有必须的理由选择其它测试风格的话，本规则并不强制。</p>\n<h1 id=\"三、-引用\"><a href=\"#三、-引用\" class=\"headerlink\" title=\"三、 引用\"></a>三、 引用</h1><p><a href=\"https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md\" target=\"_blank\" rel=\"external\">Databricks Scala 编程风格指南</a>(团队最终选择的模板)<br><a href=\"http://twitter.github.io/effectivescala/index-cn.html\" target=\"_blank\" rel=\"external\">Effective Scala</a>(Twitter Scala资料，值得参考)<br><a href=\"https://zhangyi.gitbooks.io/thinking-in-scala/content/scala-convention.html\" target=\"_blank\" rel=\"external\">Thinking in Scala–Scala编程规范</a>(个人整理，可以参考)<br><a href=\"http://www.scala-lang.org/docu/files/Scala%E8%AF%AD%E8%A8%80%E8%A7%84%E8%8C%83.pdf\" target=\"_blank\" rel=\"external\">scala-lang–Scala语言规范.pdf</a>(官方原版的中文文档，共127页，过于复杂琐碎，可以参考)<br><a href=\"http://docs.scala-lang.org/style/\" target=\"_blank\" rel=\"external\">Scala官网Style Guide</a>(官方原版，其它版本基本上都是基于此版进行的改进)<br><a href=\"https://segmentfault.com/a/1190000000420018\" target=\"_blank\" rel=\"external\">分析GitHub上托管的scala开源代码统计</a></p>\n","excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><h3 id=\"一、-scala代码规范\"><a href=\"#一、-scala代码规范\" class=\"headerlink\" title=\"一、 scala代码规范\"></a>一、 scala代码规范</h3><p>Scala 是一种强大到令人难以置信的多范式编程语言。目前我们团队有很多项目需要使用scala语言进行编程，尤其是Spark相关开发项目，为了能够统一scala相关开发，本人基于Spark 贡献者及 <a href=\"http://databricks.com/\">Databricks</a> 工程团队总结出了以下指南，本人增加了单元测试的规范，希望对大家的开发有帮助。当然，这个指南并非绝对，根据我们团队需求与实际经验，持续更新。</p>\n<h3 id=\"二、-选择此规范的理由\"><a href=\"#二、-选择此规范的理由\" class=\"headerlink\" title=\"二、 选择此规范的理由\"></a>二、 选择此规范的理由</h3><p>很早就有大家统一编程规范的想法，网上也有一些关于编程规范的文档供参考。最终选择了以 Databricks 公司的编程规范为模板制作。理由如下：</p>\n<ul>\n<li>目前公司使用scala语言主要用在Spark的开发，而这份指南是Spark 贡献者及 Databricks 工程团队一起工作时总结出来的；</li>\n<li>跟我们之前写得代码，以及IDEA等编译器自动格式化相差不大；</li>\n<li>这份指南经过多次的修改和总结，经过了实践的检验；</li>\n<li>这份指南相比较而言简明概要，易于理解，遵循 Java 的地方就没有赘述，比较解耦，适用于根据公司代码体系来修改；</li>\n</ul>\n<h2 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a><a name='TOC'>目录</a></h2><ol>\n<li><a href=\"#history\">文档历史</a></li>\n<li><a href=\"#syntactic\">语法风格</a><ul>\n<li><a href=\"#naming\">命名约定</a></li>\n<li><a href=\"#variable-naming\">变量命名约定</a></li>\n<li><a href=\"#linelength\">一行长度</a></li>\n<li><a href=\"#rule_of_30\">30 法则</a></li>\n<li><a href=\"#indent\">空格与缩进</a></li>\n<li><a href=\"#blanklines\">空行</a></li>\n<li><a href=\"#parentheses\">括号</a></li>\n<li><a href=\"#curly\">大括号</a></li>\n<li><a href=\"#long_literal\">长整型字面量</a></li>\n<li><a href=\"#doc\">文档风格</a></li>\n<li><a href=\"#ordering_class\">类内秩序</a></li>\n<li><a href=\"#imports\">Imports</a></li>\n<li><a href=\"#pattern-matching\">模式匹配</a></li>\n<li><a href=\"#infix\">中缀方法</a></li>\n<li><a href=\"#anonymous\">匿名方法</a></li>\n</ul>\n</li>\n<li><a href=\"#lang\">Scala 语言特性</a><ul>\n<li><a href=\"#case_class_immutability\">样例类与不可变性</a></li>\n<li><a href=\"#apply_method\">apply 方法</a></li>\n<li><a href=\"#override_modifier\">override 修饰符</a></li>\n<li><a href=\"#destruct_bind\">解构绑定</a></li>\n<li><a href=\"#call_by_name\">按名称传参</a></li>\n<li><a href=\"#multi-param-list\">多参数列表</a></li>\n<li><a href=\"#symbolic_methods\">符号方法 (运算符重载)</a></li>\n<li><a href=\"#type_inference\">类型推导</a></li>\n<li><a href=\"#return\">Return 语句</a></li>\n<li><a href=\"#recursion\">递归及尾递归</a></li>\n<li><a href=\"#implicits\">Implicits</a></li>\n<li><a href=\"#exception\">异常处理 (Try 还是 try)</a></li>\n<li><a href=\"#option\">Options</a></li>\n<li><a href=\"#chaining\">单子链接</a></li>\n</ul>\n</li>\n<li><a href=\"#concurrency\">并发</a><ul>\n<li><a href=\"#concurrency-scala-collection\">Scala concurrent.Map</a></li>\n<li><a href=\"#concurrency-sync-vs-map\">显式同步 vs 并发集合</a></li>\n<li><a href=\"#concurrency-sync-vs-atomic\">显式同步 vs 原子变量 vs @volatile</a></li>\n<li><a href=\"#concurrency-private-this\">私有字段</a></li>\n<li><a href=\"#concurrency-isolation\">隔离</a></li>\n</ul>\n</li>\n<li><a href=\"#perf\">性能</a><ul>\n<li><a href=\"#perf-microbenchmarks\">Microbenchmarks</a></li>\n<li><a href=\"#perf-whileloops\">Traversal 与 zipWithIndex</a></li>\n<li><a href=\"#perf-option\">Option 与 null</a></li>\n<li><a href=\"#perf-collection\">Scala 集合库</a></li>\n<li><a href=\"#perf-private\">private[this]</a></li>\n</ul>\n</li>\n<li><a href=\"#java\">与 Java 的互操作性</a><ul>\n<li><a href=\"#java-missing-features\">Scala 中缺失的 Java 特性</a></li>\n<li><a href=\"#java-traits\">Traits 与抽象类</a></li>\n<li><a href=\"#java-type-alias\">类型别名</a></li>\n<li><a href=\"#java-default-param-values\">默认参数值</a></li>\n<li><a href=\"#java-multi-param-list\">多参数列表</a></li>\n<li><a href=\"#java-varargs\">可变参数</a></li>\n<li><a href=\"#java-implicits\">Implicits</a></li>\n<li><a href=\"#java-companion-object\">伴生对象, 静态方法与字段</a></li>\n</ul>\n</li>\n<li><a href=\"#misc\">其它</a><ul>\n<li><a href=\"#misc_currentTimeMillis_vs_nanoTime\">优先使用 nanoTime 而非 currentTimeMillis</a></li>\n<li><a href=\"#misc_uri_url\">优先使用 URI 而非 URL</a></li>\n</ul>\n</li>\n<li><a href=\"#unit-test\">单元测试</a><ul>\n<li><a href=\"#unit-test-framework\">单元测试框架</a></li>\n<li><a href=\"#unit-test-style\">单元测试风格</a></li>\n</ul>\n</li>\n</ol>\n<h2 id=\"文档历史\"><a href=\"#文档历史\" class=\"headerlink\" title=\"文档历史\"></a><a name='history'>文档历史</a></h2><ul>\n<li>2017-01-10: 最初版本。<a href=\"https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md\">Databricks版本</a></li>\n<li>2017-01-11: 增加 <a href=\"#unit-test\">单元测试</a> 一节。</li>\n</ul>\n<h2 id=\"语法风格\"><a href=\"#语法风格\" class=\"headerlink\" title=\"语法风格\"></a><a name='syntactic'>语法风格</a></h2><h3 id=\"命名约定\"><a href=\"#命名约定\" class=\"headerlink\" title=\"命名约定\"></a><a name='naming'>命名约定</a></h3><p>我们主要遵循 Java 和 Scala 的标准命名约定。</p>\n<ul>\n<li><p>类，trait, 对象应该遵循 Java 中类的命名约定，即 PascalCase 风格。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ClusterManager</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Expression</span></span></div></pre></td></tr></table></figure>\n</li>\n<li><p>包名应该遵循 Java 中包名的命名约定，即使用全小写的 ASCII 字母。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.databricks.resourcemanager</div></pre></td></tr></table></figure>\n</li>\n<li><p>方法/函数应当使用驼峰式风格命名。</p>\n</li>\n<li><p>常量命名使用全大写字母，并将它们放在伴生对象中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Configuration</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> <span class=\"type\">DEFAULT_PORT</span> = <span class=\"number\">10000</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>枚举命名与类命名一致，使用 PascalCase 风格。</p>\n</li>\n<li><p>注解也应遵循 Java 中的约定，即使用 PascalCase 风格。注意，这一点与 Scala 的官方指南不同。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">final</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyAnnotation</span> <span class=\"keyword\">extends</span> <span class=\"title\">StaticAnnotation</span></span></div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"变量命名约定\"><a href=\"#变量命名约定\" class=\"headerlink\" title=\"变量命名约定\"></a><a name='variable-naming'>变量命名约定</a></h3><ul>\n<li><p>变量命名应当遵循驼峰式命名方法，并且变量名应当是不言而喻的，即变量名可以直观地反应它的涵义。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> serverPort = <span class=\"number\">1000</span></div><div class=\"line\"><span class=\"keyword\">val</span> clientPort = <span class=\"number\">2000</span></div></pre></td></tr></table></figure>\n</li>\n<li><p>可以在小段的局部代码中使用单字符的变量名，比如在小段的循环体中（例如 10 行以内的代码），“i” 常常被用作循环索引。然而，即使在小段的代码中，也不要使用 “l” （Larry 中的 l）作为标识符，因为它看起来和 “1”，“|”，“I” 很像，难以区分，容易搞错。</p>\n</li>\n</ul>\n<h3 id=\"一行长度\"><a href=\"#一行长度\" class=\"headerlink\" title=\"一行长度\"></a><a name='linelength'>一行长度</a></h3><ul>\n<li>一行长度的上限是 100 个字符。</li>\n<li>唯一的例外是 import 语句和 URL (即便如此，也尽量将它们保持在 100 个字符以下)。</li>\n</ul>\n<h3 id=\"30-法则\"><a href=\"#30-法则\" class=\"headerlink\" title=\"30 法则\"></a><a name='rule_of_30'>30 法则</a></h3><p>「如果一个元素包含的子元素超过 30 个，那么极有可能出现了严重的问题」 - <a href=\"http://www.amazon.com/Refactoring-Large-Software-Projects-Restructurings/dp/0470858923\">Refactoring in Large Software Projects</a>。</p>\n<p>一般来说:</p>\n<ul>\n<li>一个方法包含的代码行数不宜超过 30 行。</li>\n<li>一个类包含的方法数量不宜超过 30 个。</li>\n</ul>\n<h3 id=\"空格与缩进\"><a href=\"#空格与缩进\" class=\"headerlink\" title=\"空格与缩进\"></a><a name='indent'>空格与缩进</a></h3><ul>\n<li><p>一般情况下，使用两个空格的缩进。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"literal\">true</span>) &#123;</div><div class=\"line\">  println(<span class=\"string\">\"Wow!\"</span>)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>对于方法声明，如果一行无法容纳下所有的参数，那么使用 4 个空格来缩进它们。返回类型可以与最后一个参数在同一行，也可以放在下一行，使用两个空格缩进。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">newAPIHadoopFile</span></span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">F</span> &lt;: <span class=\"type\">NewInputFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]](</div><div class=\"line\">    path: <span class=\"type\">String</span>,</div><div class=\"line\">    fClass: <span class=\"type\">Class</span>[<span class=\"type\">F</span>],</div><div class=\"line\">    kClass: <span class=\"type\">Class</span>[<span class=\"type\">K</span>],</div><div class=\"line\">    vClass: <span class=\"type\">Class</span>[<span class=\"type\">V</span>],</div><div class=\"line\">    conf: <span class=\"type\">Configuration</span> = hadoopConfiguration): <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</div><div class=\"line\">  <span class=\"comment\">// method body</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">newAPIHadoopFile</span></span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">F</span> &lt;: <span class=\"type\">NewInputFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]](</div><div class=\"line\">    path: <span class=\"type\">String</span>,</div><div class=\"line\">    fClass: <span class=\"type\">Class</span>[<span class=\"type\">F</span>],</div><div class=\"line\">    kClass: <span class=\"type\">Class</span>[<span class=\"type\">K</span>],</div><div class=\"line\">    vClass: <span class=\"type\">Class</span>[<span class=\"type\">V</span>],</div><div class=\"line\">    conf: <span class=\"type\">Configuration</span> = hadoopConfiguration)</div><div class=\"line\">  : <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</div><div class=\"line\">  <span class=\"comment\">// method body</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>如果一行无法容纳下类头（即 extends 后面那部分），则把它们放到新的一行，用两个空格缩进，然后在类内空一行再开始函数或字段的定义（或是包的导入）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span>(<span class=\"params\"></div><div class=\"line\">    val param1: <span class=\"type\">String</span>,  // 4 space indent for parameters</div><div class=\"line\">    val param2: <span class=\"type\">String</span>,</div><div class=\"line\">    val param3: <span class=\"type\">Array</span>[<span class=\"type\">Byte</span>]</span>)</span></div><div class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">FooInterface</span>  <span class=\"comment\">// 2 space here</span></div><div class=\"line\">  <span class=\"keyword\">with</span> <span class=\"type\">Logging</span> &#123;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">firstMethod</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;  <span class=\"comment\">// blank line above</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>不要使用垂直对齐。它使你的注意力放在代码的错误部分并增大了后人修改代码的难度。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Don't align vertically</span></div><div class=\"line\"><span class=\"keyword\">val</span> plus     = <span class=\"string\">\"+\"</span></div><div class=\"line\"><span class=\"keyword\">val</span> minus    = <span class=\"string\">\"-\"</span></div><div class=\"line\"><span class=\"keyword\">val</span> multiply = <span class=\"string\">\"*\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Do the following</span></div><div class=\"line\"><span class=\"keyword\">val</span> plus = <span class=\"string\">\"+\"</span></div><div class=\"line\"><span class=\"keyword\">val</span> minus = <span class=\"string\">\"-\"</span></div><div class=\"line\"><span class=\"keyword\">val</span> multiply = <span class=\"string\">\"*\"</span></div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"空行\"><a href=\"#空行\" class=\"headerlink\" title=\"空行\"></a><a name='blanklines'>空行</a></h3><ul>\n<li>一个空行可以出现在：<ul>\n<li>连续的类成员或初始化器（initializers）之间：字段，构造函数，方法，嵌套类，静态初始化器及实例初始化器。<ul>\n<li>例外：连续的两个字段之间的空行是可选的（前提是它们之间没有其它代码），这一类空行主要为这些字段做逻辑上的分组。</li>\n</ul>\n</li>\n<li>在方法体内，根据需要，使用空行来为语句创建逻辑上的分组。</li>\n<li>在类的第一个成员之前或最后一个成员之后，空行都是可选的（既不鼓励也不阻止）。</li>\n</ul>\n</li>\n<li>使用一个或两个空行来分隔不同类的定义。</li>\n<li>不鼓励使用过多的空行。</li>\n</ul>\n<h3 id=\"括号\"><a href=\"#括号\" class=\"headerlink\" title=\"括号\"></a><a name='parentheses'>括号</a></h3><ul>\n<li><p>方法声明应该加括号（即使没有参数列表），除非它们是没有副作用（状态改变，IO 操作都认为是有副作用的）的访问器（accessor）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Job</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">// Wrong: killJob changes state. Should have ().</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">killJob</span></span>: <span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// Correct:</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">killJob</span></span>(): <span class=\"type\">Unit</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>函数调用应该与函数声明在形式上保持一致，也就是说，如果一个方法声明时带了括号，那调用时也要把括号带上。注意这不仅仅是语法层面的人为约定，当返回对象中定义了 <code>apply</code> 方法时，这一点还会影响正确性。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(args: <span class=\"type\">String</span>*): <span class=\"type\">Int</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Bar</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foo</span></span>: <span class=\"type\">Foo</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">new</span> <span class=\"type\">Bar</span>().foo  <span class=\"comment\">// This returns a Foo</span></div><div class=\"line\"><span class=\"keyword\">new</span> <span class=\"type\">Bar</span>().foo()  <span class=\"comment\">// This returns an Int!</span></div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"大括号\"><a href=\"#大括号\" class=\"headerlink\" title=\"大括号\"></a><a name='curly'>大括号</a></h3><p>即使条件语句或循环语句只有一行时，也请使用大括号。唯一的例外是，当你把 if/else 作为一个单行的三元操作符来使用并且没有副作用时，这时你可以不加大括号。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Correct:</span></div><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"literal\">true</span>) &#123;</div><div class=\"line\">  println(<span class=\"string\">\"Wow!\"</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Correct:</span></div><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"literal\">true</span>) statement1 <span class=\"keyword\">else</span> statement2</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Correct:</span></div><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  foo()</div><div class=\"line\">&#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong:</span></div><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"literal\">true</span>)</div><div class=\"line\">  println(<span class=\"string\">\"Wow!\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong:</span></div><div class=\"line\"><span class=\"keyword\">try</span> foo() <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"长整型字面量\"><a href=\"#长整型字面量\" class=\"headerlink\" title=\"长整型字面量\"></a><a name='long_literal'>长整型字面量</a></h3><p>长整型字面量使用大写的 <code>L</code> 作为后缀，不要使用小写，因为它和数字 <code>1</code> 长得很像，常常难以区分。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> longValue = <span class=\"number\">5432</span>L  <span class=\"comment\">// Do this</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> longValue = <span class=\"number\">5432</span>l  <span class=\"comment\">// Do NOT do this</span></div></pre></td></tr></table></figure>\n<h3 id=\"文档风格\"><a href=\"#文档风格\" class=\"headerlink\" title=\"文档风格\"></a><a name='doc'>文档风格</a></h3><p>使用 Java Doc 风格，而非 Scala Doc 风格。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/** This is a correct one-liner, short description. */</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\"> * This is correct multi-line JavaDoc comment. And</div><div class=\"line\"> * this is my second line, and if I keep typing, this would be</div><div class=\"line\"> * my third line.</div><div class=\"line\"> */</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/** In Spark, we don't use the ScalaDoc style so this</div><div class=\"line\">  * is not correct.</div><div class=\"line\">  */</span></div></pre></td></tr></table></figure>\n<h3 id=\"类内秩序\"><a href=\"#类内秩序\" class=\"headerlink\" title=\"类内秩序\"></a><a name='ordering_class'>类内秩序</a></h3><p>如果一个类很长，包含许多的方法，那么在逻辑上把它们分成不同的部分并加上注释头，以此组织它们。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DataFrame</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">///////////////////////////////////////////////////////////////////////////</span></div><div class=\"line\">  <span class=\"comment\">// DataFrame operations</span></div><div class=\"line\">  <span class=\"comment\">///////////////////////////////////////////////////////////////////////////</span></div><div class=\"line\"></div><div class=\"line\">  ...</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">///////////////////////////////////////////////////////////////////////////</span></div><div class=\"line\">  <span class=\"comment\">// RDD operations</span></div><div class=\"line\">  <span class=\"comment\">///////////////////////////////////////////////////////////////////////////</span></div><div class=\"line\"></div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>当然，强烈不建议把一个类写得这么长，一般只有在构建某些公共 API 时才允许这么做。</p>\n<h3 id=\"Imports\"><a href=\"#Imports\" class=\"headerlink\" title=\"Imports\"></a><a name='imports'>Imports</a></h3><ul>\n<li><strong>导入时避免使用通配符</strong>, 除非你需要导入超过 6 个实体或者隐式方法。通配符导入会使代码在面对外部变化时不够健壮。</li>\n<li>始终使用绝对路径来导入包 (如：<code>scala.util.Random</code>) ，而不是相对路径 (如：<code>util.Random</code>)。</li>\n<li>此外，导入语句按照以下顺序排序：<ul>\n<li><code>java.*</code> 和 <code>javax.*</code></li>\n<li><code>scala.*</code></li>\n<li>第三方库 (<code>org.*</code>, <code>com.*</code>, 等)</li>\n<li>项目中的类 (对于 Spark 项目，即 <code>com.databricks.*</code> 或 <code>org.apache.spark</code>)</li>\n</ul>\n</li>\n<li>在每一组导入语句内，按照字母序进行排序。</li>\n<li><p>你可以使用 IntelliJ 的「import organizer」来自动处理，请使用以下配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">java</div><div class=\"line\">javax</div><div class=\"line\">_______ blank line _______</div><div class=\"line\">scala</div><div class=\"line\">_______ blank line _______</div><div class=\"line\">all other imports</div><div class=\"line\">_______ blank line _______</div><div class=\"line\">com.databricks  // or org.apache.spark if you are working on spark</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"模式匹配\"><a href=\"#模式匹配\" class=\"headerlink\" title=\"模式匹配\"></a><a name='pattern-matching'>模式匹配</a></h3><ul>\n<li><p>如果整个方法就是一个模式匹配表达式，可能的话，可以把 match 关键词与方法声明放在同一行，以此减少一级缩进。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test</span></span>(msg: <span class=\"type\">Message</span>): <span class=\"type\">Unit</span> = msg <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>当以闭包形式调用一个函数时，如果只有一个 case 语句，那么把 case 语句与函数调用放在同一行。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">list.zipWithIndex.map &#123; <span class=\"keyword\">case</span> (elem, i) =&gt;</div><div class=\"line\">  <span class=\"comment\">// ...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>如果有多个 case 语句，把它们缩进并且包起来。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">list.map &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> a: <span class=\"type\">Foo</span> =&gt;  ...</div><div class=\"line\">  <span class=\"keyword\">case</span> b: <span class=\"type\">Bar</span> =&gt;  ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>如果唯一的目的就是想匹配某个对象的类型，那么不要展开所有的参数来做模式匹配，这样会使得重构变得更加困难，代码更容易出错。</p>\n</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Pokemon</span>(<span class=\"params\">name: <span class=\"type\">String</span>, weight: <span class=\"type\">Int</span>, hp: <span class=\"type\">Int</span>, attack: <span class=\"type\">Int</span>, defense: <span class=\"type\">Int</span></span>)</span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Human</span>(<span class=\"params\">name: <span class=\"type\">String</span>, hp: <span class=\"type\">Int</span></span>)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 不要像下面那样做，因为</span></div><div class=\"line\"><span class=\"comment\">// 1. 当 pokemon 加入一个新的字段，我们需要改变下面的模式匹配代码</span></div><div class=\"line\"><span class=\"comment\">// 2. 非常容易发生误匹配，尤其是当所有字段的类型都一样的时候</span></div><div class=\"line\">targets.foreach &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> target @ <span class=\"type\">Pokemon</span>(_, _, hp, _, defense) =&gt;</div><div class=\"line\">    <span class=\"keyword\">val</span> loss = sys.min(<span class=\"number\">0</span>, myAttack - defense)</div><div class=\"line\">    target.copy(hp = hp - loss)</div><div class=\"line\">  <span class=\"keyword\">case</span> target @ <span class=\"type\">Human</span>(_, hp) =&gt;</div><div class=\"line\">    target.copy(hp = hp - myAttack)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 像下面这样做就好多了:</span></div><div class=\"line\">targets.foreach &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> target: <span class=\"type\">Pokemon</span> =&gt;</div><div class=\"line\">    <span class=\"keyword\">val</span> loss = sys.min(<span class=\"number\">0</span>, myAttack - target.defense)</div><div class=\"line\">    target.copy(hp = target.hp - loss)</div><div class=\"line\">  <span class=\"keyword\">case</span> target: <span class=\"type\">Human</span> =&gt;</div><div class=\"line\">    target.copy(hp = target.hp - myAttack)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"中缀方法\"><a href=\"#中缀方法\" class=\"headerlink\" title=\"中缀方法\"></a><a name='infix'>中缀方法</a></h3><p><strong>避免中缀表示法</strong>，除非是符号方法（即运算符重载）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Correct</span></div><div class=\"line\">list.map(func)</div><div class=\"line\">string.contains(<span class=\"string\">\"foo\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong</span></div><div class=\"line\">list map (func)</div><div class=\"line\">string contains <span class=\"string\">\"foo\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 重载的运算符应该以中缀形式调用</span></div><div class=\"line\">arrayBuffer += elem</div></pre></td></tr></table></figure>\n<h3 id=\"匿名方法\"><a href=\"#匿名方法\" class=\"headerlink\" title=\"匿名方法\"></a><a name='anonymous'>匿名方法</a></h3><p>对于匿名方法，<strong>避免使用过多的小括号和花括号</strong>。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Correct</span></div><div class=\"line\">list.map &#123; item =&gt;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Correct</span></div><div class=\"line\">list.map(item =&gt; ...)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong</span></div><div class=\"line\">list.map(item =&gt; &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong</span></div><div class=\"line\">list.map &#123; item =&gt; &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Wrong</span></div><div class=\"line\">list.map(&#123; item =&gt; ... &#125;)</div></pre></td></tr></table></figure>\n<h2 id=\"Scala-语言特性\"><a href=\"#Scala-语言特性\" class=\"headerlink\" title=\"Scala 语言特性\"></a><a name='lang'>Scala 语言特性</a></h2><h3 id=\"样例类与不可变性\"><a href=\"#样例类与不可变性\" class=\"headerlink\" title=\"样例类与不可变性\"></a><a name='case_class_immutability'>样例类与不可变性</a></h3><p>样例类（case class）本质也是普通的类，编译器会自动地为它加上以下支持：</p>\n<ul>\n<li>构造器参数的公有 getter 方法</li>\n<li>拷贝构造函数</li>\n<li>构造器参数的模式匹配</li>\n<li>默认的 toString/hash/equals 实现</li>\n</ul>\n<p>对于样例类来说，构造器参数不应设为可变的，可以使用拷贝构造函数达到同样的效果。使用可变的样例类容易出错，例如，哈希表中，对象根据旧的哈希值被放在错误的位置上。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// This is OK</span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">name: <span class=\"type\">String</span>, age: <span class=\"type\">Int</span></span>)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// This is NOT OK</span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">name: <span class=\"type\">String</span>, var age: <span class=\"type\">Int</span></span>)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 通过拷贝构造函数创建一个新的实例来改变其中的值</span></div><div class=\"line\"><span class=\"keyword\">val</span> p1 = <span class=\"type\">Person</span>(<span class=\"string\">\"Peter\"</span>, <span class=\"number\">15</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> p2 = p2.copy(age = <span class=\"number\">16</span>)</div></pre></td></tr></table></figure>\n<h3 id=\"apply-方法\"><a href=\"#apply-方法\" class=\"headerlink\" title=\"apply 方法\"></a><a name='apply_method'>apply 方法</a></h3><p>避免在类里定义 apply 方法。这些方法往往会使代码的可读性变差，尤其是对于不熟悉 Scala 的人。它也难以被 IDE（或 grep）所跟踪。在最坏的情况下，它还可能影响代码的正确性，正如你在<a href=\"#parentheses\">括号</a>一节中看到的。</p>\n<p>然而，将 apply 方法作为工厂方法定义在伴生对象中是可以接受的。在这种情况下，apply 方法应该返回其伴生类的类型。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">TreeNode</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">// 下面这种定义是 OK 的</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(name: <span class=\"type\">String</span>): <span class=\"type\">TreeNode</span> = ...</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 不要像下面那样定义，因为它没有返回其伴生类的类型：TreeNode</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(name: <span class=\"type\">String</span>): <span class=\"type\">String</span> = ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"override-修饰符\"><a href=\"#override-修饰符\" class=\"headerlink\" title=\"override 修饰符\"></a><A name='override_modifier'>override 修饰符</a></h3><p>无论是覆盖具体的方法还是实现抽象的方法，始终都为方法加上 override 修饰符。实现抽象方法时，不加 override 修饰符，Scala 编译器也不会报错。即便如此，我们也应该始终把 override 修饰符加上，以此显式地表示覆盖行为。以此避免由于方法签名不同（而你也难以发现）而导致没有覆盖到本应覆盖的方法。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Parent</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hello</span></span>(data: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    print(data)</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Child</span> <span class=\"keyword\">extends</span> <span class=\"title\">Parent</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">import</span> scala.collection.<span class=\"type\">Map</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 下面的方法没有覆盖 Parent.hello,</span></div><div class=\"line\">  <span class=\"comment\">// 因为两个 Map 的类型是不同的。</span></div><div class=\"line\">  <span class=\"comment\">// 如果我们加上 override 修饰符，编译器就会帮你找出问题并报错。</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hello</span></span>(data: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    print(<span class=\"string\">\"This is supposed to override the parent method, but it is actually not!\"</span>)</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"解构绑定\"><a href=\"#解构绑定\" class=\"headerlink\" title=\"解构绑定\"></a><a name='destruct_bind'>解构绑定</a></h3><p>解构绑定（有时也叫元组提取）是一种在一个表达式中为两个变量赋值的便捷方式。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> (a, b) = (<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div></pre></td></tr></table></figure>\n<p>然而，请不要在构造函数中使用它们，尤其是当 <code>a</code> 和 <code>b</code> 需要被标记为 <code>transient</code> 的时候。Scala 编译器会产生一个额外的 Tuple2 字段，而它并不是暂态的（transient）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyClass</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">// 以下代码无法 work，因为编译器会产生一个非暂态的 Tuple2 指向 a 和 b</span></div><div class=\"line\">  <span class=\"meta\">@transient</span> <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> (a, b) = someFuncThatReturnsTuple2()</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"按名称传参\"><a href=\"#按名称传参\" class=\"headerlink\" title=\"按名称传参\"></a><a name='call_by_name'>按名称传参</a></h3><p><strong>避免使用按名传参</strong>. 显式地使用 <code>() =&gt; T</code> 。</p>\n<p>背景：Scala 允许按名称来定义方法参数，例如：以下例子是可以成功执行的：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">print</span></span>(value: =&gt; <span class=\"type\">Int</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">  println(value)</div><div class=\"line\">  println(value + <span class=\"number\">1</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">var</span> a = <span class=\"number\">0</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">inc</span></span>(): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">  a += <span class=\"number\">1</span></div><div class=\"line\">  a</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">print(inc())</div></pre></td></tr></table></figure>\n<p>在上面的代码中，<code>inc()</code> 以闭包的形式传递给 <code>print</code> 函数，并且在 <code>print</code> 函数中被执行了两次，而不是以数值 <code>1</code> 传入。按名传参的一个主要问题是在方法调用处，我们无法区分是按名传参还是按值传参。因此无法确切地知道这个表达式是否会被执行（更糟糕的是它可能会被执行多次）。对于带有副作用的表达式来说，这一点是非常危险的。</p>\n<h3 id=\"多参数列表\"><a href=\"#多参数列表\" class=\"headerlink\" title=\"多参数列表\"></a><A name='multi-param-list'>多参数列表</a></h3><p><strong>避免使用多参数列表</strong>。它们使运算符重载变得复杂，并且会使不熟悉 Scala 的程序员感到困惑。例如：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Avoid this!</span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">name: <span class=\"type\">String</span>, age: <span class=\"type\">Int</span></span>)(<span class=\"params\">secret: <span class=\"type\">String</span></span>)</span></div></pre></td></tr></table></figure>\n<p>一个值得注意的例外是，当在定义底层库时，可以使用第二个参数列表来存放隐式（implicit）参数。尽管如此，<a href=\"#implicits\">我们应该避免使用 implicits</a>！</p>\n<h3 id=\"符号方法（运算符重载）\"><a href=\"#符号方法（运算符重载）\" class=\"headerlink\" title=\"符号方法（运算符重载）\"></a><a name='symbolic_methods'>符号方法（运算符重载）</a></h3><p><strong>不要使用符号作为方法名</strong>，除非你是在定义算术运算的方法（如：<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>），否则在任何其它情况下，都不要使用。符号化的方法名让人难以理解方法的意图是什么，来看下面两个例子：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 符号化的方法名难以理解</span></div><div class=\"line\">channel ! msg</div><div class=\"line\">stream1 &gt;&gt;= stream2</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 下面的方法意图则不言而喻</span></div><div class=\"line\">channel.send(msg)</div><div class=\"line\">stream1.join(stream2)</div></pre></td></tr></table></figure>\n<h3 id=\"类型推导\"><a href=\"#类型推导\" class=\"headerlink\" title=\"类型推导\"></a><a name='type_inference'>类型推导</a></h3><p>Scala 的类型推导，尤其是左侧类型推导以及闭包推导，可以使代码变得更加简洁。尽管如此，也有一些情况我们是需要显式地声明类型的：</p>\n<ul>\n<li><strong>公有方法应该显式地声明类型</strong>，编译器推导出来的类型往往会使你大吃一惊。</li>\n<li><strong>隐式方法应该显式地声明类型</strong>，否则在增量编译时，它会使 Scala 编译器崩溃。</li>\n<li><strong>如果变量或闭包的类型并非显而易见，请显式声明类型</strong>。一个不错的判断准则是，如果评审代码的人无法在 3 秒内确定相应实体的类型，那么你就应该显式地声明类型。</li>\n</ul>\n<h3 id=\"Return-语句\"><a href=\"#Return-语句\" class=\"headerlink\" title=\"Return 语句\"></a><a name='return'>Return 语句</a></h3><p><strong>闭包中避免使用 return</strong>。<code>return</code> 会被编译器转成 <code>scala.runtime.NonLocalReturnControl</code> 异常的 <code>try/catch</code> 语句，这可能会导致意外行为。请看下面的例子：</p>\n  <figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">receive</span></span>(rpc: <span class=\"type\">WebSocketRPC</span>): <span class=\"type\">Option</span>[<span class=\"type\">Response</span>] = &#123;</div><div class=\"line\">  tableFut.onComplete &#123; table =&gt;</div><div class=\"line\">    <span class=\"keyword\">if</span> (table.isFailure) &#123;</div><div class=\"line\">      <span class=\"keyword\">return</span> <span class=\"type\">None</span> <span class=\"comment\">// Do not do that!</span></div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; ... &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p><code>.onComplete</code> 方法接收一个匿名闭包并把它传递到一个不同的线程中。这个闭包最终会抛出一个 <code>NonLocalReturnControl</code> 异常，并在 <strong>一个不同的线程中</strong>被捕获，而这里执行的方法却没有任何影响。</p>\n<p>然而，也有少数情况我们是推荐使用 <code>return</code> 的。</p>\n<ul>\n<li><p>使用 <code>return</code> 来简化控制流，避免增加一级缩进。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">doSomething</span></span>(obj: <span class=\"type\">Any</span>): <span class=\"type\">Any</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (obj eq <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">null</span></div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">// do something ...</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>使用 <code>return</code> 来提前终止循环，这样就不用额外构造状态标志。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (cond) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"递归及尾递归\"><a href=\"#递归及尾递归\" class=\"headerlink\" title=\"递归及尾递归\"></a><a name='recursion'>递归及尾递归</a></h3><p><strong>避免使用递归</strong>，除非问题可以非常自然地用递归来描述（比如，图和树的遍历）。</p>\n<p>对于那些你意欲使之成为尾递归的方法，请加上 <code>@tailrec</code> 注解以确保编译器去检查它是否真的是尾递归（你会非常惊讶地看到，由于使用了闭包和函数变换，许多看似尾递归的代码事实并非尾递归）。</p>\n<p>大多数的代码使用简单的循环和状态机会更容易推理，使用尾递归反而可能会使它更加繁琐且难以理解。例如，下面的例子中，命令式的代码比尾递归版本的代码要更加易读：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// Tail recursive version.</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">max</span></span>(data: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>]): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">  <span class=\"meta\">@tailrec</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">max0</span></span>(data: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>], pos: <span class=\"type\">Int</span>, max: <span class=\"type\">Int</span>): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (pos == data.length) &#123;</div><div class=\"line\">      max</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">      max0(data, pos + <span class=\"number\">1</span>, <span class=\"keyword\">if</span> (data(pos) &gt; max) data(pos) <span class=\"keyword\">else</span> max)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  max0(data, <span class=\"number\">0</span>, <span class=\"type\">Int</span>.<span class=\"type\">MinValue</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Explicit loop version</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">max</span></span>(data: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>]): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">  <span class=\"keyword\">var</span> max = <span class=\"type\">Int</span>.<span class=\"type\">MinValue</span></div><div class=\"line\">  <span class=\"keyword\">for</span> (v &lt;- data) &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (v &gt; max) &#123;</div><div class=\"line\">      max = v</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  max</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"Implicits\"><a href=\"#Implicits\" class=\"headerlink\" title=\"Implicits\"></a><a name='implicits'>Implicits</a></h3><p><strong>避免使用 implicit</strong>，除非：</p>\n<ul>\n<li>你在构建领域特定的语言（DSL）</li>\n<li>你在隐式类型参数中使用它（如：<code>ClassTag</code>，<code>TypeTag</code>）</li>\n<li>你在你自己的类中使用它（意指不要污染外部空间），以此减少类型转换的冗余度（如：Scala 闭包到 Java 闭包的转换）。</li>\n</ul>\n<p>当使用 implicit 时，我们应该确保另一个工程师可以直接理解使用语义，而无需去阅读隐式定义本身。Implicit 有着非常复杂的解析规则，这会使代码变得极其难以理解。Twitter 的 Effective Scala 指南中写道：「如果你发现你在使用 implicit，始终停下来问一下你自己，是否可以在不使用 implicit 的条件下达到相同的效果」。</p>\n<p>如果你必需使用它们（比如：丰富 DSL），那么不要重载隐式方法，即确保每个隐式方法有着不同的名字，这样使用者就可以选择性地导入它们。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 别这么做，这样使用者无法选择性地只导入其中一个方法。</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">ImplicitHolder</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">toRdd</span></span>(seq: <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>]): <span class=\"type\">RDD</span>[<span class=\"type\">Int</span>] = ...</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">toRdd</span></span>(seq: <span class=\"type\">Seq</span>[<span class=\"type\">Long</span>]): <span class=\"type\">RDD</span>[<span class=\"type\">Long</span>] = ...</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 应该将它们定义为不同的名字：</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">ImplicitHolder</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">intSeqToRdd</span></span>(seq: <span class=\"type\">Seq</span>[<span class=\"type\">Int</span>]): <span class=\"type\">RDD</span>[<span class=\"type\">Int</span>] = ...</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">longSeqToRdd</span></span>(seq: <span class=\"type\">Seq</span>[<span class=\"type\">Long</span>]): <span class=\"type\">RDD</span>[<span class=\"type\">Long</span>] = ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"异常处理-Try-还是-try\"><a href=\"#异常处理-Try-还是-try\" class=\"headerlink\" title=\"异常处理 (Try 还是 try)\"></a><a name='exception'>异常处理 (Try 还是 try)</a></h2><ul>\n<li><p>不要捕获 Throwable 或 Exception 类型的异常。请使用 <code>scala.util.control.NonFatal</code>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">try</span> &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">  <span class=\"keyword\">case</span> <span class=\"type\">NonFatal</span>(e) =&gt;</div><div class=\"line\">    <span class=\"comment\">// 异常处理；注意 NonFatal 无法匹配 InterruptedException 类型的异常</span></div><div class=\"line\">  <span class=\"keyword\">case</span> e: <span class=\"type\">InterruptedException</span> =&gt;</div><div class=\"line\">    <span class=\"comment\">// 处理 InterruptedException</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>这能保证我们不会去捕获 <code>NonLocalReturnControl</code> 异常（正如在<a href=\"#return\">Return 语句</a>中所解释的）。</p>\n</li>\n<li><p>不要在 API 中使用 <code>Try</code>，即，不要在任何方法中返回 Try。对于异常执行，请显式地抛出异常，并使用 Java 风格的 try/catch 做异常处理。</p>\n<p>背景资料：Scala 提供了单子（monadic）错误处理（通过 <code>Try</code>，<code>Success</code> 和 <code>Failure</code>），这样便于做链式处理。然而，根据我们的经验，发现使用它通常会带来更多的嵌套层级，使得代码难以阅读。此外，对于预期错误还是异常，在语义上常常是不明晰的。因此，我们不鼓励使用 <code>Try</code> 来做错误处理，尤其是以下情况：</p>\n<p>一个人为的例子：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">UserService</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">/** Look up a user's profile in the user database. */</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get</span></span>(userId: <span class=\"type\">Int</span>): <span class=\"type\">Try</span>[<span class=\"type\">User</span>]</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>以下的写法会更好：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">UserService</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">/**</div><div class=\"line\">   * Look up a user's profile in the user database.</div><div class=\"line\">   * @return None if the user is not found.</div><div class=\"line\">   * @throws DatabaseConnectionException when we have trouble connecting to the database/</div><div class=\"line\">   */</span></div><div class=\"line\">  <span class=\"meta\">@throws</span>(<span class=\"type\">DatabaseConnectionException</span>)</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get</span></span>(userId: <span class=\"type\">Int</span>): <span class=\"type\">Option</span>[<span class=\"type\">User</span>]</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>第二种写法非常明显地能让调用者知道需要处理哪些错误情况。</p>\n</li>\n</ul>\n<h3 id=\"Options\"><a href=\"#Options\" class=\"headerlink\" title=\"Options\"></a><a name='option'>Options</a></h3><ul>\n<li>如果一个值可能为空，那么请使用 <code>Option</code>。相对于 <code>null</code>，<code>Option</code> 显式地表明了一个 API 的返回值可能为空。</li>\n<li><p>构造 <code>Option</code> 值时，请使用 <code>Option</code> 而非 <code>Some</code>，以防那个值为 <code>null</code>。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">myMethod1</span></span>(input: <span class=\"type\">String</span>): <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">Option</span>(transform(input))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// This is not as robust because transform can return null, and then</span></div><div class=\"line\"><span class=\"comment\">// myMethod2 will return Some(null).</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">myMethod2</span></span>(input: <span class=\"type\">String</span>): <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">Some</span>(transform(input))</div></pre></td></tr></table></figure>\n</li>\n<li><p>不要使用 None 来表示异常，有异常时请显式抛出。</p>\n</li>\n<li>不要在一个 <code>Option</code> 值上直接调用 <code>get</code> 方法，除非你百分百确定那个 <code>Option</code> 值不是 <code>None</code>。</li>\n</ul>\n<h3 id=\"单子链接\"><a href=\"#单子链接\" class=\"headerlink\" title=\"单子链接\"></a><a name='chaining'>单子链接</a></h3><p>单子链接是 Scala 的一个强大特性。Scala 中几乎一切都是单子（如：集合，Option，Future，Try 等），对它们的操作可以链接在一起。这是一个非常强大的概念，但你应该谨慎使用，尤其是：</p>\n<ul>\n<li>避免链接（或嵌套）超过 3 个操作。</li>\n<li>如果需要花超过 5 秒钟来理解其中的逻辑，那么你应该尽量去想想有没什么办法在不使用单子链接的条件下来达到相同的效果。一般来说，你需要注意的是：不要滥用 <code>flatMap</code> 和 <code>fold</code>。</li>\n<li>链接应该在 flatMap 之后断开（因为类型发生了变化）。</li>\n</ul>\n<p>通过给中间结果显式地赋予一个变量名，将链接断开变成一种更加过程化的风格，能让单子链接更加易于理解。来看下面的例子：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Person</span>(<span class=\"params\">val data: <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]</span>)</span></div><div class=\"line\"><span class=\"keyword\">val</span> database = <span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">Person</span>]</div><div class=\"line\"><span class=\"comment\">// Sometimes the client can store \"null\" value in the  store \"address\"</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// A monadic chaining approach</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getAddress</span></span>(name: <span class=\"type\">String</span>): <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = &#123;</div><div class=\"line\">  database.get(name).flatMap &#123; elem =&gt;</div><div class=\"line\">    elem.data.get(<span class=\"string\">\"address\"</span>)</div><div class=\"line\">      .flatMap(<span class=\"type\">Option</span>.apply)  <span class=\"comment\">// handle null value</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 尽管代码会长一些，但以下方法可读性更高</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getAddress</span></span>(name: <span class=\"type\">String</span>): <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = &#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (!database.contains(name)) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"type\">None</span></div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  database(name).data.get(<span class=\"string\">\"address\"</span>) <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(<span class=\"literal\">null</span>) =&gt; <span class=\"type\">None</span>  <span class=\"comment\">// handle null value</span></div><div class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(addr) =&gt; <span class=\"type\">Option</span>(addr)</div><div class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt; <span class=\"type\">None</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"并发\"><a href=\"#并发\" class=\"headerlink\" title=\"并发\"></a><a name='concurrency'>并发</a></h2><h3 id=\"Scala-concurrent-Map\"><a href=\"#Scala-concurrent-Map\" class=\"headerlink\" title=\"Scala concurrent.Map\"></a><a name='concurrency-scala-collection'>Scala concurrent.Map</a></h3><p><strong>优先考虑使用 <code>java.util.concurrent.ConcurrentHashMap</code> 而非 <code>scala.collection.concurrent.Map</code></strong>。尤其是 <code>scala.collection.concurrent.Map</code> 中的 <code>getOrElseUpdate</code> 方法要慎用，它并非原子操作（这个问题在 Scala 2.11.16 中 fix 了：<a href=\"https://issues.scala-lang.org/browse/SI-7943\">SI-7943</a>）。由于我们做的所有项目都需要在 Scala 2.10 和 Scala 2.11 上使用，因此要避免使用 <code>scala.collection.concurrent.Map</code>。</p>\n<h3 id=\"显式同步-vs-并发集合\"><a href=\"#显式同步-vs-并发集合\" class=\"headerlink\" title=\"显式同步 vs 并发集合\"></a><a name='concurrency-sync-vs-map'>显式同步 vs 并发集合</a></h3><p>有 3 种推荐的方法来安全地并发访问共享状态。<strong>不要混用它们</strong>，因为这会使程序变得难以推理，并且可能导致死锁。</p>\n<ul>\n<li><p><code>java.util.concurrent.ConcurrentHashMap</code>：当所有的状态都存储在一个 map 中，并且有高程度的竞争时使用。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">val</span> map = <span class=\"keyword\">new</span> java.util.concurrent.<span class=\"type\">ConcurrentHashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]</div></pre></td></tr></table></figure>\n</li>\n<li><p><code>java.util.Collections.synchronizedMap</code>：使用情景：当所有状态都存储在一个 map 中，并且预期不存在竞争情况，但你仍想确保代码在并发下是安全的。如果没有竞争出现，JVM 的 JIT 编译器能够通过偏置锁（biased locking）移除同步开销。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">val</span> map = java.util.<span class=\"type\">Collections</span>.synchronizedMap(<span class=\"keyword\">new</span> java.util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>])</div></pre></td></tr></table></figure>\n</li>\n<li><p>通过同步所有临界区进行显式同步，可用于监视多个变量。与 2 相似，JVM 的 JIT 编译器能够通过偏置锁（biased locking）移除同步开销。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Manager</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> count = <span class=\"number\">0</span></div><div class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">val</span> map = <span class=\"keyword\">new</span> java.util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update</span></span>(key: <span class=\"type\">String</span>, value: <span class=\"type\">String</span>): <span class=\"type\">Unit</span> = synchronized &#123;</div><div class=\"line\">    map.put(key, value)</div><div class=\"line\">    count += <span class=\"number\">1</span></div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getCount</span></span>: <span class=\"type\">Int</span> = synchronized &#123; count &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>注意，对于 case 1 和 case 2，不要让集合的视图或迭代器从保护区域逃逸。这可能会以一种不明显的方式发生，比如：返回了 <code>Map.keySet</code> 或 <code>Map.values</code>。如果需要传递集合的视图或值，生成一份数据拷贝再传递。</p>\n  <figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> map = java.util.<span class=\"type\">Collections</span>.synchronizedMap(<span class=\"keyword\">new</span> java.util.<span class=\"type\">HashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>])</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// This is broken!</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">values</span></span>: <span class=\"type\">Iterable</span>[<span class=\"type\">String</span>] = map.values</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Instead, copy the elements</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">values</span></span>: <span class=\"type\">Iterable</span>[<span class=\"type\">String</span>] = map.synchronized &#123; <span class=\"type\">Seq</span>(map.values: _*) &#125;</div></pre></td></tr></table></figure>\n<h3 id=\"显式同步-vs-原子变量-vs-volatile\"><a href=\"#显式同步-vs-原子变量-vs-volatile\" class=\"headerlink\" title=\"显式同步 vs 原子变量 vs @volatile\"></a><a name='concurrency-sync-vs-atomic'>显式同步 vs 原子变量 vs @volatile</a></h3><p><code>java.util.concurrent.atomic</code> 包提供了对基本类型的无锁访问，比如：<code>AtomicBoolean</code>, <code>AtomicInteger</code> 和 <code>AtomicReference</code>。</p>\n<p>始终优先考虑使用原子变量而非 <code>@volatile</code>，它们是相关功能的严格超集并且从代码上看更加明显。原子变量的底层实现使用了 <code>@volatile</code>。</p>\n<p>优先考虑使用原子变量而非显式同步的情况：（1）一个对象的所有临界区更新都被限制在单个变量里并且预期会有竞争情况出现。原子变量是无锁的并且允许更为有效的竞争。（2）同步被明确地表示为 <code>getAndSet</code> 操作。例如：</p>\n  <figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// good: 明确又有效地表达了下面的并发代码只执行一次</span></div><div class=\"line\"><span class=\"keyword\">val</span> initialized = <span class=\"keyword\">new</span> <span class=\"type\">AtomicBoolean</span>(<span class=\"literal\">false</span>)</div><div class=\"line\">...</div><div class=\"line\"><span class=\"keyword\">if</span> (!initialized.getAndSet(<span class=\"literal\">true</span>)) &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// poor: 下面的同步就没那么明晰，而且会出现不必要的同步</span></div><div class=\"line\"><span class=\"keyword\">val</span> initialized = <span class=\"literal\">false</span></div><div class=\"line\">...</div><div class=\"line\"><span class=\"keyword\">var</span> wasInitialized = <span class=\"literal\">false</span></div><div class=\"line\">synchronized &#123;</div><div class=\"line\">  wasInitialized = initialized</div><div class=\"line\">  initialized = <span class=\"literal\">true</span></div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"keyword\">if</span> (!wasInitialized) &#123;</div><div class=\"line\">  ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"私有字段\"><a href=\"#私有字段\" class=\"headerlink\" title=\"私有字段\"></a><a name='concurrency-private-this'>私有字段</a></h3><p>注意，<code>private</code> 字段仍然可以被相同类的其它实例所访问，所以仅仅通过 <code>this.synchronized</code>（或 <code>synchronized</code>）来保护它从技术上来说是不够的，不过你可以通过 <code>private[this]</code> 修饰私有字段来达到目的。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 以下代码仍然是不安全的。</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> count: <span class=\"type\">Int</span> = <span class=\"number\">0</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">inc</span></span>(): <span class=\"type\">Unit</span> = synchronized &#123; count += <span class=\"number\">1</span> &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 以下代码是安全的。</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> count: <span class=\"type\">Int</span> = <span class=\"number\">0</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">inc</span></span>(): <span class=\"type\">Unit</span> = synchronized &#123; count += <span class=\"number\">1</span> &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"隔离\"><a href=\"#隔离\" class=\"headerlink\" title=\"隔离\"></a><a name='concurrency-isolation'>隔离</a></h3><p>一般来说，并发和同步逻辑应该尽可能地被隔离和包含起来。这实际上意味着：</p>\n<ul>\n<li>避免在 API 层面、面向用户的方法以及回调中暴露同步原语。</li>\n<li>对于复杂模块，创建一个小的内部模块来包含并发原语。</li>\n</ul>\n<h2 id=\"性能\"><a href=\"#性能\" class=\"headerlink\" title=\"性能\"></a><a name='perf'>性能</a></h2><p>对于你写的绝大多数代码，性能都不应该成为一个问题。然而，对于一些性能敏感的代码，以下有一些小建议：</p>\n<h3 id=\"Microbenchmarks\"><a href=\"#Microbenchmarks\" class=\"headerlink\" title=\"Microbenchmarks\"></a><a name='perf-microbenchmarks'>Microbenchmarks</a></h3><p>由于 Scala 编译器和 JVM JIT 编译器会对你的代码做许多神奇的事情，因此要写出一个好的微基准程序（microbenchmark）是极其困难的。更多的情况往往是你的微基准程序并没有测量你想要测量的东西。</p>\n<p>如果你要写一个微基准程序，请使用 <a href=\"http://openjdk.java.net/projects/code-tools/jmh/\">jmh</a>。请确保你阅读了<a href=\"http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/\">所有的样例</a>，这样你才理解微基准程序中「死代码」移除、常量折叠以及循环展开的效果。</p>\n<h3 id=\"Traversal-与-zipWithIndex\"><a href=\"#Traversal-与-zipWithIndex\" class=\"headerlink\" title=\"Traversal 与 zipWithIndex\"></a><a name='perf-whileloops'>Traversal 与 zipWithIndex</a></h3><p>使用 <code>while</code> 循环而非 <code>for</code> 循环或函数变换（如：<code>map</code>、<code>foreach</code>），for 循环和函数变换非常慢（由于虚函数调用和装箱的缘故）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> arr = <span class=\"comment\">// array of ints</span></div><div class=\"line\"><span class=\"comment\">// 偶数位置的数置零</span></div><div class=\"line\"><span class=\"keyword\">val</span> newArr = list.zipWithIndex.map &#123; <span class=\"keyword\">case</span> (elem, i) =&gt;</div><div class=\"line\">  <span class=\"keyword\">if</span> (i % <span class=\"number\">2</span> == <span class=\"number\">0</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> elem</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 这是上面代码的高性能版本</span></div><div class=\"line\"><span class=\"keyword\">val</span> newArr = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Int</span>](arr.length)</div><div class=\"line\"><span class=\"keyword\">var</span> i = <span class=\"number\">0</span></div><div class=\"line\"><span class=\"keyword\">val</span> len = newArr.length</div><div class=\"line\"><span class=\"keyword\">while</span> (i &lt; len) &#123;</div><div class=\"line\">  newArr(i) = <span class=\"keyword\">if</span> (i % <span class=\"number\">2</span> == <span class=\"number\">0</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> arr(i)</div><div class=\"line\">  i += <span class=\"number\">1</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"Option-与-null\"><a href=\"#Option-与-null\" class=\"headerlink\" title=\"Option 与 null\"></a><a name='perf-option'>Option 与 null</a></h3><p>对于性能有要求的代码，优先考虑使用 <code>null</code> 而不是 <code>Option</code>，以此避免虚函数调用以及装箱操作。用 Nullable 注解明确标示出可能为 <code>null</code> 的值。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"meta\">@javax</span>.annotation.<span class=\"type\">Nullable</span></div><div class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> nullableField: <span class=\"type\">Bar</span> = _</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"Scala-集合库\"><a href=\"#Scala-集合库\" class=\"headerlink\" title=\"Scala 集合库\"></a><a name='perf-collection'>Scala 集合库</a></h3><p>对于性能有要求的代码，优先考虑使用 Java 集合库而非 Scala 集合库，因为一般来说，Scala 集合库要比 Java 的集合库慢。</p>\n<h3 id=\"private-this\"><a href=\"#private-this\" class=\"headerlink\" title=\"private[this]\"></a><a name='perf-private'>private[this]</a></h3><p>对于性能有要求的代码，优先考虑使用 <code>private[this]</code> 而非 <code>private</code>。<code>private[this]</code> 生成一个字段而非生成一个访问方法。根据我们的经验，JVM JIT 编译器并不总是会内联 <code>private</code> 字段的访问方法，因此通过使用<br><code>private[this]</code> 来确保没有虚函数调用会更保险。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyClass</span> </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> field1 = ...</div><div class=\"line\">  <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">val</span> field2 = ...</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">perfSensitiveMethod</span></span>(): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></div><div class=\"line\">    <span class=\"keyword\">while</span> (i &lt; <span class=\"number\">1000000</span>) &#123;</div><div class=\"line\">      field1  <span class=\"comment\">// This might invoke a virtual method call</span></div><div class=\"line\">      field2  <span class=\"comment\">// This is just a field access</span></div><div class=\"line\">      i += <span class=\"number\">1</span></div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"与-Java-的互操作性\"><a href=\"#与-Java-的互操作性\" class=\"headerlink\" title=\"与 Java 的互操作性\"></a><a name='java'>与 Java 的互操作性</a></h2><p>本节内容介绍的是构建 Java 兼容 API 的准则。如果你构建的组件并不需要与 Java 有交互，那么请无视这一节。这一节的内容主要是从我们开发 Spark 的 Java API 的经历中得出的。</p>\n<h3 id=\"Scala-中缺失的-Java-特性\"><a href=\"#Scala-中缺失的-Java-特性\" class=\"headerlink\" title=\"Scala 中缺失的 Java 特性\"></a><a name='java-missing-features'>Scala 中缺失的 Java 特性</a></h3><p>以下的 Java 特性在 Scala 中是没有的，如果你需要使用以下特性，请在 Java 中定义它们。然而，需要提醒一点的是，你无法为 Java 源文件生成 ScalaDoc。</p>\n<ul>\n<li>静态字段</li>\n<li>静态内部类</li>\n<li>Java 枚举</li>\n<li>注解</li>\n</ul>\n<h3 id=\"Traits-与抽象类\"><a href=\"#Traits-与抽象类\" class=\"headerlink\" title=\"Traits 与抽象类\"></a><a name='java-traits'>Traits 与抽象类</a></h3><p>对于允许从外部实现的接口，请记住以下几点：</p>\n<ul>\n<li>包含了默认方法实现的 trait 是无法在 Java 中使用的，请使用抽象类来代替。</li>\n<li>一般情况下，请避免使用 trait，除非你百分百确定这个接口即使在未来也不会有默认的方法实现。</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 以下默认实现无法在 Java 中使用</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Listener</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onTermination</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 可以在 Java 中使用</span></div><div class=\"line\"><span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Listener</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">onTermination</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"类型别名\"><a href=\"#类型别名\" class=\"headerlink\" title=\"类型别名\"></a><a name='java-type-alias'>类型别名</a></h3><p>不要使用类型别名，它们在字节码和 Java 中是不可见的。</p>\n<h3 id=\"默认参数值\"><a href=\"#默认参数值\" class=\"headerlink\" title=\"默认参数值\"></a><a name='java-default-param-values'>默认参数值</a></h3><p>不要使用默认参数值，通过重载方法来代替。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 打破了与 Java 的互操作性</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample</span></span>(ratio: <span class=\"type\">Double</span>, withReplacement: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = &#123; ... &#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 以下方法是 work 的</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample</span></span>(ratio: <span class=\"type\">Double</span>, withReplacement: <span class=\"type\">Boolean</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = &#123; ... &#125;</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sample</span></span>(ratio: <span class=\"type\">Double</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = sample(ratio, withReplacement = <span class=\"literal\">false</span>)</div></pre></td></tr></table></figure>\n<h3 id=\"多参数列表-1\"><a href=\"#多参数列表-1\" class=\"headerlink\" title=\"多参数列表\"></a><a name='java-multi-param-list'>多参数列表</a></h3><p>不要使用多参数列表。</p>\n<h3 id=\"可变参数\"><a href=\"#可变参数\" class=\"headerlink\" title=\"可变参数\"></a><a name='java-varargs'>可变参数</a></h3><ul>\n<li><p>为可变参数方法添加 <code>@scala.annotation.varargs</code> 注解，以确保它能在 Java 中使用。Scala 编译器会生成两个方法，一个给 Scala 使用（字节码参数是一个 Seq），另一个给 Java 使用（字节码参数是一个数组）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@scala</span>.annotation.varargs</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">select</span></span>(exprs: <span class=\"type\">Expression</span>*): <span class=\"type\">DataFrame</span> = &#123; ... &#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>需要注意的一点是，由于 Scala 编译器的一个 bug（<a href=\"https://issues.scala-lang.org/browse/SI-1459\">SI-1459</a>，<a href=\"https://issues.scala-lang.org/browse/SI-9013\">SI-9013</a>），抽象的变参方法是无法在 Java 中使用的。</p>\n</li>\n<li><p>重载变参方法时要小心，用另一个类型去重载变参方法会破坏源码的兼容性。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Database</span> </span>&#123;</div><div class=\"line\">  <span class=\"meta\">@scala</span>.annotation.varargs</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">remove</span></span>(elems: <span class=\"type\">String</span>*): <span class=\"type\">Unit</span> = ...</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 当调用无参的 remove 方法时会出问题。</span></div><div class=\"line\">  <span class=\"meta\">@scala</span>.annotation.varargs</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">remove</span></span>(elems: <span class=\"type\">People</span>*): <span class=\"type\">Unit</span> = ...</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// remove 方法有歧义，因此编译不过。</span></div><div class=\"line\"><span class=\"keyword\">new</span> <span class=\"type\">Database</span>().remove()</div></pre></td></tr></table></figure>\n<p>一种解决方法是，在可变参数前显式地定义第一个参数：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Database</span> </span>&#123;</div><div class=\"line\">  <span class=\"meta\">@scala</span>.annotation.varargs</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">remove</span></span>(elems: <span class=\"type\">String</span>*): <span class=\"type\">Unit</span> = ...</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 以下重载是 OK 的。</span></div><div class=\"line\">  <span class=\"meta\">@scala</span>.annotation.varargs</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">remove</span></span>(elem: <span class=\"type\">People</span>, elems: <span class=\"type\">People</span>*): <span class=\"type\">Unit</span> = ...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"Implicits-1\"><a href=\"#Implicits-1\" class=\"headerlink\" title=\"Implicits\"></a><a name='java-implicits'>Implicits</a></h3><p>不要为类或方法使用 implicit，包括了不要使用 <code>ClassTag</code> 和 <code>TypeTag</code>。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JavaFriendlyAPI</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">// 以下定义对 Java 是不友好的，因为方法中包含了一个隐式参数（ClassTag）。</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convertTo</span></span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](): <span class=\"type\">T</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h3 id=\"伴生对象，静态方法与字段\"><a href=\"#伴生对象，静态方法与字段\" class=\"headerlink\" title=\"伴生对象，静态方法与字段\"></a><a name='java-companion-object'>伴生对象，静态方法与字段</a></h3><p>当涉及到伴生对象和静态方法/字段时，有几件事情是需要注意的：</p>\n<ul>\n<li><p>伴生对象在 Java 中的使用是非常别扭的（伴生对象 <code>Foo</code> 会被定义为 <code>Foo$</code> 类内的一个类型为 <code>Foo$</code> 的静态字段 <code>MODULE$</code>）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Foo</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 等价于以下的 Java 代码</span></div><div class=\"line\">public <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo$</span> </span>&#123;</div><div class=\"line\">  <span class=\"type\">Foo</span>$ <span class=\"type\">MODULE</span>$ = <span class=\"comment\">// 对象的实例化</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>如果非要使用伴生对象，可以在一个单独的类中创建一个 Java 静态字段。</p>\n</li>\n<li><p>不幸的是，没有办法在 Scala 中定义一个 JVM 静态字段。请创建一个 Java 文件来定义它。</p>\n</li>\n<li><p>伴生对象里的方法会被自动转成伴生类里的静态方法，除非方法名有冲突。确保静态方法正确生成的最好方式是用 Java 写一个测试文件，然后调用生成的静态方法。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">method2</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Foo</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">method1</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;  <span class=\"comment\">// 静态方法 Foo.method1 会被创建（字节码）</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">method2</span></span>(): <span class=\"type\">Unit</span> = &#123; ... &#125;  <span class=\"comment\">// 静态方法 Foo.method2 不会被创建</span></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// FooJavaTest.java (in test/scala/com/databricks/...)</span></div><div class=\"line\">public <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FooJavaTest</span> </span>&#123;</div><div class=\"line\">  public static void compileTest() &#123;</div><div class=\"line\">    <span class=\"type\">Foo</span>.method1();  <span class=\"comment\">// 正常编译</span></div><div class=\"line\">    <span class=\"type\">Foo</span>.method2();  <span class=\"comment\">// 编译失败，因为 method2 并没有生成</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>样例对象（case object） MyClass 的类型并不是 MyClass。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">MyClass</span></span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// Test.java</span></div><div class=\"line\"><span class=\"keyword\">if</span> (<span class=\"type\">MyClass</span>$.<span class=\"type\">MODULE</span> instanceof <span class=\"type\">MyClass</span>) &#123;</div><div class=\"line\">  <span class=\"comment\">// 上述条件始终为 false</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>要实现正确的类型层级结构，请定义一个伴生类，然后用一个样例对象去继承它：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyClass</span></span></div><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">MyClass</span> <span class=\"keyword\">extends</span> <span class=\"title\">MyClass</span></span></div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"其它\"><a href=\"#其它\" class=\"headerlink\" title=\"其它\"></a><a name='misc'>其它</a></h2><h3 id=\"优先使用-nanoTime-而非-currentTimeMillis\"><a href=\"#优先使用-nanoTime-而非-currentTimeMillis\" class=\"headerlink\" title=\"优先使用 nanoTime 而非 currentTimeMillis\"></a><a name='misc_currentTimeMillis_vs_nanoTime'>优先使用 nanoTime 而非 currentTimeMillis</a></h3><p>当要计算<em>持续时间</em>或者检查<em>超时</em>的时候，避免使用 <code>System.currentTimeMillis()</code>。请使用 <code>System.nanoTime()</code>，即使你对亚毫秒级的精度并不感兴趣。</p>\n<p><code>System.currentTimeMillis()</code> 返回的是当前的时钟时间，并且会跟进系统时钟的改变。因此，负的时钟调整可能会导致超时而挂起很长一段时间（直到时钟时间赶上先前的值）。这种情况可能发生在网络已经中断一段时间，ntpd 走过了一步之后。最典型的例子是，在系统启动的过程中，DHCP 花费的时间要比平常的长。这可能会导致非常难以理解且难以重现的问题。而 <code>System.nanoTime()</code> 则可以保证是单调递增的，与时钟变化无关。</p>\n<p>注意事项：</p>\n<ul>\n<li>永远不要序列化一个绝对的 <code>nanoTime()</code> 值或是把它传递给另一个系统。绝对的 <code>nanoTime()</code> 值是无意义的、与系统相关的，并且在系统重启时会重置。</li>\n<li>绝对的 <code>nanoTime()</code> 值并不保证总是正数（但 <code>t2 - t1</code> 能确保总是产生正确的值）。</li>\n<li><code>nanoTime()</code> 每 292 年就会重新计算起。所以，如果你的 Spark 任务需要花非常非常非常长的时间，你可能需要别的东西来处理了：）</li>\n</ul>\n<h3 id=\"优先使用-URI-而非-URL\"><a href=\"#优先使用-URI-而非-URL\" class=\"headerlink\" title=\"优先使用 URI 而非 URL\"></a><a name='misc_uri_url'>优先使用 URI 而非 URL</a></h3><p>当存储服务的 URL 时，你应当使用 <code>URI</code> 来表示。</p>\n<p><code>URL</code> 的<a href=\"http://docs.oracle.com/javase/7/docs/api/java/net/URL.html#equals(java.lang.Object\">相等性检查</a>)实际上执行了一次网络调用（这是阻塞的）来解析 IP 地址。<code>URI</code> 类在表示能力上是 <code>URL</code> 的超集，并且它执行的是字段的相等性检查。</p>\n<h2 id=\"单元测试\"><a href=\"#单元测试\" class=\"headerlink\" title=\"单元测试\"></a><a name='unit-test'>单元测试</a></h2><h3 id=\"单元测试框架\"><a href=\"#单元测试框架\" class=\"headerlink\" title=\"单元测试框架\"></a><a name='unit-test-framework'>单元测试框架</a></h3><p>ScalaTest几乎已经成为Scala语言默认的测试框架，这主要源于它提供了多种表达力超强的测试风格，能够满足各种层次的需求包括单元测试、BDD、验收测试、数据驱动测试。我们也使用<a href=\"http://www.scalatest.org/\">ScalaTest</a>测试框架。使用的时候在<code>pom.xml</code>中添加如下类似引用：</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">&lt;!-- test: https://mvnrepository.com/artifact/org.scalatest/scalatest_2.10 --&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.scalatest<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>scalatest_2.10<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>3.0.0<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">scope</span>&gt;</span>test<span class=\"tag\">&lt;/<span class=\"name\">scope</span>&gt;</span></div><div class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></div></pre></td></tr></table></figure>\n<p>测试我们遵循以下几点规则：</p>\n<ul>\n<li>测试类应该与被测试类处于同一包下，测试类的命名为：被测试类名 + Test</li>\n<li>测试含有具体实现的trait时，可以让被测试类直接继承Trait。例如：</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">RecordsGenerator</span> </span>&#123;</div><div class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generateRecords</span></span>(table: <span class=\"type\">List</span>[<span class=\"type\">List</span>[<span class=\"type\">String</span>]]): <span class=\"type\">List</span>[<span class=\"type\">Record</span>] &#123;</div><div class=\"line\">          <span class=\"comment\">//...</span></div><div class=\"line\">     &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RecordsGeneratorSpec</span> <span class=\"keyword\">extends</span> <span class=\"title\">FlatSpec</span> <span class=\"keyword\">with</span> <span class=\"title\">ShouldMatcher</span> <span class=\"keyword\">with</span> <span class=\"title\">RecordGenerator</span> </span>&#123;</div><div class=\"line\">     <span class=\"keyword\">val</span> table = <span class=\"type\">List</span>(<span class=\"type\">List</span>(<span class=\"string\">\"abc\"</span>, <span class=\"string\">\"def\"</span>), <span class=\"type\">List</span>(<span class=\"string\">\"aaa\"</span>, <span class=\"string\">\"bbb\"</span>))</div><div class=\"line\">     it should <span class=\"string\">\"generate records\"</span> in &#123;</div><div class=\"line\">          <span class=\"keyword\">val</span> records = generateRecords(table)</div><div class=\"line\">          records.size should be(<span class=\"number\">2</span>)</div><div class=\"line\">     &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>若要对文件进行测试，可以用字符串假装文件：</li>\n</ul>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">type</span> <span class=\"title\">CsvLine</span> </span>= <span class=\"type\">String</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">formatCsv</span></span>(source: <span class=\"type\">Source</span>): <span class=\"type\">List</span>[<span class=\"type\">CsvLine</span>] = &#123;</div><div class=\"line\">     source.getLines(_.replace(<span class=\"string\">\", \"</span>, <span class=\"string\">\"|\"</span>))</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>formatCsv需要接受一个文件源，例如Source.fromFile(“testdata.txt”)。但在测试时，可以通过Source.fromString方法来生成formatCsv需要接收的Source对象：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">test(<span class=\"string\">\"format csv lines\"</span>) &#123;</div><div class=\"line\">     <span class=\"keyword\">val</span> lines = <span class=\"type\">Source</span>.fromString(<span class=\"string\">\"abc, def, hgi\\n1, 2, 3\\none, two, three\"</span>)</div><div class=\"line\">     <span class=\"keyword\">val</span> result = formatCsv(lines)</div><div class=\"line\">     assert(result.mkString(<span class=\"string\">\"\\n\"</span>).equles(<span class=\"string\">\"abc|def|hgi\\n1|2|3\\none|two|three\"</span>))</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"测试风格的选择\"><a href=\"#测试风格的选择\" class=\"headerlink\" title=\"测试风格的选择\"></a><a name='unit-test-style'>测试风格的选择</a></h3><p>ScalaTest一共提供了七种测试风格，分别为：FunSuite，FlatSpec，FunSpec，WordSpec，FreeSpec，PropSpec和FeatureSpec。这就好像使用相同的原料做成不同美味乃至不同菜系的佳肴，你可以根据自己的口味进行选择。我们统一推荐使用FunSuite的方式，因为它更灵活，而且更符合传统测试方法的风格，区别仅在于test()方法可以接受一个闭包:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> org.scalatest.<span class=\"type\">FunSuite</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SetSuite</span> <span class=\"keyword\">extends</span> <span class=\"title\">FunSuite</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">  test(<span class=\"string\">\"An empty Set should have size 0\"</span>) &#123;</div><div class=\"line\">    assert(<span class=\"type\">Set</span>.empty.size == <span class=\"number\">0</span>)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  test(<span class=\"string\">\"Invoking head on an empty Set should produce NoSuchElementException\"</span>) &#123;</div><div class=\"line\">    assertThrows[<span class=\"type\">NoSuchElementException</span>] &#123;</div><div class=\"line\">      <span class=\"type\">Set</span>.empty.head</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>当然，如果你有必须的理由选择其它测试风格的话，本规则并不强制。</p>\n<h1 id=\"三、-引用\"><a href=\"#三、-引用\" class=\"headerlink\" title=\"三、 引用\"></a>三、 引用</h1><p><a href=\"https://github.com/databricks/scala-style-guide/blob/master/README-ZH.md\">Databricks Scala 编程风格指南</a>(团队最终选择的模板)<br><a href=\"http://twitter.github.io/effectivescala/index-cn.html\">Effective Scala</a>(Twitter Scala资料，值得参考)<br><a href=\"https://zhangyi.gitbooks.io/thinking-in-scala/content/scala-convention.html\">Thinking in Scala–Scala编程规范</a>(个人整理，可以参考)<br><a href=\"http://www.scala-lang.org/docu/files/Scala%E8%AF%AD%E8%A8%80%E8%A7%84%E8%8C%83.pdf\">scala-lang–Scala语言规范.pdf</a>(官方原版的中文文档，共127页，过于复杂琐碎，可以参考)<br><a href=\"http://docs.scala-lang.org/style/\">Scala官网Style Guide</a>(官方原版，其它版本基本上都是基于此版进行的改进)<br><a href=\"https://segmentfault.com/a/1190000000420018\">分析GitHub上托管的scala开源代码统计</a></p>\n"},{"title":"E20914023王小刚学期总结","toc":true,"date":"2009-12-28T11:38:37.000Z","_content":"\n注： 辅导员要求写的学期总结。。。\n—— 2009年冬于安大新区\n\n　　大学的第一学期就这样过去了，我经历了很多：有第一次一个人在外生活的兴奋与迷茫；当班委参加各种活动的激情与失落；和永远难忘的考完试后的郁闷与悲伤……来到安大，感觉我这半年经历了比过去17年还多的东西，总之，大一的上半学期是永远难忘的，虽然以悲剧收场。\n\n　　当我兴奋地来到安大，一切变得让我傻眼，几乎所有的东西都是第一次：第一次打水，第一次洗那么大的衣服，还有第一次进网吧，一切都是如此的新鲜以至我差点迷了路，但大学主要是学习啊！在这个时候我还是比较清醒，跟着室友跑去上自习，在学习上我就是一个十足的差生，能来到安大就很侥幸的我基础真的很差，室友的英语都比我高出的二三十分，当我们一起上课或者一起上自习的时候，就能看出来，我真的很佩服我的室友沈诗文，他做事很有效率，我们一起上自习，他总是会的比我多多了，我一直以沈诗文为我的目标，但我总是没有他厉害，没关系的，我一定好好努力，争取超过他。\n\n　　这段时间我是极度低落的，考得太烂，虽然我不知道成绩，但考完试之后我知道我已经完蛋了，真想消失在这个世界，感觉自己这几个月的努力全部败在了那天早上，唉，不想了，“有的人总是回忆过去，幻想未来，却忘了现在……”我必须好好学习了，今年回家必须要恶补英语和C语言，高数当然也要补。早上背英语单词，和C语言，下午看C语言和高数，主要还是英语。以我现在的状况要预习下半学期的课是真有困难，我这个人是很没有决心的，但我已经找到了克服的方法，寒假必须得恶补了。我的目标是唯一的——考研，所以下一学期我会以学习为最主要目的，一切都是为考研而准备。相信自己，我能行的！\n\n　　说活动嘛，我想我是成功的：合唱真爽，经过的突然的悲伤，然后又突然的喜悦，成绩不错就不说了\n\n　　还记得“计科院足球队”在“雀巢”上飞奔的身影，进球后抱在一起的喜悦，失球后兄弟般的团结；还记得和本班的，外班的同学打篮球的激情；还有插花比赛，交了几个很好的朋友，还意外的得了省级奖\n\n还有篮球友谊赛，辩论赛，运动会，口拓协会，象棋协会……太多了，我在这方面真充实。我想，虽然大学学习紧张，但如果你连一个活动都不参加，就太失败了。\n\n　　作为体育委员，哇，真的挺爽的，既锻炼了自己，又认识了这么多的女生，本来内向的我都不内向了，我组织参加了很多活动啊：比如运动会，跳绳比赛，篮球友谊赛，班里同学周末高兴地打篮球，我能感觉到我的各方面能力的提高，如果我不挂科的话，我还想把体育委员当下去。\n\n　　在安大，我认识了很多很多的朋友，比如我们寝室的沈诗文啊，王群啊，高龙啊，詹新成啊……太多了，感觉大学的朋友因为长时间在一起，关系真的很好，我们班的每一个人都给我留下很好的印象，不愧是一本大学啊！我们班的女生也人都很好，现在朋友一个一个得回家了，我都开始想了，我这个人学习不好，可在交朋友方面我还是挺自信的。\n\n　　啊！不说了，寝室楼也快要关门了，一不小心就写多了，殷老师，提前祝你春节快乐！\n","source":"_posts/2017-01-12-E20914023王小刚学期总结.md","raw":"---\ntitle: E20914023王小刚学期总结\ntoc: true\ndate: 2009-12-28 19:38:37\ntags:\ncategories: 单车岁月\n---\n\n注： 辅导员要求写的学期总结。。。\n—— 2009年冬于安大新区\n\n　　大学的第一学期就这样过去了，我经历了很多：有第一次一个人在外生活的兴奋与迷茫；当班委参加各种活动的激情与失落；和永远难忘的考完试后的郁闷与悲伤……来到安大，感觉我这半年经历了比过去17年还多的东西，总之，大一的上半学期是永远难忘的，虽然以悲剧收场。\n\n　　当我兴奋地来到安大，一切变得让我傻眼，几乎所有的东西都是第一次：第一次打水，第一次洗那么大的衣服，还有第一次进网吧，一切都是如此的新鲜以至我差点迷了路，但大学主要是学习啊！在这个时候我还是比较清醒，跟着室友跑去上自习，在学习上我就是一个十足的差生，能来到安大就很侥幸的我基础真的很差，室友的英语都比我高出的二三十分，当我们一起上课或者一起上自习的时候，就能看出来，我真的很佩服我的室友沈诗文，他做事很有效率，我们一起上自习，他总是会的比我多多了，我一直以沈诗文为我的目标，但我总是没有他厉害，没关系的，我一定好好努力，争取超过他。\n\n　　这段时间我是极度低落的，考得太烂，虽然我不知道成绩，但考完试之后我知道我已经完蛋了，真想消失在这个世界，感觉自己这几个月的努力全部败在了那天早上，唉，不想了，“有的人总是回忆过去，幻想未来，却忘了现在……”我必须好好学习了，今年回家必须要恶补英语和C语言，高数当然也要补。早上背英语单词，和C语言，下午看C语言和高数，主要还是英语。以我现在的状况要预习下半学期的课是真有困难，我这个人是很没有决心的，但我已经找到了克服的方法，寒假必须得恶补了。我的目标是唯一的——考研，所以下一学期我会以学习为最主要目的，一切都是为考研而准备。相信自己，我能行的！\n\n　　说活动嘛，我想我是成功的：合唱真爽，经过的突然的悲伤，然后又突然的喜悦，成绩不错就不说了\n\n　　还记得“计科院足球队”在“雀巢”上飞奔的身影，进球后抱在一起的喜悦，失球后兄弟般的团结；还记得和本班的，外班的同学打篮球的激情；还有插花比赛，交了几个很好的朋友，还意外的得了省级奖\n\n还有篮球友谊赛，辩论赛，运动会，口拓协会，象棋协会……太多了，我在这方面真充实。我想，虽然大学学习紧张，但如果你连一个活动都不参加，就太失败了。\n\n　　作为体育委员，哇，真的挺爽的，既锻炼了自己，又认识了这么多的女生，本来内向的我都不内向了，我组织参加了很多活动啊：比如运动会，跳绳比赛，篮球友谊赛，班里同学周末高兴地打篮球，我能感觉到我的各方面能力的提高，如果我不挂科的话，我还想把体育委员当下去。\n\n　　在安大，我认识了很多很多的朋友，比如我们寝室的沈诗文啊，王群啊，高龙啊，詹新成啊……太多了，感觉大学的朋友因为长时间在一起，关系真的很好，我们班的每一个人都给我留下很好的印象，不愧是一本大学啊！我们班的女生也人都很好，现在朋友一个一个得回家了，我都开始想了，我这个人学习不好，可在交朋友方面我还是挺自信的。\n\n　　啊！不说了，寝室楼也快要关门了，一不小心就写多了，殷老师，提前祝你春节快乐！\n","slug":"E20914023王小刚学期总结","published":1,"updated":"2017-01-12T11:40:12.258Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfwv002ipsgulmexvn0o","content":"<p>注： 辅导员要求写的学期总结。。。<br>—— 2009年冬于安大新区</p>\n<p>　　大学的第一学期就这样过去了，我经历了很多：有第一次一个人在外生活的兴奋与迷茫；当班委参加各种活动的激情与失落；和永远难忘的考完试后的郁闷与悲伤……来到安大，感觉我这半年经历了比过去17年还多的东西，总之，大一的上半学期是永远难忘的，虽然以悲剧收场。</p>\n<p>　　当我兴奋地来到安大，一切变得让我傻眼，几乎所有的东西都是第一次：第一次打水，第一次洗那么大的衣服，还有第一次进网吧，一切都是如此的新鲜以至我差点迷了路，但大学主要是学习啊！在这个时候我还是比较清醒，跟着室友跑去上自习，在学习上我就是一个十足的差生，能来到安大就很侥幸的我基础真的很差，室友的英语都比我高出的二三十分，当我们一起上课或者一起上自习的时候，就能看出来，我真的很佩服我的室友沈诗文，他做事很有效率，我们一起上自习，他总是会的比我多多了，我一直以沈诗文为我的目标，但我总是没有他厉害，没关系的，我一定好好努力，争取超过他。</p>\n<p>　　这段时间我是极度低落的，考得太烂，虽然我不知道成绩，但考完试之后我知道我已经完蛋了，真想消失在这个世界，感觉自己这几个月的努力全部败在了那天早上，唉，不想了，“有的人总是回忆过去，幻想未来，却忘了现在……”我必须好好学习了，今年回家必须要恶补英语和C语言，高数当然也要补。早上背英语单词，和C语言，下午看C语言和高数，主要还是英语。以我现在的状况要预习下半学期的课是真有困难，我这个人是很没有决心的，但我已经找到了克服的方法，寒假必须得恶补了。我的目标是唯一的——考研，所以下一学期我会以学习为最主要目的，一切都是为考研而准备。相信自己，我能行的！</p>\n<p>　　说活动嘛，我想我是成功的：合唱真爽，经过的突然的悲伤，然后又突然的喜悦，成绩不错就不说了</p>\n<p>　　还记得“计科院足球队”在“雀巢”上飞奔的身影，进球后抱在一起的喜悦，失球后兄弟般的团结；还记得和本班的，外班的同学打篮球的激情；还有插花比赛，交了几个很好的朋友，还意外的得了省级奖</p>\n<p>还有篮球友谊赛，辩论赛，运动会，口拓协会，象棋协会……太多了，我在这方面真充实。我想，虽然大学学习紧张，但如果你连一个活动都不参加，就太失败了。</p>\n<p>　　作为体育委员，哇，真的挺爽的，既锻炼了自己，又认识了这么多的女生，本来内向的我都不内向了，我组织参加了很多活动啊：比如运动会，跳绳比赛，篮球友谊赛，班里同学周末高兴地打篮球，我能感觉到我的各方面能力的提高，如果我不挂科的话，我还想把体育委员当下去。</p>\n<p>　　在安大，我认识了很多很多的朋友，比如我们寝室的沈诗文啊，王群啊，高龙啊，詹新成啊……太多了，感觉大学的朋友因为长时间在一起，关系真的很好，我们班的每一个人都给我留下很好的印象，不愧是一本大学啊！我们班的女生也人都很好，现在朋友一个一个得回家了，我都开始想了，我这个人学习不好，可在交朋友方面我还是挺自信的。</p>\n<p>　　啊！不说了，寝室楼也快要关门了，一不小心就写多了，殷老师，提前祝你春节快乐！</p>\n","excerpt":"","more":"<p>注： 辅导员要求写的学期总结。。。<br>—— 2009年冬于安大新区</p>\n<p>　　大学的第一学期就这样过去了，我经历了很多：有第一次一个人在外生活的兴奋与迷茫；当班委参加各种活动的激情与失落；和永远难忘的考完试后的郁闷与悲伤……来到安大，感觉我这半年经历了比过去17年还多的东西，总之，大一的上半学期是永远难忘的，虽然以悲剧收场。</p>\n<p>　　当我兴奋地来到安大，一切变得让我傻眼，几乎所有的东西都是第一次：第一次打水，第一次洗那么大的衣服，还有第一次进网吧，一切都是如此的新鲜以至我差点迷了路，但大学主要是学习啊！在这个时候我还是比较清醒，跟着室友跑去上自习，在学习上我就是一个十足的差生，能来到安大就很侥幸的我基础真的很差，室友的英语都比我高出的二三十分，当我们一起上课或者一起上自习的时候，就能看出来，我真的很佩服我的室友沈诗文，他做事很有效率，我们一起上自习，他总是会的比我多多了，我一直以沈诗文为我的目标，但我总是没有他厉害，没关系的，我一定好好努力，争取超过他。</p>\n<p>　　这段时间我是极度低落的，考得太烂，虽然我不知道成绩，但考完试之后我知道我已经完蛋了，真想消失在这个世界，感觉自己这几个月的努力全部败在了那天早上，唉，不想了，“有的人总是回忆过去，幻想未来，却忘了现在……”我必须好好学习了，今年回家必须要恶补英语和C语言，高数当然也要补。早上背英语单词，和C语言，下午看C语言和高数，主要还是英语。以我现在的状况要预习下半学期的课是真有困难，我这个人是很没有决心的，但我已经找到了克服的方法，寒假必须得恶补了。我的目标是唯一的——考研，所以下一学期我会以学习为最主要目的，一切都是为考研而准备。相信自己，我能行的！</p>\n<p>　　说活动嘛，我想我是成功的：合唱真爽，经过的突然的悲伤，然后又突然的喜悦，成绩不错就不说了</p>\n<p>　　还记得“计科院足球队”在“雀巢”上飞奔的身影，进球后抱在一起的喜悦，失球后兄弟般的团结；还记得和本班的，外班的同学打篮球的激情；还有插花比赛，交了几个很好的朋友，还意外的得了省级奖</p>\n<p>还有篮球友谊赛，辩论赛，运动会，口拓协会，象棋协会……太多了，我在这方面真充实。我想，虽然大学学习紧张，但如果你连一个活动都不参加，就太失败了。</p>\n<p>　　作为体育委员，哇，真的挺爽的，既锻炼了自己，又认识了这么多的女生，本来内向的我都不内向了，我组织参加了很多活动啊：比如运动会，跳绳比赛，篮球友谊赛，班里同学周末高兴地打篮球，我能感觉到我的各方面能力的提高，如果我不挂科的话，我还想把体育委员当下去。</p>\n<p>　　在安大，我认识了很多很多的朋友，比如我们寝室的沈诗文啊，王群啊，高龙啊，詹新成啊……太多了，感觉大学的朋友因为长时间在一起，关系真的很好，我们班的每一个人都给我留下很好的印象，不愧是一本大学啊！我们班的女生也人都很好，现在朋友一个一个得回家了，我都开始想了，我这个人学习不好，可在交朋友方面我还是挺自信的。</p>\n<p>　　啊！不说了，寝室楼也快要关门了，一不小心就写多了，殷老师，提前祝你春节快乐！</p>\n"},{"title":"羁绊","toc":false,"date":"2010-09-12T11:36:36.000Z","_content":"\n—— 2010年夏\n—— 安大新区\n\n--------\n\n合肥的天亮的真早啊！以后晚上再也不喝咖啡了，现在时刻：5:03，天已亮了大半。这是我大学里第一次真正意义上的失眠，高考前夕和高考的那段时间曾有过几次，还有一次是来这边第一次去网吧包夜的时候，当然那个不算，因为我根本没有睡。\n\n我一晚就这样静静地躺在床上，静静地听着室友的呼吸声伴随着打呼噜的rap，想着一些连我自己也搞不明白、想不清楚的事，然后听见了鸟鸣，车声，最后决定起来了，啊！好久没写日记了，是啊，我前些日子太平静太幸福了，就像大海里的小船。\n\n最近脑海里出现最多的一个词：“羁绊”，也许是我最近疯狂地看 naruto（火影忍者）的原因吧，感觉不错，令我有些着迷了，有时候把我的另一个幻想空间移植到这个空间里来了，让室友都听不清楚我到底再说什么？什么是“羁绊”？什么是朋友？什么又是兄弟？当身边的帅哥们一个又一个陷入爱情的漩涡的时候，我的想法的确显得幼稚，我好像也并不适合做一个大学生的知心朋友，是啊，我什么都不会，不会抽烟，也不会喝酒，朋友过生日的时候我是最受鄙视的：“只吃不喝，真扫兴！”，看着然后喝得最多的拖着一副醉汹汹的脸，和喝的第二多的人纠缠在一起，然后就是“兄弟，朋友！”呵呵，学对面寝室那个傻逼的一句话：“原来是这样啊！原来这就是兄弟啊！”好了，看来我是要注定被这个社会所淘汰了，因为我不会隐藏自己的感情，我永远说不出那些恭维的话，我永远不会成为这些“兄弟”，就像我很难写完一篇入党申请书一样。\n\n前天在去南体的路上，看到有一种不知道树，每一颗树上都之有一朵大花，洁白，很香，可我的确很反感，记得前些天这树上是有很多朵洁白的小花的，现在为什么变成了大花，难道为了自己的生长必须要限制旁边的花的生长么？我反对达尔文的“物竞天择，适者生存”，没有人性。\n\n说起入党申请书，对我来说可真烦，其实随便抄两页就可以了，但我总是只写一半，当写到“我的理想”的时候，我就写不下去了，每个人都知道那上面写的是假话，却还写那劳什子干甚？谁把入党当成小学的时候定下的终生目标了？党虽然好，但这些话我真的说不出来，所以一般情况下这个时候我就会在稿子上乱画，画个写轮眼什么的，不过后来又后悔了，所以又写，写到一半又画，看来我是真的继承了爸爸这种性格了：“不会说话，处处吃亏！”。\n\n我还是挺佩服鸣人的，在那么多的人鄙视的鄙视下依然能奋发图强，证明自己；佐助都要杀他了，但他还是要救佐助，想使佐助回心转意，不计回报地自愿付出，这就是羁绊，羁绊就像一根绳，可以使两个人的能力相近，又像是一根数据线，可以使人了解别人的内心，即使性格是天壤之别，志向相悖甚远，也能从眼神看到对方的内心，因为这些是建立在时间的基础上的，就像亲人一样，在大学里真正找到一个“羁绊”是非常困难的，这也许也是大学里恋爱疯狂的原因吧！……到目前为止，我认为火影忍者写得还算成功的，若岸本齐史最后要回归到现实的话，疾风传的结局应该是一场悲剧，差不多最后只有鸣人一个活着，我没有足够的说服力预测结局，但我相信我的想象能力，呵呵，还真挺令人期待！火影的每一个人都不错，我喜欢那个聪明的鹿丸，当然还有鼬、卡卡西、好色仙人……女生里我最喜欢雏田了，像我一样，不过真是太胆小了，最终应该会为鸣人而牺牲！如果作者想写好的话……不说了，真正的专家还在床上打呼噜呢！被他听到了就是我班门弄斧了。\n\n我还是有点想爸爸妈妈，还有弟弟，哥哥，以前的朋友，都好久没有打电话了，妈妈，你还好吗？虽然说出来不算是很光彩的事，额，没啥滴，这点我还是看得很开的。\n\n昨晚的那个梦真好啊，我没有进入我的那个世界，而是梦到了现实中的人，一群朦朦胧胧的人，好像见过，我们先在C205上自习，然后又一起上机，而且电脑都是新式的，就像我在创新实验是看到的变形金刚一样，今晚班委会我不小心唱歌了，不知道为什么，心里一直想、一直想、一直想，额，千万不能堕落啊，一切与学习为敌的东西我都不要！\n\n天已全亮，鸟依旧鸣，我今天是我们寝室起得最早的，相信我有充足的精力，对付早上的物理实验还是没有问题的，呵呵，我其实几天前就有点想清楚了，像友情、爱情这类东西是没有答案的，就像三毛所说：“不要说，一说就错！”每一个人的性格不一样，看法也就不一样，就像我这种放荡不羁的性格，喜欢独自一个人，至少现在，是很难找到一个羁绊的，我不想被什么东西所束缚，我不是在英语课上说了嘛！我是一个自由飞翔的鹰，喜欢一个人独自飘。\n\n世界依旧美好，又是一个鸟语花香的日子，这依然是一个充满爱的世界，我所需要做的，就是生活，啊！每一天真是值得人期待啊！\n\n","source":"_posts/2017-01-12-羁绊.md","raw":"---\ntitle: 羁绊\ntoc: false\ndate: 2010-09-12 19:36:36\ntags: 散文\ncategories: 单车岁月\n---\n\n—— 2010年夏\n—— 安大新区\n\n--------\n\n合肥的天亮的真早啊！以后晚上再也不喝咖啡了，现在时刻：5:03，天已亮了大半。这是我大学里第一次真正意义上的失眠，高考前夕和高考的那段时间曾有过几次，还有一次是来这边第一次去网吧包夜的时候，当然那个不算，因为我根本没有睡。\n\n我一晚就这样静静地躺在床上，静静地听着室友的呼吸声伴随着打呼噜的rap，想着一些连我自己也搞不明白、想不清楚的事，然后听见了鸟鸣，车声，最后决定起来了，啊！好久没写日记了，是啊，我前些日子太平静太幸福了，就像大海里的小船。\n\n最近脑海里出现最多的一个词：“羁绊”，也许是我最近疯狂地看 naruto（火影忍者）的原因吧，感觉不错，令我有些着迷了，有时候把我的另一个幻想空间移植到这个空间里来了，让室友都听不清楚我到底再说什么？什么是“羁绊”？什么是朋友？什么又是兄弟？当身边的帅哥们一个又一个陷入爱情的漩涡的时候，我的想法的确显得幼稚，我好像也并不适合做一个大学生的知心朋友，是啊，我什么都不会，不会抽烟，也不会喝酒，朋友过生日的时候我是最受鄙视的：“只吃不喝，真扫兴！”，看着然后喝得最多的拖着一副醉汹汹的脸，和喝的第二多的人纠缠在一起，然后就是“兄弟，朋友！”呵呵，学对面寝室那个傻逼的一句话：“原来是这样啊！原来这就是兄弟啊！”好了，看来我是要注定被这个社会所淘汰了，因为我不会隐藏自己的感情，我永远说不出那些恭维的话，我永远不会成为这些“兄弟”，就像我很难写完一篇入党申请书一样。\n\n前天在去南体的路上，看到有一种不知道树，每一颗树上都之有一朵大花，洁白，很香，可我的确很反感，记得前些天这树上是有很多朵洁白的小花的，现在为什么变成了大花，难道为了自己的生长必须要限制旁边的花的生长么？我反对达尔文的“物竞天择，适者生存”，没有人性。\n\n说起入党申请书，对我来说可真烦，其实随便抄两页就可以了，但我总是只写一半，当写到“我的理想”的时候，我就写不下去了，每个人都知道那上面写的是假话，却还写那劳什子干甚？谁把入党当成小学的时候定下的终生目标了？党虽然好，但这些话我真的说不出来，所以一般情况下这个时候我就会在稿子上乱画，画个写轮眼什么的，不过后来又后悔了，所以又写，写到一半又画，看来我是真的继承了爸爸这种性格了：“不会说话，处处吃亏！”。\n\n我还是挺佩服鸣人的，在那么多的人鄙视的鄙视下依然能奋发图强，证明自己；佐助都要杀他了，但他还是要救佐助，想使佐助回心转意，不计回报地自愿付出，这就是羁绊，羁绊就像一根绳，可以使两个人的能力相近，又像是一根数据线，可以使人了解别人的内心，即使性格是天壤之别，志向相悖甚远，也能从眼神看到对方的内心，因为这些是建立在时间的基础上的，就像亲人一样，在大学里真正找到一个“羁绊”是非常困难的，这也许也是大学里恋爱疯狂的原因吧！……到目前为止，我认为火影忍者写得还算成功的，若岸本齐史最后要回归到现实的话，疾风传的结局应该是一场悲剧，差不多最后只有鸣人一个活着，我没有足够的说服力预测结局，但我相信我的想象能力，呵呵，还真挺令人期待！火影的每一个人都不错，我喜欢那个聪明的鹿丸，当然还有鼬、卡卡西、好色仙人……女生里我最喜欢雏田了，像我一样，不过真是太胆小了，最终应该会为鸣人而牺牲！如果作者想写好的话……不说了，真正的专家还在床上打呼噜呢！被他听到了就是我班门弄斧了。\n\n我还是有点想爸爸妈妈，还有弟弟，哥哥，以前的朋友，都好久没有打电话了，妈妈，你还好吗？虽然说出来不算是很光彩的事，额，没啥滴，这点我还是看得很开的。\n\n昨晚的那个梦真好啊，我没有进入我的那个世界，而是梦到了现实中的人，一群朦朦胧胧的人，好像见过，我们先在C205上自习，然后又一起上机，而且电脑都是新式的，就像我在创新实验是看到的变形金刚一样，今晚班委会我不小心唱歌了，不知道为什么，心里一直想、一直想、一直想，额，千万不能堕落啊，一切与学习为敌的东西我都不要！\n\n天已全亮，鸟依旧鸣，我今天是我们寝室起得最早的，相信我有充足的精力，对付早上的物理实验还是没有问题的，呵呵，我其实几天前就有点想清楚了，像友情、爱情这类东西是没有答案的，就像三毛所说：“不要说，一说就错！”每一个人的性格不一样，看法也就不一样，就像我这种放荡不羁的性格，喜欢独自一个人，至少现在，是很难找到一个羁绊的，我不想被什么东西所束缚，我不是在英语课上说了嘛！我是一个自由飞翔的鹰，喜欢一个人独自飘。\n\n世界依旧美好，又是一个鸟语花香的日子，这依然是一个充满爱的世界，我所需要做的，就是生活，啊！每一天真是值得人期待啊！\n\n","slug":"羁绊","published":1,"updated":"2017-01-12T11:37:58.622Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfwx002mpsguiz6zivrg","content":"<p>—— 2010年夏<br>—— 安大新区</p>\n<hr>\n<p>合肥的天亮的真早啊！以后晚上再也不喝咖啡了，现在时刻：5:03，天已亮了大半。这是我大学里第一次真正意义上的失眠，高考前夕和高考的那段时间曾有过几次，还有一次是来这边第一次去网吧包夜的时候，当然那个不算，因为我根本没有睡。</p>\n<p>我一晚就这样静静地躺在床上，静静地听着室友的呼吸声伴随着打呼噜的rap，想着一些连我自己也搞不明白、想不清楚的事，然后听见了鸟鸣，车声，最后决定起来了，啊！好久没写日记了，是啊，我前些日子太平静太幸福了，就像大海里的小船。</p>\n<p>最近脑海里出现最多的一个词：“羁绊”，也许是我最近疯狂地看 naruto（火影忍者）的原因吧，感觉不错，令我有些着迷了，有时候把我的另一个幻想空间移植到这个空间里来了，让室友都听不清楚我到底再说什么？什么是“羁绊”？什么是朋友？什么又是兄弟？当身边的帅哥们一个又一个陷入爱情的漩涡的时候，我的想法的确显得幼稚，我好像也并不适合做一个大学生的知心朋友，是啊，我什么都不会，不会抽烟，也不会喝酒，朋友过生日的时候我是最受鄙视的：“只吃不喝，真扫兴！”，看着然后喝得最多的拖着一副醉汹汹的脸，和喝的第二多的人纠缠在一起，然后就是“兄弟，朋友！”呵呵，学对面寝室那个傻逼的一句话：“原来是这样啊！原来这就是兄弟啊！”好了，看来我是要注定被这个社会所淘汰了，因为我不会隐藏自己的感情，我永远说不出那些恭维的话，我永远不会成为这些“兄弟”，就像我很难写完一篇入党申请书一样。</p>\n<p>前天在去南体的路上，看到有一种不知道树，每一颗树上都之有一朵大花，洁白，很香，可我的确很反感，记得前些天这树上是有很多朵洁白的小花的，现在为什么变成了大花，难道为了自己的生长必须要限制旁边的花的生长么？我反对达尔文的“物竞天择，适者生存”，没有人性。</p>\n<p>说起入党申请书，对我来说可真烦，其实随便抄两页就可以了，但我总是只写一半，当写到“我的理想”的时候，我就写不下去了，每个人都知道那上面写的是假话，却还写那劳什子干甚？谁把入党当成小学的时候定下的终生目标了？党虽然好，但这些话我真的说不出来，所以一般情况下这个时候我就会在稿子上乱画，画个写轮眼什么的，不过后来又后悔了，所以又写，写到一半又画，看来我是真的继承了爸爸这种性格了：“不会说话，处处吃亏！”。</p>\n<p>我还是挺佩服鸣人的，在那么多的人鄙视的鄙视下依然能奋发图强，证明自己；佐助都要杀他了，但他还是要救佐助，想使佐助回心转意，不计回报地自愿付出，这就是羁绊，羁绊就像一根绳，可以使两个人的能力相近，又像是一根数据线，可以使人了解别人的内心，即使性格是天壤之别，志向相悖甚远，也能从眼神看到对方的内心，因为这些是建立在时间的基础上的，就像亲人一样，在大学里真正找到一个“羁绊”是非常困难的，这也许也是大学里恋爱疯狂的原因吧！……到目前为止，我认为火影忍者写得还算成功的，若岸本齐史最后要回归到现实的话，疾风传的结局应该是一场悲剧，差不多最后只有鸣人一个活着，我没有足够的说服力预测结局，但我相信我的想象能力，呵呵，还真挺令人期待！火影的每一个人都不错，我喜欢那个聪明的鹿丸，当然还有鼬、卡卡西、好色仙人……女生里我最喜欢雏田了，像我一样，不过真是太胆小了，最终应该会为鸣人而牺牲！如果作者想写好的话……不说了，真正的专家还在床上打呼噜呢！被他听到了就是我班门弄斧了。</p>\n<p>我还是有点想爸爸妈妈，还有弟弟，哥哥，以前的朋友，都好久没有打电话了，妈妈，你还好吗？虽然说出来不算是很光彩的事，额，没啥滴，这点我还是看得很开的。</p>\n<p>昨晚的那个梦真好啊，我没有进入我的那个世界，而是梦到了现实中的人，一群朦朦胧胧的人，好像见过，我们先在C205上自习，然后又一起上机，而且电脑都是新式的，就像我在创新实验是看到的变形金刚一样，今晚班委会我不小心唱歌了，不知道为什么，心里一直想、一直想、一直想，额，千万不能堕落啊，一切与学习为敌的东西我都不要！</p>\n<p>天已全亮，鸟依旧鸣，我今天是我们寝室起得最早的，相信我有充足的精力，对付早上的物理实验还是没有问题的，呵呵，我其实几天前就有点想清楚了，像友情、爱情这类东西是没有答案的，就像三毛所说：“不要说，一说就错！”每一个人的性格不一样，看法也就不一样，就像我这种放荡不羁的性格，喜欢独自一个人，至少现在，是很难找到一个羁绊的，我不想被什么东西所束缚，我不是在英语课上说了嘛！我是一个自由飞翔的鹰，喜欢一个人独自飘。</p>\n<p>世界依旧美好，又是一个鸟语花香的日子，这依然是一个充满爱的世界，我所需要做的，就是生活，啊！每一天真是值得人期待啊！</p>\n","excerpt":"","more":"<p>—— 2010年夏<br>—— 安大新区</p>\n<hr>\n<p>合肥的天亮的真早啊！以后晚上再也不喝咖啡了，现在时刻：5:03，天已亮了大半。这是我大学里第一次真正意义上的失眠，高考前夕和高考的那段时间曾有过几次，还有一次是来这边第一次去网吧包夜的时候，当然那个不算，因为我根本没有睡。</p>\n<p>我一晚就这样静静地躺在床上，静静地听着室友的呼吸声伴随着打呼噜的rap，想着一些连我自己也搞不明白、想不清楚的事，然后听见了鸟鸣，车声，最后决定起来了，啊！好久没写日记了，是啊，我前些日子太平静太幸福了，就像大海里的小船。</p>\n<p>最近脑海里出现最多的一个词：“羁绊”，也许是我最近疯狂地看 naruto（火影忍者）的原因吧，感觉不错，令我有些着迷了，有时候把我的另一个幻想空间移植到这个空间里来了，让室友都听不清楚我到底再说什么？什么是“羁绊”？什么是朋友？什么又是兄弟？当身边的帅哥们一个又一个陷入爱情的漩涡的时候，我的想法的确显得幼稚，我好像也并不适合做一个大学生的知心朋友，是啊，我什么都不会，不会抽烟，也不会喝酒，朋友过生日的时候我是最受鄙视的：“只吃不喝，真扫兴！”，看着然后喝得最多的拖着一副醉汹汹的脸，和喝的第二多的人纠缠在一起，然后就是“兄弟，朋友！”呵呵，学对面寝室那个傻逼的一句话：“原来是这样啊！原来这就是兄弟啊！”好了，看来我是要注定被这个社会所淘汰了，因为我不会隐藏自己的感情，我永远说不出那些恭维的话，我永远不会成为这些“兄弟”，就像我很难写完一篇入党申请书一样。</p>\n<p>前天在去南体的路上，看到有一种不知道树，每一颗树上都之有一朵大花，洁白，很香，可我的确很反感，记得前些天这树上是有很多朵洁白的小花的，现在为什么变成了大花，难道为了自己的生长必须要限制旁边的花的生长么？我反对达尔文的“物竞天择，适者生存”，没有人性。</p>\n<p>说起入党申请书，对我来说可真烦，其实随便抄两页就可以了，但我总是只写一半，当写到“我的理想”的时候，我就写不下去了，每个人都知道那上面写的是假话，却还写那劳什子干甚？谁把入党当成小学的时候定下的终生目标了？党虽然好，但这些话我真的说不出来，所以一般情况下这个时候我就会在稿子上乱画，画个写轮眼什么的，不过后来又后悔了，所以又写，写到一半又画，看来我是真的继承了爸爸这种性格了：“不会说话，处处吃亏！”。</p>\n<p>我还是挺佩服鸣人的，在那么多的人鄙视的鄙视下依然能奋发图强，证明自己；佐助都要杀他了，但他还是要救佐助，想使佐助回心转意，不计回报地自愿付出，这就是羁绊，羁绊就像一根绳，可以使两个人的能力相近，又像是一根数据线，可以使人了解别人的内心，即使性格是天壤之别，志向相悖甚远，也能从眼神看到对方的内心，因为这些是建立在时间的基础上的，就像亲人一样，在大学里真正找到一个“羁绊”是非常困难的，这也许也是大学里恋爱疯狂的原因吧！……到目前为止，我认为火影忍者写得还算成功的，若岸本齐史最后要回归到现实的话，疾风传的结局应该是一场悲剧，差不多最后只有鸣人一个活着，我没有足够的说服力预测结局，但我相信我的想象能力，呵呵，还真挺令人期待！火影的每一个人都不错，我喜欢那个聪明的鹿丸，当然还有鼬、卡卡西、好色仙人……女生里我最喜欢雏田了，像我一样，不过真是太胆小了，最终应该会为鸣人而牺牲！如果作者想写好的话……不说了，真正的专家还在床上打呼噜呢！被他听到了就是我班门弄斧了。</p>\n<p>我还是有点想爸爸妈妈，还有弟弟，哥哥，以前的朋友，都好久没有打电话了，妈妈，你还好吗？虽然说出来不算是很光彩的事，额，没啥滴，这点我还是看得很开的。</p>\n<p>昨晚的那个梦真好啊，我没有进入我的那个世界，而是梦到了现实中的人，一群朦朦胧胧的人，好像见过，我们先在C205上自习，然后又一起上机，而且电脑都是新式的，就像我在创新实验是看到的变形金刚一样，今晚班委会我不小心唱歌了，不知道为什么，心里一直想、一直想、一直想，额，千万不能堕落啊，一切与学习为敌的东西我都不要！</p>\n<p>天已全亮，鸟依旧鸣，我今天是我们寝室起得最早的，相信我有充足的精力，对付早上的物理实验还是没有问题的，呵呵，我其实几天前就有点想清楚了，像友情、爱情这类东西是没有答案的，就像三毛所说：“不要说，一说就错！”每一个人的性格不一样，看法也就不一样，就像我这种放荡不羁的性格，喜欢独自一个人，至少现在，是很难找到一个羁绊的，我不想被什么东西所束缚，我不是在英语课上说了嘛！我是一个自由飞翔的鹰，喜欢一个人独自飘。</p>\n<p>世界依旧美好，又是一个鸟语花香的日子，这依然是一个充满爱的世界，我所需要做的，就是生活，啊！每一天真是值得人期待啊！</p>\n"},{"title":"Livy-server的搭建与简单测试","toc":true,"date":"2017-02-24T09:18:30.000Z","_content":"\n半年前搭建并测试过livy，简单阅读过部分源码，现在因为openAPI项目，要深入了解livy-server了\n\n## 搭建\n搭建在官网中有，很简单，跳过 https://github.com/cloudera/livy#rest-api\n启动的时候遇到的一个问题是会报如下错误：\n```\njava.io.IOException: Cannot write log directory /home/op/livy-server-0.3.0/logs\n        at org.eclipse.jetty.util.RolloverFileOutputStream.setFile(RolloverFileOutputStream.java:219)\n        at org.eclipse.jetty.util.RolloverFileOutputStream.<init>(RolloverFileOutputStream.java:166)\n```\n很简单，由于写日志的时候该目录不存在，所以只需要手动创建 logs目录即可\n\n## 测试\n\n### 启动session\n\n我使用 Postman 来模拟它的rest api进行功能测试：\n首先需要申请一个 session：\n\n``` json\nimport json, pprint, requests, textwrap\nhost = 'http://someIp:8998'\ndata = {'kind': 'spark'}\nheaders = {'Content-Type': 'application/json'}\nr = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\nr.json()\n```\n需要注意的是，如果用postman来模拟的话，应该在Body中使用双引号的形式，单引号无法识别，比如：\n```\n{\"code\": \"1+3\"}\n```\n返回如下结果：\n```\n{\n  \"id\": 2,\n  \"appId\": null,\n  \"owner\": null,\n  \"proxyUser\": null,\n  \"state\": \"starting\",\n  \"kind\": \"spark\",\n  \"appInfo\": {\n    \"driverLogUrl\": null,\n    \"sparkUiUrl\": null\n  },\n  \"log\": []\n}\n\n返回的header也很重要：\n\n``` \nContent-Encoding → gzip\nContent-Type → application/json\nDate → Fri, 24 Feb 2017 09:17:07 GMT\nLocation → /sessions/2\nServer → Jetty(9.2.16.v20160414)\nTransfer-Encoding → chunked\n```\n\n\n进入对应机器，输入 `ps -ef | grep spark`，发现启动了三个相关的进程。\n```\nop        2002 63840  6 17:00 pts/9    00:00:37 /usr/java/jdk1.7.0_75/bin/java -cp /usr/lib/hadoop/lib/*:/usr/lib/spark/conf/:/usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar:/usr/lib/spark/lib/datanucleus-rdbms-3.2.9.jar:/usr/lib/spark/lib/datanucleus-core-3.2.10.jar:/usr/lib/spark/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/ -Xms1g -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit --properties-file /tmp/livyConf1238183037406660708.properties --class com.cloudera.livy.rsc.driver.RSCDriverBootstrapper spark-internal\nop        2472 63840 10 17:05 pts/9    00:00:30 /usr/java/jdk1.7.0_75/bin/java -cp /usr/lib/hadoop/lib/*:/usr/lib/spark/conf/:/usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar:/usr/lib/spark/lib/datanucleus-rdbms-3.2.9.jar:/usr/lib/spark/lib/datanucleus-core-3.2.10.jar:/usr/lib/spark/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/ -Xms1g -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit --properties-file /tmp/livyConf8288202035580572985.properties --class com.cloudera.livy.rsc.driver.RSCDriverBootstrapper spark-internal\n```\n这表明**每提交一个livy的session，livy都会启动一个对应的进程，所以一定要考虑它的HA**\n\n### 通过get方式查询状态\n可以通过get方式查询这个session的状态\n`http://10.142.78.39:8998/sessions/2`\n返回是它的状态为空闲\n```\n{\n  \"id\": 2,\n  \"appId\": null,\n  \"owner\": null,\n  \"proxyUser\": null,\n  \"state\": \"idle\",\n  \"kind\": \"spark\",\n  \"appInfo\": {\n    \"driverLogUrl\": null,\n    \"sparkUiUrl\": null\n  },\n  \"log\": []\n}\n```\n\n### 提交Scala计算\nsession创建好了，就可以提交Scala计算了\n\n```\nstatements_url = session_url + '/statements'\ndata = {'code': '1 + 1'}\nr = requests.post(statements_url, data=json.dumps(data), headers=headers)\nr.json()\n\n```\n\n提交后会马上返回：\n```\n{\n  \"id\": 0,\n  \"state\": \"waiting\",\n  \"output\": null\n}\n```\nwaiting 说明正在计算。。。\nheader中返回了Location\n```\nContent-Encoding →gzip\nContent-Type →application/json\nDate →Fri, 24 Feb 2017 09:45:53 GMT\nLocation →/sessions/2/statements/0\nServer →Jetty(9.2.16.v20160414)\nTransfer-Encoding →chunked\n```\n获得location后，就可以通过location来获取结果啦：\n\n```\nstatement_url = host + r.headers['location']\nr = requests.get(statement_url, headers=headers)\n```\n结果如下：\n```\n{\n  \"id\": 0,\n  \"state\": \"available\",\n  \"output\": {\n    \"status\": \"ok\",\n    \"execution_count\": 0,\n    \"data\": {\n      \"text/plain\": \"res0: Int = 4\"\n    }\n  }\n}\n```\n结果是4\n\n根据官网，跑个SparkPi吧：\n\n```\n{\"code\": \"val NUM_SAMPLES = 100000;val count = sc.parallelize(1 to NUM_SAMPLES).map { i =>  val x = Math.random();  val y = Math.random();  if (x*x + y*y < 1) 1 else 0\\n}.reduce(_ + _);println(\\\"Pi is roughly \\\" + 4.0 * count / NUM_SAMPLES)\"}\n```\n获取后结果如下：\n\n```\n{\n  \"id\": 3,\n  \"state\": \"available\",\n  \"output\": {\n    \"status\": \"ok\",\n    \"execution_count\": 3,\n    \"data\": {\n      \"text/plain\": \"Pi is roughly 3.14464\\nNUM_SAMPLES: Int = 100000\\ncount: Int = 78616\"\n    }\n  }\n}\n```\n我故意改错了代码，会返回错误的结果：\n```\n{\n  \"id\": 4,\n  \"state\": \"available\",\n  \"output\": {\n    \"status\": \"error\",\n    \"execution_count\": 4,\n    \"ename\": \"Error\",\n    \"evalue\": \"<console>:1: error: integer number too large\",\n    \"traceback\": [\n      \"       val NUM_SAMPLES = 10000000000;val count = sc.parallelize(1 to NUM_SAMPLES).map { i =>  val x = Math.random();  val y = Math.random();  if (x*x + y*y < 1) 1 else 0\\n\",\n      \"                         ^\"\n    ]\n  }\n}\n```\n我故意往代码中加了一句睡眠`Thread.sleep(10000)`，当我马上请求结果的时候，会返回正在运行：\n```\n{\n  \"id\": 6,\n  \"state\": \"running\",\n  \"output\": null\n}\n```\n\n### 关闭session\n\n好了，不玩了，关闭session吧：\n```\nsession_url = 'http://localhost:8998/sessions/0'\nrequests.delete(session_url, headers=headers)\n```\n返回：\n```\n{\n  \"msg\": \"deleted\"\n}\n```\n如果session长时间不操作的话，会自动关闭，具体这个关闭策略是什么？待以后确认。\n\n## 总结\n\n这里只对livy的restAPI做了个简单的试验，具体的API见[livy官网](https://github.com/cloudera/livy#rest-api)\n选择livy作为server端，是因为它相比于zeppelin以及jobserver功能比较专注，对yarn-cluster模式的支持使得它具有更好的HA以及可扩展性，接下来就是深入它，基于它完成spark能力的开放！\n","source":"_posts/2017-02-24-Livy-server的搭建与简单测试.md","raw":"---\ntitle: Livy-server的搭建与简单测试\ntoc: true\ndate: 2017-02-24 17:18:30\ntags:\n- spark\n- livy\ncategories: spark\n---\n\n半年前搭建并测试过livy，简单阅读过部分源码，现在因为openAPI项目，要深入了解livy-server了\n\n## 搭建\n搭建在官网中有，很简单，跳过 https://github.com/cloudera/livy#rest-api\n启动的时候遇到的一个问题是会报如下错误：\n```\njava.io.IOException: Cannot write log directory /home/op/livy-server-0.3.0/logs\n        at org.eclipse.jetty.util.RolloverFileOutputStream.setFile(RolloverFileOutputStream.java:219)\n        at org.eclipse.jetty.util.RolloverFileOutputStream.<init>(RolloverFileOutputStream.java:166)\n```\n很简单，由于写日志的时候该目录不存在，所以只需要手动创建 logs目录即可\n\n## 测试\n\n### 启动session\n\n我使用 Postman 来模拟它的rest api进行功能测试：\n首先需要申请一个 session：\n\n``` json\nimport json, pprint, requests, textwrap\nhost = 'http://someIp:8998'\ndata = {'kind': 'spark'}\nheaders = {'Content-Type': 'application/json'}\nr = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\nr.json()\n```\n需要注意的是，如果用postman来模拟的话，应该在Body中使用双引号的形式，单引号无法识别，比如：\n```\n{\"code\": \"1+3\"}\n```\n返回如下结果：\n```\n{\n  \"id\": 2,\n  \"appId\": null,\n  \"owner\": null,\n  \"proxyUser\": null,\n  \"state\": \"starting\",\n  \"kind\": \"spark\",\n  \"appInfo\": {\n    \"driverLogUrl\": null,\n    \"sparkUiUrl\": null\n  },\n  \"log\": []\n}\n\n返回的header也很重要：\n\n``` \nContent-Encoding → gzip\nContent-Type → application/json\nDate → Fri, 24 Feb 2017 09:17:07 GMT\nLocation → /sessions/2\nServer → Jetty(9.2.16.v20160414)\nTransfer-Encoding → chunked\n```\n\n\n进入对应机器，输入 `ps -ef | grep spark`，发现启动了三个相关的进程。\n```\nop        2002 63840  6 17:00 pts/9    00:00:37 /usr/java/jdk1.7.0_75/bin/java -cp /usr/lib/hadoop/lib/*:/usr/lib/spark/conf/:/usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar:/usr/lib/spark/lib/datanucleus-rdbms-3.2.9.jar:/usr/lib/spark/lib/datanucleus-core-3.2.10.jar:/usr/lib/spark/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/ -Xms1g -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit --properties-file /tmp/livyConf1238183037406660708.properties --class com.cloudera.livy.rsc.driver.RSCDriverBootstrapper spark-internal\nop        2472 63840 10 17:05 pts/9    00:00:30 /usr/java/jdk1.7.0_75/bin/java -cp /usr/lib/hadoop/lib/*:/usr/lib/spark/conf/:/usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar:/usr/lib/spark/lib/datanucleus-rdbms-3.2.9.jar:/usr/lib/spark/lib/datanucleus-core-3.2.10.jar:/usr/lib/spark/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/ -Xms1g -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit --properties-file /tmp/livyConf8288202035580572985.properties --class com.cloudera.livy.rsc.driver.RSCDriverBootstrapper spark-internal\n```\n这表明**每提交一个livy的session，livy都会启动一个对应的进程，所以一定要考虑它的HA**\n\n### 通过get方式查询状态\n可以通过get方式查询这个session的状态\n`http://10.142.78.39:8998/sessions/2`\n返回是它的状态为空闲\n```\n{\n  \"id\": 2,\n  \"appId\": null,\n  \"owner\": null,\n  \"proxyUser\": null,\n  \"state\": \"idle\",\n  \"kind\": \"spark\",\n  \"appInfo\": {\n    \"driverLogUrl\": null,\n    \"sparkUiUrl\": null\n  },\n  \"log\": []\n}\n```\n\n### 提交Scala计算\nsession创建好了，就可以提交Scala计算了\n\n```\nstatements_url = session_url + '/statements'\ndata = {'code': '1 + 1'}\nr = requests.post(statements_url, data=json.dumps(data), headers=headers)\nr.json()\n\n```\n\n提交后会马上返回：\n```\n{\n  \"id\": 0,\n  \"state\": \"waiting\",\n  \"output\": null\n}\n```\nwaiting 说明正在计算。。。\nheader中返回了Location\n```\nContent-Encoding →gzip\nContent-Type →application/json\nDate →Fri, 24 Feb 2017 09:45:53 GMT\nLocation →/sessions/2/statements/0\nServer →Jetty(9.2.16.v20160414)\nTransfer-Encoding →chunked\n```\n获得location后，就可以通过location来获取结果啦：\n\n```\nstatement_url = host + r.headers['location']\nr = requests.get(statement_url, headers=headers)\n```\n结果如下：\n```\n{\n  \"id\": 0,\n  \"state\": \"available\",\n  \"output\": {\n    \"status\": \"ok\",\n    \"execution_count\": 0,\n    \"data\": {\n      \"text/plain\": \"res0: Int = 4\"\n    }\n  }\n}\n```\n结果是4\n\n根据官网，跑个SparkPi吧：\n\n```\n{\"code\": \"val NUM_SAMPLES = 100000;val count = sc.parallelize(1 to NUM_SAMPLES).map { i =>  val x = Math.random();  val y = Math.random();  if (x*x + y*y < 1) 1 else 0\\n}.reduce(_ + _);println(\\\"Pi is roughly \\\" + 4.0 * count / NUM_SAMPLES)\"}\n```\n获取后结果如下：\n\n```\n{\n  \"id\": 3,\n  \"state\": \"available\",\n  \"output\": {\n    \"status\": \"ok\",\n    \"execution_count\": 3,\n    \"data\": {\n      \"text/plain\": \"Pi is roughly 3.14464\\nNUM_SAMPLES: Int = 100000\\ncount: Int = 78616\"\n    }\n  }\n}\n```\n我故意改错了代码，会返回错误的结果：\n```\n{\n  \"id\": 4,\n  \"state\": \"available\",\n  \"output\": {\n    \"status\": \"error\",\n    \"execution_count\": 4,\n    \"ename\": \"Error\",\n    \"evalue\": \"<console>:1: error: integer number too large\",\n    \"traceback\": [\n      \"       val NUM_SAMPLES = 10000000000;val count = sc.parallelize(1 to NUM_SAMPLES).map { i =>  val x = Math.random();  val y = Math.random();  if (x*x + y*y < 1) 1 else 0\\n\",\n      \"                         ^\"\n    ]\n  }\n}\n```\n我故意往代码中加了一句睡眠`Thread.sleep(10000)`，当我马上请求结果的时候，会返回正在运行：\n```\n{\n  \"id\": 6,\n  \"state\": \"running\",\n  \"output\": null\n}\n```\n\n### 关闭session\n\n好了，不玩了，关闭session吧：\n```\nsession_url = 'http://localhost:8998/sessions/0'\nrequests.delete(session_url, headers=headers)\n```\n返回：\n```\n{\n  \"msg\": \"deleted\"\n}\n```\n如果session长时间不操作的话，会自动关闭，具体这个关闭策略是什么？待以后确认。\n\n## 总结\n\n这里只对livy的restAPI做了个简单的试验，具体的API见[livy官网](https://github.com/cloudera/livy#rest-api)\n选择livy作为server端，是因为它相比于zeppelin以及jobserver功能比较专注，对yarn-cluster模式的支持使得它具有更好的HA以及可扩展性，接下来就是深入它，基于它完成spark能力的开放！\n","slug":"Livy-server的搭建与简单测试","published":1,"updated":"2017-02-24T10:47:57.357Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfwz002qpsgu94ktu0x0","content":"<p>半年前搭建并测试过livy，简单阅读过部分源码，现在因为openAPI项目，要深入了解livy-server了</p>\n<h2 id=\"搭建\"><a href=\"#搭建\" class=\"headerlink\" title=\"搭建\"></a>搭建</h2><p>搭建在官网中有，很简单，跳过 <a href=\"https://github.com/cloudera/livy#rest-api\" target=\"_blank\" rel=\"external\">https://github.com/cloudera/livy#rest-api</a><br>启动的时候遇到的一个问题是会报如下错误：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">java.io.IOException: Cannot write log directory /home/op/livy-server-0.3.0/logs</div><div class=\"line\">        at org.eclipse.jetty.util.RolloverFileOutputStream.setFile(RolloverFileOutputStream.java:219)</div><div class=\"line\">        at org.eclipse.jetty.util.RolloverFileOutputStream.&lt;init&gt;(RolloverFileOutputStream.java:166)</div></pre></td></tr></table></figure></p>\n<p>很简单，由于写日志的时候该目录不存在，所以只需要手动创建 logs目录即可</p>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><h3 id=\"启动session\"><a href=\"#启动session\" class=\"headerlink\" title=\"启动session\"></a>启动session</h3><p>我使用 Postman 来模拟它的rest api进行功能测试：<br>首先需要申请一个 session：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">import json, pprint, requests, textwrap</div><div class=\"line\">host = 'http://someIp:8998'</div><div class=\"line\">data = &#123;'kind': 'spark'&#125;</div><div class=\"line\">headers = &#123;'Content-Type': 'application/json'&#125;</div><div class=\"line\">r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)</div><div class=\"line\">r.json()</div></pre></td></tr></table></figure>\n<p>需要注意的是，如果用postman来模拟的话，应该在Body中使用双引号的形式，单引号无法识别，比如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;&quot;code&quot;: &quot;1+3&quot;&#125;</div></pre></td></tr></table></figure></p>\n<p>返回如下结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 2,</div><div class=\"line\">  &quot;appId&quot;: null,</div><div class=\"line\">  &quot;owner&quot;: null,</div><div class=\"line\">  &quot;proxyUser&quot;: null,</div><div class=\"line\">  &quot;state&quot;: &quot;starting&quot;,</div><div class=\"line\">  &quot;kind&quot;: &quot;spark&quot;,</div><div class=\"line\">  &quot;appInfo&quot;: &#123;</div><div class=\"line\">    &quot;driverLogUrl&quot;: null,</div><div class=\"line\">    &quot;sparkUiUrl&quot;: null</div><div class=\"line\">  &#125;,</div><div class=\"line\">  &quot;log&quot;: []</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">返回的header也很重要：</div><div class=\"line\"></div><div class=\"line\">``` </div><div class=\"line\">Content-Encoding → gzip</div><div class=\"line\">Content-Type → application/json</div><div class=\"line\">Date → Fri, 24 Feb 2017 09:17:07 GMT</div><div class=\"line\">Location → /sessions/2</div><div class=\"line\">Server → Jetty(9.2.16.v20160414)</div><div class=\"line\">Transfer-Encoding → chunked</div></pre></td></tr></table></figure></p>\n<p>进入对应机器，输入 <code>ps -ef | grep spark</code>，发现启动了三个相关的进程。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">op        2002 63840  6 17:00 pts/9    00:00:37 /usr/java/jdk1.7.0_75/bin/java -cp /usr/lib/hadoop/lib/*:/usr/lib/spark/conf/:/usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar:/usr/lib/spark/lib/datanucleus-rdbms-3.2.9.jar:/usr/lib/spark/lib/datanucleus-core-3.2.10.jar:/usr/lib/spark/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/ -Xms1g -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit --properties-file /tmp/livyConf1238183037406660708.properties --class com.cloudera.livy.rsc.driver.RSCDriverBootstrapper spark-internal</div><div class=\"line\">op        2472 63840 10 17:05 pts/9    00:00:30 /usr/java/jdk1.7.0_75/bin/java -cp /usr/lib/hadoop/lib/*:/usr/lib/spark/conf/:/usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar:/usr/lib/spark/lib/datanucleus-rdbms-3.2.9.jar:/usr/lib/spark/lib/datanucleus-core-3.2.10.jar:/usr/lib/spark/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/ -Xms1g -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit --properties-file /tmp/livyConf8288202035580572985.properties --class com.cloudera.livy.rsc.driver.RSCDriverBootstrapper spark-internal</div></pre></td></tr></table></figure></p>\n<p>这表明<strong>每提交一个livy的session，livy都会启动一个对应的进程，所以一定要考虑它的HA</strong></p>\n<h3 id=\"通过get方式查询状态\"><a href=\"#通过get方式查询状态\" class=\"headerlink\" title=\"通过get方式查询状态\"></a>通过get方式查询状态</h3><p>可以通过get方式查询这个session的状态<br><code>http://10.142.78.39:8998/sessions/2</code><br>返回是它的状态为空闲<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 2,</div><div class=\"line\">  &quot;appId&quot;: null,</div><div class=\"line\">  &quot;owner&quot;: null,</div><div class=\"line\">  &quot;proxyUser&quot;: null,</div><div class=\"line\">  &quot;state&quot;: &quot;idle&quot;,</div><div class=\"line\">  &quot;kind&quot;: &quot;spark&quot;,</div><div class=\"line\">  &quot;appInfo&quot;: &#123;</div><div class=\"line\">    &quot;driverLogUrl&quot;: null,</div><div class=\"line\">    &quot;sparkUiUrl&quot;: null</div><div class=\"line\">  &#125;,</div><div class=\"line\">  &quot;log&quot;: []</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"提交Scala计算\"><a href=\"#提交Scala计算\" class=\"headerlink\" title=\"提交Scala计算\"></a>提交Scala计算</h3><p>session创建好了，就可以提交Scala计算了</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">statements_url = session_url + &apos;/statements&apos;</div><div class=\"line\">data = &#123;&apos;code&apos;: &apos;1 + 1&apos;&#125;</div><div class=\"line\">r = requests.post(statements_url, data=json.dumps(data), headers=headers)</div><div class=\"line\">r.json()</div></pre></td></tr></table></figure>\n<p>提交后会马上返回：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 0,</div><div class=\"line\">  &quot;state&quot;: &quot;waiting&quot;,</div><div class=\"line\">  &quot;output&quot;: null</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>waiting 说明正在计算。。。<br>header中返回了Location<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Content-Encoding →gzip</div><div class=\"line\">Content-Type →application/json</div><div class=\"line\">Date →Fri, 24 Feb 2017 09:45:53 GMT</div><div class=\"line\">Location →/sessions/2/statements/0</div><div class=\"line\">Server →Jetty(9.2.16.v20160414)</div><div class=\"line\">Transfer-Encoding →chunked</div></pre></td></tr></table></figure></p>\n<p>获得location后，就可以通过location来获取结果啦：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">statement_url = host + r.headers[&apos;location&apos;]</div><div class=\"line\">r = requests.get(statement_url, headers=headers)</div></pre></td></tr></table></figure>\n<p>结果如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 0,</div><div class=\"line\">  &quot;state&quot;: &quot;available&quot;,</div><div class=\"line\">  &quot;output&quot;: &#123;</div><div class=\"line\">    &quot;status&quot;: &quot;ok&quot;,</div><div class=\"line\">    &quot;execution_count&quot;: 0,</div><div class=\"line\">    &quot;data&quot;: &#123;</div><div class=\"line\">      &quot;text/plain&quot;: &quot;res0: Int = 4&quot;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>结果是4</p>\n<p>根据官网，跑个SparkPi吧：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;&quot;code&quot;: &quot;val NUM_SAMPLES = 100000;val count = sc.parallelize(1 to NUM_SAMPLES).map &#123; i =&gt;  val x = Math.random();  val y = Math.random();  if (x*x + y*y &lt; 1) 1 else 0\\n&#125;.reduce(_ + _);println(\\&quot;Pi is roughly \\&quot; + 4.0 * count / NUM_SAMPLES)&quot;&#125;</div></pre></td></tr></table></figure>\n<p>获取后结果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 3,</div><div class=\"line\">  &quot;state&quot;: &quot;available&quot;,</div><div class=\"line\">  &quot;output&quot;: &#123;</div><div class=\"line\">    &quot;status&quot;: &quot;ok&quot;,</div><div class=\"line\">    &quot;execution_count&quot;: 3,</div><div class=\"line\">    &quot;data&quot;: &#123;</div><div class=\"line\">      &quot;text/plain&quot;: &quot;Pi is roughly 3.14464\\nNUM_SAMPLES: Int = 100000\\ncount: Int = 78616&quot;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>我故意改错了代码，会返回错误的结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 4,</div><div class=\"line\">  &quot;state&quot;: &quot;available&quot;,</div><div class=\"line\">  &quot;output&quot;: &#123;</div><div class=\"line\">    &quot;status&quot;: &quot;error&quot;,</div><div class=\"line\">    &quot;execution_count&quot;: 4,</div><div class=\"line\">    &quot;ename&quot;: &quot;Error&quot;,</div><div class=\"line\">    &quot;evalue&quot;: &quot;&lt;console&gt;:1: error: integer number too large&quot;,</div><div class=\"line\">    &quot;traceback&quot;: [</div><div class=\"line\">      &quot;       val NUM_SAMPLES = 10000000000;val count = sc.parallelize(1 to NUM_SAMPLES).map &#123; i =&gt;  val x = Math.random();  val y = Math.random();  if (x*x + y*y &lt; 1) 1 else 0\\n&quot;,</div><div class=\"line\">      &quot;                         ^&quot;</div><div class=\"line\">    ]</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>我故意往代码中加了一句睡眠<code>Thread.sleep(10000)</code>，当我马上请求结果的时候，会返回正在运行：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 6,</div><div class=\"line\">  &quot;state&quot;: &quot;running&quot;,</div><div class=\"line\">  &quot;output&quot;: null</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"关闭session\"><a href=\"#关闭session\" class=\"headerlink\" title=\"关闭session\"></a>关闭session</h3><p>好了，不玩了，关闭session吧：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">session_url = &apos;http://localhost:8998/sessions/0&apos;</div><div class=\"line\">requests.delete(session_url, headers=headers)</div></pre></td></tr></table></figure></p>\n<p>返回：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;msg&quot;: &quot;deleted&quot;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>如果session长时间不操作的话，会自动关闭，具体这个关闭策略是什么？待以后确认。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>这里只对livy的restAPI做了个简单的试验，具体的API见<a href=\"https://github.com/cloudera/livy#rest-api\" target=\"_blank\" rel=\"external\">livy官网</a><br>选择livy作为server端，是因为它相比于zeppelin以及jobserver功能比较专注，对yarn-cluster模式的支持使得它具有更好的HA以及可扩展性，接下来就是深入它，基于它完成spark能力的开放！</p>\n","excerpt":"","more":"<p>半年前搭建并测试过livy，简单阅读过部分源码，现在因为openAPI项目，要深入了解livy-server了</p>\n<h2 id=\"搭建\"><a href=\"#搭建\" class=\"headerlink\" title=\"搭建\"></a>搭建</h2><p>搭建在官网中有，很简单，跳过 <a href=\"https://github.com/cloudera/livy#rest-api\">https://github.com/cloudera/livy#rest-api</a><br>启动的时候遇到的一个问题是会报如下错误：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">java.io.IOException: Cannot write log directory /home/op/livy-server-0.3.0/logs</div><div class=\"line\">        at org.eclipse.jetty.util.RolloverFileOutputStream.setFile(RolloverFileOutputStream.java:219)</div><div class=\"line\">        at org.eclipse.jetty.util.RolloverFileOutputStream.&lt;init&gt;(RolloverFileOutputStream.java:166)</div></pre></td></tr></table></figure></p>\n<p>很简单，由于写日志的时候该目录不存在，所以只需要手动创建 logs目录即可</p>\n<h2 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h2><h3 id=\"启动session\"><a href=\"#启动session\" class=\"headerlink\" title=\"启动session\"></a>启动session</h3><p>我使用 Postman 来模拟它的rest api进行功能测试：<br>首先需要申请一个 session：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">import json, pprint, requests, textwrap</div><div class=\"line\">host = 'http://someIp:8998'</div><div class=\"line\">data = &#123;'kind': 'spark'&#125;</div><div class=\"line\">headers = &#123;'Content-Type': 'application/json'&#125;</div><div class=\"line\">r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)</div><div class=\"line\">r.json()</div></pre></td></tr></table></figure>\n<p>需要注意的是，如果用postman来模拟的话，应该在Body中使用双引号的形式，单引号无法识别，比如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;&quot;code&quot;: &quot;1+3&quot;&#125;</div></pre></td></tr></table></figure></p>\n<p>返回如下结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 2,</div><div class=\"line\">  &quot;appId&quot;: null,</div><div class=\"line\">  &quot;owner&quot;: null,</div><div class=\"line\">  &quot;proxyUser&quot;: null,</div><div class=\"line\">  &quot;state&quot;: &quot;starting&quot;,</div><div class=\"line\">  &quot;kind&quot;: &quot;spark&quot;,</div><div class=\"line\">  &quot;appInfo&quot;: &#123;</div><div class=\"line\">    &quot;driverLogUrl&quot;: null,</div><div class=\"line\">    &quot;sparkUiUrl&quot;: null</div><div class=\"line\">  &#125;,</div><div class=\"line\">  &quot;log&quot;: []</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">返回的header也很重要：</div><div class=\"line\"></div><div class=\"line\">``` </div><div class=\"line\">Content-Encoding → gzip</div><div class=\"line\">Content-Type → application/json</div><div class=\"line\">Date → Fri, 24 Feb 2017 09:17:07 GMT</div><div class=\"line\">Location → /sessions/2</div><div class=\"line\">Server → Jetty(9.2.16.v20160414)</div><div class=\"line\">Transfer-Encoding → chunked</div></pre></td></tr></table></figure></p>\n<p>进入对应机器，输入 <code>ps -ef | grep spark</code>，发现启动了三个相关的进程。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">op        2002 63840  6 17:00 pts/9    00:00:37 /usr/java/jdk1.7.0_75/bin/java -cp /usr/lib/hadoop/lib/*:/usr/lib/spark/conf/:/usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar:/usr/lib/spark/lib/datanucleus-rdbms-3.2.9.jar:/usr/lib/spark/lib/datanucleus-core-3.2.10.jar:/usr/lib/spark/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/ -Xms1g -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit --properties-file /tmp/livyConf1238183037406660708.properties --class com.cloudera.livy.rsc.driver.RSCDriverBootstrapper spark-internal</div><div class=\"line\">op        2472 63840 10 17:05 pts/9    00:00:30 /usr/java/jdk1.7.0_75/bin/java -cp /usr/lib/hadoop/lib/*:/usr/lib/spark/conf/:/usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar:/usr/lib/spark/lib/datanucleus-rdbms-3.2.9.jar:/usr/lib/spark/lib/datanucleus-core-3.2.10.jar:/usr/lib/spark/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/ -Xms1g -Xmx1g -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit --properties-file /tmp/livyConf8288202035580572985.properties --class com.cloudera.livy.rsc.driver.RSCDriverBootstrapper spark-internal</div></pre></td></tr></table></figure></p>\n<p>这表明<strong>每提交一个livy的session，livy都会启动一个对应的进程，所以一定要考虑它的HA</strong></p>\n<h3 id=\"通过get方式查询状态\"><a href=\"#通过get方式查询状态\" class=\"headerlink\" title=\"通过get方式查询状态\"></a>通过get方式查询状态</h3><p>可以通过get方式查询这个session的状态<br><code>http://10.142.78.39:8998/sessions/2</code><br>返回是它的状态为空闲<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 2,</div><div class=\"line\">  &quot;appId&quot;: null,</div><div class=\"line\">  &quot;owner&quot;: null,</div><div class=\"line\">  &quot;proxyUser&quot;: null,</div><div class=\"line\">  &quot;state&quot;: &quot;idle&quot;,</div><div class=\"line\">  &quot;kind&quot;: &quot;spark&quot;,</div><div class=\"line\">  &quot;appInfo&quot;: &#123;</div><div class=\"line\">    &quot;driverLogUrl&quot;: null,</div><div class=\"line\">    &quot;sparkUiUrl&quot;: null</div><div class=\"line\">  &#125;,</div><div class=\"line\">  &quot;log&quot;: []</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"提交Scala计算\"><a href=\"#提交Scala计算\" class=\"headerlink\" title=\"提交Scala计算\"></a>提交Scala计算</h3><p>session创建好了，就可以提交Scala计算了</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">statements_url = session_url + &apos;/statements&apos;</div><div class=\"line\">data = &#123;&apos;code&apos;: &apos;1 + 1&apos;&#125;</div><div class=\"line\">r = requests.post(statements_url, data=json.dumps(data), headers=headers)</div><div class=\"line\">r.json()</div></pre></td></tr></table></figure>\n<p>提交后会马上返回：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 0,</div><div class=\"line\">  &quot;state&quot;: &quot;waiting&quot;,</div><div class=\"line\">  &quot;output&quot;: null</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>waiting 说明正在计算。。。<br>header中返回了Location<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Content-Encoding →gzip</div><div class=\"line\">Content-Type →application/json</div><div class=\"line\">Date →Fri, 24 Feb 2017 09:45:53 GMT</div><div class=\"line\">Location →/sessions/2/statements/0</div><div class=\"line\">Server →Jetty(9.2.16.v20160414)</div><div class=\"line\">Transfer-Encoding →chunked</div></pre></td></tr></table></figure></p>\n<p>获得location后，就可以通过location来获取结果啦：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">statement_url = host + r.headers[&apos;location&apos;]</div><div class=\"line\">r = requests.get(statement_url, headers=headers)</div></pre></td></tr></table></figure>\n<p>结果如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 0,</div><div class=\"line\">  &quot;state&quot;: &quot;available&quot;,</div><div class=\"line\">  &quot;output&quot;: &#123;</div><div class=\"line\">    &quot;status&quot;: &quot;ok&quot;,</div><div class=\"line\">    &quot;execution_count&quot;: 0,</div><div class=\"line\">    &quot;data&quot;: &#123;</div><div class=\"line\">      &quot;text/plain&quot;: &quot;res0: Int = 4&quot;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>结果是4</p>\n<p>根据官网，跑个SparkPi吧：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;&quot;code&quot;: &quot;val NUM_SAMPLES = 100000;val count = sc.parallelize(1 to NUM_SAMPLES).map &#123; i =&gt;  val x = Math.random();  val y = Math.random();  if (x*x + y*y &lt; 1) 1 else 0\\n&#125;.reduce(_ + _);println(\\&quot;Pi is roughly \\&quot; + 4.0 * count / NUM_SAMPLES)&quot;&#125;</div></pre></td></tr></table></figure>\n<p>获取后结果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 3,</div><div class=\"line\">  &quot;state&quot;: &quot;available&quot;,</div><div class=\"line\">  &quot;output&quot;: &#123;</div><div class=\"line\">    &quot;status&quot;: &quot;ok&quot;,</div><div class=\"line\">    &quot;execution_count&quot;: 3,</div><div class=\"line\">    &quot;data&quot;: &#123;</div><div class=\"line\">      &quot;text/plain&quot;: &quot;Pi is roughly 3.14464\\nNUM_SAMPLES: Int = 100000\\ncount: Int = 78616&quot;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>我故意改错了代码，会返回错误的结果：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 4,</div><div class=\"line\">  &quot;state&quot;: &quot;available&quot;,</div><div class=\"line\">  &quot;output&quot;: &#123;</div><div class=\"line\">    &quot;status&quot;: &quot;error&quot;,</div><div class=\"line\">    &quot;execution_count&quot;: 4,</div><div class=\"line\">    &quot;ename&quot;: &quot;Error&quot;,</div><div class=\"line\">    &quot;evalue&quot;: &quot;&lt;console&gt;:1: error: integer number too large&quot;,</div><div class=\"line\">    &quot;traceback&quot;: [</div><div class=\"line\">      &quot;       val NUM_SAMPLES = 10000000000;val count = sc.parallelize(1 to NUM_SAMPLES).map &#123; i =&gt;  val x = Math.random();  val y = Math.random();  if (x*x + y*y &lt; 1) 1 else 0\\n&quot;,</div><div class=\"line\">      &quot;                         ^&quot;</div><div class=\"line\">    ]</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>我故意往代码中加了一句睡眠<code>Thread.sleep(10000)</code>，当我马上请求结果的时候，会返回正在运行：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;id&quot;: 6,</div><div class=\"line\">  &quot;state&quot;: &quot;running&quot;,</div><div class=\"line\">  &quot;output&quot;: null</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"关闭session\"><a href=\"#关闭session\" class=\"headerlink\" title=\"关闭session\"></a>关闭session</h3><p>好了，不玩了，关闭session吧：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">session_url = &apos;http://localhost:8998/sessions/0&apos;</div><div class=\"line\">requests.delete(session_url, headers=headers)</div></pre></td></tr></table></figure></p>\n<p>返回：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  &quot;msg&quot;: &quot;deleted&quot;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>如果session长时间不操作的话，会自动关闭，具体这个关闭策略是什么？待以后确认。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>这里只对livy的restAPI做了个简单的试验，具体的API见<a href=\"https://github.com/cloudera/livy#rest-api\">livy官网</a><br>选择livy作为server端，是因为它相比于zeppelin以及jobserver功能比较专注，对yarn-cluster模式的支持使得它具有更好的HA以及可扩展性，接下来就是深入它，基于它完成spark能力的开放！</p>\n"},{"title":"spark on yarn 作业提交源码分析","toc":true,"date":"2017-02-28T11:08:16.000Z","_content":"\n最近因为上线hdfs的federation功能，测试spark程序的时候遇到了问题，在分析此问题的过程中对spark on yarn提交作业的过程记录一下，以SparkPi为例，从spark-submit开始，通过debug日志分析详细的过程：\n整个过程中主要涉及：spark源码（1.6.2）hadoop源码（2.6.0-cdh5.4.7） Kerberos相关\n#### 从spark-sumbit开始\n提交命令：\n``` bash\nspark-submit --master yarn-client --class SparkPi ./testJars/my.jar\n```\n进入`./bin/spark-submit`\n``` bash\nexec \"${SPARK_HOME}\"/bin/spark-class org.apache.spark.deploy.SparkSubmit \"$@\"\n```\n以上脚本调用了spark-class脚本，并增加了参数 org.apache.spark.deploy.SparkSubmit。\n进入 `./bin/spark-class`\n\n``` bash\n// 首先执行load-spark-env.sh载入 ./conf/spark-env.sh，保证只载入一次，如果之前手动载入过一次的话，就不会再覆盖载入\n. \"${SPARK_HOME}\"/bin/load-spark-env.sh\n// 寻找spark安装包，是这个样子：`spark-assembly.*hadoop.*\\.jar$`，而且只能有一个\n...\n// 最后会通过java调用spark的 `org.apache.spark.launcher.Main`作为spark应用程序的主入口，首先循环读取ARG参数，加入到CMD中：\nCMD=()\nwhile IFS= read -d '' -r ARG; do\n  CMD+=(\"$ARG\")\ndone < <(\"$RUNNER\" -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\")\nexec \"${CMD[@]}\"\n```\n翻译过来就是：\n```\n/bin/java -cp /usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar org.apache.spark.launcher.Main org.apache.spark.deploy.SparkSubmit --master yarn-client --class SparkPi ./testJars/my.jar\n```\n将这个命令执行的结果返回给 CMD参数，然后执行\n\n#### launcher.Main\n\n这个类的目的是同时适应unix和windows操作系统\n\n``` java org.apache.spark.launcher.Main.java\nif (className.equals(\"org.apache.spark.deploy.SparkSubmit\")) {\n      try {\n      // 创建一个命令解析器，这里会优先将提交的命令中的 --master之类的参数解析，然后保存到 SparkSubmitCommandBuilder 中\n        builder = new SparkSubmitCommandBuilder(args);\n      } catch (IllegalArgumentException e) {\n        ...\n      }\n    } else {\n      builder = new SparkClassCommandBuilder(className, args);\n    }\n    Map<String, String> env = new HashMap<String, String>();\n    // 使用解析器来解析参数，并且得到环境变量的HashMap值\n    List<String> cmd = builder.buildCommand(env);\n    ...\n    if (isWindows()) {\n      System.out.println(prepareWindowsCommand(cmd, env));\n    } else {\n      // In bash, use NULL as the arg separator since it cannot be used in an argument.\n      // 根据输入的参数，准备bash的命令，然后打印出来，传给 spark-class脚本中的$CMD 变量，然后执行，\n      List<String> bashCmd = prepareBashCommand(cmd, env);\n      for (String c : bashCmd) {\n        System.out.print(c);\n        System.out.print('\\0');\n      }\n    }\n```\n再来看看这个cmd到底是怎么build的：\n``` java org.apache.spark.launcher.SparkSubmitCommandBuilder.java\n  // 这段代码进入 buildSparkSubmitCommand\n  @Override\n  public List<String> buildCommand(Map<String, String> env) throws IOException {\n    if (PYSPARK_SHELL_RESOURCE.equals(appResource) && !printInfo) {\n      return buildPySparkShellCommand(env);\n    } else if (SPARKR_SHELL_RESOURCE.equals(appResource) && !printInfo) {\n      return buildSparkRCommand(env);\n    } else {\n      return buildSparkSubmitCommand(env);\n    }\n  }\n ```\n 看看buildSparkSubmitCommand函数\n ``` java\n  ...\n   private List<String> buildSparkSubmitCommand(Map<String, String> env) throws IOException {\n    // Load the properties file and check whether spark-submit will be running the app's driver\n    // or just launching a cluster app. When running the driver, the JVM's argument will be\n    // modified to cover the driver's configuration.\n    // 先获取有效的配置，如果用户不自己制定的话，默认情况下会去 $SPARK_HOME/conf/spark-defaults.conf 中拿\n    Map<String, String> config = getEffectiveConfig();\n    boolean isClientMode = isClientMode(config);\n    String extraClassPath = isClientMode ? config.get(SparkLauncher.DRIVER_EXTRA_CLASSPATH) : null;\n\n    List<String> cmd = buildJavaCommand(extraClassPath);\n    // Take Thrift Server as daemon\n    if (isThriftServer(mainClass)) {\n      addOptionString(cmd, System.getenv(\"SPARK_DAEMON_JAVA_OPTS\"));\n    }\n    addOptionString(cmd, System.getenv(\"SPARK_SUBMIT_OPTS\"));\n    addOptionString(cmd, System.getenv(\"SPARK_JAVA_OPTS\"));\n\n    // 接下来的代码的意思是，很多没有手动指定的参数，会根据配置文件中拿，如果配置文件中没有，会给个默认值\n    if (isClientMode) {\n      // Figuring out where the memory value come from is a little tricky due to precedence.\n      // Precedence is observed in the following order:\n      // - explicit configuration (setConf()), which also covers --driver-memory cli argument.\n      // - properties file.\n      // - SPARK_DRIVER_MEMORY env variable\n      // - SPARK_MEM env variable\n      // - default value (1g)\n      // Take Thrift Server as daemon\n      String tsMemory =\n        isThriftServer(mainClass) ? System.getenv(\"SPARK_DAEMON_MEMORY\") : null;\n      String memory = firstNonEmpty(tsMemory, config.get(SparkLauncher.DRIVER_MEMORY),\n        System.getenv(\"SPARK_DRIVER_MEMORY\"), System.getenv(\"SPARK_MEM\"), DEFAULT_MEM);\n      cmd.add(\"-Xms\" + memory);\n      cmd.add(\"-Xmx\" + memory);\n      addOptionString(cmd, config.get(SparkLauncher.DRIVER_EXTRA_JAVA_OPTIONS));\n      mergeEnvPathList(env, getLibPathEnvName(),\n        config.get(SparkLauncher.DRIVER_EXTRA_LIBRARY_PATH));\n    }\n\n    addPermGenSizeOpt(cmd);\n    cmd.add(\"org.apache.spark.deploy.SparkSubmit\");\n    cmd.addAll(buildSparkSubmitArgs());\n    return cmd;\n  }\n```\n#### SparkSubmit类\n通过上述方法生成了命令到 $CMD 参数中后，就通过 `exec \"${CMD[@]}\"` 命令执行之前生成的命令，也就是 `org.apache.spark.deploy.SparkSubmit` 类。大概是这样子：\n``` sh\n// 在Unix中，分隔符为'\\0'，以下是大概写法\n/bin/java -Xms1g -XX:MaxPermSize=256m -cp /usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar org.apache.spark.deploy.SparkSubmit --master yarn-client --class SparkPi ./testJars/my.jar\n```\n进入main函数：\n``` scala\ndef main(args: Array[String]): Unit = {\n    val appArgs = new SparkSubmitArguments(args)\n    if (appArgs.verbose) {\n      // scalastyle:off println\n      printStream.println(appArgs)\n      // scalastyle:on println\n    }\n    appArgs.action match {\n      case SparkSubmitAction.SUBMIT => submit(appArgs)\n      case SparkSubmitAction.KILL => kill(appArgs)\n      case SparkSubmitAction.REQUEST_STATUS => requestStatus(appArgs)\n    }\n  }\n```\n咱们调用的是 submit(appArgs)：如下代码，主要分为两步，第一步，准备提交的参数四元组，第二步，\n``` scala\nprivate def submit(args: SparkSubmitArguments): Unit = {\n    // 这里的代码很长，主要目的是准备提交应用的环境，针对 yarn standalone Mesos等各类环境进行针对性处理，并对输入的args进行一些校验和修改，返回一个 4-tuple：\n    // 1. 子程序的参数列表； 2. 子程序的classpath列表 3. 系统环境变量HashMap 4. mainClass\n    val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)\n\n    // 如果有代理晕乎的话，需要创建一个代理用户，然后验证，后运行runMain\n    def doRunMain(): Unit = {\n      if (args.proxyUser != null) {\n        val proxyUser = UserGroupInformation.createProxyUser(args.proxyUser,\n          UserGroupInformation.getCurrentUser())\n        try {\n          proxyUser.doAs(new PrivilegedExceptionAction[Unit]() {\n            override def run(): Unit = {\n              runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)\n            }\n          })\n        } catch {\n          case e: Exception =>\n            // Hadoop's AuthorizationException suppresses the exception's stack trace, which\n            // makes the message printed to the output by the JVM not very helpful. Instead,\n            // detect exceptions with empty stack traces here, and treat them differently.\n            if (e.getStackTrace().length == 0) {\n              // scalastyle:off println\n              printStream.println(s\"ERROR: ${e.getClass().getName()}: ${e.getMessage()}\")\n              // scalastyle:on println\n              exitFn(1)\n            } else {\n              throw e\n            }\n        }\n      } else {\n        runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)\n      }\n    }\n\n     // In standalone cluster mode, there are two submission gateways:\n     //   (1) The traditional RPC gateway using o.a.s.deploy.Client as a wrapper\n     //   (2) The new REST-based gateway introduced in Spark 1.3\n     // The latter is the default behavior as of Spark 1.3, but Spark submit will fail over\n     // to use the legacy gateway if the master endpoint turns out to be not a REST server.\n。。。\n  }\n```\n接下来执行的是runMain方法：\n\n``` scala\n//复用反射加载childMainClass\n//调用反射机制加载main方法\n//执行main方法,进入 SparkPi 的main方法，执行spark应用程序\n```\n至此，正式完成spark应用程序的提交。\n\n\n#### 引用\nhttp://www.cnblogs.com/xing901022/p/6426408.html\nhttp://blog.csdn.net/lovehuangjiaju/article/details/49123975","source":"_posts/2017-02-28-spark-on-yarn-作业提交源码分析.md","raw":"---\ntitle: spark on yarn 作业提交源码分析\ntoc: true\ndate: 2017-02-28 19:08:16\ntags:\n- spark\n- yarn\ncategories: spark\n---\n\n最近因为上线hdfs的federation功能，测试spark程序的时候遇到了问题，在分析此问题的过程中对spark on yarn提交作业的过程记录一下，以SparkPi为例，从spark-submit开始，通过debug日志分析详细的过程：\n整个过程中主要涉及：spark源码（1.6.2）hadoop源码（2.6.0-cdh5.4.7） Kerberos相关\n#### 从spark-sumbit开始\n提交命令：\n``` bash\nspark-submit --master yarn-client --class SparkPi ./testJars/my.jar\n```\n进入`./bin/spark-submit`\n``` bash\nexec \"${SPARK_HOME}\"/bin/spark-class org.apache.spark.deploy.SparkSubmit \"$@\"\n```\n以上脚本调用了spark-class脚本，并增加了参数 org.apache.spark.deploy.SparkSubmit。\n进入 `./bin/spark-class`\n\n``` bash\n// 首先执行load-spark-env.sh载入 ./conf/spark-env.sh，保证只载入一次，如果之前手动载入过一次的话，就不会再覆盖载入\n. \"${SPARK_HOME}\"/bin/load-spark-env.sh\n// 寻找spark安装包，是这个样子：`spark-assembly.*hadoop.*\\.jar$`，而且只能有一个\n...\n// 最后会通过java调用spark的 `org.apache.spark.launcher.Main`作为spark应用程序的主入口，首先循环读取ARG参数，加入到CMD中：\nCMD=()\nwhile IFS= read -d '' -r ARG; do\n  CMD+=(\"$ARG\")\ndone < <(\"$RUNNER\" -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\")\nexec \"${CMD[@]}\"\n```\n翻译过来就是：\n```\n/bin/java -cp /usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar org.apache.spark.launcher.Main org.apache.spark.deploy.SparkSubmit --master yarn-client --class SparkPi ./testJars/my.jar\n```\n将这个命令执行的结果返回给 CMD参数，然后执行\n\n#### launcher.Main\n\n这个类的目的是同时适应unix和windows操作系统\n\n``` java org.apache.spark.launcher.Main.java\nif (className.equals(\"org.apache.spark.deploy.SparkSubmit\")) {\n      try {\n      // 创建一个命令解析器，这里会优先将提交的命令中的 --master之类的参数解析，然后保存到 SparkSubmitCommandBuilder 中\n        builder = new SparkSubmitCommandBuilder(args);\n      } catch (IllegalArgumentException e) {\n        ...\n      }\n    } else {\n      builder = new SparkClassCommandBuilder(className, args);\n    }\n    Map<String, String> env = new HashMap<String, String>();\n    // 使用解析器来解析参数，并且得到环境变量的HashMap值\n    List<String> cmd = builder.buildCommand(env);\n    ...\n    if (isWindows()) {\n      System.out.println(prepareWindowsCommand(cmd, env));\n    } else {\n      // In bash, use NULL as the arg separator since it cannot be used in an argument.\n      // 根据输入的参数，准备bash的命令，然后打印出来，传给 spark-class脚本中的$CMD 变量，然后执行，\n      List<String> bashCmd = prepareBashCommand(cmd, env);\n      for (String c : bashCmd) {\n        System.out.print(c);\n        System.out.print('\\0');\n      }\n    }\n```\n再来看看这个cmd到底是怎么build的：\n``` java org.apache.spark.launcher.SparkSubmitCommandBuilder.java\n  // 这段代码进入 buildSparkSubmitCommand\n  @Override\n  public List<String> buildCommand(Map<String, String> env) throws IOException {\n    if (PYSPARK_SHELL_RESOURCE.equals(appResource) && !printInfo) {\n      return buildPySparkShellCommand(env);\n    } else if (SPARKR_SHELL_RESOURCE.equals(appResource) && !printInfo) {\n      return buildSparkRCommand(env);\n    } else {\n      return buildSparkSubmitCommand(env);\n    }\n  }\n ```\n 看看buildSparkSubmitCommand函数\n ``` java\n  ...\n   private List<String> buildSparkSubmitCommand(Map<String, String> env) throws IOException {\n    // Load the properties file and check whether spark-submit will be running the app's driver\n    // or just launching a cluster app. When running the driver, the JVM's argument will be\n    // modified to cover the driver's configuration.\n    // 先获取有效的配置，如果用户不自己制定的话，默认情况下会去 $SPARK_HOME/conf/spark-defaults.conf 中拿\n    Map<String, String> config = getEffectiveConfig();\n    boolean isClientMode = isClientMode(config);\n    String extraClassPath = isClientMode ? config.get(SparkLauncher.DRIVER_EXTRA_CLASSPATH) : null;\n\n    List<String> cmd = buildJavaCommand(extraClassPath);\n    // Take Thrift Server as daemon\n    if (isThriftServer(mainClass)) {\n      addOptionString(cmd, System.getenv(\"SPARK_DAEMON_JAVA_OPTS\"));\n    }\n    addOptionString(cmd, System.getenv(\"SPARK_SUBMIT_OPTS\"));\n    addOptionString(cmd, System.getenv(\"SPARK_JAVA_OPTS\"));\n\n    // 接下来的代码的意思是，很多没有手动指定的参数，会根据配置文件中拿，如果配置文件中没有，会给个默认值\n    if (isClientMode) {\n      // Figuring out where the memory value come from is a little tricky due to precedence.\n      // Precedence is observed in the following order:\n      // - explicit configuration (setConf()), which also covers --driver-memory cli argument.\n      // - properties file.\n      // - SPARK_DRIVER_MEMORY env variable\n      // - SPARK_MEM env variable\n      // - default value (1g)\n      // Take Thrift Server as daemon\n      String tsMemory =\n        isThriftServer(mainClass) ? System.getenv(\"SPARK_DAEMON_MEMORY\") : null;\n      String memory = firstNonEmpty(tsMemory, config.get(SparkLauncher.DRIVER_MEMORY),\n        System.getenv(\"SPARK_DRIVER_MEMORY\"), System.getenv(\"SPARK_MEM\"), DEFAULT_MEM);\n      cmd.add(\"-Xms\" + memory);\n      cmd.add(\"-Xmx\" + memory);\n      addOptionString(cmd, config.get(SparkLauncher.DRIVER_EXTRA_JAVA_OPTIONS));\n      mergeEnvPathList(env, getLibPathEnvName(),\n        config.get(SparkLauncher.DRIVER_EXTRA_LIBRARY_PATH));\n    }\n\n    addPermGenSizeOpt(cmd);\n    cmd.add(\"org.apache.spark.deploy.SparkSubmit\");\n    cmd.addAll(buildSparkSubmitArgs());\n    return cmd;\n  }\n```\n#### SparkSubmit类\n通过上述方法生成了命令到 $CMD 参数中后，就通过 `exec \"${CMD[@]}\"` 命令执行之前生成的命令，也就是 `org.apache.spark.deploy.SparkSubmit` 类。大概是这样子：\n``` sh\n// 在Unix中，分隔符为'\\0'，以下是大概写法\n/bin/java -Xms1g -XX:MaxPermSize=256m -cp /usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar org.apache.spark.deploy.SparkSubmit --master yarn-client --class SparkPi ./testJars/my.jar\n```\n进入main函数：\n``` scala\ndef main(args: Array[String]): Unit = {\n    val appArgs = new SparkSubmitArguments(args)\n    if (appArgs.verbose) {\n      // scalastyle:off println\n      printStream.println(appArgs)\n      // scalastyle:on println\n    }\n    appArgs.action match {\n      case SparkSubmitAction.SUBMIT => submit(appArgs)\n      case SparkSubmitAction.KILL => kill(appArgs)\n      case SparkSubmitAction.REQUEST_STATUS => requestStatus(appArgs)\n    }\n  }\n```\n咱们调用的是 submit(appArgs)：如下代码，主要分为两步，第一步，准备提交的参数四元组，第二步，\n``` scala\nprivate def submit(args: SparkSubmitArguments): Unit = {\n    // 这里的代码很长，主要目的是准备提交应用的环境，针对 yarn standalone Mesos等各类环境进行针对性处理，并对输入的args进行一些校验和修改，返回一个 4-tuple：\n    // 1. 子程序的参数列表； 2. 子程序的classpath列表 3. 系统环境变量HashMap 4. mainClass\n    val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)\n\n    // 如果有代理晕乎的话，需要创建一个代理用户，然后验证，后运行runMain\n    def doRunMain(): Unit = {\n      if (args.proxyUser != null) {\n        val proxyUser = UserGroupInformation.createProxyUser(args.proxyUser,\n          UserGroupInformation.getCurrentUser())\n        try {\n          proxyUser.doAs(new PrivilegedExceptionAction[Unit]() {\n            override def run(): Unit = {\n              runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)\n            }\n          })\n        } catch {\n          case e: Exception =>\n            // Hadoop's AuthorizationException suppresses the exception's stack trace, which\n            // makes the message printed to the output by the JVM not very helpful. Instead,\n            // detect exceptions with empty stack traces here, and treat them differently.\n            if (e.getStackTrace().length == 0) {\n              // scalastyle:off println\n              printStream.println(s\"ERROR: ${e.getClass().getName()}: ${e.getMessage()}\")\n              // scalastyle:on println\n              exitFn(1)\n            } else {\n              throw e\n            }\n        }\n      } else {\n        runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)\n      }\n    }\n\n     // In standalone cluster mode, there are two submission gateways:\n     //   (1) The traditional RPC gateway using o.a.s.deploy.Client as a wrapper\n     //   (2) The new REST-based gateway introduced in Spark 1.3\n     // The latter is the default behavior as of Spark 1.3, but Spark submit will fail over\n     // to use the legacy gateway if the master endpoint turns out to be not a REST server.\n。。。\n  }\n```\n接下来执行的是runMain方法：\n\n``` scala\n//复用反射加载childMainClass\n//调用反射机制加载main方法\n//执行main方法,进入 SparkPi 的main方法，执行spark应用程序\n```\n至此，正式完成spark应用程序的提交。\n\n\n#### 引用\nhttp://www.cnblogs.com/xing901022/p/6426408.html\nhttp://blog.csdn.net/lovehuangjiaju/article/details/49123975","slug":"spark-on-yarn-作业提交源码分析","published":1,"updated":"2017-04-18T06:31:16.487Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfx2002upsguokqnhuml","content":"<p>最近因为上线hdfs的federation功能，测试spark程序的时候遇到了问题，在分析此问题的过程中对spark on yarn提交作业的过程记录一下，以SparkPi为例，从spark-submit开始，通过debug日志分析详细的过程：<br>整个过程中主要涉及：spark源码（1.6.2）hadoop源码（2.6.0-cdh5.4.7） Kerberos相关</p>\n<h4 id=\"从spark-sumbit开始\"><a href=\"#从spark-sumbit开始\" class=\"headerlink\" title=\"从spark-sumbit开始\"></a>从spark-sumbit开始</h4><p>提交命令：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">spark-submit --master yarn-client --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar</div></pre></td></tr></table></figure></p>\n<p>进入<code>./bin/spark-submit</code><br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">exec</span> <span class=\"string\">\"<span class=\"variable\">$&#123;SPARK_HOME&#125;</span>\"</span>/bin/spark-class org.apache.spark.deploy.SparkSubmit <span class=\"string\">\"<span class=\"variable\">$@</span>\"</span></div></pre></td></tr></table></figure></p>\n<p>以上脚本调用了spark-class脚本，并增加了参数 org.apache.spark.deploy.SparkSubmit。<br>进入 <code>./bin/spark-class</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 首先执行load-spark-env.sh载入 ./conf/spark-env.sh，保证只载入一次，如果之前手动载入过一次的话，就不会再覆盖载入</div><div class=\"line\">. <span class=\"string\">\"<span class=\"variable\">$&#123;SPARK_HOME&#125;</span>\"</span>/bin/load-spark-env.sh</div><div class=\"line\">// 寻找spark安装包，是这个样子：`spark-assembly.*hadoop.*\\.jar$`，而且只能有一个</div><div class=\"line\">...</div><div class=\"line\">// 最后会通过java调用spark的 `org.apache.spark.launcher.Main`作为spark应用程序的主入口，首先循环读取ARG参数，加入到CMD中：</div><div class=\"line\">CMD=()</div><div class=\"line\"><span class=\"keyword\">while</span> IFS= <span class=\"built_in\">read</span> <span class=\"_\">-d</span> <span class=\"string\">''</span> -r ARG; <span class=\"keyword\">do</span></div><div class=\"line\">  CMD+=(<span class=\"string\">\"<span class=\"variable\">$ARG</span>\"</span>)</div><div class=\"line\"><span class=\"keyword\">done</span> &lt; &lt;(<span class=\"string\">\"<span class=\"variable\">$RUNNER</span>\"</span> -cp <span class=\"string\">\"<span class=\"variable\">$LAUNCH_CLASSPATH</span>\"</span> org.apache.spark.launcher.Main <span class=\"string\">\"<span class=\"variable\">$@</span>\"</span>)</div><div class=\"line\"><span class=\"built_in\">exec</span> <span class=\"string\">\"<span class=\"variable\">$&#123;CMD[@]&#125;</span>\"</span></div></pre></td></tr></table></figure>\n<p>翻译过来就是：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/bin/java -cp /usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar org.apache.spark.launcher.Main org.apache.spark.deploy.SparkSubmit --master yarn-client --class SparkPi ./testJars/my.jar</div></pre></td></tr></table></figure></p>\n<p>将这个命令执行的结果返回给 CMD参数，然后执行</p>\n<h4 id=\"launcher-Main\"><a href=\"#launcher-Main\" class=\"headerlink\" title=\"launcher.Main\"></a>launcher.Main</h4><p>这个类的目的是同时适应unix和windows操作系统</p>\n<figure class=\"highlight java\"><figcaption><span>org.apache.spark.launcher.Main.java</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (className.equals(<span class=\"string\">\"org.apache.spark.deploy.SparkSubmit\"</span>)) &#123;</div><div class=\"line\">      <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">      <span class=\"comment\">// 创建一个命令解析器，这里会优先将提交的命令中的 --master之类的参数解析，然后保存到 SparkSubmitCommandBuilder 中</span></div><div class=\"line\">        builder = <span class=\"keyword\">new</span> SparkSubmitCommandBuilder(args);</div><div class=\"line\">      &#125; <span class=\"keyword\">catch</span> (IllegalArgumentException e) &#123;</div><div class=\"line\">        ...</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">      builder = <span class=\"keyword\">new</span> SparkClassCommandBuilder(className, args);</div><div class=\"line\">    &#125;</div><div class=\"line\">    Map&lt;String, String&gt; env = <span class=\"keyword\">new</span> HashMap&lt;String, String&gt;();</div><div class=\"line\">    <span class=\"comment\">// 使用解析器来解析参数，并且得到环境变量的HashMap值</span></div><div class=\"line\">    List&lt;String&gt; cmd = builder.buildCommand(env);</div><div class=\"line\">    ...</div><div class=\"line\">    <span class=\"keyword\">if</span> (isWindows()) &#123;</div><div class=\"line\">      System.out.println(prepareWindowsCommand(cmd, env));</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">      <span class=\"comment\">// In bash, use NULL as the arg separator since it cannot be used in an argument.</span></div><div class=\"line\">      <span class=\"comment\">// 根据输入的参数，准备bash的命令，然后打印出来，传给 spark-class脚本中的$CMD 变量，然后执行，</span></div><div class=\"line\">      List&lt;String&gt; bashCmd = prepareBashCommand(cmd, env);</div><div class=\"line\">      <span class=\"keyword\">for</span> (String c : bashCmd) &#123;</div><div class=\"line\">        System.out.print(c);</div><div class=\"line\">        System.out.print(<span class=\"string\">'\\0'</span>);</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div></pre></td></tr></table></figure>\n<p>再来看看这个cmd到底是怎么build的：<br><figure class=\"highlight java\"><figcaption><span>org.apache.spark.launcher.SparkSubmitCommandBuilder.java</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 这段代码进入 buildSparkSubmitCommand</span></div><div class=\"line\"><span class=\"meta\">@Override</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> List&lt;String&gt; <span class=\"title\">buildCommand</span><span class=\"params\">(Map&lt;String, String&gt; env)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (PYSPARK_SHELL_RESOURCE.equals(appResource) &amp;&amp; !printInfo) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> buildPySparkShellCommand(env);</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (SPARKR_SHELL_RESOURCE.equals(appResource) &amp;&amp; !printInfo) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> buildSparkRCommand(env);</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> buildSparkSubmitCommand(env);</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p> 看看buildSparkSubmitCommand函数<br> <figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div></pre></td><td class=\"code\"><pre><div class=\"line\">...</div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">private</span> List&lt;String&gt; <span class=\"title\">buildSparkSubmitCommand</span><span class=\"params\">(Map&lt;String, String&gt; env)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</div><div class=\"line\">  <span class=\"comment\">// Load the properties file and check whether spark-submit will be running the app's driver</span></div><div class=\"line\">  <span class=\"comment\">// or just launching a cluster app. When running the driver, the JVM's argument will be</span></div><div class=\"line\">  <span class=\"comment\">// modified to cover the driver's configuration.</span></div><div class=\"line\">  <span class=\"comment\">// 先获取有效的配置，如果用户不自己制定的话，默认情况下会去 $SPARK_HOME/conf/spark-defaults.conf 中拿</span></div><div class=\"line\">  Map&lt;String, String&gt; config = getEffectiveConfig();</div><div class=\"line\">  <span class=\"keyword\">boolean</span> isClientMode = isClientMode(config);</div><div class=\"line\">  String extraClassPath = isClientMode ? config.get(SparkLauncher.DRIVER_EXTRA_CLASSPATH) : <span class=\"keyword\">null</span>;</div><div class=\"line\"></div><div class=\"line\">  List&lt;String&gt; cmd = buildJavaCommand(extraClassPath);</div><div class=\"line\">  <span class=\"comment\">// Take Thrift Server as daemon</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (isThriftServer(mainClass)) &#123;</div><div class=\"line\">    addOptionString(cmd, System.getenv(<span class=\"string\">\"SPARK_DAEMON_JAVA_OPTS\"</span>));</div><div class=\"line\">  &#125;</div><div class=\"line\">  addOptionString(cmd, System.getenv(<span class=\"string\">\"SPARK_SUBMIT_OPTS\"</span>));</div><div class=\"line\">  addOptionString(cmd, System.getenv(<span class=\"string\">\"SPARK_JAVA_OPTS\"</span>));</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 接下来的代码的意思是，很多没有手动指定的参数，会根据配置文件中拿，如果配置文件中没有，会给个默认值</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (isClientMode) &#123;</div><div class=\"line\">    <span class=\"comment\">// Figuring out where the memory value come from is a little tricky due to precedence.</span></div><div class=\"line\">    <span class=\"comment\">// Precedence is observed in the following order:</span></div><div class=\"line\">    <span class=\"comment\">// - explicit configuration (setConf()), which also covers --driver-memory cli argument.</span></div><div class=\"line\">    <span class=\"comment\">// - properties file.</span></div><div class=\"line\">    <span class=\"comment\">// - SPARK_DRIVER_MEMORY env variable</span></div><div class=\"line\">    <span class=\"comment\">// - SPARK_MEM env variable</span></div><div class=\"line\">    <span class=\"comment\">// - default value (1g)</span></div><div class=\"line\">    <span class=\"comment\">// Take Thrift Server as daemon</span></div><div class=\"line\">    String tsMemory =</div><div class=\"line\">      isThriftServer(mainClass) ? System.getenv(<span class=\"string\">\"SPARK_DAEMON_MEMORY\"</span>) : <span class=\"keyword\">null</span>;</div><div class=\"line\">    String memory = firstNonEmpty(tsMemory, config.get(SparkLauncher.DRIVER_MEMORY),</div><div class=\"line\">      System.getenv(<span class=\"string\">\"SPARK_DRIVER_MEMORY\"</span>), System.getenv(<span class=\"string\">\"SPARK_MEM\"</span>), DEFAULT_MEM);</div><div class=\"line\">    cmd.add(<span class=\"string\">\"-Xms\"</span> + memory);</div><div class=\"line\">    cmd.add(<span class=\"string\">\"-Xmx\"</span> + memory);</div><div class=\"line\">    addOptionString(cmd, config.get(SparkLauncher.DRIVER_EXTRA_JAVA_OPTIONS));</div><div class=\"line\">    mergeEnvPathList(env, getLibPathEnvName(),</div><div class=\"line\">      config.get(SparkLauncher.DRIVER_EXTRA_LIBRARY_PATH));</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  addPermGenSizeOpt(cmd);</div><div class=\"line\">  cmd.add(<span class=\"string\">\"org.apache.spark.deploy.SparkSubmit\"</span>);</div><div class=\"line\">  cmd.addAll(buildSparkSubmitArgs());</div><div class=\"line\">  <span class=\"keyword\">return</span> cmd;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"SparkSubmit类\"><a href=\"#SparkSubmit类\" class=\"headerlink\" title=\"SparkSubmit类\"></a>SparkSubmit类</h4><p>通过上述方法生成了命令到 $CMD 参数中后，就通过 <code>exec &quot;${CMD[@]}&quot;</code> 命令执行之前生成的命令，也就是 <code>org.apache.spark.deploy.SparkSubmit</code> 类。大概是这样子：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 在Unix中，分隔符为<span class=\"string\">'\\0'</span>，以下是大概写法</div><div class=\"line\">/bin/java -Xms1g -XX:MaxPermSize=256m -cp /usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar org.apache.spark.deploy.SparkSubmit --master yarn-client --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar</div></pre></td></tr></table></figure></p>\n<p>进入main函数：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> appArgs = <span class=\"keyword\">new</span> <span class=\"type\">SparkSubmitArguments</span>(args)</div><div class=\"line\">    <span class=\"keyword\">if</span> (appArgs.verbose) &#123;</div><div class=\"line\">      <span class=\"comment\">// scalastyle:off println</span></div><div class=\"line\">      printStream.println(appArgs)</div><div class=\"line\">      <span class=\"comment\">// scalastyle:on println</span></div><div class=\"line\">    &#125;</div><div class=\"line\">    appArgs.action <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">SparkSubmitAction</span>.<span class=\"type\">SUBMIT</span> =&gt; submit(appArgs)</div><div class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">SparkSubmitAction</span>.<span class=\"type\">KILL</span> =&gt; kill(appArgs)</div><div class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">SparkSubmitAction</span>.<span class=\"type\">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure></p>\n<p>咱们调用的是 submit(appArgs)：如下代码，主要分为两步，第一步，准备提交的参数四元组，第二步，<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">submit</span></span>(args: <span class=\"type\">SparkSubmitArguments</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"comment\">// 这里的代码很长，主要目的是准备提交应用的环境，针对 yarn standalone Mesos等各类环境进行针对性处理，并对输入的args进行一些校验和修改，返回一个 4-tuple：</span></div><div class=\"line\">    <span class=\"comment\">// 1. 子程序的参数列表； 2. 子程序的classpath列表 3. 系统环境变量HashMap 4. mainClass</span></div><div class=\"line\">    <span class=\"keyword\">val</span> (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// 如果有代理晕乎的话，需要创建一个代理用户，然后验证，后运行runMain</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">doRunMain</span></span>(): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">      <span class=\"keyword\">if</span> (args.proxyUser != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">        <span class=\"keyword\">val</span> proxyUser = <span class=\"type\">UserGroupInformation</span>.createProxyUser(args.proxyUser,</div><div class=\"line\">          <span class=\"type\">UserGroupInformation</span>.getCurrentUser())</div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">          proxyUser.doAs(<span class=\"keyword\">new</span> <span class=\"type\">PrivilegedExceptionAction</span>[<span class=\"type\">Unit</span>]() &#123;</div><div class=\"line\">            <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>(): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">              runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</div><div class=\"line\">            &#125;</div><div class=\"line\">          &#125;)</div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">          <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</div><div class=\"line\">            <span class=\"comment\">// Hadoop's AuthorizationException suppresses the exception's stack trace, which</span></div><div class=\"line\">            <span class=\"comment\">// makes the message printed to the output by the JVM not very helpful. Instead,</span></div><div class=\"line\">            <span class=\"comment\">// detect exceptions with empty stack traces here, and treat them differently.</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (e.getStackTrace().length == <span class=\"number\">0</span>) &#123;</div><div class=\"line\">              <span class=\"comment\">// scalastyle:off println</span></div><div class=\"line\">              printStream.println(<span class=\"string\">s\"ERROR: <span class=\"subst\">$&#123;e.getClass().getName()&#125;</span>: <span class=\"subst\">$&#123;e.getMessage()&#125;</span>\"</span>)</div><div class=\"line\">              <span class=\"comment\">// scalastyle:on println</span></div><div class=\"line\">              exitFn(<span class=\"number\">1</span>)</div><div class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">              <span class=\"keyword\">throw</span> e</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">        runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">     <span class=\"comment\">// In standalone cluster mode, there are two submission gateways:</span></div><div class=\"line\">     <span class=\"comment\">//   (1) The traditional RPC gateway using o.a.s.deploy.Client as a wrapper</span></div><div class=\"line\">     <span class=\"comment\">//   (2) The new REST-based gateway introduced in Spark 1.3</span></div><div class=\"line\">     <span class=\"comment\">// The latter is the default behavior as of Spark 1.3, but Spark submit will fail over</span></div><div class=\"line\">     <span class=\"comment\">// to use the legacy gateway if the master endpoint turns out to be not a REST server.</span></div><div class=\"line\">。。。</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure></p>\n<p>接下来执行的是runMain方法：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">//复用反射加载childMainClass</span></div><div class=\"line\"><span class=\"comment\">//调用反射机制加载main方法</span></div><div class=\"line\"><span class=\"comment\">//执行main方法,进入 SparkPi 的main方法，执行spark应用程序</span></div></pre></td></tr></table></figure>\n<p>至此，正式完成spark应用程序的提交。</p>\n<h4 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h4><p><a href=\"http://www.cnblogs.com/xing901022/p/6426408.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/xing901022/p/6426408.html</a><br><a href=\"http://blog.csdn.net/lovehuangjiaju/article/details/49123975\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/lovehuangjiaju/article/details/49123975</a></p>\n","excerpt":"","more":"<p>最近因为上线hdfs的federation功能，测试spark程序的时候遇到了问题，在分析此问题的过程中对spark on yarn提交作业的过程记录一下，以SparkPi为例，从spark-submit开始，通过debug日志分析详细的过程：<br>整个过程中主要涉及：spark源码（1.6.2）hadoop源码（2.6.0-cdh5.4.7） Kerberos相关</p>\n<h4 id=\"从spark-sumbit开始\"><a href=\"#从spark-sumbit开始\" class=\"headerlink\" title=\"从spark-sumbit开始\"></a>从spark-sumbit开始</h4><p>提交命令：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">spark-submit --master yarn-client --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar</div></pre></td></tr></table></figure></p>\n<p>进入<code>./bin/spark-submit</code><br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">exec</span> <span class=\"string\">\"<span class=\"variable\">$&#123;SPARK_HOME&#125;</span>\"</span>/bin/spark-class org.apache.spark.deploy.SparkSubmit <span class=\"string\">\"<span class=\"variable\">$@</span>\"</span></div></pre></td></tr></table></figure></p>\n<p>以上脚本调用了spark-class脚本，并增加了参数 org.apache.spark.deploy.SparkSubmit。<br>进入 <code>./bin/spark-class</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 首先执行load-spark-env.sh载入 ./conf/spark-env.sh，保证只载入一次，如果之前手动载入过一次的话，就不会再覆盖载入</div><div class=\"line\">. <span class=\"string\">\"<span class=\"variable\">$&#123;SPARK_HOME&#125;</span>\"</span>/bin/load-spark-env.sh</div><div class=\"line\">// 寻找spark安装包，是这个样子：`spark-assembly.*hadoop.*\\.jar$`，而且只能有一个</div><div class=\"line\">...</div><div class=\"line\">// 最后会通过java调用spark的 `org.apache.spark.launcher.Main`作为spark应用程序的主入口，首先循环读取ARG参数，加入到CMD中：</div><div class=\"line\">CMD=()</div><div class=\"line\"><span class=\"keyword\">while</span> IFS= <span class=\"built_in\">read</span> <span class=\"_\">-d</span> <span class=\"string\">''</span> -r ARG; <span class=\"keyword\">do</span></div><div class=\"line\">  CMD+=(<span class=\"string\">\"<span class=\"variable\">$ARG</span>\"</span>)</div><div class=\"line\"><span class=\"keyword\">done</span> &lt; &lt;(<span class=\"string\">\"<span class=\"variable\">$RUNNER</span>\"</span> -cp <span class=\"string\">\"<span class=\"variable\">$LAUNCH_CLASSPATH</span>\"</span> org.apache.spark.launcher.Main <span class=\"string\">\"<span class=\"variable\">$@</span>\"</span>)</div><div class=\"line\"><span class=\"built_in\">exec</span> <span class=\"string\">\"<span class=\"variable\">$&#123;CMD[@]&#125;</span>\"</span></div></pre></td></tr></table></figure>\n<p>翻译过来就是：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/bin/java -cp /usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar org.apache.spark.launcher.Main org.apache.spark.deploy.SparkSubmit --master yarn-client --class SparkPi ./testJars/my.jar</div></pre></td></tr></table></figure></p>\n<p>将这个命令执行的结果返回给 CMD参数，然后执行</p>\n<h4 id=\"launcher-Main\"><a href=\"#launcher-Main\" class=\"headerlink\" title=\"launcher.Main\"></a>launcher.Main</h4><p>这个类的目的是同时适应unix和windows操作系统</p>\n<figure class=\"highlight java\"><figcaption><span>org.apache.spark.launcher.Main.java</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (className.equals(<span class=\"string\">\"org.apache.spark.deploy.SparkSubmit\"</span>)) &#123;</div><div class=\"line\">      <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">      <span class=\"comment\">// 创建一个命令解析器，这里会优先将提交的命令中的 --master之类的参数解析，然后保存到 SparkSubmitCommandBuilder 中</span></div><div class=\"line\">        builder = <span class=\"keyword\">new</span> SparkSubmitCommandBuilder(args);</div><div class=\"line\">      &#125; <span class=\"keyword\">catch</span> (IllegalArgumentException e) &#123;</div><div class=\"line\">        ...</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">      builder = <span class=\"keyword\">new</span> SparkClassCommandBuilder(className, args);</div><div class=\"line\">    &#125;</div><div class=\"line\">    Map&lt;String, String&gt; env = <span class=\"keyword\">new</span> HashMap&lt;String, String&gt;();</div><div class=\"line\">    <span class=\"comment\">// 使用解析器来解析参数，并且得到环境变量的HashMap值</span></div><div class=\"line\">    List&lt;String&gt; cmd = builder.buildCommand(env);</div><div class=\"line\">    ...</div><div class=\"line\">    <span class=\"keyword\">if</span> (isWindows()) &#123;</div><div class=\"line\">      System.out.println(prepareWindowsCommand(cmd, env));</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">      <span class=\"comment\">// In bash, use NULL as the arg separator since it cannot be used in an argument.</span></div><div class=\"line\">      <span class=\"comment\">// 根据输入的参数，准备bash的命令，然后打印出来，传给 spark-class脚本中的$CMD 变量，然后执行，</span></div><div class=\"line\">      List&lt;String&gt; bashCmd = prepareBashCommand(cmd, env);</div><div class=\"line\">      <span class=\"keyword\">for</span> (String c : bashCmd) &#123;</div><div class=\"line\">        System.out.print(c);</div><div class=\"line\">        System.out.print(<span class=\"string\">'\\0'</span>);</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div></pre></td></tr></table></figure>\n<p>再来看看这个cmd到底是怎么build的：<br><figure class=\"highlight java\"><figcaption><span>org.apache.spark.launcher.SparkSubmitCommandBuilder.java</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 这段代码进入 buildSparkSubmitCommand</span></div><div class=\"line\"><span class=\"meta\">@Override</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> List&lt;String&gt; <span class=\"title\">buildCommand</span><span class=\"params\">(Map&lt;String, String&gt; env)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</div><div class=\"line\">  <span class=\"keyword\">if</span> (PYSPARK_SHELL_RESOURCE.equals(appResource) &amp;&amp; !printInfo) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> buildPySparkShellCommand(env);</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (SPARKR_SHELL_RESOURCE.equals(appResource) &amp;&amp; !printInfo) &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> buildSparkRCommand(env);</div><div class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">    <span class=\"keyword\">return</span> buildSparkSubmitCommand(env);</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p> 看看buildSparkSubmitCommand函数<br> <figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div></pre></td><td class=\"code\"><pre><div class=\"line\">...</div><div class=\"line\"> <span class=\"function\"><span class=\"keyword\">private</span> List&lt;String&gt; <span class=\"title\">buildSparkSubmitCommand</span><span class=\"params\">(Map&lt;String, String&gt; env)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</div><div class=\"line\">  <span class=\"comment\">// Load the properties file and check whether spark-submit will be running the app's driver</span></div><div class=\"line\">  <span class=\"comment\">// or just launching a cluster app. When running the driver, the JVM's argument will be</span></div><div class=\"line\">  <span class=\"comment\">// modified to cover the driver's configuration.</span></div><div class=\"line\">  <span class=\"comment\">// 先获取有效的配置，如果用户不自己制定的话，默认情况下会去 $SPARK_HOME/conf/spark-defaults.conf 中拿</span></div><div class=\"line\">  Map&lt;String, String&gt; config = getEffectiveConfig();</div><div class=\"line\">  <span class=\"keyword\">boolean</span> isClientMode = isClientMode(config);</div><div class=\"line\">  String extraClassPath = isClientMode ? config.get(SparkLauncher.DRIVER_EXTRA_CLASSPATH) : <span class=\"keyword\">null</span>;</div><div class=\"line\"></div><div class=\"line\">  List&lt;String&gt; cmd = buildJavaCommand(extraClassPath);</div><div class=\"line\">  <span class=\"comment\">// Take Thrift Server as daemon</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (isThriftServer(mainClass)) &#123;</div><div class=\"line\">    addOptionString(cmd, System.getenv(<span class=\"string\">\"SPARK_DAEMON_JAVA_OPTS\"</span>));</div><div class=\"line\">  &#125;</div><div class=\"line\">  addOptionString(cmd, System.getenv(<span class=\"string\">\"SPARK_SUBMIT_OPTS\"</span>));</div><div class=\"line\">  addOptionString(cmd, System.getenv(<span class=\"string\">\"SPARK_JAVA_OPTS\"</span>));</div><div class=\"line\"></div><div class=\"line\">  <span class=\"comment\">// 接下来的代码的意思是，很多没有手动指定的参数，会根据配置文件中拿，如果配置文件中没有，会给个默认值</span></div><div class=\"line\">  <span class=\"keyword\">if</span> (isClientMode) &#123;</div><div class=\"line\">    <span class=\"comment\">// Figuring out where the memory value come from is a little tricky due to precedence.</span></div><div class=\"line\">    <span class=\"comment\">// Precedence is observed in the following order:</span></div><div class=\"line\">    <span class=\"comment\">// - explicit configuration (setConf()), which also covers --driver-memory cli argument.</span></div><div class=\"line\">    <span class=\"comment\">// - properties file.</span></div><div class=\"line\">    <span class=\"comment\">// - SPARK_DRIVER_MEMORY env variable</span></div><div class=\"line\">    <span class=\"comment\">// - SPARK_MEM env variable</span></div><div class=\"line\">    <span class=\"comment\">// - default value (1g)</span></div><div class=\"line\">    <span class=\"comment\">// Take Thrift Server as daemon</span></div><div class=\"line\">    String tsMemory =</div><div class=\"line\">      isThriftServer(mainClass) ? System.getenv(<span class=\"string\">\"SPARK_DAEMON_MEMORY\"</span>) : <span class=\"keyword\">null</span>;</div><div class=\"line\">    String memory = firstNonEmpty(tsMemory, config.get(SparkLauncher.DRIVER_MEMORY),</div><div class=\"line\">      System.getenv(<span class=\"string\">\"SPARK_DRIVER_MEMORY\"</span>), System.getenv(<span class=\"string\">\"SPARK_MEM\"</span>), DEFAULT_MEM);</div><div class=\"line\">    cmd.add(<span class=\"string\">\"-Xms\"</span> + memory);</div><div class=\"line\">    cmd.add(<span class=\"string\">\"-Xmx\"</span> + memory);</div><div class=\"line\">    addOptionString(cmd, config.get(SparkLauncher.DRIVER_EXTRA_JAVA_OPTIONS));</div><div class=\"line\">    mergeEnvPathList(env, getLibPathEnvName(),</div><div class=\"line\">      config.get(SparkLauncher.DRIVER_EXTRA_LIBRARY_PATH));</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  addPermGenSizeOpt(cmd);</div><div class=\"line\">  cmd.add(<span class=\"string\">\"org.apache.spark.deploy.SparkSubmit\"</span>);</div><div class=\"line\">  cmd.addAll(buildSparkSubmitArgs());</div><div class=\"line\">  <span class=\"keyword\">return</span> cmd;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"SparkSubmit类\"><a href=\"#SparkSubmit类\" class=\"headerlink\" title=\"SparkSubmit类\"></a>SparkSubmit类</h4><p>通过上述方法生成了命令到 $CMD 参数中后，就通过 <code>exec &quot;${CMD[@]}&quot;</code> 命令执行之前生成的命令，也就是 <code>org.apache.spark.deploy.SparkSubmit</code> 类。大概是这样子：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 在Unix中，分隔符为<span class=\"string\">'\\0'</span>，以下是大概写法</div><div class=\"line\">/bin/java -Xms1g -XX:MaxPermSize=256m -cp /usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar org.apache.spark.deploy.SparkSubmit --master yarn-client --class SparkPi ./<span class=\"built_in\">test</span>Jars/my.jar</div></pre></td></tr></table></figure></p>\n<p>进入main函数：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> appArgs = <span class=\"keyword\">new</span> <span class=\"type\">SparkSubmitArguments</span>(args)</div><div class=\"line\">    <span class=\"keyword\">if</span> (appArgs.verbose) &#123;</div><div class=\"line\">      <span class=\"comment\">// scalastyle:off println</span></div><div class=\"line\">      printStream.println(appArgs)</div><div class=\"line\">      <span class=\"comment\">// scalastyle:on println</span></div><div class=\"line\">    &#125;</div><div class=\"line\">    appArgs.action <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">SparkSubmitAction</span>.<span class=\"type\">SUBMIT</span> =&gt; submit(appArgs)</div><div class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">SparkSubmitAction</span>.<span class=\"type\">KILL</span> =&gt; kill(appArgs)</div><div class=\"line\">      <span class=\"keyword\">case</span> <span class=\"type\">SparkSubmitAction</span>.<span class=\"type\">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure></p>\n<p>咱们调用的是 submit(appArgs)：如下代码，主要分为两步，第一步，准备提交的参数四元组，第二步，<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">submit</span></span>(args: <span class=\"type\">SparkSubmitArguments</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"comment\">// 这里的代码很长，主要目的是准备提交应用的环境，针对 yarn standalone Mesos等各类环境进行针对性处理，并对输入的args进行一些校验和修改，返回一个 4-tuple：</span></div><div class=\"line\">    <span class=\"comment\">// 1. 子程序的参数列表； 2. 子程序的classpath列表 3. 系统环境变量HashMap 4. mainClass</span></div><div class=\"line\">    <span class=\"keyword\">val</span> (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// 如果有代理晕乎的话，需要创建一个代理用户，然后验证，后运行runMain</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">doRunMain</span></span>(): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">      <span class=\"keyword\">if</span> (args.proxyUser != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">        <span class=\"keyword\">val</span> proxyUser = <span class=\"type\">UserGroupInformation</span>.createProxyUser(args.proxyUser,</div><div class=\"line\">          <span class=\"type\">UserGroupInformation</span>.getCurrentUser())</div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">          proxyUser.doAs(<span class=\"keyword\">new</span> <span class=\"type\">PrivilegedExceptionAction</span>[<span class=\"type\">Unit</span>]() &#123;</div><div class=\"line\">            <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run</span></span>(): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">              runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</div><div class=\"line\">            &#125;</div><div class=\"line\">          &#125;)</div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> &#123;</div><div class=\"line\">          <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</div><div class=\"line\">            <span class=\"comment\">// Hadoop's AuthorizationException suppresses the exception's stack trace, which</span></div><div class=\"line\">            <span class=\"comment\">// makes the message printed to the output by the JVM not very helpful. Instead,</span></div><div class=\"line\">            <span class=\"comment\">// detect exceptions with empty stack traces here, and treat them differently.</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (e.getStackTrace().length == <span class=\"number\">0</span>) &#123;</div><div class=\"line\">              <span class=\"comment\">// scalastyle:off println</span></div><div class=\"line\">              printStream.println(<span class=\"string\">s\"ERROR: <span class=\"subst\">$&#123;e.getClass().getName()&#125;</span>: <span class=\"subst\">$&#123;e.getMessage()&#125;</span>\"</span>)</div><div class=\"line\">              <span class=\"comment\">// scalastyle:on println</span></div><div class=\"line\">              exitFn(<span class=\"number\">1</span>)</div><div class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">              <span class=\"keyword\">throw</span> e</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">        runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">     <span class=\"comment\">// In standalone cluster mode, there are two submission gateways:</span></div><div class=\"line\">     <span class=\"comment\">//   (1) The traditional RPC gateway using o.a.s.deploy.Client as a wrapper</span></div><div class=\"line\">     <span class=\"comment\">//   (2) The new REST-based gateway introduced in Spark 1.3</span></div><div class=\"line\">     <span class=\"comment\">// The latter is the default behavior as of Spark 1.3, but Spark submit will fail over</span></div><div class=\"line\">     <span class=\"comment\">// to use the legacy gateway if the master endpoint turns out to be not a REST server.</span></div><div class=\"line\">。。。</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure></p>\n<p>接下来执行的是runMain方法：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">//复用反射加载childMainClass</span></div><div class=\"line\"><span class=\"comment\">//调用反射机制加载main方法</span></div><div class=\"line\"><span class=\"comment\">//执行main方法,进入 SparkPi 的main方法，执行spark应用程序</span></div></pre></td></tr></table></figure>\n<p>至此，正式完成spark应用程序的提交。</p>\n<h4 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h4><p><a href=\"http://www.cnblogs.com/xing901022/p/6426408.html\">http://www.cnblogs.com/xing901022/p/6426408.html</a><br><a href=\"http://blog.csdn.net/lovehuangjiaju/article/details/49123975\">http://blog.csdn.net/lovehuangjiaju/article/details/49123975</a></p>\n"},{"title":"Apache-Eagle定义一个Application","toc":true,"date":"2017-03-10T10:18:23.000Z","_content":"\n为了监控大数据平台的大量组件与应用，我们决定引入 Apache Eagle()，Apache Eagle是由Ebay贡献并在2017年初成为了顶级项目。。。它的核心就是用一个实时计算平台（Storm），接收数据（kafka等），然后处理，然后存hbase，然后报警。由于缺乏文档，所以最简单的配置使用也是踩了不少坑：\n当前版本：Eagle 0.5.0 Spark 1.6.2\n\n\n## 配置使用\n\nhttp://10.142.78.74:8090/#/integration/applicationList\n\nhttp://10.142.78.100:8080/topology.html?id=SPARK_HISTORY_JOB_APP_TESTENV-38-1489136484\n\nhttp://10.142.78.100:60010/master-status\n\nhttp://www.yiibai.com/hbase/\n\n\n## 设计\n### alert engine\n#### 高层次设计\n\n从高层来看，alert engine 是一个元数据驱动的storm拓扑，它包括了多个模块协同工作：\n\n- Admin Service - 提供了元数据管理和拓扑管理的API。其中：Metadata store 是 admin service API的具体实现.\n- Alert Engine Topology on Storm ：通用的Storm 拓扑\n- Coordinator 协调器： alert engine拓扑的调度程序。它是一个后端调度程序，用于新策略的加载，资源分配，并且暴露了一些内部的API用于管理；\n- Zookeeper：作为通信和警报引擎之间的通信。\n\n{% asset_img eagle_1.png %}\n\n## 原理\nSpark History Job Monitor主要分为两大步骤，在代码中的表现为一个 SparkHistoryJobSpout 和一个 SparkHistoryJobParseBolt：\n\nSparkHistoryJobSpout：从 rm 提供的metric中，解析出 已经完成的spark job：COMPLETE_SPARK_JOB，得到 applicationID，发给下一个Bolt\nSparkHistoryJobParseBolt：接收到上游发过来的appId后，通过spark的规则，将其\n\n## 配置 Spark History Job Monitor\n\n分别点击 `Integration` -> `Sites` -> `Edit` 进入应用配置界面；\n找到 `Spark History Job Monitor` 配置，点击右边的 `编辑`按钮，即可编辑\n对于 Spark History Job Monitor，由于我们集群是Kerberos的，需要配置以下参数，\n\n#### Environment\n##### Execution Mode  选择 Cluster Mode\n\n##### Execution File  \n默认为：  /usr/local/eagle-0.5.0-SNAPSHOT/lib/eagle-topology-0.5.0-SNAPSHOT-assembly.jar \n可以不用改\n\n#### General\n##### resource manager url  指的是yarn的那个界面\n\thttp://10.142.78.36:8090/\n##### hdfs url 指的是 hdfs的路径 \n\thdfs://10.142.78.98:54310/\n##### hdfs base path for spark job data  指的是spark history server配置的日志写在hdfs中的路径\n\thdfs:///user/op/sparkHistoryServe\n\n#### Advanced\n这里主要配的是一些storm以及spark的一些相关参数，可以先不用配\n\n#### Custom 如果hdfs是Kerberos的话，需要配置Kerberos相关参数\n需要增加如下：\n\n``` bash\ndataSourceConfig.hdfs.keytab.file hdfs@HADOOP.CHINATELECOM.CN\ndataSourceConfig.hdfs.kerberos.principal /etc/hadoop/conf/hdfs.keytab\n```\n\n#### custom\n``` json\n{\n    \"workers\": \"3\",\n    \"topology.numOfSpoutExecutors\": \"1\",\n    \"topology.numOfSpoutTasks\": \"1\",\n    \"topology.numOfParseBoltExecutors\": \"6\",\n    \"topology.numOfParserBoltTasks\": \"6\",\n    \"topology.spoutCrawlInterval\": \"60000\",\n    \"topology.requestLimit\": \"100\",\n    \"topology.message.timeout.secs\": \"600\",\n    \"service.flushLimit\": \"500\",\n    \"dataSourceConfig.rm.url\": \"\",\n    \"dataSourceConfig.hdfs.fs.defaultFS\": \"hdfs://xxxxx\",\n    \"dataSourceConfig.hdfs.baseDir\": \"/logs/spark-events\",\n    \"spark.jobConf.additional.info\": \"\",\n    \"spark.defaultVal.spark.executor.memory\": \"1g\",\n    \"spark.defaultVal.spark.driver.memory\": \"1g\",\n    \"spark.defaultVal.spark.driver.cores\": \"1\",\n    \"spark.defaultVal.spark.executor.cores\": \"1\",\n    \"spark.defaultVal.spark.yarn.am.memory\": \"512m\",\n    \"spark.defaultVal.spark.yarn.am.cores\": \"1\",\n    \"spark.defaultVal.spark.yarn.executor.memoryOverhead.factor\": \"10\",\n    \"spark.defaultVal.spark.yarn.driver.memoryOverhead.factor\": \"10\",\n    \"spark.defaultVal.spark.yarn.am.memoryOverhead.factor\": \"10\",\n    \"spark.defaultVal.spark.yarn.overhead.min\": \"384m\",\n    \"dataSourceConfig.hdfs.dfs.namenode.rpc-address.xxxxx.nn1\": \"yyyyy\",\n    \"dataSourceConfig.hdfs.dfs.namenode.rpc-address.xxxxx.nn2\": \"zzzzz\",\n    \"dataSourceConfig.hdfs.dfs.client.failover.proxy.provider.xxxxx\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\n    \"dataSourceConfig.hdfs.dfs.nameservices\": \"xxxxx\",\n    \"dataSourceConfig.hdfs.dfs.ha.namenodes.xxxxx\": \"nn1,nn2\",\n    \"dataSourceConfig.hdfs.hdfs.kerberos.principal\": \"aaaaaaa\",\n    \"dataSourceConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\": \"org.apache.hadoop.security.WhitelistBasedResolver\",\n    \"dataSourceConfig.hdfs.hdfs.keytab.file\": \"/home/storm/.keytab/b_eagle.keytab\",\n    \"dataSourceConfig.hdfs.dfs.data.transfer.protection\": \"authentication,privacy\",\n    \"dataSourceConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\": \"AES/CTR/NoPadding\"\n}\n```\n\n``` json 我们的\n{\n\t\"workers\": \"1\",\n\t\"topology.numOfSpoutExecutors\": \"1\",\n\t\"topology.numOfSpoutTasks\": \"4\",\n\t\"topology.numOfParseBoltExecutors\": \"1\",\n\t\"topology.numOfParserBoltTasks\": \"4\",\n\t\"topology.spoutCrawlInterval\": \"10000\",\n\t\"topology.message.timeout.secs\": \"300\",\n\t\"service.flushLimit\": \"500\",\n\t\"dataSourceConfig.rm.url\": \"http://10.142.78.36:8090/\",\n\t\"dataSourceConfig.hdfs.fs.defaultFS\": \"hdfs://ns\",\n\t\"dataSourceConfig.hdfs.baseDir\": \"/user/op/sparkHistoryServer\",\n\t\"spark.jobConf.additional.info\": \"\",\n\t\"spark.defaultVal.spark.executor.memory\": \"1g\",\n\t\"spark.defaultVal.spark.driver.memory\": \"1g\",\n\t\"spark.defaultVal.spark.driver.cores\": \"1\",\n\t\"spark.defaultVal.spark.executor.cores\": \"1\",\n\t\"spark.defaultVal.spark.yarn.am.memory\": \"512m\",\n\t\"spark.defaultVal.spark.yarn.am.cores\": \"1\",\n\t\"spark.defaultVal.spark.yarn.executor.memoryOverhead.factor\": \"10\",\n\t\"spark.defaultVal.spark.yarn.driver.memoryOverhead.factor\": \"10\",\n\t\"spark.defaultVal.spark.yarn.am.memoryOverhead.factor\": \"10\",\n\t\"spark.defaultVal.spark.yarn.overhead.min\": \"384m\",\n\n\t\"dataSourceConfig.hdfs.dfs.namenode.kerberos.principal\": \"hdfs/_HOST@HADOOP.CHINATELECOM.CN\",\n\t\"dataSourceConfig.hdfs.hadoop.security.authorization\": \"true\",\n\t\"dataSourceConfig.hdfs.dfs.data.transfer.protection\": \"authentication,privacy\",\n\t\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.ns.nn1\": \"NM-ITC-NF8460M3-303-011:54310\",\n\t\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.ns.nn2\": \"NM-ITC-NF8460M3-303-012:54310\",\n\t\"dataSourceConfig.hdfs.dfs.nameservices\": \"ns\",\n\t\"dataSourceConfig.hdfs.dfs.client.failover.proxy.provider.ns\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\n\t\"dataSourceConfig.hdfs.java.security.krb5.kdc\": \"test-bdd-073\",\n\t\"dataSourceConfig.hdfs.dfs.ha.namenodes.ns\": \"nn1,nn2\",\n\t\"dataSourceConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\": \"AES/CTR/NoPadding\",\n\t\"dataSourceConfig.hdfs.hdfs.kerberos.principal\": \"hdfs@HADOOP.CHINATELECOM.CN\",\n\t\"dataSourceConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\": \"org.apache.hadoop.security.WhitelistBasedResolver\",\n\t\"dataSourceConfig.hdfs.hadoop.security.authentication\": \"kerberos\",\n\t\"dataSourceConfig.hdfs.java.security.krb5.conf\": \"/etc/krb5.conf\",\n\t\"dataSourceConfig.hdfs.java.security.krb5.realm\": \"HADOOP.CHINATELECOM.CN\",\n\t\"dataSourceConfig.hdfs.hdfs.keytab.file\": \"/tmp/hdfs.keytab\"\n}\n```\n``` json ,我们的 MR history job\n{\n\t\"workers\": \"2\",\n\t\"stormConfig.mrHistoryJobSpoutTasks\": \"4\",\n\t\"stormConfig.jobKafkaSinkTasks\": \"1\",\n\t\"stormConfig.taskAttemptKafkaSinkTasks\": \"1\",\n\t\"endpointConfig.hdfs.fs.defaultFS\": \"hdfs://ns\",\n\t\"endpointConfig.basePath\": \"/history-yarn/done\",\n\t\"endpointConfig.mrHistoryServerUrl\": \"http://10.142.78.40:19890\",\n\t\"endpointConfig.timeZone\": \"Etc/GMT-8\",\n\t\"dataSinkConfig.MAP_REDUCE_JOB_STREAM.topic\": \"map_reduce_job_testenv\",\n\t\"dataSinkConfig.MAP_REDUCE_TASK_ATTEMPT_STREAM.topic\": \"map_reduce_task_attempt_testenv\",\n\t\"dataSinkConfig.brokerList\": \"10.142.78.100:9092\",\n\t\"dataSourceConfig.zkConnection\": \"10.142.78.98:2181,10.142.78.99:2181,10.142.78.100:2181\",\n\t\"dataSinkConfig.serializerClass\": \"kafka.serializer.StringEncoder\",\n\t\"dataSinkConfig.keySerializerClass\": \"kafka.serializer.StringEncoder\",\n\t\"dataSinkConfig.producerType\": \"async\",\n\t\"dataSinkConfig.numBatchMessages\": \"4096\",\n\t\"dataSinkConfig.maxQueueBufferMs\": \"5000\",\n\t\"dataSinkConfig.requestRequiredAcks\": \"0\",\n\t\n\t\"endpointConfig.hdfs.dfs.namenode.kerberos.principal\": \"hdfs/_HOST@HADOOP.CHINATELECOM.CN\",\n\t\"endpointConfig.hdfs.hadoop.security.authorization\": \"true\",\n\t\"endpointConfig.hdfs.dfs.data.transfer.protection\": \"authentication,privacy\",\n\t\"endpointConfig.hdfs.dfs.namenode.rpc-address.ns.nn1\": \"NM-ITC-NF8460M3-303-011:54310\",\n\t\"endpointConfig.hdfs.dfs.namenode.rpc-address.ns.nn2\": \"NM-ITC-NF8460M3-303-012:54310\",\n\t\"endpointConfig.hdfs.dfs.nameservices\": \"ns\",\n\t\"endpointConfig.hdfs.dfs.client.failover.proxy.provider.ns\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\n\t\"endpointConfig.hdfs.java.security.krb5.kdc\": \"test-bdd-073\",\n\t\"endpointConfig.hdfs.dfs.ha.namenodes.ns\": \"nn1,nn2\",\n\t\"endpointConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\": \"AES/CTR/NoPadding\",\n\t\"endpointConfig.hdfs.hdfs.kerberos.principal\": \"hdfs@HADOOP.CHINATELECOM.CN\",\n\t\"endpointConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\": \"org.apache.hadoop.security.WhitelistBasedResolver\",\n\t\"endpointConfig.hdfs.hadoop.security.authentication\": \"kerberos\",\n\t\"endpointConfig.hdfs.java.security.krb5.conf\": \"/etc/krb5.conf\",\n\t\"endpointConfig.hdfs.java.security.krb5.realm\": \"HADOOP.CHINATELECOM.CN\",\n\t\"endpointConfig.hdfs.hdfs.keytab.file\": \"/tmp/hdfs.keytab\"\n}\n```\n\n\n#### 引用\nhttps://eagle.apache.org/\nhttp://www.csdn.net/article/2015-10-29/2826076","source":"_posts/2017-03-10-Apache-Eagle定义一个Application.md","raw":"---\ntitle: Apache-Eagle定义一个Application\ntoc: true\ndate: 2017-03-10 18:18:23\ntags: \n- spark\n- eagle \ncategories: eagle\n---\n\n为了监控大数据平台的大量组件与应用，我们决定引入 Apache Eagle()，Apache Eagle是由Ebay贡献并在2017年初成为了顶级项目。。。它的核心就是用一个实时计算平台（Storm），接收数据（kafka等），然后处理，然后存hbase，然后报警。由于缺乏文档，所以最简单的配置使用也是踩了不少坑：\n当前版本：Eagle 0.5.0 Spark 1.6.2\n\n\n## 配置使用\n\nhttp://10.142.78.74:8090/#/integration/applicationList\n\nhttp://10.142.78.100:8080/topology.html?id=SPARK_HISTORY_JOB_APP_TESTENV-38-1489136484\n\nhttp://10.142.78.100:60010/master-status\n\nhttp://www.yiibai.com/hbase/\n\n\n## 设计\n### alert engine\n#### 高层次设计\n\n从高层来看，alert engine 是一个元数据驱动的storm拓扑，它包括了多个模块协同工作：\n\n- Admin Service - 提供了元数据管理和拓扑管理的API。其中：Metadata store 是 admin service API的具体实现.\n- Alert Engine Topology on Storm ：通用的Storm 拓扑\n- Coordinator 协调器： alert engine拓扑的调度程序。它是一个后端调度程序，用于新策略的加载，资源分配，并且暴露了一些内部的API用于管理；\n- Zookeeper：作为通信和警报引擎之间的通信。\n\n{% asset_img eagle_1.png %}\n\n## 原理\nSpark History Job Monitor主要分为两大步骤，在代码中的表现为一个 SparkHistoryJobSpout 和一个 SparkHistoryJobParseBolt：\n\nSparkHistoryJobSpout：从 rm 提供的metric中，解析出 已经完成的spark job：COMPLETE_SPARK_JOB，得到 applicationID，发给下一个Bolt\nSparkHistoryJobParseBolt：接收到上游发过来的appId后，通过spark的规则，将其\n\n## 配置 Spark History Job Monitor\n\n分别点击 `Integration` -> `Sites` -> `Edit` 进入应用配置界面；\n找到 `Spark History Job Monitor` 配置，点击右边的 `编辑`按钮，即可编辑\n对于 Spark History Job Monitor，由于我们集群是Kerberos的，需要配置以下参数，\n\n#### Environment\n##### Execution Mode  选择 Cluster Mode\n\n##### Execution File  \n默认为：  /usr/local/eagle-0.5.0-SNAPSHOT/lib/eagle-topology-0.5.0-SNAPSHOT-assembly.jar \n可以不用改\n\n#### General\n##### resource manager url  指的是yarn的那个界面\n\thttp://10.142.78.36:8090/\n##### hdfs url 指的是 hdfs的路径 \n\thdfs://10.142.78.98:54310/\n##### hdfs base path for spark job data  指的是spark history server配置的日志写在hdfs中的路径\n\thdfs:///user/op/sparkHistoryServe\n\n#### Advanced\n这里主要配的是一些storm以及spark的一些相关参数，可以先不用配\n\n#### Custom 如果hdfs是Kerberos的话，需要配置Kerberos相关参数\n需要增加如下：\n\n``` bash\ndataSourceConfig.hdfs.keytab.file hdfs@HADOOP.CHINATELECOM.CN\ndataSourceConfig.hdfs.kerberos.principal /etc/hadoop/conf/hdfs.keytab\n```\n\n#### custom\n``` json\n{\n    \"workers\": \"3\",\n    \"topology.numOfSpoutExecutors\": \"1\",\n    \"topology.numOfSpoutTasks\": \"1\",\n    \"topology.numOfParseBoltExecutors\": \"6\",\n    \"topology.numOfParserBoltTasks\": \"6\",\n    \"topology.spoutCrawlInterval\": \"60000\",\n    \"topology.requestLimit\": \"100\",\n    \"topology.message.timeout.secs\": \"600\",\n    \"service.flushLimit\": \"500\",\n    \"dataSourceConfig.rm.url\": \"\",\n    \"dataSourceConfig.hdfs.fs.defaultFS\": \"hdfs://xxxxx\",\n    \"dataSourceConfig.hdfs.baseDir\": \"/logs/spark-events\",\n    \"spark.jobConf.additional.info\": \"\",\n    \"spark.defaultVal.spark.executor.memory\": \"1g\",\n    \"spark.defaultVal.spark.driver.memory\": \"1g\",\n    \"spark.defaultVal.spark.driver.cores\": \"1\",\n    \"spark.defaultVal.spark.executor.cores\": \"1\",\n    \"spark.defaultVal.spark.yarn.am.memory\": \"512m\",\n    \"spark.defaultVal.spark.yarn.am.cores\": \"1\",\n    \"spark.defaultVal.spark.yarn.executor.memoryOverhead.factor\": \"10\",\n    \"spark.defaultVal.spark.yarn.driver.memoryOverhead.factor\": \"10\",\n    \"spark.defaultVal.spark.yarn.am.memoryOverhead.factor\": \"10\",\n    \"spark.defaultVal.spark.yarn.overhead.min\": \"384m\",\n    \"dataSourceConfig.hdfs.dfs.namenode.rpc-address.xxxxx.nn1\": \"yyyyy\",\n    \"dataSourceConfig.hdfs.dfs.namenode.rpc-address.xxxxx.nn2\": \"zzzzz\",\n    \"dataSourceConfig.hdfs.dfs.client.failover.proxy.provider.xxxxx\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\n    \"dataSourceConfig.hdfs.dfs.nameservices\": \"xxxxx\",\n    \"dataSourceConfig.hdfs.dfs.ha.namenodes.xxxxx\": \"nn1,nn2\",\n    \"dataSourceConfig.hdfs.hdfs.kerberos.principal\": \"aaaaaaa\",\n    \"dataSourceConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\": \"org.apache.hadoop.security.WhitelistBasedResolver\",\n    \"dataSourceConfig.hdfs.hdfs.keytab.file\": \"/home/storm/.keytab/b_eagle.keytab\",\n    \"dataSourceConfig.hdfs.dfs.data.transfer.protection\": \"authentication,privacy\",\n    \"dataSourceConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\": \"AES/CTR/NoPadding\"\n}\n```\n\n``` json 我们的\n{\n\t\"workers\": \"1\",\n\t\"topology.numOfSpoutExecutors\": \"1\",\n\t\"topology.numOfSpoutTasks\": \"4\",\n\t\"topology.numOfParseBoltExecutors\": \"1\",\n\t\"topology.numOfParserBoltTasks\": \"4\",\n\t\"topology.spoutCrawlInterval\": \"10000\",\n\t\"topology.message.timeout.secs\": \"300\",\n\t\"service.flushLimit\": \"500\",\n\t\"dataSourceConfig.rm.url\": \"http://10.142.78.36:8090/\",\n\t\"dataSourceConfig.hdfs.fs.defaultFS\": \"hdfs://ns\",\n\t\"dataSourceConfig.hdfs.baseDir\": \"/user/op/sparkHistoryServer\",\n\t\"spark.jobConf.additional.info\": \"\",\n\t\"spark.defaultVal.spark.executor.memory\": \"1g\",\n\t\"spark.defaultVal.spark.driver.memory\": \"1g\",\n\t\"spark.defaultVal.spark.driver.cores\": \"1\",\n\t\"spark.defaultVal.spark.executor.cores\": \"1\",\n\t\"spark.defaultVal.spark.yarn.am.memory\": \"512m\",\n\t\"spark.defaultVal.spark.yarn.am.cores\": \"1\",\n\t\"spark.defaultVal.spark.yarn.executor.memoryOverhead.factor\": \"10\",\n\t\"spark.defaultVal.spark.yarn.driver.memoryOverhead.factor\": \"10\",\n\t\"spark.defaultVal.spark.yarn.am.memoryOverhead.factor\": \"10\",\n\t\"spark.defaultVal.spark.yarn.overhead.min\": \"384m\",\n\n\t\"dataSourceConfig.hdfs.dfs.namenode.kerberos.principal\": \"hdfs/_HOST@HADOOP.CHINATELECOM.CN\",\n\t\"dataSourceConfig.hdfs.hadoop.security.authorization\": \"true\",\n\t\"dataSourceConfig.hdfs.dfs.data.transfer.protection\": \"authentication,privacy\",\n\t\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.ns.nn1\": \"NM-ITC-NF8460M3-303-011:54310\",\n\t\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.ns.nn2\": \"NM-ITC-NF8460M3-303-012:54310\",\n\t\"dataSourceConfig.hdfs.dfs.nameservices\": \"ns\",\n\t\"dataSourceConfig.hdfs.dfs.client.failover.proxy.provider.ns\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\n\t\"dataSourceConfig.hdfs.java.security.krb5.kdc\": \"test-bdd-073\",\n\t\"dataSourceConfig.hdfs.dfs.ha.namenodes.ns\": \"nn1,nn2\",\n\t\"dataSourceConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\": \"AES/CTR/NoPadding\",\n\t\"dataSourceConfig.hdfs.hdfs.kerberos.principal\": \"hdfs@HADOOP.CHINATELECOM.CN\",\n\t\"dataSourceConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\": \"org.apache.hadoop.security.WhitelistBasedResolver\",\n\t\"dataSourceConfig.hdfs.hadoop.security.authentication\": \"kerberos\",\n\t\"dataSourceConfig.hdfs.java.security.krb5.conf\": \"/etc/krb5.conf\",\n\t\"dataSourceConfig.hdfs.java.security.krb5.realm\": \"HADOOP.CHINATELECOM.CN\",\n\t\"dataSourceConfig.hdfs.hdfs.keytab.file\": \"/tmp/hdfs.keytab\"\n}\n```\n``` json ,我们的 MR history job\n{\n\t\"workers\": \"2\",\n\t\"stormConfig.mrHistoryJobSpoutTasks\": \"4\",\n\t\"stormConfig.jobKafkaSinkTasks\": \"1\",\n\t\"stormConfig.taskAttemptKafkaSinkTasks\": \"1\",\n\t\"endpointConfig.hdfs.fs.defaultFS\": \"hdfs://ns\",\n\t\"endpointConfig.basePath\": \"/history-yarn/done\",\n\t\"endpointConfig.mrHistoryServerUrl\": \"http://10.142.78.40:19890\",\n\t\"endpointConfig.timeZone\": \"Etc/GMT-8\",\n\t\"dataSinkConfig.MAP_REDUCE_JOB_STREAM.topic\": \"map_reduce_job_testenv\",\n\t\"dataSinkConfig.MAP_REDUCE_TASK_ATTEMPT_STREAM.topic\": \"map_reduce_task_attempt_testenv\",\n\t\"dataSinkConfig.brokerList\": \"10.142.78.100:9092\",\n\t\"dataSourceConfig.zkConnection\": \"10.142.78.98:2181,10.142.78.99:2181,10.142.78.100:2181\",\n\t\"dataSinkConfig.serializerClass\": \"kafka.serializer.StringEncoder\",\n\t\"dataSinkConfig.keySerializerClass\": \"kafka.serializer.StringEncoder\",\n\t\"dataSinkConfig.producerType\": \"async\",\n\t\"dataSinkConfig.numBatchMessages\": \"4096\",\n\t\"dataSinkConfig.maxQueueBufferMs\": \"5000\",\n\t\"dataSinkConfig.requestRequiredAcks\": \"0\",\n\t\n\t\"endpointConfig.hdfs.dfs.namenode.kerberos.principal\": \"hdfs/_HOST@HADOOP.CHINATELECOM.CN\",\n\t\"endpointConfig.hdfs.hadoop.security.authorization\": \"true\",\n\t\"endpointConfig.hdfs.dfs.data.transfer.protection\": \"authentication,privacy\",\n\t\"endpointConfig.hdfs.dfs.namenode.rpc-address.ns.nn1\": \"NM-ITC-NF8460M3-303-011:54310\",\n\t\"endpointConfig.hdfs.dfs.namenode.rpc-address.ns.nn2\": \"NM-ITC-NF8460M3-303-012:54310\",\n\t\"endpointConfig.hdfs.dfs.nameservices\": \"ns\",\n\t\"endpointConfig.hdfs.dfs.client.failover.proxy.provider.ns\": \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\",\n\t\"endpointConfig.hdfs.java.security.krb5.kdc\": \"test-bdd-073\",\n\t\"endpointConfig.hdfs.dfs.ha.namenodes.ns\": \"nn1,nn2\",\n\t\"endpointConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\": \"AES/CTR/NoPadding\",\n\t\"endpointConfig.hdfs.hdfs.kerberos.principal\": \"hdfs@HADOOP.CHINATELECOM.CN\",\n\t\"endpointConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\": \"org.apache.hadoop.security.WhitelistBasedResolver\",\n\t\"endpointConfig.hdfs.hadoop.security.authentication\": \"kerberos\",\n\t\"endpointConfig.hdfs.java.security.krb5.conf\": \"/etc/krb5.conf\",\n\t\"endpointConfig.hdfs.java.security.krb5.realm\": \"HADOOP.CHINATELECOM.CN\",\n\t\"endpointConfig.hdfs.hdfs.keytab.file\": \"/tmp/hdfs.keytab\"\n}\n```\n\n\n#### 引用\nhttps://eagle.apache.org/\nhttp://www.csdn.net/article/2015-10-29/2826076","slug":"Apache-Eagle定义一个Application","published":1,"updated":"2017-03-24T01:54:05.847Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfx3002ypsgu5whkp2or","content":"<p>为了监控大数据平台的大量组件与应用，我们决定引入 Apache Eagle()，Apache Eagle是由Ebay贡献并在2017年初成为了顶级项目。。。它的核心就是用一个实时计算平台（Storm），接收数据（kafka等），然后处理，然后存hbase，然后报警。由于缺乏文档，所以最简单的配置使用也是踩了不少坑：<br>当前版本：Eagle 0.5.0 Spark 1.6.2</p>\n<h2 id=\"配置使用\"><a href=\"#配置使用\" class=\"headerlink\" title=\"配置使用\"></a>配置使用</h2><p><a href=\"http://10.142.78.74:8090/#/integration/applicationList\" target=\"_blank\" rel=\"external\">http://10.142.78.74:8090/#/integration/applicationList</a></p>\n<p><a href=\"http://10.142.78.100:8080/topology.html?id=SPARK_HISTORY_JOB_APP_TESTENV-38-1489136484\" target=\"_blank\" rel=\"external\">http://10.142.78.100:8080/topology.html?id=SPARK_HISTORY_JOB_APP_TESTENV-38-1489136484</a></p>\n<p><a href=\"http://10.142.78.100:60010/master-status\" target=\"_blank\" rel=\"external\">http://10.142.78.100:60010/master-status</a></p>\n<p><a href=\"http://www.yiibai.com/hbase/\" target=\"_blank\" rel=\"external\">http://www.yiibai.com/hbase/</a></p>\n<h2 id=\"设计\"><a href=\"#设计\" class=\"headerlink\" title=\"设计\"></a>设计</h2><h3 id=\"alert-engine\"><a href=\"#alert-engine\" class=\"headerlink\" title=\"alert engine\"></a>alert engine</h3><h4 id=\"高层次设计\"><a href=\"#高层次设计\" class=\"headerlink\" title=\"高层次设计\"></a>高层次设计</h4><p>从高层来看，alert engine 是一个元数据驱动的storm拓扑，它包括了多个模块协同工作：</p>\n<ul>\n<li>Admin Service - 提供了元数据管理和拓扑管理的API。其中：Metadata store 是 admin service API的具体实现.</li>\n<li>Alert Engine Topology on Storm ：通用的Storm 拓扑</li>\n<li>Coordinator 协调器： alert engine拓扑的调度程序。它是一个后端调度程序，用于新策略的加载，资源分配，并且暴露了一些内部的API用于管理；</li>\n<li>Zookeeper：作为通信和警报引擎之间的通信。</li>\n</ul>\n<img src=\"/2017/03/10/Apache-Eagle定义一个Application/eagle_1.png\" alt=\"eagle_1.png\" title=\"\">\n<h2 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h2><p>Spark History Job Monitor主要分为两大步骤，在代码中的表现为一个 SparkHistoryJobSpout 和一个 SparkHistoryJobParseBolt：</p>\n<p>SparkHistoryJobSpout：从 rm 提供的metric中，解析出 已经完成的spark job：COMPLETE_SPARK_JOB，得到 applicationID，发给下一个Bolt<br>SparkHistoryJobParseBolt：接收到上游发过来的appId后，通过spark的规则，将其</p>\n<h2 id=\"配置-Spark-History-Job-Monitor\"><a href=\"#配置-Spark-History-Job-Monitor\" class=\"headerlink\" title=\"配置 Spark History Job Monitor\"></a>配置 Spark History Job Monitor</h2><p>分别点击 <code>Integration</code> -&gt; <code>Sites</code> -&gt; <code>Edit</code> 进入应用配置界面；<br>找到 <code>Spark History Job Monitor</code> 配置，点击右边的 <code>编辑</code>按钮，即可编辑<br>对于 Spark History Job Monitor，由于我们集群是Kerberos的，需要配置以下参数，</p>\n<h4 id=\"Environment\"><a href=\"#Environment\" class=\"headerlink\" title=\"Environment\"></a>Environment</h4><h5 id=\"Execution-Mode-选择-Cluster-Mode\"><a href=\"#Execution-Mode-选择-Cluster-Mode\" class=\"headerlink\" title=\"Execution Mode  选择 Cluster Mode\"></a>Execution Mode  选择 Cluster Mode</h5><h5 id=\"Execution-File\"><a href=\"#Execution-File\" class=\"headerlink\" title=\"Execution File\"></a>Execution File</h5><p>默认为：  /usr/local/eagle-0.5.0-SNAPSHOT/lib/eagle-topology-0.5.0-SNAPSHOT-assembly.jar<br>可以不用改</p>\n<h4 id=\"General\"><a href=\"#General\" class=\"headerlink\" title=\"General\"></a>General</h4><h5 id=\"resource-manager-url-指的是yarn的那个界面\"><a href=\"#resource-manager-url-指的是yarn的那个界面\" class=\"headerlink\" title=\"resource manager url  指的是yarn的那个界面\"></a>resource manager url  指的是yarn的那个界面</h5><pre><code>http://10.142.78.36:8090/\n</code></pre><h5 id=\"hdfs-url-指的是-hdfs的路径\"><a href=\"#hdfs-url-指的是-hdfs的路径\" class=\"headerlink\" title=\"hdfs url 指的是 hdfs的路径\"></a>hdfs url 指的是 hdfs的路径</h5><pre><code>hdfs://10.142.78.98:54310/\n</code></pre><h5 id=\"hdfs-base-path-for-spark-job-data-指的是spark-history-server配置的日志写在hdfs中的路径\"><a href=\"#hdfs-base-path-for-spark-job-data-指的是spark-history-server配置的日志写在hdfs中的路径\" class=\"headerlink\" title=\"hdfs base path for spark job data  指的是spark history server配置的日志写在hdfs中的路径\"></a>hdfs base path for spark job data  指的是spark history server配置的日志写在hdfs中的路径</h5><pre><code>hdfs:///user/op/sparkHistoryServe\n</code></pre><h4 id=\"Advanced\"><a href=\"#Advanced\" class=\"headerlink\" title=\"Advanced\"></a>Advanced</h4><p>这里主要配的是一些storm以及spark的一些相关参数，可以先不用配</p>\n<h4 id=\"Custom-如果hdfs是Kerberos的话，需要配置Kerberos相关参数\"><a href=\"#Custom-如果hdfs是Kerberos的话，需要配置Kerberos相关参数\" class=\"headerlink\" title=\"Custom 如果hdfs是Kerberos的话，需要配置Kerberos相关参数\"></a>Custom 如果hdfs是Kerberos的话，需要配置Kerberos相关参数</h4><p>需要增加如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">dataSourceConfig.hdfs.keytab.file hdfs@HADOOP.CHINATELECOM.CN</div><div class=\"line\">dataSourceConfig.hdfs.kerberos.principal /etc/hadoop/conf/hdfs.keytab</div></pre></td></tr></table></figure>\n<h4 id=\"custom\"><a href=\"#custom\" class=\"headerlink\" title=\"custom\"></a>custom</h4><figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"attr\">\"workers\"</span>: <span class=\"string\">\"3\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.numOfSpoutExecutors\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.numOfSpoutTasks\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.numOfParseBoltExecutors\"</span>: <span class=\"string\">\"6\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.numOfParserBoltTasks\"</span>: <span class=\"string\">\"6\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.spoutCrawlInterval\"</span>: <span class=\"string\">\"60000\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.requestLimit\"</span>: <span class=\"string\">\"100\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.message.timeout.secs\"</span>: <span class=\"string\">\"600\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"service.flushLimit\"</span>: <span class=\"string\">\"500\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.rm.url\"</span>: <span class=\"string\">\"\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.fs.defaultFS\"</span>: <span class=\"string\">\"hdfs://xxxxx\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.baseDir\"</span>: <span class=\"string\">\"/logs/spark-events\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.jobConf.additional.info\"</span>: <span class=\"string\">\"\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.executor.memory\"</span>: <span class=\"string\">\"1g\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.driver.memory\"</span>: <span class=\"string\">\"1g\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.driver.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.executor.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.am.memory\"</span>: <span class=\"string\">\"512m\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.am.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.executor.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.driver.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.am.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.overhead.min\"</span>: <span class=\"string\">\"384m\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.xxxxx.nn1\"</span>: <span class=\"string\">\"yyyyy\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.xxxxx.nn2\"</span>: <span class=\"string\">\"zzzzz\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.client.failover.proxy.provider.xxxxx\"</span>: <span class=\"string\">\"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.nameservices\"</span>: <span class=\"string\">\"xxxxx\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.ha.namenodes.xxxxx\"</span>: <span class=\"string\">\"nn1,nn2\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.hdfs.kerberos.principal\"</span>: <span class=\"string\">\"aaaaaaa\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\"</span>: <span class=\"string\">\"org.apache.hadoop.security.WhitelistBasedResolver\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.hdfs.keytab.file\"</span>: <span class=\"string\">\"/home/storm/.keytab/b_eagle.keytab\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.data.transfer.protection\"</span>: <span class=\"string\">\"authentication,privacy\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\"</span>: <span class=\"string\">\"AES/CTR/NoPadding\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<figure class=\"highlight json\"><figcaption><span>我们的</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">\t<span class=\"attr\">\"workers\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.numOfSpoutExecutors\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.numOfSpoutTasks\"</span>: <span class=\"string\">\"4\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.numOfParseBoltExecutors\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.numOfParserBoltTasks\"</span>: <span class=\"string\">\"4\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.spoutCrawlInterval\"</span>: <span class=\"string\">\"10000\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.message.timeout.secs\"</span>: <span class=\"string\">\"300\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"service.flushLimit\"</span>: <span class=\"string\">\"500\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.rm.url\"</span>: <span class=\"string\">\"http://10.142.78.36:8090/\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.fs.defaultFS\"</span>: <span class=\"string\">\"hdfs://ns\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.baseDir\"</span>: <span class=\"string\">\"/user/op/sparkHistoryServer\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.jobConf.additional.info\"</span>: <span class=\"string\">\"\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.executor.memory\"</span>: <span class=\"string\">\"1g\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.driver.memory\"</span>: <span class=\"string\">\"1g\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.driver.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.executor.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.am.memory\"</span>: <span class=\"string\">\"512m\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.am.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.executor.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.driver.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.am.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.overhead.min\"</span>: <span class=\"string\">\"384m\"</span>,</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.namenode.kerberos.principal\"</span>: <span class=\"string\">\"hdfs/_HOST@HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.hadoop.security.authorization\"</span>: <span class=\"string\">\"true\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.data.transfer.protection\"</span>: <span class=\"string\">\"authentication,privacy\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.ns.nn1\"</span>: <span class=\"string\">\"NM-ITC-NF8460M3-303-011:54310\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.ns.nn2\"</span>: <span class=\"string\">\"NM-ITC-NF8460M3-303-012:54310\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.nameservices\"</span>: <span class=\"string\">\"ns\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.client.failover.proxy.provider.ns\"</span>: <span class=\"string\">\"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.java.security.krb5.kdc\"</span>: <span class=\"string\">\"test-bdd-073\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.ha.namenodes.ns\"</span>: <span class=\"string\">\"nn1,nn2\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\"</span>: <span class=\"string\">\"AES/CTR/NoPadding\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.hdfs.kerberos.principal\"</span>: <span class=\"string\">\"hdfs@HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\"</span>: <span class=\"string\">\"org.apache.hadoop.security.WhitelistBasedResolver\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.hadoop.security.authentication\"</span>: <span class=\"string\">\"kerberos\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.java.security.krb5.conf\"</span>: <span class=\"string\">\"/etc/krb5.conf\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.java.security.krb5.realm\"</span>: <span class=\"string\">\"HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.hdfs.keytab.file\"</span>: <span class=\"string\">\"/tmp/hdfs.keytab\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<figure class=\"highlight json\"><figcaption><span>,我们的 MR history job</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">\t<span class=\"attr\">\"workers\"</span>: <span class=\"string\">\"2\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"stormConfig.mrHistoryJobSpoutTasks\"</span>: <span class=\"string\">\"4\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"stormConfig.jobKafkaSinkTasks\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"stormConfig.taskAttemptKafkaSinkTasks\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.fs.defaultFS\"</span>: <span class=\"string\">\"hdfs://ns\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.basePath\"</span>: <span class=\"string\">\"/history-yarn/done\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.mrHistoryServerUrl\"</span>: <span class=\"string\">\"http://10.142.78.40:19890\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.timeZone\"</span>: <span class=\"string\">\"Etc/GMT-8\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.MAP_REDUCE_JOB_STREAM.topic\"</span>: <span class=\"string\">\"map_reduce_job_testenv\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.MAP_REDUCE_TASK_ATTEMPT_STREAM.topic\"</span>: <span class=\"string\">\"map_reduce_task_attempt_testenv\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.brokerList\"</span>: <span class=\"string\">\"10.142.78.100:9092\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.zkConnection\"</span>: <span class=\"string\">\"10.142.78.98:2181,10.142.78.99:2181,10.142.78.100:2181\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.serializerClass\"</span>: <span class=\"string\">\"kafka.serializer.StringEncoder\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.keySerializerClass\"</span>: <span class=\"string\">\"kafka.serializer.StringEncoder\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.producerType\"</span>: <span class=\"string\">\"async\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.numBatchMessages\"</span>: <span class=\"string\">\"4096\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.maxQueueBufferMs\"</span>: <span class=\"string\">\"5000\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.requestRequiredAcks\"</span>: <span class=\"string\">\"0\"</span>,</div><div class=\"line\">\t</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.namenode.kerberos.principal\"</span>: <span class=\"string\">\"hdfs/_HOST@HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.hadoop.security.authorization\"</span>: <span class=\"string\">\"true\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.data.transfer.protection\"</span>: <span class=\"string\">\"authentication,privacy\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.namenode.rpc-address.ns.nn1\"</span>: <span class=\"string\">\"NM-ITC-NF8460M3-303-011:54310\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.namenode.rpc-address.ns.nn2\"</span>: <span class=\"string\">\"NM-ITC-NF8460M3-303-012:54310\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.nameservices\"</span>: <span class=\"string\">\"ns\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.client.failover.proxy.provider.ns\"</span>: <span class=\"string\">\"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.java.security.krb5.kdc\"</span>: <span class=\"string\">\"test-bdd-073\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.ha.namenodes.ns\"</span>: <span class=\"string\">\"nn1,nn2\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\"</span>: <span class=\"string\">\"AES/CTR/NoPadding\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.hdfs.kerberos.principal\"</span>: <span class=\"string\">\"hdfs@HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\"</span>: <span class=\"string\">\"org.apache.hadoop.security.WhitelistBasedResolver\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.hadoop.security.authentication\"</span>: <span class=\"string\">\"kerberos\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.java.security.krb5.conf\"</span>: <span class=\"string\">\"/etc/krb5.conf\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.java.security.krb5.realm\"</span>: <span class=\"string\">\"HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.hdfs.keytab.file\"</span>: <span class=\"string\">\"/tmp/hdfs.keytab\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h4><p><a href=\"https://eagle.apache.org/\" target=\"_blank\" rel=\"external\">https://eagle.apache.org/</a><br><a href=\"http://www.csdn.net/article/2015-10-29/2826076\" target=\"_blank\" rel=\"external\">http://www.csdn.net/article/2015-10-29/2826076</a></p>\n","excerpt":"","more":"<p>为了监控大数据平台的大量组件与应用，我们决定引入 Apache Eagle()，Apache Eagle是由Ebay贡献并在2017年初成为了顶级项目。。。它的核心就是用一个实时计算平台（Storm），接收数据（kafka等），然后处理，然后存hbase，然后报警。由于缺乏文档，所以最简单的配置使用也是踩了不少坑：<br>当前版本：Eagle 0.5.0 Spark 1.6.2</p>\n<h2 id=\"配置使用\"><a href=\"#配置使用\" class=\"headerlink\" title=\"配置使用\"></a>配置使用</h2><p><a href=\"http://10.142.78.74:8090/#/integration/applicationList\">http://10.142.78.74:8090/#/integration/applicationList</a></p>\n<p><a href=\"http://10.142.78.100:8080/topology.html?id=SPARK_HISTORY_JOB_APP_TESTENV-38-1489136484\">http://10.142.78.100:8080/topology.html?id=SPARK_HISTORY_JOB_APP_TESTENV-38-1489136484</a></p>\n<p><a href=\"http://10.142.78.100:60010/master-status\">http://10.142.78.100:60010/master-status</a></p>\n<p><a href=\"http://www.yiibai.com/hbase/\">http://www.yiibai.com/hbase/</a></p>\n<h2 id=\"设计\"><a href=\"#设计\" class=\"headerlink\" title=\"设计\"></a>设计</h2><h3 id=\"alert-engine\"><a href=\"#alert-engine\" class=\"headerlink\" title=\"alert engine\"></a>alert engine</h3><h4 id=\"高层次设计\"><a href=\"#高层次设计\" class=\"headerlink\" title=\"高层次设计\"></a>高层次设计</h4><p>从高层来看，alert engine 是一个元数据驱动的storm拓扑，它包括了多个模块协同工作：</p>\n<ul>\n<li>Admin Service - 提供了元数据管理和拓扑管理的API。其中：Metadata store 是 admin service API的具体实现.</li>\n<li>Alert Engine Topology on Storm ：通用的Storm 拓扑</li>\n<li>Coordinator 协调器： alert engine拓扑的调度程序。它是一个后端调度程序，用于新策略的加载，资源分配，并且暴露了一些内部的API用于管理；</li>\n<li>Zookeeper：作为通信和警报引擎之间的通信。</li>\n</ul>\n<img src=\"/2017/03/10/Apache-Eagle定义一个Application/eagle_1.png\" alt=\"eagle_1.png\" title=\"\">\n<h2 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h2><p>Spark History Job Monitor主要分为两大步骤，在代码中的表现为一个 SparkHistoryJobSpout 和一个 SparkHistoryJobParseBolt：</p>\n<p>SparkHistoryJobSpout：从 rm 提供的metric中，解析出 已经完成的spark job：COMPLETE_SPARK_JOB，得到 applicationID，发给下一个Bolt<br>SparkHistoryJobParseBolt：接收到上游发过来的appId后，通过spark的规则，将其</p>\n<h2 id=\"配置-Spark-History-Job-Monitor\"><a href=\"#配置-Spark-History-Job-Monitor\" class=\"headerlink\" title=\"配置 Spark History Job Monitor\"></a>配置 Spark History Job Monitor</h2><p>分别点击 <code>Integration</code> -&gt; <code>Sites</code> -&gt; <code>Edit</code> 进入应用配置界面；<br>找到 <code>Spark History Job Monitor</code> 配置，点击右边的 <code>编辑</code>按钮，即可编辑<br>对于 Spark History Job Monitor，由于我们集群是Kerberos的，需要配置以下参数，</p>\n<h4 id=\"Environment\"><a href=\"#Environment\" class=\"headerlink\" title=\"Environment\"></a>Environment</h4><h5 id=\"Execution-Mode-选择-Cluster-Mode\"><a href=\"#Execution-Mode-选择-Cluster-Mode\" class=\"headerlink\" title=\"Execution Mode  选择 Cluster Mode\"></a>Execution Mode  选择 Cluster Mode</h5><h5 id=\"Execution-File\"><a href=\"#Execution-File\" class=\"headerlink\" title=\"Execution File\"></a>Execution File</h5><p>默认为：  /usr/local/eagle-0.5.0-SNAPSHOT/lib/eagle-topology-0.5.0-SNAPSHOT-assembly.jar<br>可以不用改</p>\n<h4 id=\"General\"><a href=\"#General\" class=\"headerlink\" title=\"General\"></a>General</h4><h5 id=\"resource-manager-url-指的是yarn的那个界面\"><a href=\"#resource-manager-url-指的是yarn的那个界面\" class=\"headerlink\" title=\"resource manager url  指的是yarn的那个界面\"></a>resource manager url  指的是yarn的那个界面</h5><pre><code>http://10.142.78.36:8090/\n</code></pre><h5 id=\"hdfs-url-指的是-hdfs的路径\"><a href=\"#hdfs-url-指的是-hdfs的路径\" class=\"headerlink\" title=\"hdfs url 指的是 hdfs的路径\"></a>hdfs url 指的是 hdfs的路径</h5><pre><code>hdfs://10.142.78.98:54310/\n</code></pre><h5 id=\"hdfs-base-path-for-spark-job-data-指的是spark-history-server配置的日志写在hdfs中的路径\"><a href=\"#hdfs-base-path-for-spark-job-data-指的是spark-history-server配置的日志写在hdfs中的路径\" class=\"headerlink\" title=\"hdfs base path for spark job data  指的是spark history server配置的日志写在hdfs中的路径\"></a>hdfs base path for spark job data  指的是spark history server配置的日志写在hdfs中的路径</h5><pre><code>hdfs:///user/op/sparkHistoryServe\n</code></pre><h4 id=\"Advanced\"><a href=\"#Advanced\" class=\"headerlink\" title=\"Advanced\"></a>Advanced</h4><p>这里主要配的是一些storm以及spark的一些相关参数，可以先不用配</p>\n<h4 id=\"Custom-如果hdfs是Kerberos的话，需要配置Kerberos相关参数\"><a href=\"#Custom-如果hdfs是Kerberos的话，需要配置Kerberos相关参数\" class=\"headerlink\" title=\"Custom 如果hdfs是Kerberos的话，需要配置Kerberos相关参数\"></a>Custom 如果hdfs是Kerberos的话，需要配置Kerberos相关参数</h4><p>需要增加如下：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">dataSourceConfig.hdfs.keytab.file hdfs@HADOOP.CHINATELECOM.CN</div><div class=\"line\">dataSourceConfig.hdfs.kerberos.principal /etc/hadoop/conf/hdfs.keytab</div></pre></td></tr></table></figure>\n<h4 id=\"custom\"><a href=\"#custom\" class=\"headerlink\" title=\"custom\"></a>custom</h4><figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"attr\">\"workers\"</span>: <span class=\"string\">\"3\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.numOfSpoutExecutors\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.numOfSpoutTasks\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.numOfParseBoltExecutors\"</span>: <span class=\"string\">\"6\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.numOfParserBoltTasks\"</span>: <span class=\"string\">\"6\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.spoutCrawlInterval\"</span>: <span class=\"string\">\"60000\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.requestLimit\"</span>: <span class=\"string\">\"100\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"topology.message.timeout.secs\"</span>: <span class=\"string\">\"600\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"service.flushLimit\"</span>: <span class=\"string\">\"500\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.rm.url\"</span>: <span class=\"string\">\"\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.fs.defaultFS\"</span>: <span class=\"string\">\"hdfs://xxxxx\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.baseDir\"</span>: <span class=\"string\">\"/logs/spark-events\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.jobConf.additional.info\"</span>: <span class=\"string\">\"\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.executor.memory\"</span>: <span class=\"string\">\"1g\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.driver.memory\"</span>: <span class=\"string\">\"1g\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.driver.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.executor.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.am.memory\"</span>: <span class=\"string\">\"512m\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.am.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.executor.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.driver.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.am.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"spark.defaultVal.spark.yarn.overhead.min\"</span>: <span class=\"string\">\"384m\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.xxxxx.nn1\"</span>: <span class=\"string\">\"yyyyy\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.xxxxx.nn2\"</span>: <span class=\"string\">\"zzzzz\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.client.failover.proxy.provider.xxxxx\"</span>: <span class=\"string\">\"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.nameservices\"</span>: <span class=\"string\">\"xxxxx\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.ha.namenodes.xxxxx\"</span>: <span class=\"string\">\"nn1,nn2\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.hdfs.kerberos.principal\"</span>: <span class=\"string\">\"aaaaaaa\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\"</span>: <span class=\"string\">\"org.apache.hadoop.security.WhitelistBasedResolver\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.hdfs.keytab.file\"</span>: <span class=\"string\">\"/home/storm/.keytab/b_eagle.keytab\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.data.transfer.protection\"</span>: <span class=\"string\">\"authentication,privacy\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"dataSourceConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\"</span>: <span class=\"string\">\"AES/CTR/NoPadding\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<figure class=\"highlight json\"><figcaption><span>我们的</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">\t<span class=\"attr\">\"workers\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.numOfSpoutExecutors\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.numOfSpoutTasks\"</span>: <span class=\"string\">\"4\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.numOfParseBoltExecutors\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.numOfParserBoltTasks\"</span>: <span class=\"string\">\"4\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.spoutCrawlInterval\"</span>: <span class=\"string\">\"10000\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"topology.message.timeout.secs\"</span>: <span class=\"string\">\"300\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"service.flushLimit\"</span>: <span class=\"string\">\"500\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.rm.url\"</span>: <span class=\"string\">\"http://10.142.78.36:8090/\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.fs.defaultFS\"</span>: <span class=\"string\">\"hdfs://ns\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.baseDir\"</span>: <span class=\"string\">\"/user/op/sparkHistoryServer\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.jobConf.additional.info\"</span>: <span class=\"string\">\"\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.executor.memory\"</span>: <span class=\"string\">\"1g\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.driver.memory\"</span>: <span class=\"string\">\"1g\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.driver.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.executor.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.am.memory\"</span>: <span class=\"string\">\"512m\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.am.cores\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.executor.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.driver.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.am.memoryOverhead.factor\"</span>: <span class=\"string\">\"10\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"spark.defaultVal.spark.yarn.overhead.min\"</span>: <span class=\"string\">\"384m\"</span>,</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.namenode.kerberos.principal\"</span>: <span class=\"string\">\"hdfs/_HOST@HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.hadoop.security.authorization\"</span>: <span class=\"string\">\"true\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.data.transfer.protection\"</span>: <span class=\"string\">\"authentication,privacy\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.ns.nn1\"</span>: <span class=\"string\">\"NM-ITC-NF8460M3-303-011:54310\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.namenode.rpc-address.ns.nn2\"</span>: <span class=\"string\">\"NM-ITC-NF8460M3-303-012:54310\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.nameservices\"</span>: <span class=\"string\">\"ns\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.client.failover.proxy.provider.ns\"</span>: <span class=\"string\">\"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.java.security.krb5.kdc\"</span>: <span class=\"string\">\"test-bdd-073\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.ha.namenodes.ns\"</span>: <span class=\"string\">\"nn1,nn2\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\"</span>: <span class=\"string\">\"AES/CTR/NoPadding\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.hdfs.kerberos.principal\"</span>: <span class=\"string\">\"hdfs@HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\"</span>: <span class=\"string\">\"org.apache.hadoop.security.WhitelistBasedResolver\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.hadoop.security.authentication\"</span>: <span class=\"string\">\"kerberos\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.java.security.krb5.conf\"</span>: <span class=\"string\">\"/etc/krb5.conf\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.java.security.krb5.realm\"</span>: <span class=\"string\">\"HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.hdfs.hdfs.keytab.file\"</span>: <span class=\"string\">\"/tmp/hdfs.keytab\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<figure class=\"highlight json\"><figcaption><span>,我们的 MR history job</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">\t<span class=\"attr\">\"workers\"</span>: <span class=\"string\">\"2\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"stormConfig.mrHistoryJobSpoutTasks\"</span>: <span class=\"string\">\"4\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"stormConfig.jobKafkaSinkTasks\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"stormConfig.taskAttemptKafkaSinkTasks\"</span>: <span class=\"string\">\"1\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.fs.defaultFS\"</span>: <span class=\"string\">\"hdfs://ns\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.basePath\"</span>: <span class=\"string\">\"/history-yarn/done\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.mrHistoryServerUrl\"</span>: <span class=\"string\">\"http://10.142.78.40:19890\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.timeZone\"</span>: <span class=\"string\">\"Etc/GMT-8\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.MAP_REDUCE_JOB_STREAM.topic\"</span>: <span class=\"string\">\"map_reduce_job_testenv\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.MAP_REDUCE_TASK_ATTEMPT_STREAM.topic\"</span>: <span class=\"string\">\"map_reduce_task_attempt_testenv\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.brokerList\"</span>: <span class=\"string\">\"10.142.78.100:9092\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSourceConfig.zkConnection\"</span>: <span class=\"string\">\"10.142.78.98:2181,10.142.78.99:2181,10.142.78.100:2181\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.serializerClass\"</span>: <span class=\"string\">\"kafka.serializer.StringEncoder\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.keySerializerClass\"</span>: <span class=\"string\">\"kafka.serializer.StringEncoder\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.producerType\"</span>: <span class=\"string\">\"async\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.numBatchMessages\"</span>: <span class=\"string\">\"4096\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.maxQueueBufferMs\"</span>: <span class=\"string\">\"5000\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"dataSinkConfig.requestRequiredAcks\"</span>: <span class=\"string\">\"0\"</span>,</div><div class=\"line\">\t</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.namenode.kerberos.principal\"</span>: <span class=\"string\">\"hdfs/_HOST@HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.hadoop.security.authorization\"</span>: <span class=\"string\">\"true\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.data.transfer.protection\"</span>: <span class=\"string\">\"authentication,privacy\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.namenode.rpc-address.ns.nn1\"</span>: <span class=\"string\">\"NM-ITC-NF8460M3-303-011:54310\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.namenode.rpc-address.ns.nn2\"</span>: <span class=\"string\">\"NM-ITC-NF8460M3-303-012:54310\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.nameservices\"</span>: <span class=\"string\">\"ns\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.client.failover.proxy.provider.ns\"</span>: <span class=\"string\">\"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.java.security.krb5.kdc\"</span>: <span class=\"string\">\"test-bdd-073\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.ha.namenodes.ns\"</span>: <span class=\"string\">\"nn1,nn2\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.encrypt.data.transfer.cipher.suites\"</span>: <span class=\"string\">\"AES/CTR/NoPadding\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.hdfs.kerberos.principal\"</span>: <span class=\"string\">\"hdfs@HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.dfs.data.transfer.saslproperties.resolver.class\"</span>: <span class=\"string\">\"org.apache.hadoop.security.WhitelistBasedResolver\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.hadoop.security.authentication\"</span>: <span class=\"string\">\"kerberos\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.java.security.krb5.conf\"</span>: <span class=\"string\">\"/etc/krb5.conf\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.java.security.krb5.realm\"</span>: <span class=\"string\">\"HADOOP.CHINATELECOM.CN\"</span>,</div><div class=\"line\">\t<span class=\"attr\">\"endpointConfig.hdfs.hdfs.keytab.file\"</span>: <span class=\"string\">\"/tmp/hdfs.keytab\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h4><p><a href=\"https://eagle.apache.org/\">https://eagle.apache.org/</a><br><a href=\"http://www.csdn.net/article/2015-10-29/2826076\">http://www.csdn.net/article/2015-10-29/2826076</a></p>\n"},{"title":"flume1.7使用KafkaSource采集大量数据","toc":true,"date":"2017-03-23T03:01:43.000Z","_content":"\n我们需要把spark Streaming的大量数据写入HDFS或者ES，为了增加稳定性，先将数据写入 kafka集群，然后使用 flume导入HDFS，数据量平均为 20W/s;\n\n由于kafka版本为0.10，为了提高效率，我们使用了1.7版本的flume，它是直接消费kafka集群的，有以下坑：\n\n#### 配置方法\n先说说高效的配置方法吧：\n同时参照[flume官网文档](https://flume.apache.org/FlumeUserGuide.html#kafka-source)和[kafka官网文档](http://kafka.apache.org/documentation/#consumerapi)。由于新出来，所以官网是最好的，并且因为kafka的consumer配置非常多，在flume官网中默认的配置很少，所以还是需要根据具体情况配置一些好用的参数。\n\n#### Commit cannot be completed due to group rebalance\n\n```\n2017-03-23 10:32:32,509 (PollableSourceRunner-KafkaSource-r1) [ERROR - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:550)] Error ILLEGAL_GENERATION occurred while committing offsets for group flume_test\n2017-03-23 10:32:32,509 (PollableSourceRunner-KafkaSource-r1) [ERROR - org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:314)] KafkaSource EXCEPTION, {}\norg.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed due to group rebalance\n        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:552)\n        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:493)\n        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:665)\n        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:644)\n        at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167)\n        at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133)\n        at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107)\n        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:380)\n        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:274)\n        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:320)\n        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:213)\n        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:193)\n        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163)\n        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:358)\n        at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:968)\n        at org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:304)\n        at org.apache.flume.source.AbstractPollableSource.process(AbstractPollableSource.java:60)\n        at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:133)\n        at java.lang.Thread.run(Thread.java:745)\n```\n\n原因是我的数据量特别大，导致每次消费consumer进行poll的时候耗时太久，导致发送hearbeat间隔太长，coordinator认为consumer死了，就发生了rebalance；\n\n解决方法：\n增大参数：heartbeat.interval.ms - This tells Kafka wait the specified amount of milliseconds before it consider the consumer will be considered \"dead\"\n\n缩小参数：max.partition.fetch.bytes - This will limit the amount of messages (up to) the consumer will receive when polling. \n\n实际解决的时候，我并没有缩小 max.partition.fetch.bytes 参数，因为我觉得一次多拉点也好\n\n\n## 引用\nhttp://blog.csdn.net/xianzhen376/article/details/51802736\nhttps://github.com/ajkret/kafka-sample\nhttp://stackoverflow.com/questions/35658171/kafka-commitfailedexception-consumer-exception\nhttp://kaimingwan.com/post/kafka/kafkawen-ti-shou-ji","source":"_posts/2017-03-23-flume1-7使用KafkaSource采集大量数据.md","raw":"---\ntitle: flume1.7使用KafkaSource采集大量数据\ntoc: true\ndate: 2017-03-23 11:01:43\ntags: flume\ncategories: flume\n---\n\n我们需要把spark Streaming的大量数据写入HDFS或者ES，为了增加稳定性，先将数据写入 kafka集群，然后使用 flume导入HDFS，数据量平均为 20W/s;\n\n由于kafka版本为0.10，为了提高效率，我们使用了1.7版本的flume，它是直接消费kafka集群的，有以下坑：\n\n#### 配置方法\n先说说高效的配置方法吧：\n同时参照[flume官网文档](https://flume.apache.org/FlumeUserGuide.html#kafka-source)和[kafka官网文档](http://kafka.apache.org/documentation/#consumerapi)。由于新出来，所以官网是最好的，并且因为kafka的consumer配置非常多，在flume官网中默认的配置很少，所以还是需要根据具体情况配置一些好用的参数。\n\n#### Commit cannot be completed due to group rebalance\n\n```\n2017-03-23 10:32:32,509 (PollableSourceRunner-KafkaSource-r1) [ERROR - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:550)] Error ILLEGAL_GENERATION occurred while committing offsets for group flume_test\n2017-03-23 10:32:32,509 (PollableSourceRunner-KafkaSource-r1) [ERROR - org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:314)] KafkaSource EXCEPTION, {}\norg.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed due to group rebalance\n        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:552)\n        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:493)\n        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:665)\n        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:644)\n        at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167)\n        at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133)\n        at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107)\n        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:380)\n        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:274)\n        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:320)\n        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:213)\n        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:193)\n        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163)\n        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:358)\n        at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:968)\n        at org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:304)\n        at org.apache.flume.source.AbstractPollableSource.process(AbstractPollableSource.java:60)\n        at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:133)\n        at java.lang.Thread.run(Thread.java:745)\n```\n\n原因是我的数据量特别大，导致每次消费consumer进行poll的时候耗时太久，导致发送hearbeat间隔太长，coordinator认为consumer死了，就发生了rebalance；\n\n解决方法：\n增大参数：heartbeat.interval.ms - This tells Kafka wait the specified amount of milliseconds before it consider the consumer will be considered \"dead\"\n\n缩小参数：max.partition.fetch.bytes - This will limit the amount of messages (up to) the consumer will receive when polling. \n\n实际解决的时候，我并没有缩小 max.partition.fetch.bytes 参数，因为我觉得一次多拉点也好\n\n\n## 引用\nhttp://blog.csdn.net/xianzhen376/article/details/51802736\nhttps://github.com/ajkret/kafka-sample\nhttp://stackoverflow.com/questions/35658171/kafka-commitfailedexception-consumer-exception\nhttp://kaimingwan.com/post/kafka/kafkawen-ti-shou-ji","slug":"flume1-7使用KafkaSource采集大量数据","published":1,"updated":"2017-03-23T03:18:13.222Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfx50032psgu9jskzjbj","content":"<p>我们需要把spark Streaming的大量数据写入HDFS或者ES，为了增加稳定性，先将数据写入 kafka集群，然后使用 flume导入HDFS，数据量平均为 20W/s;</p>\n<p>由于kafka版本为0.10，为了提高效率，我们使用了1.7版本的flume，它是直接消费kafka集群的，有以下坑：</p>\n<h4 id=\"配置方法\"><a href=\"#配置方法\" class=\"headerlink\" title=\"配置方法\"></a>配置方法</h4><p>先说说高效的配置方法吧：<br>同时参照<a href=\"https://flume.apache.org/FlumeUserGuide.html#kafka-source\" target=\"_blank\" rel=\"external\">flume官网文档</a>和<a href=\"http://kafka.apache.org/documentation/#consumerapi\" target=\"_blank\" rel=\"external\">kafka官网文档</a>。由于新出来，所以官网是最好的，并且因为kafka的consumer配置非常多，在flume官网中默认的配置很少，所以还是需要根据具体情况配置一些好用的参数。</p>\n<h4 id=\"Commit-cannot-be-completed-due-to-group-rebalance\"><a href=\"#Commit-cannot-be-completed-due-to-group-rebalance\" class=\"headerlink\" title=\"Commit cannot be completed due to group rebalance\"></a>Commit cannot be completed due to group rebalance</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">2017-03-23 10:32:32,509 (PollableSourceRunner-KafkaSource-r1) [ERROR - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:550)] Error ILLEGAL_GENERATION occurred while committing offsets for group flume_test</div><div class=\"line\">2017-03-23 10:32:32,509 (PollableSourceRunner-KafkaSource-r1) [ERROR - org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:314)] KafkaSource EXCEPTION, &#123;&#125;</div><div class=\"line\">org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed due to group rebalance</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:552)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:493)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:665)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:644)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:380)</div><div class=\"line\">        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:274)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:320)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:213)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:193)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:358)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:968)</div><div class=\"line\">        at org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:304)</div><div class=\"line\">        at org.apache.flume.source.AbstractPollableSource.process(AbstractPollableSource.java:60)</div><div class=\"line\">        at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:133)</div><div class=\"line\">        at java.lang.Thread.run(Thread.java:745)</div></pre></td></tr></table></figure>\n<p>原因是我的数据量特别大，导致每次消费consumer进行poll的时候耗时太久，导致发送hearbeat间隔太长，coordinator认为consumer死了，就发生了rebalance；</p>\n<p>解决方法：<br>增大参数：heartbeat.interval.ms - This tells Kafka wait the specified amount of milliseconds before it consider the consumer will be considered “dead”</p>\n<p>缩小参数：max.partition.fetch.bytes - This will limit the amount of messages (up to) the consumer will receive when polling. </p>\n<p>实际解决的时候，我并没有缩小 max.partition.fetch.bytes 参数，因为我觉得一次多拉点也好</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"http://blog.csdn.net/xianzhen376/article/details/51802736\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/xianzhen376/article/details/51802736</a><br><a href=\"https://github.com/ajkret/kafka-sample\" target=\"_blank\" rel=\"external\">https://github.com/ajkret/kafka-sample</a><br><a href=\"http://stackoverflow.com/questions/35658171/kafka-commitfailedexception-consumer-exception\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/35658171/kafka-commitfailedexception-consumer-exception</a><br><a href=\"http://kaimingwan.com/post/kafka/kafkawen-ti-shou-ji\" target=\"_blank\" rel=\"external\">http://kaimingwan.com/post/kafka/kafkawen-ti-shou-ji</a></p>\n","excerpt":"","more":"<p>我们需要把spark Streaming的大量数据写入HDFS或者ES，为了增加稳定性，先将数据写入 kafka集群，然后使用 flume导入HDFS，数据量平均为 20W/s;</p>\n<p>由于kafka版本为0.10，为了提高效率，我们使用了1.7版本的flume，它是直接消费kafka集群的，有以下坑：</p>\n<h4 id=\"配置方法\"><a href=\"#配置方法\" class=\"headerlink\" title=\"配置方法\"></a>配置方法</h4><p>先说说高效的配置方法吧：<br>同时参照<a href=\"https://flume.apache.org/FlumeUserGuide.html#kafka-source\">flume官网文档</a>和<a href=\"http://kafka.apache.org/documentation/#consumerapi\">kafka官网文档</a>。由于新出来，所以官网是最好的，并且因为kafka的consumer配置非常多，在flume官网中默认的配置很少，所以还是需要根据具体情况配置一些好用的参数。</p>\n<h4 id=\"Commit-cannot-be-completed-due-to-group-rebalance\"><a href=\"#Commit-cannot-be-completed-due-to-group-rebalance\" class=\"headerlink\" title=\"Commit cannot be completed due to group rebalance\"></a>Commit cannot be completed due to group rebalance</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">2017-03-23 10:32:32,509 (PollableSourceRunner-KafkaSource-r1) [ERROR - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:550)] Error ILLEGAL_GENERATION occurred while committing offsets for group flume_test</div><div class=\"line\">2017-03-23 10:32:32,509 (PollableSourceRunner-KafkaSource-r1) [ERROR - org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:314)] KafkaSource EXCEPTION, &#123;&#125;</div><div class=\"line\">org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed due to group rebalance</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:552)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:493)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:665)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:644)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:380)</div><div class=\"line\">        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:274)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:320)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:213)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:193)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:358)</div><div class=\"line\">        at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:968)</div><div class=\"line\">        at org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:304)</div><div class=\"line\">        at org.apache.flume.source.AbstractPollableSource.process(AbstractPollableSource.java:60)</div><div class=\"line\">        at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:133)</div><div class=\"line\">        at java.lang.Thread.run(Thread.java:745)</div></pre></td></tr></table></figure>\n<p>原因是我的数据量特别大，导致每次消费consumer进行poll的时候耗时太久，导致发送hearbeat间隔太长，coordinator认为consumer死了，就发生了rebalance；</p>\n<p>解决方法：<br>增大参数：heartbeat.interval.ms - This tells Kafka wait the specified amount of milliseconds before it consider the consumer will be considered “dead”</p>\n<p>缩小参数：max.partition.fetch.bytes - This will limit the amount of messages (up to) the consumer will receive when polling. </p>\n<p>实际解决的时候，我并没有缩小 max.partition.fetch.bytes 参数，因为我觉得一次多拉点也好</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><p><a href=\"http://blog.csdn.net/xianzhen376/article/details/51802736\">http://blog.csdn.net/xianzhen376/article/details/51802736</a><br><a href=\"https://github.com/ajkret/kafka-sample\">https://github.com/ajkret/kafka-sample</a><br><a href=\"http://stackoverflow.com/questions/35658171/kafka-commitfailedexception-consumer-exception\">http://stackoverflow.com/questions/35658171/kafka-commitfailedexception-consumer-exception</a><br><a href=\"http://kaimingwan.com/post/kafka/kafkawen-ti-shou-ji\">http://kaimingwan.com/post/kafka/kafkawen-ti-shou-ji</a></p>\n"},{"title":"阿里巴巴Java开发手册学习笔记（上）","toc":false,"date":"2017-03-13T11:47:30.000Z","_content":"\n本人自认为代码还算写得比较符合规范，但是相信阿里会有总结更深的见解，因此对自己之前没有注意到的，自己没有遵循的，做一个笔记与整理，以加深理解。\n# 编程规约\n\n## 命名规约\n#### 【强制】常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。\n重点在，不要嫌名字长，我以前写得比较短 \n正例： `MAX _ STOCK _ COUNT`\n\n#### 【强制】抽象类命名使用 Abstract 或 Base 开头 ； 异常类命名使用 Exception 结尾 ； 测试类命名以它要测试的类的名称开始，以 Test 结尾。\n如果是接口的话，如何？\n\n#### 接口和实现类的命名有两套规则\n> 1 ） 【强制】对于 Service 和 DAO 类，基于 SOA 的理念，暴露出来的服务一定是接口，内部\n> 的实现类用 Impl 的后缀与接口区别。\n> 正例： CacheServiceImpl 实现 CacheService 接口。\n> 2 ）  【推荐】 如果是形容能力的接口名称，取对应的形容词做接口名 （ 通常是– able 的形式 ） 。\n> 正例： AbstractTranslator 实现  Translatable 。\n\n一定注意，之前写scala的时候，并没有考虑到这一点，DAO和Service暴露出来一定是接口，然后用Impl后缀的实现类实现。\n\n#### 【参考】枚举类名建议带上 Enum 后缀，枚举成员名称需要全大写，单词间用下划线隔开。\n> 说明：枚举其实就是特殊的常量类，且构造方法被默认强制是私有。\n> 正例：枚举名字： DealStatusEnum， 成员名称： SUCCESS /  UNKOWN _ REASON 。\n\n#### 【强制】 POJO 类中布尔类型的变量，都不要加 is ，否则部分框架解析会引起序列化错误。\n> 反例：定义为基本数据类型 boolean isSuccess； 的属性，它的方法也是 isSuccess() ， RPC\n> 框架在反向解析的时候，“以为”对应的属性名称是 success ，导致属性获取不到，进而抛出异\n> 常。\n\n#### 【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。\n点分隔符之间仅有一个自然语义的英语单词，包名统一使用单数形式，但类名可以使用复数形式\n\n#### 【强制】杜绝完全不规范的缩写，避免望文不知义\n> 反例：  AbstractClass “缩写”命名成 AbsClass；condition “缩写”命名成  condi ，此类\n> 随意缩写严重降低了代码的可阅读性。\n\n的确是，一定要注意\n\n#### 【推荐】如果使用到了设计模式，建议在类名中体现出具体模式。\n> 说明：将设计模式体现在名字中，有利于阅读者快速理解架构设计思想。\n> 正例： public class OrderFactory;\n> public class LoginProxy;\n> public class ResourceObserver;\n\n#### 【参考】各层命名规约：\n\n> A) Service / DAO 层方法命名规约\n> 1 ） 获取单个对象的方法用 get 做前缀。\n> 2 ） 获取多个对象的方法用 list 做前缀。\n> 3 ） 获取统计值的方法用 count 做前缀。\n> 4 ） 插入的方法用 save（ 推荐 ） 或 insert 做前缀。\n> 5 ） 删除的方法用 remove（ 推荐 ） 或 delete 做前缀。\n> 6 ） 修改的方法用 update 做前缀。\n> B) 领域模型命名规约\n> 1 ） 数据对象： xxxDO ， xxx 即为数据表名。\n> 2 ） 数据传输对象： xxxDTO ， xxx 为业务领域相关的名称。\n> 3 ） 展示对象： xxxVO ， xxx 一般为网页名称。\n> 4 ） POJO 是 DO / DTO / BO / VO 的统称，禁止命名成 xxxPOJO 。\n\n到时候命名的时候再查阅一遍\n\n####  【推荐】接口类中的方法和属性不要加任何修饰符号 （public 也不要加 ） ，保持代码的简洁性，并加上有效的 Javadoc 注释。尽量不要在接口里定义变量，如果一定要定义变量，肯定是与接口方法相关，并且是整个应用的基础常量。\n> 正例：接口方法签名： void f();\n> 接口基础常量表示： String COMPANY = \" alibaba \" ;\n> 反例：接口方法定义： public abstract void f();\n> 说明： JDK 8 中接口允许有默认实现，那么这个 default 方法，是对所有实现类都有价值的默认实现。\n\n定义接口的时候，方法和属性，不要加任何修饰符号，注释要写清楚\n\n## 常量定义\n\n#### 【强制】不允许出现任何魔法值 （ 即未经定义的常量 ） 直接出现在代码中。\n反例：  String key =\"Id#taobao_\"+ tradeId；\ncache . put(key ,  value);\n\n为什么要这样做？\n\n#### 如果变量值仅在一个范围内变化用 Enum 类。如果还带有名称之外的延伸属性，必须使用 Enum 类，下面正例中的数字就是延伸信息，表示星期几。\n正例： public Enum {  MONDAY( 1 ) ,  TUESDAY( 2 ) ,  WEDNESDAY( 3 ) ,  THURSDAY( 4 ) ,  FRIDAY( 5 ) ,SATURDAY( 6 ) ,  SUNDAY( 7 ); }\n\n## 格式规范\n\n#### 【强制】单行字符数限制不超过 120 个，超出需要换行，换行时遵循如下原则：\n1） 第二行相对第一行缩进 4 个空格，从第三行开始，不再继续缩进，参考示例。\n2 ） 运算符与下文一起换行。\n3 ） 方法调用的点符号与下文一起换行。\n4 ） 在多个参数超长，逗号后进行换行。\n5 ） 在括号前不要换行，见反例。\n\n我之前是不能超过90个，目的是笔记本电脑看代码的时候比较方便，现在涨到120个了\n\n#### 【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。\n说明：没有必要插入多行空格进行隔开。不能有多插几行的强迫症\n\n## OOP规约\n\n#### 【强制】避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。\n\n#### 【强制】所有的覆写方法，必须加 @Override 注解。\n反例： getObject() 与 get0bject() 的问题。一个是字母的 O ，一个是数字的 0，加@Override\n可以准确判断是否覆盖成功。另外，如果在抽象类中对方法签名进行修改，其实现类会马上编\n译报错。\n\n#### 【强制】相同参数类型，相同业务含义，才可以使用 Java 的可变参数，避免使用 Object 。\n说明：可变参数必须放置在参数列表的最后。 （ 提倡同学们尽量不用可变参数编程 ）\n正例： public User getUsers(String type, Integer... ids)\n\n#### 【强制】对外暴露的接口签名，原则上不允许修改方法签名，避免对接口调用方产生影响。\n接口过时必须加@Deprecated 注解，并清晰地说明采用的新接口或者新服务是什么。\n\n#### 【强制】不能使用过时的类或方法。\n\n#### 【强制】 Object 的 equals 方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals 。\n正例： \" test \" .equals(object);\n反例：  object.equals( \" test \" );\n说明：推荐使用 java . util . Objects # equals （JDK 7 引入的工具类 ）\n\n#### 【强制】所有的相同类型的包装类对象之间值的比较，全部使用 equals 方法比较。\n说明：对于 Integer var =?在-128 至 127 之间的赋值， Integer 对象是在\nIntegerCache . cache 产生，会复用已有对象，这个区间内的 Integer 值可以直接使用==进行\n判断，但是这个区间之外的所有数据，都会在堆上产生，并不会复用已有对象，这是一个大坑，\n推荐使用 equals 方法进行判断。\n\n为什么这是个大坑？？？因为使用 == 比较的话，会判断内存地址是否相同，这样会返回False\n\n#### 8. 【强制】关于基本数据类型与包装数据类型的使用标准如下：\n1 ） 所有的 POJO 类属性必须使用包装数据类型。\n2 ） RPC 方法的返回值和参数必须使用包装数据类型。\n3 ） 所有的局部变量【推荐】使用基本数据类型。\n\n包装数据类型的 null 值，能够表示额外的信息，如：远程调用失败，异常退出。\n\n#### 【强制】定义 DO / DTO / VO 等 POJO 类时，不要设定任何属性默认值。\n反例： POJO 类的 gmtCreate 默认值为 new Date(); 但是这个属性在数据提取时并没有置入具\n体值，在更新其它字段时又附带更新了此字段，导致创建时间被修改成当前时间。\n\n#### 【强制】构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在 init 方法中。\n\n#### 【强制】 POJO 类必须写 toString 方法。使用 IDE 的中工具： source >  generate toString时，如果继承了另一个 POJO 类，注意在前面加一下 super . toString 。\n说明：在方法执行抛出异常时，可以直接调用 POJO 的 toString() 方法打印其属性值，便于排\n查问题。\n\n#### 【推荐】使用索引访问用 String 的 split 方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛 IndexOutOfBoundsException 的风险。\n说明：\nString str = \"a,b,c,,\";\nString[] ary = str.split(\",\");\n//预期大于 3，结果是 3\nSystem.out.println(ary.length);\n\n#### 【推荐】类内方法定义顺序依次是：公有方法或保护方法 > 私有方法 >  getter / setter方法。\n说明：构造方法放到最前面，公有方法是类的调用者和维护者最关心的方法，首屏展示最好 ； 保护方法虽然只是子类关心，也可能是“模板设计模式”下的核心方法 ； 而私有方法外部一般不需要特别关心，是一个黑盒实现 ； 因为方法信息价值较低，所有 Service 和 DAO 的 getter / setter 方法放在类体最后。\n\n#### 【推荐】在getter / setter 方法中，尽量不要增加业务逻辑，增加排查问题的难度。\n\n#### 【推荐】循环体内，字符串的联接方式，使用 StringBuilder 的 append 方法进行扩展。\n\n#### 【推荐】 final 可提高程序响应效率，声明成 final 的情况：\n1 ） 不需要重新赋值的变量，包括类属性、局部变量。\n2 ） 对象参数前加 final ，表示不允许修改引用的指向。\n3 ） 类方法确定不允许被重写。\n\n#### 【推荐】慎用 Object 的 clone 方法来拷贝对象。\n说明：对象的 clone 方法默认是浅拷贝，若想实现深拷贝需要重写 clone 方法实现属性对象\n的拷贝。\n\n#### 【推荐】类成员与方法访问控制从严：\n1 ） 如果不允许外部直接通过 new 来创建对象，那么构造方法必须是 private 。\n2 ） 工具类不允许有 public 或 default 构造方法。\n3 ） 类非 static 成员变量并且与子类共享，必须是 protected 。\n4 ） 类非 static 成员变量并且仅在本类使用，必须是 private 。\n5 ） 类 static 成员变量如果仅在本类使用，必须是 private 。\n6 ） 若是 static 成员变量，必须考虑是否为 final 。\n7 ） 类成员方法只供类内部调用，必须是 private 。\n8 ） 类成员方法只对继承类公开，那么限制为 protected 。\n说明：任何类、方法、参数、变量，严控访问范围。过宽泛的访问范围，不利于模块解耦。\n思\n考：如果是一个 private 的方法，想删除就删除，可是一个 public 的 Service 方法，或者一\n个 public 的成员变量，删除一下，不得手心冒点汗吗？变量像自己的小孩，尽量在自己的视\n线内，变量作用域太大，如果无限制的到处跑，那么你会担心的。\n\n**这一点是非常重要的习惯，一定要加以养成**\n\n## 集合处理\n\n#### 【强制】关于 hashCode 和 equals 的处理，遵循如下规则：\n1） 只要重写 equals ，就必须重写 hashCode 。\n2） 因为 Set 存储的是不重复的对象，依据 hashCode 和 equals 进行判断，所以 Set 存储的\n对象必须重写这两个方法。\n3） 如果自定义对象做为 Map 的键，那么必须重写 hashCode 和 equals 。\n正例： String 重写了 hashCode 和 equals 方法，所以我们可以非常愉快地使用 String 对象\n作为 key 来使用。\n\n#### 【强制】  ArrayList 的 subList 结果不可强转成 ArrayList ，否则会抛出 ClassCastException\n异常： java . util . RandomAccessSubList cannot be cast to java . util . ArrayList ;\n说明： subList 返回的是  ArrayList 的内部类  SubList ，并不是  ArrayList ，而是\nArrayList 的一个视图，对于 SubList 子列表的所有操作最终会反映到原列表上。\n\n#### 【强制】 在 subList 场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生 ConcurrentModificationException 异常。\n\n#### 【强制】使用集合转数组的方法，必须使用集合的 toArray(T[] array) ，传入的是类型完全一样的数组，大小就是 list . size() 。\n反例：直接使用 toArray 无参方法存在问题，此方法返回值只能是 Object[] 类，若强转其它\n类型数组将出现 ClassCastException 错误。\n正例：\nList<String> list = new ArrayList<String>(2);\nlist.add(\"guan\");\nlist.add(\"bao\");\nString[] array = new String[list.size()];\narray = list.toArray(array);\n说明：使用 toArray 带参方法，入参分配的数组空间不够大时， toArray 方法内部将重新分配\n内存空间，并返回新数组地址 ； 如果数组元素大于实际所需，下标为 [ list . size() ] 的数组\n元素将被置为 null ，其它数组元素保持原值，因此最好将方法入参数组大小定义与集合元素\n个数一致。\n\ntoArray 方法，一定要带参数，指定类型和大小；\n\n#### 【强制】使用工具类 Arrays . asList() 把数组转换成集合时，不能使用其修改集合相关的方法，它的 add / remove / clear 方法会抛出 UnsupportedOperationException 异常。\n\n说明： asList 的返回对象是一个 Arrays 内部类，并没有实现集合的修改方法。 Arrays . asList\n体现的是适配器模式，只是转换接口，后台的数据仍是数组。\nString[] str = new String[] { \"a\", \"b\" };\nList list = Arrays.asList(str);\n第一种情况： list.add(\"c\");  运行时异常。\n第二种情况： str[0]= \"gujin\"; 那么 list.get(0) 也会随之修改。\n\n#### 【强制】泛型通配符<?  extends T >来接收返回的数据，此写法的泛型集合不能使用 add 方法。\n说明：苹果装箱后返回一个`<?  extends Fruits >` 对象，此对象就不能往里加任何水果，包括苹果。\n\n\n#### 【强制】不要在 foreach 循环里进行元素的 remove / add 操作。 remove 元素请使用 Iterator方式，如果并发操作，需要对 Iterator 对象加锁。\n这个坑我以前踩过，如果早点看到就不会踩到了。。。\n\n#### 【强制】 在 JDK 7 版本以上， Comparator 要满足自反性，传递性，对称性，不然 Arrays.sort ，Collections.sort 会报 IllegalArgumentException 异常。\n说明：\n1 ） 自反性： x ， y 的比较结果和 y ， x 的比较结果相反。\n2 ） 传递性： x > y , y > z ,则 x > z 。\n3 ） 对称性： x = y ,则 x , z 比较结果和 y ， z 比较结果相同。\n\n#### 【推荐】集合初始化时，尽量指定集合初始值大小。\n说明： ArrayList 尽量使用 ArrayList(int initialCapacity) 初始化。\n\n#### 【推荐】使用 entrySet 遍历 Map 类集合 KV ，而不是 keySet 方式进行遍历。\n说明： keySet 其实是遍历了 2 次，一次是转为 Iterator 对象，另一次是从 hashMap 中取出key 所对应的 value 。而 entrySet 只是遍历了一次就把 key 和 value 都放到了 entry 中，效率更高。如果是 JDK 8，使用 Map . foreach 方法。\n正例： values() 返回的是 V 值集合，是一个 list 集合对象 ；keySet() 返回的是 K 值集合，是一个 Set 集合对象；entrySet() 返回的是 K - V 值组合集合。\n\n#### 【推荐】高度注意 Map 类集合 K / V 能不能存储 null 值的情况，如下表格：\n\n|集合类|Key|Value|Super|说明|\n|-----------------------------------|\n|Hashtable|不允许为 null|不允许为 null|Dictionary|线程安全|\n|ConcurrentHashMap|不允许为 null|不允许为 null|AbstractMap|分段锁技术|\n|TreeMap|不允许为 null|允许为 null|AbstractMap|线程不安全|\n|HashMap|允许为 null|允许为 null|AbstractMap|线程不安全|\n\n反例： 由于 HashMap 的干扰，很多人认为 ConcurrentHashMap 是可以置入 null 值，注意存储null 值时会抛出 NPE 异常。\n\n#### 【参考】合理利用好集合的有序性 (sort) 和稳定性 (order) ，避免集合的无序性 (unsort) 和不稳定性 (unorder) 带来的负面影响。\n说明：稳定性指集合每次遍历的元素次序是一定的。有序性是指遍历的结果是按某种比较规则依次排列的。如： ArrayList 是 order / unsort；HashMap 是 unorder / unsort；TreeSet 是order / sort 。\n\n#### 【参考】利用 Set 元素唯一的特性，可以快速对一个集合进行去重操作，避免使用 List 的contains 方法进行遍历、对比、去重操作。\n自评：这个好：使用 s.addAll(list); 方法。\n\n## 并发处理\n#### 【强制】获取单例对象需要保证线程安全，其中的方法也要保证线程安全。\n说明：资源驱动类、工具类、单例工厂类都需要注意。\n\n#### 【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。\n``` java\npublic class TimerTaskThread extends Thread {\n\tpublic TimerTaskThread(){\n\tsuper.setName(\"TimerTaskThread\"); ...\n\t}\n```\n\n#### 【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\n说明：使用线程池的好处是减少在创建和销毁线程上所花的时间以及系统资源的开销，解决资\n源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者\n“过度切换”的问题。\n\n#### 【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。\n说明： Executors 返回的线程池对象的弊端如下：\n1） FixedThreadPool 和 SingleThreadPool :\n允许的请求队列长度为 Integer.MAX_VALUE ，可能会堆积大量的请求，从而导致 OOM 。\n2） CachedThreadPool 和 ScheduledThreadPool :\n允许的创建线程数量为 Integer.MAX_VALUE ，可能会创建大量的线程，从而导致 OOM 。\n\n#### 【强制】 SimpleDateFormat 是线程不安全的类，一般不要定义为 static 变量，如果定义为static ，必须加锁，或者使用 DateUtils 工具类。\n\n#### 【强制】高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁 ； 能锁区块，就不要锁整个方法体 ； 能用对象锁，就不要用类锁。\n\n#### 【强制】对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁\n\n说明：线程一需要对表 A 、 B 、 C 依次全部加锁后才可以进行更新操作，那么线程二的加锁顺序\n也必须是 A 、 B 、 C ，否则可能出现死锁。\n\n#### 【强制】并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用 version 作为更新依据。\n说明：如果每次访问冲突概率小于 20%，推荐使用乐观锁，否则使用悲观锁。乐观锁的重试次\n数不得小于 3 次。\n\n#### 【强制】多线程并行处理定时任务时， Timer 运行多个 TimeTask 时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用 ScheduledExecutorService 则没有这个问题。\n都是经验啊，我还没有碰到这种坑\n\n#### 【推荐】使用 CountDownLatch 进行异步转同步操作，每个线程退出前必须调用 countDown 方法，线程执行代码注意 catch 异常，确保 countDown 方法可以执行，避免主线程无法执行至 countDown 方法，直到超时才返回结果。\n说明：注意，子线程抛出异常堆栈，不能在主线程 try - catch 到。\n\n#### 【推荐】避免 Random 实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed 导致的性能下降。\n说明： Random 实例包括 java . util . Random 的实例或者  Math . random() 实例。\n正例：在 JDK 7 之后，可以直接使用 API ThreadLocalRandom ，在  JDK 7 之前，可以做到每个\n线程一个实例。\n\n#### 【参考】 volatile 解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。\n如果是 count ++操作，使用如下类实现：AtomicInteger count =  new AtomicInteger(); count . addAndGet( 1 );  如果是 JDK 8，推荐使用 LongAdder 对象，比 AtomicLong 性能更好 （ 减少乐观锁的重试次数 ） 。\n\n#### 【参考】  HashMap 在容量不够进行 resize 时由于高并发可能出现死链，导致 CPU 飙升，在开发过程中注意规避此风险。\n\n## 控制语句\n\n#### 【强制】在一个 switch 块内，每个 case 要么通过 break / return 等来终止，要么注释说明程序将继续执行到哪一个 case 为止 ； 在一个 switch 块内，都必须包含一个 default 语句并且放在最后，即使它什么代码也没有。\n\n#### 【推荐】推荐尽量少用 else ，  if - else 的方式可以改写成：\n```\nif(condition){\n\t...\n\treturn obj;\n}\n// 接着写 else 的业务逻辑代码;\n```\n说明：如果非得使用 if()...else if()...else... 方式表达逻辑，【强制】请勿超过 3 层，\n超过请使用状态设计模式。\n正例：逻辑上超过 3 层的 if-else 代码可以使用卫语句，或者状态模式来实现。\n思考：以后实现一定注意，我以前没有注意过这一点。\n\n#### 【推荐】除常用方法（如 getXxx/isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。\n说明：很多 if 语句内的逻辑相当复杂，阅读者需要分析条件表达式的最终结果，才能明确什么\n样的条件执行什么样的语句，那么，如果阅读者分析逻辑表达式错误呢？\n\n正例：\n```\n//伪代码如下\nboolean existed = (file.open(fileName, \"w\") != null) && (...) || (...);\nif (existed) {\n\t...\n}\n反例：\nif ((file.open(fileName, \"w\") != null) && (...) || (...)) {\n\t...\n}\n```\n\n#### 【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的 try - catch 操作 （ 这个 try - catch 是否可以移至循环体外 ） 。\n\n#### 【推荐】接口入参保护，这种场景常见的是用于做批量操作的接口。\n\n#### 【参考】方法中需要进行参数校验的场景：\n1 ） 调用频次低的方法。\n2 ） 执行时间开销很大的方法，参数校验时间几乎可以忽略不计，但如果因为参数错误导致\n中间执行回退，或者错误，那得不偿失。\n3 ） 需要极高稳定性和可用性的方法。\n4 ） 对外提供的开放接口，不管是 RPC / API / HTTP 接口。\n5） 敏感权限入口。\n\n#### 【参考】方法中不需要参数校验的场景：\n1 ） 极有可能被循环调用的方法，不建议对参数进行校验。但在方法说明里必须注明外部参\n数检查。\n2 ） 底层的方法调用频度都比较高，一般不校验。毕竟是像纯净水过滤的最后一道，参数错\n误不太可能到底层才会暴露问题。一般 DAO 层与 Service 层都在同一个应用中，部署在同一\n台服务器中，所以 DAO 的参数校验，可以省略。\n3 ） 被声明成 private 只会被自己代码所调用的方法，如果能够确定调用方法的代码传入参\n数已经做过检查或者肯定不会有问题，此时可以不校验参数。\n\n## 注释规约\n\n#### 【强制】类、类属性、类方法的注释必须使用 Javadoc 规范，使用/**内容*/格式，不得使用 // xxx 方式。\n说明：在 IDE 编辑窗口中， Javadoc 方式会提示相关注释，生成 Javadoc 可以正确输出相应注\n释 ； 在 IDE 中，工程调用方法时，不进入方法即可悬浮提示方法、参数、返回值的意义，提高\n阅读效率。\n\n#### 【强制】所有的抽象方法 （ 包括接口中的方法 ） 必须要用 Javadoc 注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。\n说明：对子类的实现要求，或者调用注意事项，请一并说明。\n\n#### 【强制】所有的枚举类型字段必须要有注释，说明每个数据项的用途。\n\n#### 【推荐】与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。\n反例：“ TCP 连接超时”解释成“传输控制协议连接超时”，理解反而费脑筋。\n\n#### 【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。\n说明：代码与注释更新不同步，就像路网与导航软件更新不同步一样，如果导航软件严重滞后，\n就失去了导航的意义。\n\n#### 【参考】注释掉的代码尽量要配合说明，而不是简单的注释掉。\n说明：代码被注释掉有两种可能性：1 ） 后续会恢复此段代码逻辑。2 ） 永久不用。前者如果没\n有备注信息，难以知晓注释动机。后者建议直接删掉 （ 代码仓库保存了历史代码 ） 。\n思考：这一点我需要反思，很多时候代码舍不得删，搞得很乱，所以应该勤提交，使用git来保存历史代码；\n\n#### 【参考】对于注释的要求：第一、能够准确反应设计思想和代码逻辑 ； 第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。\n完全没有注释的大段代码对于阅读者形同\n天书，注释是给自己看的，即使隔很长时间，也能清晰理解当时的思路 ； 注释也是给继任者看\n的，使其能够快速接替自己的工作。\n\n#### 【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。\n1 ） 待办事宜 （TODO） : （ 标记人，标记时间， [ 预计处理时间 ]）\n表示需要实现，但目前还未实现的功能。这实际上是一个 Javadoc 的标签，目前的 Javadoc\n还没有实现，但已经被广泛使用。只能应用于类，接口和方法 （ 因为它是一个 Javadoc 标签 ） 。\n2 ） 错误，不能工作 （FIXME） : （ 标记人，标记时间， [ 预计处理时间 ]）\n在注释中用 FIXME 标记某代码是错误的，而且不能工作，需要及时纠正的情况。\n\n## 其它\n\n#### 【强制】在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。\n说明：不要在方法体内定义： Pattern pattern =  Pattern . compile( 规则 );\n\n#### 【强制】后台输送给页面的变量必须加 $!{var} ——中间的感叹号。\n说明：如果 var = null 或者不存在，那么 ${var} 会直接显示在页面上。\n思考：这里是 Velocity 模板引擎的一些内容，Velocity是一个基于Java的模板引擎，通过特定的语法，Velocity可以获取在java语言中定义的对象，从而实现界面和java代码的真正分离，这意味着可以使用velocity替代jsp的开发模式了。这使得前端开发人员可以和 Java 程序开发人员同步开发一个遵循 MVC 架构的 web 站点，在实际应用中，velocity还可以应用于很多其他的场景。，比如源代码生成、自动email和转换xml等。\n\n#### 【强制】注意  Math . random() 这个方法返回是 double 类型，注意取值的范围 0≤ x <1 （ 能够取到零值，注意除零异常 ） ，如果想获取整数类型的随机数，不要将 x 放大 10 的若干倍然后取整，直接使用 Random 对象的 nextInt 或者 nextLong 方法。\n\n#### 【强制】获取当前毫秒数 System . currentTimeMillis(); 而不是 new Date() . getTime();\n说明：如果想获取更加精确的纳秒级时间值，用 System . nanoTime() 。在 JDK 8 中，针对统计时间等场景，推荐使用 Instant类。\n\n#### 【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。\n\n#### 【推荐】对于“明确停止使用的代码和配置”，如方法、变量、类、配置文件、动态配置属性等要坚决从程序中清理出去，避免造成过多垃圾。 \n\n","source":"_posts/2017-03-13-阿里巴巴Java开发手册学习笔记.md","raw":"---\ntitle: 阿里巴巴Java开发手册学习笔记（上）\ntoc: false\ndate: 2017-03-13 19:47:30\ntags: java\ncategories: java\n---\n\n本人自认为代码还算写得比较符合规范，但是相信阿里会有总结更深的见解，因此对自己之前没有注意到的，自己没有遵循的，做一个笔记与整理，以加深理解。\n# 编程规约\n\n## 命名规约\n#### 【强制】常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。\n重点在，不要嫌名字长，我以前写得比较短 \n正例： `MAX _ STOCK _ COUNT`\n\n#### 【强制】抽象类命名使用 Abstract 或 Base 开头 ； 异常类命名使用 Exception 结尾 ； 测试类命名以它要测试的类的名称开始，以 Test 结尾。\n如果是接口的话，如何？\n\n#### 接口和实现类的命名有两套规则\n> 1 ） 【强制】对于 Service 和 DAO 类，基于 SOA 的理念，暴露出来的服务一定是接口，内部\n> 的实现类用 Impl 的后缀与接口区别。\n> 正例： CacheServiceImpl 实现 CacheService 接口。\n> 2 ）  【推荐】 如果是形容能力的接口名称，取对应的形容词做接口名 （ 通常是– able 的形式 ） 。\n> 正例： AbstractTranslator 实现  Translatable 。\n\n一定注意，之前写scala的时候，并没有考虑到这一点，DAO和Service暴露出来一定是接口，然后用Impl后缀的实现类实现。\n\n#### 【参考】枚举类名建议带上 Enum 后缀，枚举成员名称需要全大写，单词间用下划线隔开。\n> 说明：枚举其实就是特殊的常量类，且构造方法被默认强制是私有。\n> 正例：枚举名字： DealStatusEnum， 成员名称： SUCCESS /  UNKOWN _ REASON 。\n\n#### 【强制】 POJO 类中布尔类型的变量，都不要加 is ，否则部分框架解析会引起序列化错误。\n> 反例：定义为基本数据类型 boolean isSuccess； 的属性，它的方法也是 isSuccess() ， RPC\n> 框架在反向解析的时候，“以为”对应的属性名称是 success ，导致属性获取不到，进而抛出异\n> 常。\n\n#### 【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。\n点分隔符之间仅有一个自然语义的英语单词，包名统一使用单数形式，但类名可以使用复数形式\n\n#### 【强制】杜绝完全不规范的缩写，避免望文不知义\n> 反例：  AbstractClass “缩写”命名成 AbsClass；condition “缩写”命名成  condi ，此类\n> 随意缩写严重降低了代码的可阅读性。\n\n的确是，一定要注意\n\n#### 【推荐】如果使用到了设计模式，建议在类名中体现出具体模式。\n> 说明：将设计模式体现在名字中，有利于阅读者快速理解架构设计思想。\n> 正例： public class OrderFactory;\n> public class LoginProxy;\n> public class ResourceObserver;\n\n#### 【参考】各层命名规约：\n\n> A) Service / DAO 层方法命名规约\n> 1 ） 获取单个对象的方法用 get 做前缀。\n> 2 ） 获取多个对象的方法用 list 做前缀。\n> 3 ） 获取统计值的方法用 count 做前缀。\n> 4 ） 插入的方法用 save（ 推荐 ） 或 insert 做前缀。\n> 5 ） 删除的方法用 remove（ 推荐 ） 或 delete 做前缀。\n> 6 ） 修改的方法用 update 做前缀。\n> B) 领域模型命名规约\n> 1 ） 数据对象： xxxDO ， xxx 即为数据表名。\n> 2 ） 数据传输对象： xxxDTO ， xxx 为业务领域相关的名称。\n> 3 ） 展示对象： xxxVO ， xxx 一般为网页名称。\n> 4 ） POJO 是 DO / DTO / BO / VO 的统称，禁止命名成 xxxPOJO 。\n\n到时候命名的时候再查阅一遍\n\n####  【推荐】接口类中的方法和属性不要加任何修饰符号 （public 也不要加 ） ，保持代码的简洁性，并加上有效的 Javadoc 注释。尽量不要在接口里定义变量，如果一定要定义变量，肯定是与接口方法相关，并且是整个应用的基础常量。\n> 正例：接口方法签名： void f();\n> 接口基础常量表示： String COMPANY = \" alibaba \" ;\n> 反例：接口方法定义： public abstract void f();\n> 说明： JDK 8 中接口允许有默认实现，那么这个 default 方法，是对所有实现类都有价值的默认实现。\n\n定义接口的时候，方法和属性，不要加任何修饰符号，注释要写清楚\n\n## 常量定义\n\n#### 【强制】不允许出现任何魔法值 （ 即未经定义的常量 ） 直接出现在代码中。\n反例：  String key =\"Id#taobao_\"+ tradeId；\ncache . put(key ,  value);\n\n为什么要这样做？\n\n#### 如果变量值仅在一个范围内变化用 Enum 类。如果还带有名称之外的延伸属性，必须使用 Enum 类，下面正例中的数字就是延伸信息，表示星期几。\n正例： public Enum {  MONDAY( 1 ) ,  TUESDAY( 2 ) ,  WEDNESDAY( 3 ) ,  THURSDAY( 4 ) ,  FRIDAY( 5 ) ,SATURDAY( 6 ) ,  SUNDAY( 7 ); }\n\n## 格式规范\n\n#### 【强制】单行字符数限制不超过 120 个，超出需要换行，换行时遵循如下原则：\n1） 第二行相对第一行缩进 4 个空格，从第三行开始，不再继续缩进，参考示例。\n2 ） 运算符与下文一起换行。\n3 ） 方法调用的点符号与下文一起换行。\n4 ） 在多个参数超长，逗号后进行换行。\n5 ） 在括号前不要换行，见反例。\n\n我之前是不能超过90个，目的是笔记本电脑看代码的时候比较方便，现在涨到120个了\n\n#### 【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。\n说明：没有必要插入多行空格进行隔开。不能有多插几行的强迫症\n\n## OOP规约\n\n#### 【强制】避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。\n\n#### 【强制】所有的覆写方法，必须加 @Override 注解。\n反例： getObject() 与 get0bject() 的问题。一个是字母的 O ，一个是数字的 0，加@Override\n可以准确判断是否覆盖成功。另外，如果在抽象类中对方法签名进行修改，其实现类会马上编\n译报错。\n\n#### 【强制】相同参数类型，相同业务含义，才可以使用 Java 的可变参数，避免使用 Object 。\n说明：可变参数必须放置在参数列表的最后。 （ 提倡同学们尽量不用可变参数编程 ）\n正例： public User getUsers(String type, Integer... ids)\n\n#### 【强制】对外暴露的接口签名，原则上不允许修改方法签名，避免对接口调用方产生影响。\n接口过时必须加@Deprecated 注解，并清晰地说明采用的新接口或者新服务是什么。\n\n#### 【强制】不能使用过时的类或方法。\n\n#### 【强制】 Object 的 equals 方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals 。\n正例： \" test \" .equals(object);\n反例：  object.equals( \" test \" );\n说明：推荐使用 java . util . Objects # equals （JDK 7 引入的工具类 ）\n\n#### 【强制】所有的相同类型的包装类对象之间值的比较，全部使用 equals 方法比较。\n说明：对于 Integer var =?在-128 至 127 之间的赋值， Integer 对象是在\nIntegerCache . cache 产生，会复用已有对象，这个区间内的 Integer 值可以直接使用==进行\n判断，但是这个区间之外的所有数据，都会在堆上产生，并不会复用已有对象，这是一个大坑，\n推荐使用 equals 方法进行判断。\n\n为什么这是个大坑？？？因为使用 == 比较的话，会判断内存地址是否相同，这样会返回False\n\n#### 8. 【强制】关于基本数据类型与包装数据类型的使用标准如下：\n1 ） 所有的 POJO 类属性必须使用包装数据类型。\n2 ） RPC 方法的返回值和参数必须使用包装数据类型。\n3 ） 所有的局部变量【推荐】使用基本数据类型。\n\n包装数据类型的 null 值，能够表示额外的信息，如：远程调用失败，异常退出。\n\n#### 【强制】定义 DO / DTO / VO 等 POJO 类时，不要设定任何属性默认值。\n反例： POJO 类的 gmtCreate 默认值为 new Date(); 但是这个属性在数据提取时并没有置入具\n体值，在更新其它字段时又附带更新了此字段，导致创建时间被修改成当前时间。\n\n#### 【强制】构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在 init 方法中。\n\n#### 【强制】 POJO 类必须写 toString 方法。使用 IDE 的中工具： source >  generate toString时，如果继承了另一个 POJO 类，注意在前面加一下 super . toString 。\n说明：在方法执行抛出异常时，可以直接调用 POJO 的 toString() 方法打印其属性值，便于排\n查问题。\n\n#### 【推荐】使用索引访问用 String 的 split 方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛 IndexOutOfBoundsException 的风险。\n说明：\nString str = \"a,b,c,,\";\nString[] ary = str.split(\",\");\n//预期大于 3，结果是 3\nSystem.out.println(ary.length);\n\n#### 【推荐】类内方法定义顺序依次是：公有方法或保护方法 > 私有方法 >  getter / setter方法。\n说明：构造方法放到最前面，公有方法是类的调用者和维护者最关心的方法，首屏展示最好 ； 保护方法虽然只是子类关心，也可能是“模板设计模式”下的核心方法 ； 而私有方法外部一般不需要特别关心，是一个黑盒实现 ； 因为方法信息价值较低，所有 Service 和 DAO 的 getter / setter 方法放在类体最后。\n\n#### 【推荐】在getter / setter 方法中，尽量不要增加业务逻辑，增加排查问题的难度。\n\n#### 【推荐】循环体内，字符串的联接方式，使用 StringBuilder 的 append 方法进行扩展。\n\n#### 【推荐】 final 可提高程序响应效率，声明成 final 的情况：\n1 ） 不需要重新赋值的变量，包括类属性、局部变量。\n2 ） 对象参数前加 final ，表示不允许修改引用的指向。\n3 ） 类方法确定不允许被重写。\n\n#### 【推荐】慎用 Object 的 clone 方法来拷贝对象。\n说明：对象的 clone 方法默认是浅拷贝，若想实现深拷贝需要重写 clone 方法实现属性对象\n的拷贝。\n\n#### 【推荐】类成员与方法访问控制从严：\n1 ） 如果不允许外部直接通过 new 来创建对象，那么构造方法必须是 private 。\n2 ） 工具类不允许有 public 或 default 构造方法。\n3 ） 类非 static 成员变量并且与子类共享，必须是 protected 。\n4 ） 类非 static 成员变量并且仅在本类使用，必须是 private 。\n5 ） 类 static 成员变量如果仅在本类使用，必须是 private 。\n6 ） 若是 static 成员变量，必须考虑是否为 final 。\n7 ） 类成员方法只供类内部调用，必须是 private 。\n8 ） 类成员方法只对继承类公开，那么限制为 protected 。\n说明：任何类、方法、参数、变量，严控访问范围。过宽泛的访问范围，不利于模块解耦。\n思\n考：如果是一个 private 的方法，想删除就删除，可是一个 public 的 Service 方法，或者一\n个 public 的成员变量，删除一下，不得手心冒点汗吗？变量像自己的小孩，尽量在自己的视\n线内，变量作用域太大，如果无限制的到处跑，那么你会担心的。\n\n**这一点是非常重要的习惯，一定要加以养成**\n\n## 集合处理\n\n#### 【强制】关于 hashCode 和 equals 的处理，遵循如下规则：\n1） 只要重写 equals ，就必须重写 hashCode 。\n2） 因为 Set 存储的是不重复的对象，依据 hashCode 和 equals 进行判断，所以 Set 存储的\n对象必须重写这两个方法。\n3） 如果自定义对象做为 Map 的键，那么必须重写 hashCode 和 equals 。\n正例： String 重写了 hashCode 和 equals 方法，所以我们可以非常愉快地使用 String 对象\n作为 key 来使用。\n\n#### 【强制】  ArrayList 的 subList 结果不可强转成 ArrayList ，否则会抛出 ClassCastException\n异常： java . util . RandomAccessSubList cannot be cast to java . util . ArrayList ;\n说明： subList 返回的是  ArrayList 的内部类  SubList ，并不是  ArrayList ，而是\nArrayList 的一个视图，对于 SubList 子列表的所有操作最终会反映到原列表上。\n\n#### 【强制】 在 subList 场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生 ConcurrentModificationException 异常。\n\n#### 【强制】使用集合转数组的方法，必须使用集合的 toArray(T[] array) ，传入的是类型完全一样的数组，大小就是 list . size() 。\n反例：直接使用 toArray 无参方法存在问题，此方法返回值只能是 Object[] 类，若强转其它\n类型数组将出现 ClassCastException 错误。\n正例：\nList<String> list = new ArrayList<String>(2);\nlist.add(\"guan\");\nlist.add(\"bao\");\nString[] array = new String[list.size()];\narray = list.toArray(array);\n说明：使用 toArray 带参方法，入参分配的数组空间不够大时， toArray 方法内部将重新分配\n内存空间，并返回新数组地址 ； 如果数组元素大于实际所需，下标为 [ list . size() ] 的数组\n元素将被置为 null ，其它数组元素保持原值，因此最好将方法入参数组大小定义与集合元素\n个数一致。\n\ntoArray 方法，一定要带参数，指定类型和大小；\n\n#### 【强制】使用工具类 Arrays . asList() 把数组转换成集合时，不能使用其修改集合相关的方法，它的 add / remove / clear 方法会抛出 UnsupportedOperationException 异常。\n\n说明： asList 的返回对象是一个 Arrays 内部类，并没有实现集合的修改方法。 Arrays . asList\n体现的是适配器模式，只是转换接口，后台的数据仍是数组。\nString[] str = new String[] { \"a\", \"b\" };\nList list = Arrays.asList(str);\n第一种情况： list.add(\"c\");  运行时异常。\n第二种情况： str[0]= \"gujin\"; 那么 list.get(0) 也会随之修改。\n\n#### 【强制】泛型通配符<?  extends T >来接收返回的数据，此写法的泛型集合不能使用 add 方法。\n说明：苹果装箱后返回一个`<?  extends Fruits >` 对象，此对象就不能往里加任何水果，包括苹果。\n\n\n#### 【强制】不要在 foreach 循环里进行元素的 remove / add 操作。 remove 元素请使用 Iterator方式，如果并发操作，需要对 Iterator 对象加锁。\n这个坑我以前踩过，如果早点看到就不会踩到了。。。\n\n#### 【强制】 在 JDK 7 版本以上， Comparator 要满足自反性，传递性，对称性，不然 Arrays.sort ，Collections.sort 会报 IllegalArgumentException 异常。\n说明：\n1 ） 自反性： x ， y 的比较结果和 y ， x 的比较结果相反。\n2 ） 传递性： x > y , y > z ,则 x > z 。\n3 ） 对称性： x = y ,则 x , z 比较结果和 y ， z 比较结果相同。\n\n#### 【推荐】集合初始化时，尽量指定集合初始值大小。\n说明： ArrayList 尽量使用 ArrayList(int initialCapacity) 初始化。\n\n#### 【推荐】使用 entrySet 遍历 Map 类集合 KV ，而不是 keySet 方式进行遍历。\n说明： keySet 其实是遍历了 2 次，一次是转为 Iterator 对象，另一次是从 hashMap 中取出key 所对应的 value 。而 entrySet 只是遍历了一次就把 key 和 value 都放到了 entry 中，效率更高。如果是 JDK 8，使用 Map . foreach 方法。\n正例： values() 返回的是 V 值集合，是一个 list 集合对象 ；keySet() 返回的是 K 值集合，是一个 Set 集合对象；entrySet() 返回的是 K - V 值组合集合。\n\n#### 【推荐】高度注意 Map 类集合 K / V 能不能存储 null 值的情况，如下表格：\n\n|集合类|Key|Value|Super|说明|\n|-----------------------------------|\n|Hashtable|不允许为 null|不允许为 null|Dictionary|线程安全|\n|ConcurrentHashMap|不允许为 null|不允许为 null|AbstractMap|分段锁技术|\n|TreeMap|不允许为 null|允许为 null|AbstractMap|线程不安全|\n|HashMap|允许为 null|允许为 null|AbstractMap|线程不安全|\n\n反例： 由于 HashMap 的干扰，很多人认为 ConcurrentHashMap 是可以置入 null 值，注意存储null 值时会抛出 NPE 异常。\n\n#### 【参考】合理利用好集合的有序性 (sort) 和稳定性 (order) ，避免集合的无序性 (unsort) 和不稳定性 (unorder) 带来的负面影响。\n说明：稳定性指集合每次遍历的元素次序是一定的。有序性是指遍历的结果是按某种比较规则依次排列的。如： ArrayList 是 order / unsort；HashMap 是 unorder / unsort；TreeSet 是order / sort 。\n\n#### 【参考】利用 Set 元素唯一的特性，可以快速对一个集合进行去重操作，避免使用 List 的contains 方法进行遍历、对比、去重操作。\n自评：这个好：使用 s.addAll(list); 方法。\n\n## 并发处理\n#### 【强制】获取单例对象需要保证线程安全，其中的方法也要保证线程安全。\n说明：资源驱动类、工具类、单例工厂类都需要注意。\n\n#### 【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。\n``` java\npublic class TimerTaskThread extends Thread {\n\tpublic TimerTaskThread(){\n\tsuper.setName(\"TimerTaskThread\"); ...\n\t}\n```\n\n#### 【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\n说明：使用线程池的好处是减少在创建和销毁线程上所花的时间以及系统资源的开销，解决资\n源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者\n“过度切换”的问题。\n\n#### 【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。\n说明： Executors 返回的线程池对象的弊端如下：\n1） FixedThreadPool 和 SingleThreadPool :\n允许的请求队列长度为 Integer.MAX_VALUE ，可能会堆积大量的请求，从而导致 OOM 。\n2） CachedThreadPool 和 ScheduledThreadPool :\n允许的创建线程数量为 Integer.MAX_VALUE ，可能会创建大量的线程，从而导致 OOM 。\n\n#### 【强制】 SimpleDateFormat 是线程不安全的类，一般不要定义为 static 变量，如果定义为static ，必须加锁，或者使用 DateUtils 工具类。\n\n#### 【强制】高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁 ； 能锁区块，就不要锁整个方法体 ； 能用对象锁，就不要用类锁。\n\n#### 【强制】对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁\n\n说明：线程一需要对表 A 、 B 、 C 依次全部加锁后才可以进行更新操作，那么线程二的加锁顺序\n也必须是 A 、 B 、 C ，否则可能出现死锁。\n\n#### 【强制】并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用 version 作为更新依据。\n说明：如果每次访问冲突概率小于 20%，推荐使用乐观锁，否则使用悲观锁。乐观锁的重试次\n数不得小于 3 次。\n\n#### 【强制】多线程并行处理定时任务时， Timer 运行多个 TimeTask 时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用 ScheduledExecutorService 则没有这个问题。\n都是经验啊，我还没有碰到这种坑\n\n#### 【推荐】使用 CountDownLatch 进行异步转同步操作，每个线程退出前必须调用 countDown 方法，线程执行代码注意 catch 异常，确保 countDown 方法可以执行，避免主线程无法执行至 countDown 方法，直到超时才返回结果。\n说明：注意，子线程抛出异常堆栈，不能在主线程 try - catch 到。\n\n#### 【推荐】避免 Random 实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed 导致的性能下降。\n说明： Random 实例包括 java . util . Random 的实例或者  Math . random() 实例。\n正例：在 JDK 7 之后，可以直接使用 API ThreadLocalRandom ，在  JDK 7 之前，可以做到每个\n线程一个实例。\n\n#### 【参考】 volatile 解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。\n如果是 count ++操作，使用如下类实现：AtomicInteger count =  new AtomicInteger(); count . addAndGet( 1 );  如果是 JDK 8，推荐使用 LongAdder 对象，比 AtomicLong 性能更好 （ 减少乐观锁的重试次数 ） 。\n\n#### 【参考】  HashMap 在容量不够进行 resize 时由于高并发可能出现死链，导致 CPU 飙升，在开发过程中注意规避此风险。\n\n## 控制语句\n\n#### 【强制】在一个 switch 块内，每个 case 要么通过 break / return 等来终止，要么注释说明程序将继续执行到哪一个 case 为止 ； 在一个 switch 块内，都必须包含一个 default 语句并且放在最后，即使它什么代码也没有。\n\n#### 【推荐】推荐尽量少用 else ，  if - else 的方式可以改写成：\n```\nif(condition){\n\t...\n\treturn obj;\n}\n// 接着写 else 的业务逻辑代码;\n```\n说明：如果非得使用 if()...else if()...else... 方式表达逻辑，【强制】请勿超过 3 层，\n超过请使用状态设计模式。\n正例：逻辑上超过 3 层的 if-else 代码可以使用卫语句，或者状态模式来实现。\n思考：以后实现一定注意，我以前没有注意过这一点。\n\n#### 【推荐】除常用方法（如 getXxx/isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。\n说明：很多 if 语句内的逻辑相当复杂，阅读者需要分析条件表达式的最终结果，才能明确什么\n样的条件执行什么样的语句，那么，如果阅读者分析逻辑表达式错误呢？\n\n正例：\n```\n//伪代码如下\nboolean existed = (file.open(fileName, \"w\") != null) && (...) || (...);\nif (existed) {\n\t...\n}\n反例：\nif ((file.open(fileName, \"w\") != null) && (...) || (...)) {\n\t...\n}\n```\n\n#### 【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的 try - catch 操作 （ 这个 try - catch 是否可以移至循环体外 ） 。\n\n#### 【推荐】接口入参保护，这种场景常见的是用于做批量操作的接口。\n\n#### 【参考】方法中需要进行参数校验的场景：\n1 ） 调用频次低的方法。\n2 ） 执行时间开销很大的方法，参数校验时间几乎可以忽略不计，但如果因为参数错误导致\n中间执行回退，或者错误，那得不偿失。\n3 ） 需要极高稳定性和可用性的方法。\n4 ） 对外提供的开放接口，不管是 RPC / API / HTTP 接口。\n5） 敏感权限入口。\n\n#### 【参考】方法中不需要参数校验的场景：\n1 ） 极有可能被循环调用的方法，不建议对参数进行校验。但在方法说明里必须注明外部参\n数检查。\n2 ） 底层的方法调用频度都比较高，一般不校验。毕竟是像纯净水过滤的最后一道，参数错\n误不太可能到底层才会暴露问题。一般 DAO 层与 Service 层都在同一个应用中，部署在同一\n台服务器中，所以 DAO 的参数校验，可以省略。\n3 ） 被声明成 private 只会被自己代码所调用的方法，如果能够确定调用方法的代码传入参\n数已经做过检查或者肯定不会有问题，此时可以不校验参数。\n\n## 注释规约\n\n#### 【强制】类、类属性、类方法的注释必须使用 Javadoc 规范，使用/**内容*/格式，不得使用 // xxx 方式。\n说明：在 IDE 编辑窗口中， Javadoc 方式会提示相关注释，生成 Javadoc 可以正确输出相应注\n释 ； 在 IDE 中，工程调用方法时，不进入方法即可悬浮提示方法、参数、返回值的意义，提高\n阅读效率。\n\n#### 【强制】所有的抽象方法 （ 包括接口中的方法 ） 必须要用 Javadoc 注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。\n说明：对子类的实现要求，或者调用注意事项，请一并说明。\n\n#### 【强制】所有的枚举类型字段必须要有注释，说明每个数据项的用途。\n\n#### 【推荐】与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。\n反例：“ TCP 连接超时”解释成“传输控制协议连接超时”，理解反而费脑筋。\n\n#### 【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。\n说明：代码与注释更新不同步，就像路网与导航软件更新不同步一样，如果导航软件严重滞后，\n就失去了导航的意义。\n\n#### 【参考】注释掉的代码尽量要配合说明，而不是简单的注释掉。\n说明：代码被注释掉有两种可能性：1 ） 后续会恢复此段代码逻辑。2 ） 永久不用。前者如果没\n有备注信息，难以知晓注释动机。后者建议直接删掉 （ 代码仓库保存了历史代码 ） 。\n思考：这一点我需要反思，很多时候代码舍不得删，搞得很乱，所以应该勤提交，使用git来保存历史代码；\n\n#### 【参考】对于注释的要求：第一、能够准确反应设计思想和代码逻辑 ； 第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。\n完全没有注释的大段代码对于阅读者形同\n天书，注释是给自己看的，即使隔很长时间，也能清晰理解当时的思路 ； 注释也是给继任者看\n的，使其能够快速接替自己的工作。\n\n#### 【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。\n1 ） 待办事宜 （TODO） : （ 标记人，标记时间， [ 预计处理时间 ]）\n表示需要实现，但目前还未实现的功能。这实际上是一个 Javadoc 的标签，目前的 Javadoc\n还没有实现，但已经被广泛使用。只能应用于类，接口和方法 （ 因为它是一个 Javadoc 标签 ） 。\n2 ） 错误，不能工作 （FIXME） : （ 标记人，标记时间， [ 预计处理时间 ]）\n在注释中用 FIXME 标记某代码是错误的，而且不能工作，需要及时纠正的情况。\n\n## 其它\n\n#### 【强制】在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。\n说明：不要在方法体内定义： Pattern pattern =  Pattern . compile( 规则 );\n\n#### 【强制】后台输送给页面的变量必须加 $!{var} ——中间的感叹号。\n说明：如果 var = null 或者不存在，那么 ${var} 会直接显示在页面上。\n思考：这里是 Velocity 模板引擎的一些内容，Velocity是一个基于Java的模板引擎，通过特定的语法，Velocity可以获取在java语言中定义的对象，从而实现界面和java代码的真正分离，这意味着可以使用velocity替代jsp的开发模式了。这使得前端开发人员可以和 Java 程序开发人员同步开发一个遵循 MVC 架构的 web 站点，在实际应用中，velocity还可以应用于很多其他的场景。，比如源代码生成、自动email和转换xml等。\n\n#### 【强制】注意  Math . random() 这个方法返回是 double 类型，注意取值的范围 0≤ x <1 （ 能够取到零值，注意除零异常 ） ，如果想获取整数类型的随机数，不要将 x 放大 10 的若干倍然后取整，直接使用 Random 对象的 nextInt 或者 nextLong 方法。\n\n#### 【强制】获取当前毫秒数 System . currentTimeMillis(); 而不是 new Date() . getTime();\n说明：如果想获取更加精确的纳秒级时间值，用 System . nanoTime() 。在 JDK 8 中，针对统计时间等场景，推荐使用 Instant类。\n\n#### 【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。\n\n#### 【推荐】对于“明确停止使用的代码和配置”，如方法、变量、类、配置文件、动态配置属性等要坚决从程序中清理出去，避免造成过多垃圾。 \n\n","slug":"阿里巴巴Java开发手册学习笔记","published":1,"updated":"2017-03-24T09:04:00.718Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfx70036psguqwzpts96","content":"<p>本人自认为代码还算写得比较符合规范，但是相信阿里会有总结更深的见解，因此对自己之前没有注意到的，自己没有遵循的，做一个笔记与整理，以加深理解。</p>\n<h1 id=\"编程规约\"><a href=\"#编程规约\" class=\"headerlink\" title=\"编程规约\"></a>编程规约</h1><h2 id=\"命名规约\"><a href=\"#命名规约\" class=\"headerlink\" title=\"命名规约\"></a>命名规约</h2><h4 id=\"【强制】常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。\"><a href=\"#【强制】常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。\" class=\"headerlink\" title=\"【强制】常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。\"></a>【强制】常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。</h4><p>重点在，不要嫌名字长，我以前写得比较短<br>正例： <code>MAX _ STOCK _ COUNT</code></p>\n<h4 id=\"【强制】抽象类命名使用-Abstract-或-Base-开头-；-异常类命名使用-Exception-结尾-；-测试类命名以它要测试的类的名称开始，以-Test-结尾。\"><a href=\"#【强制】抽象类命名使用-Abstract-或-Base-开头-；-异常类命名使用-Exception-结尾-；-测试类命名以它要测试的类的名称开始，以-Test-结尾。\" class=\"headerlink\" title=\"【强制】抽象类命名使用 Abstract 或 Base 开头 ； 异常类命名使用 Exception 结尾 ； 测试类命名以它要测试的类的名称开始，以 Test 结尾。\"></a>【强制】抽象类命名使用 Abstract 或 Base 开头 ； 异常类命名使用 Exception 结尾 ； 测试类命名以它要测试的类的名称开始，以 Test 结尾。</h4><p>如果是接口的话，如何？</p>\n<h4 id=\"接口和实现类的命名有两套规则\"><a href=\"#接口和实现类的命名有两套规则\" class=\"headerlink\" title=\"接口和实现类的命名有两套规则\"></a>接口和实现类的命名有两套规则</h4><blockquote>\n<p>1 ） 【强制】对于 Service 和 DAO 类，基于 SOA 的理念，暴露出来的服务一定是接口，内部<br>的实现类用 Impl 的后缀与接口区别。<br>正例： CacheServiceImpl 实现 CacheService 接口。<br>2 ）  【推荐】 如果是形容能力的接口名称，取对应的形容词做接口名 （ 通常是– able 的形式 ） 。<br>正例： AbstractTranslator 实现  Translatable 。</p>\n</blockquote>\n<p>一定注意，之前写scala的时候，并没有考虑到这一点，DAO和Service暴露出来一定是接口，然后用Impl后缀的实现类实现。</p>\n<h4 id=\"【参考】枚举类名建议带上-Enum-后缀，枚举成员名称需要全大写，单词间用下划线隔开。\"><a href=\"#【参考】枚举类名建议带上-Enum-后缀，枚举成员名称需要全大写，单词间用下划线隔开。\" class=\"headerlink\" title=\"【参考】枚举类名建议带上 Enum 后缀，枚举成员名称需要全大写，单词间用下划线隔开。\"></a>【参考】枚举类名建议带上 Enum 后缀，枚举成员名称需要全大写，单词间用下划线隔开。</h4><blockquote>\n<p>说明：枚举其实就是特殊的常量类，且构造方法被默认强制是私有。<br>正例：枚举名字： DealStatusEnum， 成员名称： SUCCESS /  UNKOWN _ REASON 。</p>\n</blockquote>\n<h4 id=\"【强制】-POJO-类中布尔类型的变量，都不要加-is-，否则部分框架解析会引起序列化错误。\"><a href=\"#【强制】-POJO-类中布尔类型的变量，都不要加-is-，否则部分框架解析会引起序列化错误。\" class=\"headerlink\" title=\"【强制】 POJO 类中布尔类型的变量，都不要加 is ，否则部分框架解析会引起序列化错误。\"></a>【强制】 POJO 类中布尔类型的变量，都不要加 is ，否则部分框架解析会引起序列化错误。</h4><blockquote>\n<p>反例：定义为基本数据类型 boolean isSuccess； 的属性，它的方法也是 isSuccess() ， RPC<br>框架在反向解析的时候，“以为”对应的属性名称是 success ，导致属性获取不到，进而抛出异<br>常。</p>\n</blockquote>\n<h4 id=\"【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。\"><a href=\"#【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。\" class=\"headerlink\" title=\"【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。\"></a>【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。</h4><p>点分隔符之间仅有一个自然语义的英语单词，包名统一使用单数形式，但类名可以使用复数形式</p>\n<h4 id=\"【强制】杜绝完全不规范的缩写，避免望文不知义\"><a href=\"#【强制】杜绝完全不规范的缩写，避免望文不知义\" class=\"headerlink\" title=\"【强制】杜绝完全不规范的缩写，避免望文不知义\"></a>【强制】杜绝完全不规范的缩写，避免望文不知义</h4><blockquote>\n<p>反例：  AbstractClass “缩写”命名成 AbsClass；condition “缩写”命名成  condi ，此类<br>随意缩写严重降低了代码的可阅读性。</p>\n</blockquote>\n<p>的确是，一定要注意</p>\n<h4 id=\"【推荐】如果使用到了设计模式，建议在类名中体现出具体模式。\"><a href=\"#【推荐】如果使用到了设计模式，建议在类名中体现出具体模式。\" class=\"headerlink\" title=\"【推荐】如果使用到了设计模式，建议在类名中体现出具体模式。\"></a>【推荐】如果使用到了设计模式，建议在类名中体现出具体模式。</h4><blockquote>\n<p>说明：将设计模式体现在名字中，有利于阅读者快速理解架构设计思想。<br>正例： public class OrderFactory;<br>public class LoginProxy;<br>public class ResourceObserver;</p>\n</blockquote>\n<h4 id=\"【参考】各层命名规约：\"><a href=\"#【参考】各层命名规约：\" class=\"headerlink\" title=\"【参考】各层命名规约：\"></a>【参考】各层命名规约：</h4><blockquote>\n<p>A) Service / DAO 层方法命名规约<br>1 ） 获取单个对象的方法用 get 做前缀。<br>2 ） 获取多个对象的方法用 list 做前缀。<br>3 ） 获取统计值的方法用 count 做前缀。<br>4 ） 插入的方法用 save（ 推荐 ） 或 insert 做前缀。<br>5 ） 删除的方法用 remove（ 推荐 ） 或 delete 做前缀。<br>6 ） 修改的方法用 update 做前缀。<br>B) 领域模型命名规约<br>1 ） 数据对象： xxxDO ， xxx 即为数据表名。<br>2 ） 数据传输对象： xxxDTO ， xxx 为业务领域相关的名称。<br>3 ） 展示对象： xxxVO ， xxx 一般为网页名称。<br>4 ） POJO 是 DO / DTO / BO / VO 的统称，禁止命名成 xxxPOJO 。</p>\n</blockquote>\n<p>到时候命名的时候再查阅一遍</p>\n<h4 id=\"【推荐】接口类中的方法和属性不要加任何修饰符号-（public-也不要加-）-，保持代码的简洁性，并加上有效的-Javadoc-注释。尽量不要在接口里定义变量，如果一定要定义变量，肯定是与接口方法相关，并且是整个应用的基础常量。\"><a href=\"#【推荐】接口类中的方法和属性不要加任何修饰符号-（public-也不要加-）-，保持代码的简洁性，并加上有效的-Javadoc-注释。尽量不要在接口里定义变量，如果一定要定义变量，肯定是与接口方法相关，并且是整个应用的基础常量。\" class=\"headerlink\" title=\"【推荐】接口类中的方法和属性不要加任何修饰符号 （public 也不要加 ） ，保持代码的简洁性，并加上有效的 Javadoc 注释。尽量不要在接口里定义变量，如果一定要定义变量，肯定是与接口方法相关，并且是整个应用的基础常量。\"></a>【推荐】接口类中的方法和属性不要加任何修饰符号 （public 也不要加 ） ，保持代码的简洁性，并加上有效的 Javadoc 注释。尽量不要在接口里定义变量，如果一定要定义变量，肯定是与接口方法相关，并且是整个应用的基础常量。</h4><blockquote>\n<p>正例：接口方法签名： void f();<br>接口基础常量表示： String COMPANY = “ alibaba “ ;<br>反例：接口方法定义： public abstract void f();<br>说明： JDK 8 中接口允许有默认实现，那么这个 default 方法，是对所有实现类都有价值的默认实现。</p>\n</blockquote>\n<p>定义接口的时候，方法和属性，不要加任何修饰符号，注释要写清楚</p>\n<h2 id=\"常量定义\"><a href=\"#常量定义\" class=\"headerlink\" title=\"常量定义\"></a>常量定义</h2><h4 id=\"【强制】不允许出现任何魔法值-（-即未经定义的常量-）-直接出现在代码中。\"><a href=\"#【强制】不允许出现任何魔法值-（-即未经定义的常量-）-直接出现在代码中。\" class=\"headerlink\" title=\"【强制】不允许出现任何魔法值 （ 即未经定义的常量 ） 直接出现在代码中。\"></a>【强制】不允许出现任何魔法值 （ 即未经定义的常量 ） 直接出现在代码中。</h4><p>反例：  String key =”Id#taobao_”+ tradeId；<br>cache . put(key ,  value);</p>\n<p>为什么要这样做？</p>\n<h4 id=\"如果变量值仅在一个范围内变化用-Enum-类。如果还带有名称之外的延伸属性，必须使用-Enum-类，下面正例中的数字就是延伸信息，表示星期几。\"><a href=\"#如果变量值仅在一个范围内变化用-Enum-类。如果还带有名称之外的延伸属性，必须使用-Enum-类，下面正例中的数字就是延伸信息，表示星期几。\" class=\"headerlink\" title=\"如果变量值仅在一个范围内变化用 Enum 类。如果还带有名称之外的延伸属性，必须使用 Enum 类，下面正例中的数字就是延伸信息，表示星期几。\"></a>如果变量值仅在一个范围内变化用 Enum 类。如果还带有名称之外的延伸属性，必须使用 Enum 类，下面正例中的数字就是延伸信息，表示星期几。</h4><p>正例： public Enum {  MONDAY( 1 ) ,  TUESDAY( 2 ) ,  WEDNESDAY( 3 ) ,  THURSDAY( 4 ) ,  FRIDAY( 5 ) ,SATURDAY( 6 ) ,  SUNDAY( 7 ); }</p>\n<h2 id=\"格式规范\"><a href=\"#格式规范\" class=\"headerlink\" title=\"格式规范\"></a>格式规范</h2><h4 id=\"【强制】单行字符数限制不超过-120-个，超出需要换行，换行时遵循如下原则：\"><a href=\"#【强制】单行字符数限制不超过-120-个，超出需要换行，换行时遵循如下原则：\" class=\"headerlink\" title=\"【强制】单行字符数限制不超过 120 个，超出需要换行，换行时遵循如下原则：\"></a>【强制】单行字符数限制不超过 120 个，超出需要换行，换行时遵循如下原则：</h4><p>1） 第二行相对第一行缩进 4 个空格，从第三行开始，不再继续缩进，参考示例。<br>2 ） 运算符与下文一起换行。<br>3 ） 方法调用的点符号与下文一起换行。<br>4 ） 在多个参数超长，逗号后进行换行。<br>5 ） 在括号前不要换行，见反例。</p>\n<p>我之前是不能超过90个，目的是笔记本电脑看代码的时候比较方便，现在涨到120个了</p>\n<h4 id=\"【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。\"><a href=\"#【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。\" class=\"headerlink\" title=\"【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。\"></a>【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。</h4><p>说明：没有必要插入多行空格进行隔开。不能有多插几行的强迫症</p>\n<h2 id=\"OOP规约\"><a href=\"#OOP规约\" class=\"headerlink\" title=\"OOP规约\"></a>OOP规约</h2><h4 id=\"【强制】避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。\"><a href=\"#【强制】避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。\" class=\"headerlink\" title=\"【强制】避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。\"></a>【强制】避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。</h4><h4 id=\"【强制】所有的覆写方法，必须加-Override-注解。\"><a href=\"#【强制】所有的覆写方法，必须加-Override-注解。\" class=\"headerlink\" title=\"【强制】所有的覆写方法，必须加 @Override 注解。\"></a>【强制】所有的覆写方法，必须加 @Override 注解。</h4><p>反例： getObject() 与 get0bject() 的问题。一个是字母的 O ，一个是数字的 0，加@Override<br>可以准确判断是否覆盖成功。另外，如果在抽象类中对方法签名进行修改，其实现类会马上编<br>译报错。</p>\n<h4 id=\"【强制】相同参数类型，相同业务含义，才可以使用-Java-的可变参数，避免使用-Object-。\"><a href=\"#【强制】相同参数类型，相同业务含义，才可以使用-Java-的可变参数，避免使用-Object-。\" class=\"headerlink\" title=\"【强制】相同参数类型，相同业务含义，才可以使用 Java 的可变参数，避免使用 Object 。\"></a>【强制】相同参数类型，相同业务含义，才可以使用 Java 的可变参数，避免使用 Object 。</h4><p>说明：可变参数必须放置在参数列表的最后。 （ 提倡同学们尽量不用可变参数编程 ）<br>正例： public User getUsers(String type, Integer… ids)</p>\n<h4 id=\"【强制】对外暴露的接口签名，原则上不允许修改方法签名，避免对接口调用方产生影响。\"><a href=\"#【强制】对外暴露的接口签名，原则上不允许修改方法签名，避免对接口调用方产生影响。\" class=\"headerlink\" title=\"【强制】对外暴露的接口签名，原则上不允许修改方法签名，避免对接口调用方产生影响。\"></a>【强制】对外暴露的接口签名，原则上不允许修改方法签名，避免对接口调用方产生影响。</h4><p>接口过时必须加@Deprecated 注解，并清晰地说明采用的新接口或者新服务是什么。</p>\n<h4 id=\"【强制】不能使用过时的类或方法。\"><a href=\"#【强制】不能使用过时的类或方法。\" class=\"headerlink\" title=\"【强制】不能使用过时的类或方法。\"></a>【强制】不能使用过时的类或方法。</h4><h4 id=\"【强制】-Object-的-equals-方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals-。\"><a href=\"#【强制】-Object-的-equals-方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals-。\" class=\"headerlink\" title=\"【强制】 Object 的 equals 方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals 。\"></a>【强制】 Object 的 equals 方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals 。</h4><p>正例： “ test “ .equals(object);<br>反例：  object.equals( “ test “ );<br>说明：推荐使用 java . util . Objects # equals （JDK 7 引入的工具类 ）</p>\n<h4 id=\"【强制】所有的相同类型的包装类对象之间值的比较，全部使用-equals-方法比较。\"><a href=\"#【强制】所有的相同类型的包装类对象之间值的比较，全部使用-equals-方法比较。\" class=\"headerlink\" title=\"【强制】所有的相同类型的包装类对象之间值的比较，全部使用 equals 方法比较。\"></a>【强制】所有的相同类型的包装类对象之间值的比较，全部使用 equals 方法比较。</h4><p>说明：对于 Integer var =?在-128 至 127 之间的赋值， Integer 对象是在<br>IntegerCache . cache 产生，会复用已有对象，这个区间内的 Integer 值可以直接使用==进行<br>判断，但是这个区间之外的所有数据，都会在堆上产生，并不会复用已有对象，这是一个大坑，<br>推荐使用 equals 方法进行判断。</p>\n<p>为什么这是个大坑？？？因为使用 == 比较的话，会判断内存地址是否相同，这样会返回False</p>\n<h4 id=\"8-【强制】关于基本数据类型与包装数据类型的使用标准如下：\"><a href=\"#8-【强制】关于基本数据类型与包装数据类型的使用标准如下：\" class=\"headerlink\" title=\"8. 【强制】关于基本数据类型与包装数据类型的使用标准如下：\"></a>8. 【强制】关于基本数据类型与包装数据类型的使用标准如下：</h4><p>1 ） 所有的 POJO 类属性必须使用包装数据类型。<br>2 ） RPC 方法的返回值和参数必须使用包装数据类型。<br>3 ） 所有的局部变量【推荐】使用基本数据类型。</p>\n<p>包装数据类型的 null 值，能够表示额外的信息，如：远程调用失败，异常退出。</p>\n<h4 id=\"【强制】定义-DO-DTO-VO-等-POJO-类时，不要设定任何属性默认值。\"><a href=\"#【强制】定义-DO-DTO-VO-等-POJO-类时，不要设定任何属性默认值。\" class=\"headerlink\" title=\"【强制】定义 DO / DTO / VO 等 POJO 类时，不要设定任何属性默认值。\"></a>【强制】定义 DO / DTO / VO 等 POJO 类时，不要设定任何属性默认值。</h4><p>反例： POJO 类的 gmtCreate 默认值为 new Date(); 但是这个属性在数据提取时并没有置入具<br>体值，在更新其它字段时又附带更新了此字段，导致创建时间被修改成当前时间。</p>\n<h4 id=\"【强制】构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在-init-方法中。\"><a href=\"#【强制】构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在-init-方法中。\" class=\"headerlink\" title=\"【强制】构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在 init 方法中。\"></a>【强制】构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在 init 方法中。</h4><h4 id=\"【强制】-POJO-类必须写-toString-方法。使用-IDE-的中工具：-source-gt-generate-toString时，如果继承了另一个-POJO-类，注意在前面加一下-super-toString-。\"><a href=\"#【强制】-POJO-类必须写-toString-方法。使用-IDE-的中工具：-source-gt-generate-toString时，如果继承了另一个-POJO-类，注意在前面加一下-super-toString-。\" class=\"headerlink\" title=\"【强制】 POJO 类必须写 toString 方法。使用 IDE 的中工具： source &gt;  generate toString时，如果继承了另一个 POJO 类，注意在前面加一下 super . toString 。\"></a>【强制】 POJO 类必须写 toString 方法。使用 IDE 的中工具： source &gt;  generate toString时，如果继承了另一个 POJO 类，注意在前面加一下 super . toString 。</h4><p>说明：在方法执行抛出异常时，可以直接调用 POJO 的 toString() 方法打印其属性值，便于排<br>查问题。</p>\n<h4 id=\"【推荐】使用索引访问用-String-的-split-方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛-IndexOutOfBoundsException-的风险。\"><a href=\"#【推荐】使用索引访问用-String-的-split-方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛-IndexOutOfBoundsException-的风险。\" class=\"headerlink\" title=\"【推荐】使用索引访问用 String 的 split 方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛 IndexOutOfBoundsException 的风险。\"></a>【推荐】使用索引访问用 String 的 split 方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛 IndexOutOfBoundsException 的风险。</h4><p>说明：<br>String str = “a,b,c,,”;<br>String[] ary = str.split(“,”);<br>//预期大于 3，结果是 3<br>System.out.println(ary.length);</p>\n<h4 id=\"【推荐】类内方法定义顺序依次是：公有方法或保护方法-gt-私有方法-gt-getter-setter方法。\"><a href=\"#【推荐】类内方法定义顺序依次是：公有方法或保护方法-gt-私有方法-gt-getter-setter方法。\" class=\"headerlink\" title=\"【推荐】类内方法定义顺序依次是：公有方法或保护方法 &gt; 私有方法 &gt;  getter / setter方法。\"></a>【推荐】类内方法定义顺序依次是：公有方法或保护方法 &gt; 私有方法 &gt;  getter / setter方法。</h4><p>说明：构造方法放到最前面，公有方法是类的调用者和维护者最关心的方法，首屏展示最好 ； 保护方法虽然只是子类关心，也可能是“模板设计模式”下的核心方法 ； 而私有方法外部一般不需要特别关心，是一个黑盒实现 ； 因为方法信息价值较低，所有 Service 和 DAO 的 getter / setter 方法放在类体最后。</p>\n<h4 id=\"【推荐】在getter-setter-方法中，尽量不要增加业务逻辑，增加排查问题的难度。\"><a href=\"#【推荐】在getter-setter-方法中，尽量不要增加业务逻辑，增加排查问题的难度。\" class=\"headerlink\" title=\"【推荐】在getter / setter 方法中，尽量不要增加业务逻辑，增加排查问题的难度。\"></a>【推荐】在getter / setter 方法中，尽量不要增加业务逻辑，增加排查问题的难度。</h4><h4 id=\"【推荐】循环体内，字符串的联接方式，使用-StringBuilder-的-append-方法进行扩展。\"><a href=\"#【推荐】循环体内，字符串的联接方式，使用-StringBuilder-的-append-方法进行扩展。\" class=\"headerlink\" title=\"【推荐】循环体内，字符串的联接方式，使用 StringBuilder 的 append 方法进行扩展。\"></a>【推荐】循环体内，字符串的联接方式，使用 StringBuilder 的 append 方法进行扩展。</h4><h4 id=\"【推荐】-final-可提高程序响应效率，声明成-final-的情况：\"><a href=\"#【推荐】-final-可提高程序响应效率，声明成-final-的情况：\" class=\"headerlink\" title=\"【推荐】 final 可提高程序响应效率，声明成 final 的情况：\"></a>【推荐】 final 可提高程序响应效率，声明成 final 的情况：</h4><p>1 ） 不需要重新赋值的变量，包括类属性、局部变量。<br>2 ） 对象参数前加 final ，表示不允许修改引用的指向。<br>3 ） 类方法确定不允许被重写。</p>\n<h4 id=\"【推荐】慎用-Object-的-clone-方法来拷贝对象。\"><a href=\"#【推荐】慎用-Object-的-clone-方法来拷贝对象。\" class=\"headerlink\" title=\"【推荐】慎用 Object 的 clone 方法来拷贝对象。\"></a>【推荐】慎用 Object 的 clone 方法来拷贝对象。</h4><p>说明：对象的 clone 方法默认是浅拷贝，若想实现深拷贝需要重写 clone 方法实现属性对象<br>的拷贝。</p>\n<h4 id=\"【推荐】类成员与方法访问控制从严：\"><a href=\"#【推荐】类成员与方法访问控制从严：\" class=\"headerlink\" title=\"【推荐】类成员与方法访问控制从严：\"></a>【推荐】类成员与方法访问控制从严：</h4><p>1 ） 如果不允许外部直接通过 new 来创建对象，那么构造方法必须是 private 。<br>2 ） 工具类不允许有 public 或 default 构造方法。<br>3 ） 类非 static 成员变量并且与子类共享，必须是 protected 。<br>4 ） 类非 static 成员变量并且仅在本类使用，必须是 private 。<br>5 ） 类 static 成员变量如果仅在本类使用，必须是 private 。<br>6 ） 若是 static 成员变量，必须考虑是否为 final 。<br>7 ） 类成员方法只供类内部调用，必须是 private 。<br>8 ） 类成员方法只对继承类公开，那么限制为 protected 。<br>说明：任何类、方法、参数、变量，严控访问范围。过宽泛的访问范围，不利于模块解耦。<br>思<br>考：如果是一个 private 的方法，想删除就删除，可是一个 public 的 Service 方法，或者一<br>个 public 的成员变量，删除一下，不得手心冒点汗吗？变量像自己的小孩，尽量在自己的视<br>线内，变量作用域太大，如果无限制的到处跑，那么你会担心的。</p>\n<p><strong>这一点是非常重要的习惯，一定要加以养成</strong></p>\n<h2 id=\"集合处理\"><a href=\"#集合处理\" class=\"headerlink\" title=\"集合处理\"></a>集合处理</h2><h4 id=\"【强制】关于-hashCode-和-equals-的处理，遵循如下规则：\"><a href=\"#【强制】关于-hashCode-和-equals-的处理，遵循如下规则：\" class=\"headerlink\" title=\"【强制】关于 hashCode 和 equals 的处理，遵循如下规则：\"></a>【强制】关于 hashCode 和 equals 的处理，遵循如下规则：</h4><p>1） 只要重写 equals ，就必须重写 hashCode 。<br>2） 因为 Set 存储的是不重复的对象，依据 hashCode 和 equals 进行判断，所以 Set 存储的<br>对象必须重写这两个方法。<br>3） 如果自定义对象做为 Map 的键，那么必须重写 hashCode 和 equals 。<br>正例： String 重写了 hashCode 和 equals 方法，所以我们可以非常愉快地使用 String 对象<br>作为 key 来使用。</p>\n<h4 id=\"【强制】-ArrayList-的-subList-结果不可强转成-ArrayList-，否则会抛出-ClassCastException\"><a href=\"#【强制】-ArrayList-的-subList-结果不可强转成-ArrayList-，否则会抛出-ClassCastException\" class=\"headerlink\" title=\"【强制】  ArrayList 的 subList 结果不可强转成 ArrayList ，否则会抛出 ClassCastException\"></a>【强制】  ArrayList 的 subList 结果不可强转成 ArrayList ，否则会抛出 ClassCastException</h4><p>异常： java . util . RandomAccessSubList cannot be cast to java . util . ArrayList ;<br>说明： subList 返回的是  ArrayList 的内部类  SubList ，并不是  ArrayList ，而是<br>ArrayList 的一个视图，对于 SubList 子列表的所有操作最终会反映到原列表上。</p>\n<h4 id=\"【强制】-在-subList-场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生-ConcurrentModificationException-异常。\"><a href=\"#【强制】-在-subList-场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生-ConcurrentModificationException-异常。\" class=\"headerlink\" title=\"【强制】 在 subList 场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生 ConcurrentModificationException 异常。\"></a>【强制】 在 subList 场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生 ConcurrentModificationException 异常。</h4><h4 id=\"【强制】使用集合转数组的方法，必须使用集合的-toArray-T-array-，传入的是类型完全一样的数组，大小就是-list-size-。\"><a href=\"#【强制】使用集合转数组的方法，必须使用集合的-toArray-T-array-，传入的是类型完全一样的数组，大小就是-list-size-。\" class=\"headerlink\" title=\"【强制】使用集合转数组的方法，必须使用集合的 toArray(T[] array) ，传入的是类型完全一样的数组，大小就是 list . size() 。\"></a>【强制】使用集合转数组的方法，必须使用集合的 toArray(T[] array) ，传入的是类型完全一样的数组，大小就是 list . size() 。</h4><p>反例：直接使用 toArray 无参方法存在问题，此方法返回值只能是 Object[] 类，若强转其它<br>类型数组将出现 ClassCastException 错误。<br>正例：<br>List<string> list = new ArrayList<string>(2);<br>list.add(“guan”);<br>list.add(“bao”);<br>String[] array = new String[list.size()];<br>array = list.toArray(array);<br>说明：使用 toArray 带参方法，入参分配的数组空间不够大时， toArray 方法内部将重新分配<br>内存空间，并返回新数组地址 ； 如果数组元素大于实际所需，下标为 [ list . size() ] 的数组<br>元素将被置为 null ，其它数组元素保持原值，因此最好将方法入参数组大小定义与集合元素<br>个数一致。</string></string></p>\n<p>toArray 方法，一定要带参数，指定类型和大小；</p>\n<h4 id=\"【强制】使用工具类-Arrays-asList-把数组转换成集合时，不能使用其修改集合相关的方法，它的-add-remove-clear-方法会抛出-UnsupportedOperationException-异常。\"><a href=\"#【强制】使用工具类-Arrays-asList-把数组转换成集合时，不能使用其修改集合相关的方法，它的-add-remove-clear-方法会抛出-UnsupportedOperationException-异常。\" class=\"headerlink\" title=\"【强制】使用工具类 Arrays . asList() 把数组转换成集合时，不能使用其修改集合相关的方法，它的 add / remove / clear 方法会抛出 UnsupportedOperationException 异常。\"></a>【强制】使用工具类 Arrays . asList() 把数组转换成集合时，不能使用其修改集合相关的方法，它的 add / remove / clear 方法会抛出 UnsupportedOperationException 异常。</h4><p>说明： asList 的返回对象是一个 Arrays 内部类，并没有实现集合的修改方法。 Arrays . asList<br>体现的是适配器模式，只是转换接口，后台的数据仍是数组。<br>String[] str = new String[] { “a”, “b” };<br>List list = Arrays.asList(str);<br>第一种情况： list.add(“c”);  运行时异常。<br>第二种情况： str[0]= “gujin”; 那么 list.get(0) 也会随之修改。</p>\n<h4 id=\"【强制】泛型通配符-lt-extends-T-gt-来接收返回的数据，此写法的泛型集合不能使用-add-方法。\"><a href=\"#【强制】泛型通配符-lt-extends-T-gt-来接收返回的数据，此写法的泛型集合不能使用-add-方法。\" class=\"headerlink\" title=\"【强制】泛型通配符&lt;?  extends T &gt;来接收返回的数据，此写法的泛型集合不能使用 add 方法。\"></a>【强制】泛型通配符&lt;?  extends T &gt;来接收返回的数据，此写法的泛型集合不能使用 add 方法。</h4><p>说明：苹果装箱后返回一个<code>&lt;?  extends Fruits &gt;</code> 对象，此对象就不能往里加任何水果，包括苹果。</p>\n<h4 id=\"【强制】不要在-foreach-循环里进行元素的-remove-add-操作。-remove-元素请使用-Iterator方式，如果并发操作，需要对-Iterator-对象加锁。\"><a href=\"#【强制】不要在-foreach-循环里进行元素的-remove-add-操作。-remove-元素请使用-Iterator方式，如果并发操作，需要对-Iterator-对象加锁。\" class=\"headerlink\" title=\"【强制】不要在 foreach 循环里进行元素的 remove / add 操作。 remove 元素请使用 Iterator方式，如果并发操作，需要对 Iterator 对象加锁。\"></a>【强制】不要在 foreach 循环里进行元素的 remove / add 操作。 remove 元素请使用 Iterator方式，如果并发操作，需要对 Iterator 对象加锁。</h4><p>这个坑我以前踩过，如果早点看到就不会踩到了。。。</p>\n<h4 id=\"【强制】-在-JDK-7-版本以上，-Comparator-要满足自反性，传递性，对称性，不然-Arrays-sort-，Collections-sort-会报-IllegalArgumentException-异常。\"><a href=\"#【强制】-在-JDK-7-版本以上，-Comparator-要满足自反性，传递性，对称性，不然-Arrays-sort-，Collections-sort-会报-IllegalArgumentException-异常。\" class=\"headerlink\" title=\"【强制】 在 JDK 7 版本以上， Comparator 要满足自反性，传递性，对称性，不然 Arrays.sort ，Collections.sort 会报 IllegalArgumentException 异常。\"></a>【强制】 在 JDK 7 版本以上， Comparator 要满足自反性，传递性，对称性，不然 Arrays.sort ，Collections.sort 会报 IllegalArgumentException 异常。</h4><p>说明：<br>1 ） 自反性： x ， y 的比较结果和 y ， x 的比较结果相反。<br>2 ） 传递性： x &gt; y , y &gt; z ,则 x &gt; z 。<br>3 ） 对称性： x = y ,则 x , z 比较结果和 y ， z 比较结果相同。</p>\n<h4 id=\"【推荐】集合初始化时，尽量指定集合初始值大小。\"><a href=\"#【推荐】集合初始化时，尽量指定集合初始值大小。\" class=\"headerlink\" title=\"【推荐】集合初始化时，尽量指定集合初始值大小。\"></a>【推荐】集合初始化时，尽量指定集合初始值大小。</h4><p>说明： ArrayList 尽量使用 ArrayList(int initialCapacity) 初始化。</p>\n<h4 id=\"【推荐】使用-entrySet-遍历-Map-类集合-KV-，而不是-keySet-方式进行遍历。\"><a href=\"#【推荐】使用-entrySet-遍历-Map-类集合-KV-，而不是-keySet-方式进行遍历。\" class=\"headerlink\" title=\"【推荐】使用 entrySet 遍历 Map 类集合 KV ，而不是 keySet 方式进行遍历。\"></a>【推荐】使用 entrySet 遍历 Map 类集合 KV ，而不是 keySet 方式进行遍历。</h4><p>说明： keySet 其实是遍历了 2 次，一次是转为 Iterator 对象，另一次是从 hashMap 中取出key 所对应的 value 。而 entrySet 只是遍历了一次就把 key 和 value 都放到了 entry 中，效率更高。如果是 JDK 8，使用 Map . foreach 方法。<br>正例： values() 返回的是 V 值集合，是一个 list 集合对象 ；keySet() 返回的是 K 值集合，是一个 Set 集合对象；entrySet() 返回的是 K - V 值组合集合。</p>\n<h4 id=\"【推荐】高度注意-Map-类集合-K-V-能不能存储-null-值的情况，如下表格：\"><a href=\"#【推荐】高度注意-Map-类集合-K-V-能不能存储-null-值的情况，如下表格：\" class=\"headerlink\" title=\"【推荐】高度注意 Map 类集合 K / V 能不能存储 null 值的情况，如下表格：\"></a>【推荐】高度注意 Map 类集合 K / V 能不能存储 null 值的情况，如下表格：</h4><table>\n<thead>\n<tr>\n<th>集合类</th>\n<th>Key</th>\n<th>Value</th>\n<th>Super</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Hashtable</td>\n<td>不允许为 null</td>\n<td>不允许为 null</td>\n<td>Dictionary</td>\n<td>线程安全</td>\n</tr>\n<tr>\n<td>ConcurrentHashMap</td>\n<td>不允许为 null</td>\n<td>不允许为 null</td>\n<td>AbstractMap</td>\n<td>分段锁技术</td>\n</tr>\n<tr>\n<td>TreeMap</td>\n<td>不允许为 null</td>\n<td>允许为 null</td>\n<td>AbstractMap</td>\n<td>线程不安全</td>\n</tr>\n<tr>\n<td>HashMap</td>\n<td>允许为 null</td>\n<td>允许为 null</td>\n<td>AbstractMap</td>\n<td>线程不安全</td>\n</tr>\n</tbody>\n</table>\n<p>反例： 由于 HashMap 的干扰，很多人认为 ConcurrentHashMap 是可以置入 null 值，注意存储null 值时会抛出 NPE 异常。</p>\n<h4 id=\"【参考】合理利用好集合的有序性-sort-和稳定性-order-，避免集合的无序性-unsort-和不稳定性-unorder-带来的负面影响。\"><a href=\"#【参考】合理利用好集合的有序性-sort-和稳定性-order-，避免集合的无序性-unsort-和不稳定性-unorder-带来的负面影响。\" class=\"headerlink\" title=\"【参考】合理利用好集合的有序性 (sort) 和稳定性 (order) ，避免集合的无序性 (unsort) 和不稳定性 (unorder) 带来的负面影响。\"></a>【参考】合理利用好集合的有序性 (sort) 和稳定性 (order) ，避免集合的无序性 (unsort) 和不稳定性 (unorder) 带来的负面影响。</h4><p>说明：稳定性指集合每次遍历的元素次序是一定的。有序性是指遍历的结果是按某种比较规则依次排列的。如： ArrayList 是 order / unsort；HashMap 是 unorder / unsort；TreeSet 是order / sort 。</p>\n<h4 id=\"【参考】利用-Set-元素唯一的特性，可以快速对一个集合进行去重操作，避免使用-List-的contains-方法进行遍历、对比、去重操作。\"><a href=\"#【参考】利用-Set-元素唯一的特性，可以快速对一个集合进行去重操作，避免使用-List-的contains-方法进行遍历、对比、去重操作。\" class=\"headerlink\" title=\"【参考】利用 Set 元素唯一的特性，可以快速对一个集合进行去重操作，避免使用 List 的contains 方法进行遍历、对比、去重操作。\"></a>【参考】利用 Set 元素唯一的特性，可以快速对一个集合进行去重操作，避免使用 List 的contains 方法进行遍历、对比、去重操作。</h4><p>自评：这个好：使用 s.addAll(list); 方法。</p>\n<h2 id=\"并发处理\"><a href=\"#并发处理\" class=\"headerlink\" title=\"并发处理\"></a>并发处理</h2><h4 id=\"【强制】获取单例对象需要保证线程安全，其中的方法也要保证线程安全。\"><a href=\"#【强制】获取单例对象需要保证线程安全，其中的方法也要保证线程安全。\" class=\"headerlink\" title=\"【强制】获取单例对象需要保证线程安全，其中的方法也要保证线程安全。\"></a>【强制】获取单例对象需要保证线程安全，其中的方法也要保证线程安全。</h4><p>说明：资源驱动类、工具类、单例工厂类都需要注意。</p>\n<h4 id=\"【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。\"><a href=\"#【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。\" class=\"headerlink\" title=\"【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。\"></a>【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TimerTaskThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">TimerTaskThread</span><span class=\"params\">()</span></span>&#123;</div><div class=\"line\">\t<span class=\"keyword\">super</span>.setName(<span class=\"string\">\"TimerTaskThread\"</span>); ...</div><div class=\"line\">\t&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\"><a href=\"#【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\" class=\"headerlink\" title=\"【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\"></a>【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。</h4><p>说明：使用线程池的好处是减少在创建和销毁线程上所花的时间以及系统资源的开销，解决资<br>源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者<br>“过度切换”的问题。</p>\n<h4 id=\"【强制】线程池不允许使用-Executors-去创建，而是通过-ThreadPoolExecutor-的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。\"><a href=\"#【强制】线程池不允许使用-Executors-去创建，而是通过-ThreadPoolExecutor-的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。\" class=\"headerlink\" title=\"【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。\"></a>【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。</h4><p>说明： Executors 返回的线程池对象的弊端如下：<br>1） FixedThreadPool 和 SingleThreadPool :<br>允许的请求队列长度为 Integer.MAX_VALUE ，可能会堆积大量的请求，从而导致 OOM 。<br>2） CachedThreadPool 和 ScheduledThreadPool :<br>允许的创建线程数量为 Integer.MAX_VALUE ，可能会创建大量的线程，从而导致 OOM 。</p>\n<h4 id=\"【强制】-SimpleDateFormat-是线程不安全的类，一般不要定义为-static-变量，如果定义为static-，必须加锁，或者使用-DateUtils-工具类。\"><a href=\"#【强制】-SimpleDateFormat-是线程不安全的类，一般不要定义为-static-变量，如果定义为static-，必须加锁，或者使用-DateUtils-工具类。\" class=\"headerlink\" title=\"【强制】 SimpleDateFormat 是线程不安全的类，一般不要定义为 static 变量，如果定义为static ，必须加锁，或者使用 DateUtils 工具类。\"></a>【强制】 SimpleDateFormat 是线程不安全的类，一般不要定义为 static 变量，如果定义为static ，必须加锁，或者使用 DateUtils 工具类。</h4><h4 id=\"【强制】高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁-；-能锁区块，就不要锁整个方法体-；-能用对象锁，就不要用类锁。\"><a href=\"#【强制】高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁-；-能锁区块，就不要锁整个方法体-；-能用对象锁，就不要用类锁。\" class=\"headerlink\" title=\"【强制】高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁 ； 能锁区块，就不要锁整个方法体 ； 能用对象锁，就不要用类锁。\"></a>【强制】高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁 ； 能锁区块，就不要锁整个方法体 ； 能用对象锁，就不要用类锁。</h4><h4 id=\"【强制】对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁\"><a href=\"#【强制】对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁\" class=\"headerlink\" title=\"【强制】对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁\"></a>【强制】对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁</h4><p>说明：线程一需要对表 A 、 B 、 C 依次全部加锁后才可以进行更新操作，那么线程二的加锁顺序<br>也必须是 A 、 B 、 C ，否则可能出现死锁。</p>\n<h4 id=\"【强制】并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用-version-作为更新依据。\"><a href=\"#【强制】并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用-version-作为更新依据。\" class=\"headerlink\" title=\"【强制】并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用 version 作为更新依据。\"></a>【强制】并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用 version 作为更新依据。</h4><p>说明：如果每次访问冲突概率小于 20%，推荐使用乐观锁，否则使用悲观锁。乐观锁的重试次<br>数不得小于 3 次。</p>\n<h4 id=\"【强制】多线程并行处理定时任务时，-Timer-运行多个-TimeTask-时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用-ScheduledExecutorService-则没有这个问题。\"><a href=\"#【强制】多线程并行处理定时任务时，-Timer-运行多个-TimeTask-时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用-ScheduledExecutorService-则没有这个问题。\" class=\"headerlink\" title=\"【强制】多线程并行处理定时任务时， Timer 运行多个 TimeTask 时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用 ScheduledExecutorService 则没有这个问题。\"></a>【强制】多线程并行处理定时任务时， Timer 运行多个 TimeTask 时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用 ScheduledExecutorService 则没有这个问题。</h4><p>都是经验啊，我还没有碰到这种坑</p>\n<h4 id=\"【推荐】使用-CountDownLatch-进行异步转同步操作，每个线程退出前必须调用-countDown-方法，线程执行代码注意-catch-异常，确保-countDown-方法可以执行，避免主线程无法执行至-countDown-方法，直到超时才返回结果。\"><a href=\"#【推荐】使用-CountDownLatch-进行异步转同步操作，每个线程退出前必须调用-countDown-方法，线程执行代码注意-catch-异常，确保-countDown-方法可以执行，避免主线程无法执行至-countDown-方法，直到超时才返回结果。\" class=\"headerlink\" title=\"【推荐】使用 CountDownLatch 进行异步转同步操作，每个线程退出前必须调用 countDown 方法，线程执行代码注意 catch 异常，确保 countDown 方法可以执行，避免主线程无法执行至 countDown 方法，直到超时才返回结果。\"></a>【推荐】使用 CountDownLatch 进行异步转同步操作，每个线程退出前必须调用 countDown 方法，线程执行代码注意 catch 异常，确保 countDown 方法可以执行，避免主线程无法执行至 countDown 方法，直到超时才返回结果。</h4><p>说明：注意，子线程抛出异常堆栈，不能在主线程 try - catch 到。</p>\n<h4 id=\"【推荐】避免-Random-实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed-导致的性能下降。\"><a href=\"#【推荐】避免-Random-实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed-导致的性能下降。\" class=\"headerlink\" title=\"【推荐】避免 Random 实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed 导致的性能下降。\"></a>【推荐】避免 Random 实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed 导致的性能下降。</h4><p>说明： Random 实例包括 java . util . Random 的实例或者  Math . random() 实例。<br>正例：在 JDK 7 之后，可以直接使用 API ThreadLocalRandom ，在  JDK 7 之前，可以做到每个<br>线程一个实例。</p>\n<h4 id=\"【参考】-volatile-解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。\"><a href=\"#【参考】-volatile-解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。\" class=\"headerlink\" title=\"【参考】 volatile 解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。\"></a>【参考】 volatile 解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。</h4><p>如果是 count ++操作，使用如下类实现：AtomicInteger count =  new AtomicInteger(); count . addAndGet( 1 );  如果是 JDK 8，推荐使用 LongAdder 对象，比 AtomicLong 性能更好 （ 减少乐观锁的重试次数 ） 。</p>\n<h4 id=\"【参考】-HashMap-在容量不够进行-resize-时由于高并发可能出现死链，导致-CPU-飙升，在开发过程中注意规避此风险。\"><a href=\"#【参考】-HashMap-在容量不够进行-resize-时由于高并发可能出现死链，导致-CPU-飙升，在开发过程中注意规避此风险。\" class=\"headerlink\" title=\"【参考】  HashMap 在容量不够进行 resize 时由于高并发可能出现死链，导致 CPU 飙升，在开发过程中注意规避此风险。\"></a>【参考】  HashMap 在容量不够进行 resize 时由于高并发可能出现死链，导致 CPU 飙升，在开发过程中注意规避此风险。</h4><h2 id=\"控制语句\"><a href=\"#控制语句\" class=\"headerlink\" title=\"控制语句\"></a>控制语句</h2><h4 id=\"【强制】在一个-switch-块内，每个-case-要么通过-break-return-等来终止，要么注释说明程序将继续执行到哪一个-case-为止-；-在一个-switch-块内，都必须包含一个-default-语句并且放在最后，即使它什么代码也没有。\"><a href=\"#【强制】在一个-switch-块内，每个-case-要么通过-break-return-等来终止，要么注释说明程序将继续执行到哪一个-case-为止-；-在一个-switch-块内，都必须包含一个-default-语句并且放在最后，即使它什么代码也没有。\" class=\"headerlink\" title=\"【强制】在一个 switch 块内，每个 case 要么通过 break / return 等来终止，要么注释说明程序将继续执行到哪一个 case 为止 ； 在一个 switch 块内，都必须包含一个 default 语句并且放在最后，即使它什么代码也没有。\"></a>【强制】在一个 switch 块内，每个 case 要么通过 break / return 等来终止，要么注释说明程序将继续执行到哪一个 case 为止 ； 在一个 switch 块内，都必须包含一个 default 语句并且放在最后，即使它什么代码也没有。</h4><h4 id=\"【推荐】推荐尽量少用-else-，-if-else-的方式可以改写成：\"><a href=\"#【推荐】推荐尽量少用-else-，-if-else-的方式可以改写成：\" class=\"headerlink\" title=\"【推荐】推荐尽量少用 else ，  if - else 的方式可以改写成：\"></a>【推荐】推荐尽量少用 else ，  if - else 的方式可以改写成：</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">if(condition)&#123;</div><div class=\"line\">\t...</div><div class=\"line\">\treturn obj;</div><div class=\"line\">&#125;</div><div class=\"line\">// 接着写 else 的业务逻辑代码;</div></pre></td></tr></table></figure>\n<p>说明：如果非得使用 if()…else if()…else… 方式表达逻辑，【强制】请勿超过 3 层，<br>超过请使用状态设计模式。<br>正例：逻辑上超过 3 层的 if-else 代码可以使用卫语句，或者状态模式来实现。<br>思考：以后实现一定注意，我以前没有注意过这一点。</p>\n<h4 id=\"【推荐】除常用方法（如-getXxx-isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。\"><a href=\"#【推荐】除常用方法（如-getXxx-isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。\" class=\"headerlink\" title=\"【推荐】除常用方法（如 getXxx/isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。\"></a>【推荐】除常用方法（如 getXxx/isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。</h4><p>说明：很多 if 语句内的逻辑相当复杂，阅读者需要分析条件表达式的最终结果，才能明确什么<br>样的条件执行什么样的语句，那么，如果阅读者分析逻辑表达式错误呢？</p>\n<p>正例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">//伪代码如下</div><div class=\"line\">boolean existed = (file.open(fileName, &quot;w&quot;) != null) &amp;&amp; (...) || (...);</div><div class=\"line\">if (existed) &#123;</div><div class=\"line\">\t...</div><div class=\"line\">&#125;</div><div class=\"line\">反例：</div><div class=\"line\">if ((file.open(fileName, &quot;w&quot;) != null) &amp;&amp; (...) || (...)) &#123;</div><div class=\"line\">\t...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的-try-catch-操作-（-这个-try-catch-是否可以移至循环体外-）-。\"><a href=\"#【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的-try-catch-操作-（-这个-try-catch-是否可以移至循环体外-）-。\" class=\"headerlink\" title=\"【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的 try - catch 操作 （ 这个 try - catch 是否可以移至循环体外 ） 。\"></a>【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的 try - catch 操作 （ 这个 try - catch 是否可以移至循环体外 ） 。</h4><h4 id=\"【推荐】接口入参保护，这种场景常见的是用于做批量操作的接口。\"><a href=\"#【推荐】接口入参保护，这种场景常见的是用于做批量操作的接口。\" class=\"headerlink\" title=\"【推荐】接口入参保护，这种场景常见的是用于做批量操作的接口。\"></a>【推荐】接口入参保护，这种场景常见的是用于做批量操作的接口。</h4><h4 id=\"【参考】方法中需要进行参数校验的场景：\"><a href=\"#【参考】方法中需要进行参数校验的场景：\" class=\"headerlink\" title=\"【参考】方法中需要进行参数校验的场景：\"></a>【参考】方法中需要进行参数校验的场景：</h4><p>1 ） 调用频次低的方法。<br>2 ） 执行时间开销很大的方法，参数校验时间几乎可以忽略不计，但如果因为参数错误导致<br>中间执行回退，或者错误，那得不偿失。<br>3 ） 需要极高稳定性和可用性的方法。<br>4 ） 对外提供的开放接口，不管是 RPC / API / HTTP 接口。<br>5） 敏感权限入口。</p>\n<h4 id=\"【参考】方法中不需要参数校验的场景：\"><a href=\"#【参考】方法中不需要参数校验的场景：\" class=\"headerlink\" title=\"【参考】方法中不需要参数校验的场景：\"></a>【参考】方法中不需要参数校验的场景：</h4><p>1 ） 极有可能被循环调用的方法，不建议对参数进行校验。但在方法说明里必须注明外部参<br>数检查。<br>2 ） 底层的方法调用频度都比较高，一般不校验。毕竟是像纯净水过滤的最后一道，参数错<br>误不太可能到底层才会暴露问题。一般 DAO 层与 Service 层都在同一个应用中，部署在同一<br>台服务器中，所以 DAO 的参数校验，可以省略。<br>3 ） 被声明成 private 只会被自己代码所调用的方法，如果能够确定调用方法的代码传入参<br>数已经做过检查或者肯定不会有问题，此时可以不校验参数。</p>\n<h2 id=\"注释规约\"><a href=\"#注释规约\" class=\"headerlink\" title=\"注释规约\"></a>注释规约</h2><h4 id=\"【强制】类、类属性、类方法的注释必须使用-Javadoc-规范，使用-内容-格式，不得使用-xxx-方式。\"><a href=\"#【强制】类、类属性、类方法的注释必须使用-Javadoc-规范，使用-内容-格式，不得使用-xxx-方式。\" class=\"headerlink\" title=\"【强制】类、类属性、类方法的注释必须使用 Javadoc 规范，使用/*内容/格式，不得使用 // xxx 方式。\"></a>【强制】类、类属性、类方法的注释必须使用 Javadoc 规范，使用/<em>*内容</em>/格式，不得使用 // xxx 方式。</h4><p>说明：在 IDE 编辑窗口中， Javadoc 方式会提示相关注释，生成 Javadoc 可以正确输出相应注<br>释 ； 在 IDE 中，工程调用方法时，不进入方法即可悬浮提示方法、参数、返回值的意义，提高<br>阅读效率。</p>\n<h4 id=\"【强制】所有的抽象方法-（-包括接口中的方法-）-必须要用-Javadoc-注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。\"><a href=\"#【强制】所有的抽象方法-（-包括接口中的方法-）-必须要用-Javadoc-注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。\" class=\"headerlink\" title=\"【强制】所有的抽象方法 （ 包括接口中的方法 ） 必须要用 Javadoc 注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。\"></a>【强制】所有的抽象方法 （ 包括接口中的方法 ） 必须要用 Javadoc 注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。</h4><p>说明：对子类的实现要求，或者调用注意事项，请一并说明。</p>\n<h4 id=\"【强制】所有的枚举类型字段必须要有注释，说明每个数据项的用途。\"><a href=\"#【强制】所有的枚举类型字段必须要有注释，说明每个数据项的用途。\" class=\"headerlink\" title=\"【强制】所有的枚举类型字段必须要有注释，说明每个数据项的用途。\"></a>【强制】所有的枚举类型字段必须要有注释，说明每个数据项的用途。</h4><h4 id=\"【推荐】与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。\"><a href=\"#【推荐】与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。\" class=\"headerlink\" title=\"【推荐】与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。\"></a>【推荐】与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。</h4><p>反例：“ TCP 连接超时”解释成“传输控制协议连接超时”，理解反而费脑筋。</p>\n<h4 id=\"【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。\"><a href=\"#【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。\" class=\"headerlink\" title=\"【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。\"></a>【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。</h4><p>说明：代码与注释更新不同步，就像路网与导航软件更新不同步一样，如果导航软件严重滞后，<br>就失去了导航的意义。</p>\n<h4 id=\"【参考】注释掉的代码尽量要配合说明，而不是简单的注释掉。\"><a href=\"#【参考】注释掉的代码尽量要配合说明，而不是简单的注释掉。\" class=\"headerlink\" title=\"【参考】注释掉的代码尽量要配合说明，而不是简单的注释掉。\"></a>【参考】注释掉的代码尽量要配合说明，而不是简单的注释掉。</h4><p>说明：代码被注释掉有两种可能性：1 ） 后续会恢复此段代码逻辑。2 ） 永久不用。前者如果没<br>有备注信息，难以知晓注释动机。后者建议直接删掉 （ 代码仓库保存了历史代码 ） 。<br>思考：这一点我需要反思，很多时候代码舍不得删，搞得很乱，所以应该勤提交，使用git来保存历史代码；</p>\n<h4 id=\"【参考】对于注释的要求：第一、能够准确反应设计思想和代码逻辑-；-第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。\"><a href=\"#【参考】对于注释的要求：第一、能够准确反应设计思想和代码逻辑-；-第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。\" class=\"headerlink\" title=\"【参考】对于注释的要求：第一、能够准确反应设计思想和代码逻辑 ； 第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。\"></a>【参考】对于注释的要求：第一、能够准确反应设计思想和代码逻辑 ； 第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。</h4><p>完全没有注释的大段代码对于阅读者形同<br>天书，注释是给自己看的，即使隔很长时间，也能清晰理解当时的思路 ； 注释也是给继任者看<br>的，使其能够快速接替自己的工作。</p>\n<h4 id=\"【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。\"><a href=\"#【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。\" class=\"headerlink\" title=\"【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。\"></a>【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。</h4><p>1 ） 待办事宜 （TODO） : （ 标记人，标记时间， [ 预计处理时间 ]）<br>表示需要实现，但目前还未实现的功能。这实际上是一个 Javadoc 的标签，目前的 Javadoc<br>还没有实现，但已经被广泛使用。只能应用于类，接口和方法 （ 因为它是一个 Javadoc 标签 ） 。<br>2 ） 错误，不能工作 （FIXME） : （ 标记人，标记时间， [ 预计处理时间 ]）<br>在注释中用 FIXME 标记某代码是错误的，而且不能工作，需要及时纠正的情况。</p>\n<h2 id=\"其它\"><a href=\"#其它\" class=\"headerlink\" title=\"其它\"></a>其它</h2><h4 id=\"【强制】在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。\"><a href=\"#【强制】在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。\" class=\"headerlink\" title=\"【强制】在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。\"></a>【强制】在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。</h4><p>说明：不要在方法体内定义： Pattern pattern =  Pattern . compile( 规则 );</p>\n<h4 id=\"【强制】后台输送给页面的变量必须加-var-——中间的感叹号。\"><a href=\"#【强制】后台输送给页面的变量必须加-var-——中间的感叹号。\" class=\"headerlink\" title=\"【强制】后台输送给页面的变量必须加 $!{var} ——中间的感叹号。\"></a>【强制】后台输送给页面的变量必须加 $!{var} ——中间的感叹号。</h4><p>说明：如果 var = null 或者不存在，那么 ${var} 会直接显示在页面上。<br>思考：这里是 Velocity 模板引擎的一些内容，Velocity是一个基于Java的模板引擎，通过特定的语法，Velocity可以获取在java语言中定义的对象，从而实现界面和java代码的真正分离，这意味着可以使用velocity替代jsp的开发模式了。这使得前端开发人员可以和 Java 程序开发人员同步开发一个遵循 MVC 架构的 web 站点，在实际应用中，velocity还可以应用于很多其他的场景。，比如源代码生成、自动email和转换xml等。</p>\n<h4 id=\"【强制】注意-Math-random-这个方法返回是-double-类型，注意取值的范围-0≤-x-lt-1-（-能够取到零值，注意除零异常-）-，如果想获取整数类型的随机数，不要将-x-放大-10-的若干倍然后取整，直接使用-Random-对象的-nextInt-或者-nextLong-方法。\"><a href=\"#【强制】注意-Math-random-这个方法返回是-double-类型，注意取值的范围-0≤-x-lt-1-（-能够取到零值，注意除零异常-）-，如果想获取整数类型的随机数，不要将-x-放大-10-的若干倍然后取整，直接使用-Random-对象的-nextInt-或者-nextLong-方法。\" class=\"headerlink\" title=\"【强制】注意  Math . random() 这个方法返回是 double 类型，注意取值的范围 0≤ x &lt;1 （ 能够取到零值，注意除零异常 ） ，如果想获取整数类型的随机数，不要将 x 放大 10 的若干倍然后取整，直接使用 Random 对象的 nextInt 或者 nextLong 方法。\"></a>【强制】注意  Math . random() 这个方法返回是 double 类型，注意取值的范围 0≤ x &lt;1 （ 能够取到零值，注意除零异常 ） ，如果想获取整数类型的随机数，不要将 x 放大 10 的若干倍然后取整，直接使用 Random 对象的 nextInt 或者 nextLong 方法。</h4><h4 id=\"【强制】获取当前毫秒数-System-currentTimeMillis-而不是-new-Date-getTime\"><a href=\"#【强制】获取当前毫秒数-System-currentTimeMillis-而不是-new-Date-getTime\" class=\"headerlink\" title=\"【强制】获取当前毫秒数 System . currentTimeMillis(); 而不是 new Date() . getTime();\"></a>【强制】获取当前毫秒数 System . currentTimeMillis(); 而不是 new Date() . getTime();</h4><p>说明：如果想获取更加精确的纳秒级时间值，用 System . nanoTime() 。在 JDK 8 中，针对统计时间等场景，推荐使用 Instant类。</p>\n<h4 id=\"【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。\"><a href=\"#【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。\" class=\"headerlink\" title=\"【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。\"></a>【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。</h4><h4 id=\"【推荐】对于“明确停止使用的代码和配置”，如方法、变量、类、配置文件、动态配置属性等要坚决从程序中清理出去，避免造成过多垃圾。\"><a href=\"#【推荐】对于“明确停止使用的代码和配置”，如方法、变量、类、配置文件、动态配置属性等要坚决从程序中清理出去，避免造成过多垃圾。\" class=\"headerlink\" title=\"【推荐】对于“明确停止使用的代码和配置”，如方法、变量、类、配置文件、动态配置属性等要坚决从程序中清理出去，避免造成过多垃圾。\"></a>【推荐】对于“明确停止使用的代码和配置”，如方法、变量、类、配置文件、动态配置属性等要坚决从程序中清理出去，避免造成过多垃圾。</h4>","excerpt":"","more":"<p>本人自认为代码还算写得比较符合规范，但是相信阿里会有总结更深的见解，因此对自己之前没有注意到的，自己没有遵循的，做一个笔记与整理，以加深理解。</p>\n<h1 id=\"编程规约\"><a href=\"#编程规约\" class=\"headerlink\" title=\"编程规约\"></a>编程规约</h1><h2 id=\"命名规约\"><a href=\"#命名规约\" class=\"headerlink\" title=\"命名规约\"></a>命名规约</h2><h4 id=\"【强制】常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。\"><a href=\"#【强制】常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。\" class=\"headerlink\" title=\"【强制】常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。\"></a>【强制】常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。</h4><p>重点在，不要嫌名字长，我以前写得比较短<br>正例： <code>MAX _ STOCK _ COUNT</code></p>\n<h4 id=\"【强制】抽象类命名使用-Abstract-或-Base-开头-；-异常类命名使用-Exception-结尾-；-测试类命名以它要测试的类的名称开始，以-Test-结尾。\"><a href=\"#【强制】抽象类命名使用-Abstract-或-Base-开头-；-异常类命名使用-Exception-结尾-；-测试类命名以它要测试的类的名称开始，以-Test-结尾。\" class=\"headerlink\" title=\"【强制】抽象类命名使用 Abstract 或 Base 开头 ； 异常类命名使用 Exception 结尾 ； 测试类命名以它要测试的类的名称开始，以 Test 结尾。\"></a>【强制】抽象类命名使用 Abstract 或 Base 开头 ； 异常类命名使用 Exception 结尾 ； 测试类命名以它要测试的类的名称开始，以 Test 结尾。</h4><p>如果是接口的话，如何？</p>\n<h4 id=\"接口和实现类的命名有两套规则\"><a href=\"#接口和实现类的命名有两套规则\" class=\"headerlink\" title=\"接口和实现类的命名有两套规则\"></a>接口和实现类的命名有两套规则</h4><blockquote>\n<p>1 ） 【强制】对于 Service 和 DAO 类，基于 SOA 的理念，暴露出来的服务一定是接口，内部<br>的实现类用 Impl 的后缀与接口区别。<br>正例： CacheServiceImpl 实现 CacheService 接口。<br>2 ）  【推荐】 如果是形容能力的接口名称，取对应的形容词做接口名 （ 通常是– able 的形式 ） 。<br>正例： AbstractTranslator 实现  Translatable 。</p>\n</blockquote>\n<p>一定注意，之前写scala的时候，并没有考虑到这一点，DAO和Service暴露出来一定是接口，然后用Impl后缀的实现类实现。</p>\n<h4 id=\"【参考】枚举类名建议带上-Enum-后缀，枚举成员名称需要全大写，单词间用下划线隔开。\"><a href=\"#【参考】枚举类名建议带上-Enum-后缀，枚举成员名称需要全大写，单词间用下划线隔开。\" class=\"headerlink\" title=\"【参考】枚举类名建议带上 Enum 后缀，枚举成员名称需要全大写，单词间用下划线隔开。\"></a>【参考】枚举类名建议带上 Enum 后缀，枚举成员名称需要全大写，单词间用下划线隔开。</h4><blockquote>\n<p>说明：枚举其实就是特殊的常量类，且构造方法被默认强制是私有。<br>正例：枚举名字： DealStatusEnum， 成员名称： SUCCESS /  UNKOWN _ REASON 。</p>\n</blockquote>\n<h4 id=\"【强制】-POJO-类中布尔类型的变量，都不要加-is-，否则部分框架解析会引起序列化错误。\"><a href=\"#【强制】-POJO-类中布尔类型的变量，都不要加-is-，否则部分框架解析会引起序列化错误。\" class=\"headerlink\" title=\"【强制】 POJO 类中布尔类型的变量，都不要加 is ，否则部分框架解析会引起序列化错误。\"></a>【强制】 POJO 类中布尔类型的变量，都不要加 is ，否则部分框架解析会引起序列化错误。</h4><blockquote>\n<p>反例：定义为基本数据类型 boolean isSuccess； 的属性，它的方法也是 isSuccess() ， RPC<br>框架在反向解析的时候，“以为”对应的属性名称是 success ，导致属性获取不到，进而抛出异<br>常。</p>\n</blockquote>\n<h4 id=\"【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。\"><a href=\"#【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。\" class=\"headerlink\" title=\"【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。\"></a>【强制】包名统一使用小写，点分隔符之间有且仅有一个自然语义的英语单词。包名统一使用单数形式，但是类名如果有复数含义，类名可以使用复数形式。</h4><p>点分隔符之间仅有一个自然语义的英语单词，包名统一使用单数形式，但类名可以使用复数形式</p>\n<h4 id=\"【强制】杜绝完全不规范的缩写，避免望文不知义\"><a href=\"#【强制】杜绝完全不规范的缩写，避免望文不知义\" class=\"headerlink\" title=\"【强制】杜绝完全不规范的缩写，避免望文不知义\"></a>【强制】杜绝完全不规范的缩写，避免望文不知义</h4><blockquote>\n<p>反例：  AbstractClass “缩写”命名成 AbsClass；condition “缩写”命名成  condi ，此类<br>随意缩写严重降低了代码的可阅读性。</p>\n</blockquote>\n<p>的确是，一定要注意</p>\n<h4 id=\"【推荐】如果使用到了设计模式，建议在类名中体现出具体模式。\"><a href=\"#【推荐】如果使用到了设计模式，建议在类名中体现出具体模式。\" class=\"headerlink\" title=\"【推荐】如果使用到了设计模式，建议在类名中体现出具体模式。\"></a>【推荐】如果使用到了设计模式，建议在类名中体现出具体模式。</h4><blockquote>\n<p>说明：将设计模式体现在名字中，有利于阅读者快速理解架构设计思想。<br>正例： public class OrderFactory;<br>public class LoginProxy;<br>public class ResourceObserver;</p>\n</blockquote>\n<h4 id=\"【参考】各层命名规约：\"><a href=\"#【参考】各层命名规约：\" class=\"headerlink\" title=\"【参考】各层命名规约：\"></a>【参考】各层命名规约：</h4><blockquote>\n<p>A) Service / DAO 层方法命名规约<br>1 ） 获取单个对象的方法用 get 做前缀。<br>2 ） 获取多个对象的方法用 list 做前缀。<br>3 ） 获取统计值的方法用 count 做前缀。<br>4 ） 插入的方法用 save（ 推荐 ） 或 insert 做前缀。<br>5 ） 删除的方法用 remove（ 推荐 ） 或 delete 做前缀。<br>6 ） 修改的方法用 update 做前缀。<br>B) 领域模型命名规约<br>1 ） 数据对象： xxxDO ， xxx 即为数据表名。<br>2 ） 数据传输对象： xxxDTO ， xxx 为业务领域相关的名称。<br>3 ） 展示对象： xxxVO ， xxx 一般为网页名称。<br>4 ） POJO 是 DO / DTO / BO / VO 的统称，禁止命名成 xxxPOJO 。</p>\n</blockquote>\n<p>到时候命名的时候再查阅一遍</p>\n<h4 id=\"【推荐】接口类中的方法和属性不要加任何修饰符号-（public-也不要加-）-，保持代码的简洁性，并加上有效的-Javadoc-注释。尽量不要在接口里定义变量，如果一定要定义变量，肯定是与接口方法相关，并且是整个应用的基础常量。\"><a href=\"#【推荐】接口类中的方法和属性不要加任何修饰符号-（public-也不要加-）-，保持代码的简洁性，并加上有效的-Javadoc-注释。尽量不要在接口里定义变量，如果一定要定义变量，肯定是与接口方法相关，并且是整个应用的基础常量。\" class=\"headerlink\" title=\"【推荐】接口类中的方法和属性不要加任何修饰符号 （public 也不要加 ） ，保持代码的简洁性，并加上有效的 Javadoc 注释。尽量不要在接口里定义变量，如果一定要定义变量，肯定是与接口方法相关，并且是整个应用的基础常量。\"></a>【推荐】接口类中的方法和属性不要加任何修饰符号 （public 也不要加 ） ，保持代码的简洁性，并加上有效的 Javadoc 注释。尽量不要在接口里定义变量，如果一定要定义变量，肯定是与接口方法相关，并且是整个应用的基础常量。</h4><blockquote>\n<p>正例：接口方法签名： void f();<br>接口基础常量表示： String COMPANY = “ alibaba “ ;<br>反例：接口方法定义： public abstract void f();<br>说明： JDK 8 中接口允许有默认实现，那么这个 default 方法，是对所有实现类都有价值的默认实现。</p>\n</blockquote>\n<p>定义接口的时候，方法和属性，不要加任何修饰符号，注释要写清楚</p>\n<h2 id=\"常量定义\"><a href=\"#常量定义\" class=\"headerlink\" title=\"常量定义\"></a>常量定义</h2><h4 id=\"【强制】不允许出现任何魔法值-（-即未经定义的常量-）-直接出现在代码中。\"><a href=\"#【强制】不允许出现任何魔法值-（-即未经定义的常量-）-直接出现在代码中。\" class=\"headerlink\" title=\"【强制】不允许出现任何魔法值 （ 即未经定义的常量 ） 直接出现在代码中。\"></a>【强制】不允许出现任何魔法值 （ 即未经定义的常量 ） 直接出现在代码中。</h4><p>反例：  String key =”Id#taobao_”+ tradeId；<br>cache . put(key ,  value);</p>\n<p>为什么要这样做？</p>\n<h4 id=\"如果变量值仅在一个范围内变化用-Enum-类。如果还带有名称之外的延伸属性，必须使用-Enum-类，下面正例中的数字就是延伸信息，表示星期几。\"><a href=\"#如果变量值仅在一个范围内变化用-Enum-类。如果还带有名称之外的延伸属性，必须使用-Enum-类，下面正例中的数字就是延伸信息，表示星期几。\" class=\"headerlink\" title=\"如果变量值仅在一个范围内变化用 Enum 类。如果还带有名称之外的延伸属性，必须使用 Enum 类，下面正例中的数字就是延伸信息，表示星期几。\"></a>如果变量值仅在一个范围内变化用 Enum 类。如果还带有名称之外的延伸属性，必须使用 Enum 类，下面正例中的数字就是延伸信息，表示星期几。</h4><p>正例： public Enum {  MONDAY( 1 ) ,  TUESDAY( 2 ) ,  WEDNESDAY( 3 ) ,  THURSDAY( 4 ) ,  FRIDAY( 5 ) ,SATURDAY( 6 ) ,  SUNDAY( 7 ); }</p>\n<h2 id=\"格式规范\"><a href=\"#格式规范\" class=\"headerlink\" title=\"格式规范\"></a>格式规范</h2><h4 id=\"【强制】单行字符数限制不超过-120-个，超出需要换行，换行时遵循如下原则：\"><a href=\"#【强制】单行字符数限制不超过-120-个，超出需要换行，换行时遵循如下原则：\" class=\"headerlink\" title=\"【强制】单行字符数限制不超过 120 个，超出需要换行，换行时遵循如下原则：\"></a>【强制】单行字符数限制不超过 120 个，超出需要换行，换行时遵循如下原则：</h4><p>1） 第二行相对第一行缩进 4 个空格，从第三行开始，不再继续缩进，参考示例。<br>2 ） 运算符与下文一起换行。<br>3 ） 方法调用的点符号与下文一起换行。<br>4 ） 在多个参数超长，逗号后进行换行。<br>5 ） 在括号前不要换行，见反例。</p>\n<p>我之前是不能超过90个，目的是笔记本电脑看代码的时候比较方便，现在涨到120个了</p>\n<h4 id=\"【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。\"><a href=\"#【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。\" class=\"headerlink\" title=\"【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。\"></a>【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。</h4><p>说明：没有必要插入多行空格进行隔开。不能有多插几行的强迫症</p>\n<h2 id=\"OOP规约\"><a href=\"#OOP规约\" class=\"headerlink\" title=\"OOP规约\"></a>OOP规约</h2><h4 id=\"【强制】避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。\"><a href=\"#【强制】避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。\" class=\"headerlink\" title=\"【强制】避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。\"></a>【强制】避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。</h4><h4 id=\"【强制】所有的覆写方法，必须加-Override-注解。\"><a href=\"#【强制】所有的覆写方法，必须加-Override-注解。\" class=\"headerlink\" title=\"【强制】所有的覆写方法，必须加 @Override 注解。\"></a>【强制】所有的覆写方法，必须加 @Override 注解。</h4><p>反例： getObject() 与 get0bject() 的问题。一个是字母的 O ，一个是数字的 0，加@Override<br>可以准确判断是否覆盖成功。另外，如果在抽象类中对方法签名进行修改，其实现类会马上编<br>译报错。</p>\n<h4 id=\"【强制】相同参数类型，相同业务含义，才可以使用-Java-的可变参数，避免使用-Object-。\"><a href=\"#【强制】相同参数类型，相同业务含义，才可以使用-Java-的可变参数，避免使用-Object-。\" class=\"headerlink\" title=\"【强制】相同参数类型，相同业务含义，才可以使用 Java 的可变参数，避免使用 Object 。\"></a>【强制】相同参数类型，相同业务含义，才可以使用 Java 的可变参数，避免使用 Object 。</h4><p>说明：可变参数必须放置在参数列表的最后。 （ 提倡同学们尽量不用可变参数编程 ）<br>正例： public User getUsers(String type, Integer… ids)</p>\n<h4 id=\"【强制】对外暴露的接口签名，原则上不允许修改方法签名，避免对接口调用方产生影响。\"><a href=\"#【强制】对外暴露的接口签名，原则上不允许修改方法签名，避免对接口调用方产生影响。\" class=\"headerlink\" title=\"【强制】对外暴露的接口签名，原则上不允许修改方法签名，避免对接口调用方产生影响。\"></a>【强制】对外暴露的接口签名，原则上不允许修改方法签名，避免对接口调用方产生影响。</h4><p>接口过时必须加@Deprecated 注解，并清晰地说明采用的新接口或者新服务是什么。</p>\n<h4 id=\"【强制】不能使用过时的类或方法。\"><a href=\"#【强制】不能使用过时的类或方法。\" class=\"headerlink\" title=\"【强制】不能使用过时的类或方法。\"></a>【强制】不能使用过时的类或方法。</h4><h4 id=\"【强制】-Object-的-equals-方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals-。\"><a href=\"#【强制】-Object-的-equals-方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals-。\" class=\"headerlink\" title=\"【强制】 Object 的 equals 方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals 。\"></a>【强制】 Object 的 equals 方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals 。</h4><p>正例： “ test “ .equals(object);<br>反例：  object.equals( “ test “ );<br>说明：推荐使用 java . util . Objects # equals （JDK 7 引入的工具类 ）</p>\n<h4 id=\"【强制】所有的相同类型的包装类对象之间值的比较，全部使用-equals-方法比较。\"><a href=\"#【强制】所有的相同类型的包装类对象之间值的比较，全部使用-equals-方法比较。\" class=\"headerlink\" title=\"【强制】所有的相同类型的包装类对象之间值的比较，全部使用 equals 方法比较。\"></a>【强制】所有的相同类型的包装类对象之间值的比较，全部使用 equals 方法比较。</h4><p>说明：对于 Integer var =?在-128 至 127 之间的赋值， Integer 对象是在<br>IntegerCache . cache 产生，会复用已有对象，这个区间内的 Integer 值可以直接使用==进行<br>判断，但是这个区间之外的所有数据，都会在堆上产生，并不会复用已有对象，这是一个大坑，<br>推荐使用 equals 方法进行判断。</p>\n<p>为什么这是个大坑？？？因为使用 == 比较的话，会判断内存地址是否相同，这样会返回False</p>\n<h4 id=\"8-【强制】关于基本数据类型与包装数据类型的使用标准如下：\"><a href=\"#8-【强制】关于基本数据类型与包装数据类型的使用标准如下：\" class=\"headerlink\" title=\"8. 【强制】关于基本数据类型与包装数据类型的使用标准如下：\"></a>8. 【强制】关于基本数据类型与包装数据类型的使用标准如下：</h4><p>1 ） 所有的 POJO 类属性必须使用包装数据类型。<br>2 ） RPC 方法的返回值和参数必须使用包装数据类型。<br>3 ） 所有的局部变量【推荐】使用基本数据类型。</p>\n<p>包装数据类型的 null 值，能够表示额外的信息，如：远程调用失败，异常退出。</p>\n<h4 id=\"【强制】定义-DO-DTO-VO-等-POJO-类时，不要设定任何属性默认值。\"><a href=\"#【强制】定义-DO-DTO-VO-等-POJO-类时，不要设定任何属性默认值。\" class=\"headerlink\" title=\"【强制】定义 DO / DTO / VO 等 POJO 类时，不要设定任何属性默认值。\"></a>【强制】定义 DO / DTO / VO 等 POJO 类时，不要设定任何属性默认值。</h4><p>反例： POJO 类的 gmtCreate 默认值为 new Date(); 但是这个属性在数据提取时并没有置入具<br>体值，在更新其它字段时又附带更新了此字段，导致创建时间被修改成当前时间。</p>\n<h4 id=\"【强制】构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在-init-方法中。\"><a href=\"#【强制】构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在-init-方法中。\" class=\"headerlink\" title=\"【强制】构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在 init 方法中。\"></a>【强制】构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在 init 方法中。</h4><h4 id=\"【强制】-POJO-类必须写-toString-方法。使用-IDE-的中工具：-source-gt-generate-toString时，如果继承了另一个-POJO-类，注意在前面加一下-super-toString-。\"><a href=\"#【强制】-POJO-类必须写-toString-方法。使用-IDE-的中工具：-source-gt-generate-toString时，如果继承了另一个-POJO-类，注意在前面加一下-super-toString-。\" class=\"headerlink\" title=\"【强制】 POJO 类必须写 toString 方法。使用 IDE 的中工具： source &gt;  generate toString时，如果继承了另一个 POJO 类，注意在前面加一下 super . toString 。\"></a>【强制】 POJO 类必须写 toString 方法。使用 IDE 的中工具： source &gt;  generate toString时，如果继承了另一个 POJO 类，注意在前面加一下 super . toString 。</h4><p>说明：在方法执行抛出异常时，可以直接调用 POJO 的 toString() 方法打印其属性值，便于排<br>查问题。</p>\n<h4 id=\"【推荐】使用索引访问用-String-的-split-方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛-IndexOutOfBoundsException-的风险。\"><a href=\"#【推荐】使用索引访问用-String-的-split-方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛-IndexOutOfBoundsException-的风险。\" class=\"headerlink\" title=\"【推荐】使用索引访问用 String 的 split 方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛 IndexOutOfBoundsException 的风险。\"></a>【推荐】使用索引访问用 String 的 split 方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛 IndexOutOfBoundsException 的风险。</h4><p>说明：<br>String str = “a,b,c,,”;<br>String[] ary = str.split(“,”);<br>//预期大于 3，结果是 3<br>System.out.println(ary.length);</p>\n<h4 id=\"【推荐】类内方法定义顺序依次是：公有方法或保护方法-gt-私有方法-gt-getter-setter方法。\"><a href=\"#【推荐】类内方法定义顺序依次是：公有方法或保护方法-gt-私有方法-gt-getter-setter方法。\" class=\"headerlink\" title=\"【推荐】类内方法定义顺序依次是：公有方法或保护方法 &gt; 私有方法 &gt;  getter / setter方法。\"></a>【推荐】类内方法定义顺序依次是：公有方法或保护方法 &gt; 私有方法 &gt;  getter / setter方法。</h4><p>说明：构造方法放到最前面，公有方法是类的调用者和维护者最关心的方法，首屏展示最好 ； 保护方法虽然只是子类关心，也可能是“模板设计模式”下的核心方法 ； 而私有方法外部一般不需要特别关心，是一个黑盒实现 ； 因为方法信息价值较低，所有 Service 和 DAO 的 getter / setter 方法放在类体最后。</p>\n<h4 id=\"【推荐】在getter-setter-方法中，尽量不要增加业务逻辑，增加排查问题的难度。\"><a href=\"#【推荐】在getter-setter-方法中，尽量不要增加业务逻辑，增加排查问题的难度。\" class=\"headerlink\" title=\"【推荐】在getter / setter 方法中，尽量不要增加业务逻辑，增加排查问题的难度。\"></a>【推荐】在getter / setter 方法中，尽量不要增加业务逻辑，增加排查问题的难度。</h4><h4 id=\"【推荐】循环体内，字符串的联接方式，使用-StringBuilder-的-append-方法进行扩展。\"><a href=\"#【推荐】循环体内，字符串的联接方式，使用-StringBuilder-的-append-方法进行扩展。\" class=\"headerlink\" title=\"【推荐】循环体内，字符串的联接方式，使用 StringBuilder 的 append 方法进行扩展。\"></a>【推荐】循环体内，字符串的联接方式，使用 StringBuilder 的 append 方法进行扩展。</h4><h4 id=\"【推荐】-final-可提高程序响应效率，声明成-final-的情况：\"><a href=\"#【推荐】-final-可提高程序响应效率，声明成-final-的情况：\" class=\"headerlink\" title=\"【推荐】 final 可提高程序响应效率，声明成 final 的情况：\"></a>【推荐】 final 可提高程序响应效率，声明成 final 的情况：</h4><p>1 ） 不需要重新赋值的变量，包括类属性、局部变量。<br>2 ） 对象参数前加 final ，表示不允许修改引用的指向。<br>3 ） 类方法确定不允许被重写。</p>\n<h4 id=\"【推荐】慎用-Object-的-clone-方法来拷贝对象。\"><a href=\"#【推荐】慎用-Object-的-clone-方法来拷贝对象。\" class=\"headerlink\" title=\"【推荐】慎用 Object 的 clone 方法来拷贝对象。\"></a>【推荐】慎用 Object 的 clone 方法来拷贝对象。</h4><p>说明：对象的 clone 方法默认是浅拷贝，若想实现深拷贝需要重写 clone 方法实现属性对象<br>的拷贝。</p>\n<h4 id=\"【推荐】类成员与方法访问控制从严：\"><a href=\"#【推荐】类成员与方法访问控制从严：\" class=\"headerlink\" title=\"【推荐】类成员与方法访问控制从严：\"></a>【推荐】类成员与方法访问控制从严：</h4><p>1 ） 如果不允许外部直接通过 new 来创建对象，那么构造方法必须是 private 。<br>2 ） 工具类不允许有 public 或 default 构造方法。<br>3 ） 类非 static 成员变量并且与子类共享，必须是 protected 。<br>4 ） 类非 static 成员变量并且仅在本类使用，必须是 private 。<br>5 ） 类 static 成员变量如果仅在本类使用，必须是 private 。<br>6 ） 若是 static 成员变量，必须考虑是否为 final 。<br>7 ） 类成员方法只供类内部调用，必须是 private 。<br>8 ） 类成员方法只对继承类公开，那么限制为 protected 。<br>说明：任何类、方法、参数、变量，严控访问范围。过宽泛的访问范围，不利于模块解耦。<br>思<br>考：如果是一个 private 的方法，想删除就删除，可是一个 public 的 Service 方法，或者一<br>个 public 的成员变量，删除一下，不得手心冒点汗吗？变量像自己的小孩，尽量在自己的视<br>线内，变量作用域太大，如果无限制的到处跑，那么你会担心的。</p>\n<p><strong>这一点是非常重要的习惯，一定要加以养成</strong></p>\n<h2 id=\"集合处理\"><a href=\"#集合处理\" class=\"headerlink\" title=\"集合处理\"></a>集合处理</h2><h4 id=\"【强制】关于-hashCode-和-equals-的处理，遵循如下规则：\"><a href=\"#【强制】关于-hashCode-和-equals-的处理，遵循如下规则：\" class=\"headerlink\" title=\"【强制】关于 hashCode 和 equals 的处理，遵循如下规则：\"></a>【强制】关于 hashCode 和 equals 的处理，遵循如下规则：</h4><p>1） 只要重写 equals ，就必须重写 hashCode 。<br>2） 因为 Set 存储的是不重复的对象，依据 hashCode 和 equals 进行判断，所以 Set 存储的<br>对象必须重写这两个方法。<br>3） 如果自定义对象做为 Map 的键，那么必须重写 hashCode 和 equals 。<br>正例： String 重写了 hashCode 和 equals 方法，所以我们可以非常愉快地使用 String 对象<br>作为 key 来使用。</p>\n<h4 id=\"【强制】-ArrayList-的-subList-结果不可强转成-ArrayList-，否则会抛出-ClassCastException\"><a href=\"#【强制】-ArrayList-的-subList-结果不可强转成-ArrayList-，否则会抛出-ClassCastException\" class=\"headerlink\" title=\"【强制】  ArrayList 的 subList 结果不可强转成 ArrayList ，否则会抛出 ClassCastException\"></a>【强制】  ArrayList 的 subList 结果不可强转成 ArrayList ，否则会抛出 ClassCastException</h4><p>异常： java . util . RandomAccessSubList cannot be cast to java . util . ArrayList ;<br>说明： subList 返回的是  ArrayList 的内部类  SubList ，并不是  ArrayList ，而是<br>ArrayList 的一个视图，对于 SubList 子列表的所有操作最终会反映到原列表上。</p>\n<h4 id=\"【强制】-在-subList-场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生-ConcurrentModificationException-异常。\"><a href=\"#【强制】-在-subList-场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生-ConcurrentModificationException-异常。\" class=\"headerlink\" title=\"【强制】 在 subList 场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生 ConcurrentModificationException 异常。\"></a>【强制】 在 subList 场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生 ConcurrentModificationException 异常。</h4><h4 id=\"【强制】使用集合转数组的方法，必须使用集合的-toArray-T-array-，传入的是类型完全一样的数组，大小就是-list-size-。\"><a href=\"#【强制】使用集合转数组的方法，必须使用集合的-toArray-T-array-，传入的是类型完全一样的数组，大小就是-list-size-。\" class=\"headerlink\" title=\"【强制】使用集合转数组的方法，必须使用集合的 toArray(T[] array) ，传入的是类型完全一样的数组，大小就是 list . size() 。\"></a>【强制】使用集合转数组的方法，必须使用集合的 toArray(T[] array) ，传入的是类型完全一样的数组，大小就是 list . size() 。</h4><p>反例：直接使用 toArray 无参方法存在问题，此方法返回值只能是 Object[] 类，若强转其它<br>类型数组将出现 ClassCastException 错误。<br>正例：<br>List<String> list = new ArrayList<String>(2);<br>list.add(“guan”);<br>list.add(“bao”);<br>String[] array = new String[list.size()];<br>array = list.toArray(array);<br>说明：使用 toArray 带参方法，入参分配的数组空间不够大时， toArray 方法内部将重新分配<br>内存空间，并返回新数组地址 ； 如果数组元素大于实际所需，下标为 [ list . size() ] 的数组<br>元素将被置为 null ，其它数组元素保持原值，因此最好将方法入参数组大小定义与集合元素<br>个数一致。</p>\n<p>toArray 方法，一定要带参数，指定类型和大小；</p>\n<h4 id=\"【强制】使用工具类-Arrays-asList-把数组转换成集合时，不能使用其修改集合相关的方法，它的-add-remove-clear-方法会抛出-UnsupportedOperationException-异常。\"><a href=\"#【强制】使用工具类-Arrays-asList-把数组转换成集合时，不能使用其修改集合相关的方法，它的-add-remove-clear-方法会抛出-UnsupportedOperationException-异常。\" class=\"headerlink\" title=\"【强制】使用工具类 Arrays . asList() 把数组转换成集合时，不能使用其修改集合相关的方法，它的 add / remove / clear 方法会抛出 UnsupportedOperationException 异常。\"></a>【强制】使用工具类 Arrays . asList() 把数组转换成集合时，不能使用其修改集合相关的方法，它的 add / remove / clear 方法会抛出 UnsupportedOperationException 异常。</h4><p>说明： asList 的返回对象是一个 Arrays 内部类，并没有实现集合的修改方法。 Arrays . asList<br>体现的是适配器模式，只是转换接口，后台的数据仍是数组。<br>String[] str = new String[] { “a”, “b” };<br>List list = Arrays.asList(str);<br>第一种情况： list.add(“c”);  运行时异常。<br>第二种情况： str[0]= “gujin”; 那么 list.get(0) 也会随之修改。</p>\n<h4 id=\"【强制】泛型通配符-lt-extends-T-gt-来接收返回的数据，此写法的泛型集合不能使用-add-方法。\"><a href=\"#【强制】泛型通配符-lt-extends-T-gt-来接收返回的数据，此写法的泛型集合不能使用-add-方法。\" class=\"headerlink\" title=\"【强制】泛型通配符&lt;?  extends T &gt;来接收返回的数据，此写法的泛型集合不能使用 add 方法。\"></a>【强制】泛型通配符&lt;?  extends T &gt;来接收返回的数据，此写法的泛型集合不能使用 add 方法。</h4><p>说明：苹果装箱后返回一个<code>&lt;?  extends Fruits &gt;</code> 对象，此对象就不能往里加任何水果，包括苹果。</p>\n<h4 id=\"【强制】不要在-foreach-循环里进行元素的-remove-add-操作。-remove-元素请使用-Iterator方式，如果并发操作，需要对-Iterator-对象加锁。\"><a href=\"#【强制】不要在-foreach-循环里进行元素的-remove-add-操作。-remove-元素请使用-Iterator方式，如果并发操作，需要对-Iterator-对象加锁。\" class=\"headerlink\" title=\"【强制】不要在 foreach 循环里进行元素的 remove / add 操作。 remove 元素请使用 Iterator方式，如果并发操作，需要对 Iterator 对象加锁。\"></a>【强制】不要在 foreach 循环里进行元素的 remove / add 操作。 remove 元素请使用 Iterator方式，如果并发操作，需要对 Iterator 对象加锁。</h4><p>这个坑我以前踩过，如果早点看到就不会踩到了。。。</p>\n<h4 id=\"【强制】-在-JDK-7-版本以上，-Comparator-要满足自反性，传递性，对称性，不然-Arrays-sort-，Collections-sort-会报-IllegalArgumentException-异常。\"><a href=\"#【强制】-在-JDK-7-版本以上，-Comparator-要满足自反性，传递性，对称性，不然-Arrays-sort-，Collections-sort-会报-IllegalArgumentException-异常。\" class=\"headerlink\" title=\"【强制】 在 JDK 7 版本以上， Comparator 要满足自反性，传递性，对称性，不然 Arrays.sort ，Collections.sort 会报 IllegalArgumentException 异常。\"></a>【强制】 在 JDK 7 版本以上， Comparator 要满足自反性，传递性，对称性，不然 Arrays.sort ，Collections.sort 会报 IllegalArgumentException 异常。</h4><p>说明：<br>1 ） 自反性： x ， y 的比较结果和 y ， x 的比较结果相反。<br>2 ） 传递性： x &gt; y , y &gt; z ,则 x &gt; z 。<br>3 ） 对称性： x = y ,则 x , z 比较结果和 y ， z 比较结果相同。</p>\n<h4 id=\"【推荐】集合初始化时，尽量指定集合初始值大小。\"><a href=\"#【推荐】集合初始化时，尽量指定集合初始值大小。\" class=\"headerlink\" title=\"【推荐】集合初始化时，尽量指定集合初始值大小。\"></a>【推荐】集合初始化时，尽量指定集合初始值大小。</h4><p>说明： ArrayList 尽量使用 ArrayList(int initialCapacity) 初始化。</p>\n<h4 id=\"【推荐】使用-entrySet-遍历-Map-类集合-KV-，而不是-keySet-方式进行遍历。\"><a href=\"#【推荐】使用-entrySet-遍历-Map-类集合-KV-，而不是-keySet-方式进行遍历。\" class=\"headerlink\" title=\"【推荐】使用 entrySet 遍历 Map 类集合 KV ，而不是 keySet 方式进行遍历。\"></a>【推荐】使用 entrySet 遍历 Map 类集合 KV ，而不是 keySet 方式进行遍历。</h4><p>说明： keySet 其实是遍历了 2 次，一次是转为 Iterator 对象，另一次是从 hashMap 中取出key 所对应的 value 。而 entrySet 只是遍历了一次就把 key 和 value 都放到了 entry 中，效率更高。如果是 JDK 8，使用 Map . foreach 方法。<br>正例： values() 返回的是 V 值集合，是一个 list 集合对象 ；keySet() 返回的是 K 值集合，是一个 Set 集合对象；entrySet() 返回的是 K - V 值组合集合。</p>\n<h4 id=\"【推荐】高度注意-Map-类集合-K-V-能不能存储-null-值的情况，如下表格：\"><a href=\"#【推荐】高度注意-Map-类集合-K-V-能不能存储-null-值的情况，如下表格：\" class=\"headerlink\" title=\"【推荐】高度注意 Map 类集合 K / V 能不能存储 null 值的情况，如下表格：\"></a>【推荐】高度注意 Map 类集合 K / V 能不能存储 null 值的情况，如下表格：</h4><table>\n<thead>\n<tr>\n<th>集合类</th>\n<th>Key</th>\n<th>Value</th>\n<th>Super</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Hashtable</td>\n<td>不允许为 null</td>\n<td>不允许为 null</td>\n<td>Dictionary</td>\n<td>线程安全</td>\n</tr>\n<tr>\n<td>ConcurrentHashMap</td>\n<td>不允许为 null</td>\n<td>不允许为 null</td>\n<td>AbstractMap</td>\n<td>分段锁技术</td>\n</tr>\n<tr>\n<td>TreeMap</td>\n<td>不允许为 null</td>\n<td>允许为 null</td>\n<td>AbstractMap</td>\n<td>线程不安全</td>\n</tr>\n<tr>\n<td>HashMap</td>\n<td>允许为 null</td>\n<td>允许为 null</td>\n<td>AbstractMap</td>\n<td>线程不安全</td>\n</tr>\n</tbody>\n</table>\n<p>反例： 由于 HashMap 的干扰，很多人认为 ConcurrentHashMap 是可以置入 null 值，注意存储null 值时会抛出 NPE 异常。</p>\n<h4 id=\"【参考】合理利用好集合的有序性-sort-和稳定性-order-，避免集合的无序性-unsort-和不稳定性-unorder-带来的负面影响。\"><a href=\"#【参考】合理利用好集合的有序性-sort-和稳定性-order-，避免集合的无序性-unsort-和不稳定性-unorder-带来的负面影响。\" class=\"headerlink\" title=\"【参考】合理利用好集合的有序性 (sort) 和稳定性 (order) ，避免集合的无序性 (unsort) 和不稳定性 (unorder) 带来的负面影响。\"></a>【参考】合理利用好集合的有序性 (sort) 和稳定性 (order) ，避免集合的无序性 (unsort) 和不稳定性 (unorder) 带来的负面影响。</h4><p>说明：稳定性指集合每次遍历的元素次序是一定的。有序性是指遍历的结果是按某种比较规则依次排列的。如： ArrayList 是 order / unsort；HashMap 是 unorder / unsort；TreeSet 是order / sort 。</p>\n<h4 id=\"【参考】利用-Set-元素唯一的特性，可以快速对一个集合进行去重操作，避免使用-List-的contains-方法进行遍历、对比、去重操作。\"><a href=\"#【参考】利用-Set-元素唯一的特性，可以快速对一个集合进行去重操作，避免使用-List-的contains-方法进行遍历、对比、去重操作。\" class=\"headerlink\" title=\"【参考】利用 Set 元素唯一的特性，可以快速对一个集合进行去重操作，避免使用 List 的contains 方法进行遍历、对比、去重操作。\"></a>【参考】利用 Set 元素唯一的特性，可以快速对一个集合进行去重操作，避免使用 List 的contains 方法进行遍历、对比、去重操作。</h4><p>自评：这个好：使用 s.addAll(list); 方法。</p>\n<h2 id=\"并发处理\"><a href=\"#并发处理\" class=\"headerlink\" title=\"并发处理\"></a>并发处理</h2><h4 id=\"【强制】获取单例对象需要保证线程安全，其中的方法也要保证线程安全。\"><a href=\"#【强制】获取单例对象需要保证线程安全，其中的方法也要保证线程安全。\" class=\"headerlink\" title=\"【强制】获取单例对象需要保证线程安全，其中的方法也要保证线程安全。\"></a>【强制】获取单例对象需要保证线程安全，其中的方法也要保证线程安全。</h4><p>说明：资源驱动类、工具类、单例工厂类都需要注意。</p>\n<h4 id=\"【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。\"><a href=\"#【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。\" class=\"headerlink\" title=\"【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。\"></a>【强制】创建线程或线程池时请指定有意义的线程名称，方便出错时回溯。</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TimerTaskThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">TimerTaskThread</span><span class=\"params\">()</span></span>&#123;</div><div class=\"line\">\t<span class=\"keyword\">super</span>.setName(<span class=\"string\">\"TimerTaskThread\"</span>); ...</div><div class=\"line\">\t&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\"><a href=\"#【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\" class=\"headerlink\" title=\"【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\"></a>【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。</h4><p>说明：使用线程池的好处是减少在创建和销毁线程上所花的时间以及系统资源的开销，解决资<br>源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者<br>“过度切换”的问题。</p>\n<h4 id=\"【强制】线程池不允许使用-Executors-去创建，而是通过-ThreadPoolExecutor-的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。\"><a href=\"#【强制】线程池不允许使用-Executors-去创建，而是通过-ThreadPoolExecutor-的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。\" class=\"headerlink\" title=\"【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。\"></a>【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。</h4><p>说明： Executors 返回的线程池对象的弊端如下：<br>1） FixedThreadPool 和 SingleThreadPool :<br>允许的请求队列长度为 Integer.MAX_VALUE ，可能会堆积大量的请求，从而导致 OOM 。<br>2） CachedThreadPool 和 ScheduledThreadPool :<br>允许的创建线程数量为 Integer.MAX_VALUE ，可能会创建大量的线程，从而导致 OOM 。</p>\n<h4 id=\"【强制】-SimpleDateFormat-是线程不安全的类，一般不要定义为-static-变量，如果定义为static-，必须加锁，或者使用-DateUtils-工具类。\"><a href=\"#【强制】-SimpleDateFormat-是线程不安全的类，一般不要定义为-static-变量，如果定义为static-，必须加锁，或者使用-DateUtils-工具类。\" class=\"headerlink\" title=\"【强制】 SimpleDateFormat 是线程不安全的类，一般不要定义为 static 变量，如果定义为static ，必须加锁，或者使用 DateUtils 工具类。\"></a>【强制】 SimpleDateFormat 是线程不安全的类，一般不要定义为 static 变量，如果定义为static ，必须加锁，或者使用 DateUtils 工具类。</h4><h4 id=\"【强制】高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁-；-能锁区块，就不要锁整个方法体-；-能用对象锁，就不要用类锁。\"><a href=\"#【强制】高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁-；-能锁区块，就不要锁整个方法体-；-能用对象锁，就不要用类锁。\" class=\"headerlink\" title=\"【强制】高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁 ； 能锁区块，就不要锁整个方法体 ； 能用对象锁，就不要用类锁。\"></a>【强制】高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁 ； 能锁区块，就不要锁整个方法体 ； 能用对象锁，就不要用类锁。</h4><h4 id=\"【强制】对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁\"><a href=\"#【强制】对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁\" class=\"headerlink\" title=\"【强制】对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁\"></a>【强制】对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁</h4><p>说明：线程一需要对表 A 、 B 、 C 依次全部加锁后才可以进行更新操作，那么线程二的加锁顺序<br>也必须是 A 、 B 、 C ，否则可能出现死锁。</p>\n<h4 id=\"【强制】并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用-version-作为更新依据。\"><a href=\"#【强制】并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用-version-作为更新依据。\" class=\"headerlink\" title=\"【强制】并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用 version 作为更新依据。\"></a>【强制】并发修改同一记录时，避免更新丢失，要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用 version 作为更新依据。</h4><p>说明：如果每次访问冲突概率小于 20%，推荐使用乐观锁，否则使用悲观锁。乐观锁的重试次<br>数不得小于 3 次。</p>\n<h4 id=\"【强制】多线程并行处理定时任务时，-Timer-运行多个-TimeTask-时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用-ScheduledExecutorService-则没有这个问题。\"><a href=\"#【强制】多线程并行处理定时任务时，-Timer-运行多个-TimeTask-时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用-ScheduledExecutorService-则没有这个问题。\" class=\"headerlink\" title=\"【强制】多线程并行处理定时任务时， Timer 运行多个 TimeTask 时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用 ScheduledExecutorService 则没有这个问题。\"></a>【强制】多线程并行处理定时任务时， Timer 运行多个 TimeTask 时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用 ScheduledExecutorService 则没有这个问题。</h4><p>都是经验啊，我还没有碰到这种坑</p>\n<h4 id=\"【推荐】使用-CountDownLatch-进行异步转同步操作，每个线程退出前必须调用-countDown-方法，线程执行代码注意-catch-异常，确保-countDown-方法可以执行，避免主线程无法执行至-countDown-方法，直到超时才返回结果。\"><a href=\"#【推荐】使用-CountDownLatch-进行异步转同步操作，每个线程退出前必须调用-countDown-方法，线程执行代码注意-catch-异常，确保-countDown-方法可以执行，避免主线程无法执行至-countDown-方法，直到超时才返回结果。\" class=\"headerlink\" title=\"【推荐】使用 CountDownLatch 进行异步转同步操作，每个线程退出前必须调用 countDown 方法，线程执行代码注意 catch 异常，确保 countDown 方法可以执行，避免主线程无法执行至 countDown 方法，直到超时才返回结果。\"></a>【推荐】使用 CountDownLatch 进行异步转同步操作，每个线程退出前必须调用 countDown 方法，线程执行代码注意 catch 异常，确保 countDown 方法可以执行，避免主线程无法执行至 countDown 方法，直到超时才返回结果。</h4><p>说明：注意，子线程抛出异常堆栈，不能在主线程 try - catch 到。</p>\n<h4 id=\"【推荐】避免-Random-实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed-导致的性能下降。\"><a href=\"#【推荐】避免-Random-实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed-导致的性能下降。\" class=\"headerlink\" title=\"【推荐】避免 Random 实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed 导致的性能下降。\"></a>【推荐】避免 Random 实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed 导致的性能下降。</h4><p>说明： Random 实例包括 java . util . Random 的实例或者  Math . random() 实例。<br>正例：在 JDK 7 之后，可以直接使用 API ThreadLocalRandom ，在  JDK 7 之前，可以做到每个<br>线程一个实例。</p>\n<h4 id=\"【参考】-volatile-解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。\"><a href=\"#【参考】-volatile-解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。\" class=\"headerlink\" title=\"【参考】 volatile 解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。\"></a>【参考】 volatile 解决多线程内存不可见问题。对于一写多读，是可以解决变量同步问题，但是如果多写，同样无法解决线程安全问题。</h4><p>如果是 count ++操作，使用如下类实现：AtomicInteger count =  new AtomicInteger(); count . addAndGet( 1 );  如果是 JDK 8，推荐使用 LongAdder 对象，比 AtomicLong 性能更好 （ 减少乐观锁的重试次数 ） 。</p>\n<h4 id=\"【参考】-HashMap-在容量不够进行-resize-时由于高并发可能出现死链，导致-CPU-飙升，在开发过程中注意规避此风险。\"><a href=\"#【参考】-HashMap-在容量不够进行-resize-时由于高并发可能出现死链，导致-CPU-飙升，在开发过程中注意规避此风险。\" class=\"headerlink\" title=\"【参考】  HashMap 在容量不够进行 resize 时由于高并发可能出现死链，导致 CPU 飙升，在开发过程中注意规避此风险。\"></a>【参考】  HashMap 在容量不够进行 resize 时由于高并发可能出现死链，导致 CPU 飙升，在开发过程中注意规避此风险。</h4><h2 id=\"控制语句\"><a href=\"#控制语句\" class=\"headerlink\" title=\"控制语句\"></a>控制语句</h2><h4 id=\"【强制】在一个-switch-块内，每个-case-要么通过-break-return-等来终止，要么注释说明程序将继续执行到哪一个-case-为止-；-在一个-switch-块内，都必须包含一个-default-语句并且放在最后，即使它什么代码也没有。\"><a href=\"#【强制】在一个-switch-块内，每个-case-要么通过-break-return-等来终止，要么注释说明程序将继续执行到哪一个-case-为止-；-在一个-switch-块内，都必须包含一个-default-语句并且放在最后，即使它什么代码也没有。\" class=\"headerlink\" title=\"【强制】在一个 switch 块内，每个 case 要么通过 break / return 等来终止，要么注释说明程序将继续执行到哪一个 case 为止 ； 在一个 switch 块内，都必须包含一个 default 语句并且放在最后，即使它什么代码也没有。\"></a>【强制】在一个 switch 块内，每个 case 要么通过 break / return 等来终止，要么注释说明程序将继续执行到哪一个 case 为止 ； 在一个 switch 块内，都必须包含一个 default 语句并且放在最后，即使它什么代码也没有。</h4><h4 id=\"【推荐】推荐尽量少用-else-，-if-else-的方式可以改写成：\"><a href=\"#【推荐】推荐尽量少用-else-，-if-else-的方式可以改写成：\" class=\"headerlink\" title=\"【推荐】推荐尽量少用 else ，  if - else 的方式可以改写成：\"></a>【推荐】推荐尽量少用 else ，  if - else 的方式可以改写成：</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">if(condition)&#123;</div><div class=\"line\">\t...</div><div class=\"line\">\treturn obj;</div><div class=\"line\">&#125;</div><div class=\"line\">// 接着写 else 的业务逻辑代码;</div></pre></td></tr></table></figure>\n<p>说明：如果非得使用 if()…else if()…else… 方式表达逻辑，【强制】请勿超过 3 层，<br>超过请使用状态设计模式。<br>正例：逻辑上超过 3 层的 if-else 代码可以使用卫语句，或者状态模式来实现。<br>思考：以后实现一定注意，我以前没有注意过这一点。</p>\n<h4 id=\"【推荐】除常用方法（如-getXxx-isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。\"><a href=\"#【推荐】除常用方法（如-getXxx-isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。\" class=\"headerlink\" title=\"【推荐】除常用方法（如 getXxx/isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。\"></a>【推荐】除常用方法（如 getXxx/isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。</h4><p>说明：很多 if 语句内的逻辑相当复杂，阅读者需要分析条件表达式的最终结果，才能明确什么<br>样的条件执行什么样的语句，那么，如果阅读者分析逻辑表达式错误呢？</p>\n<p>正例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">//伪代码如下</div><div class=\"line\">boolean existed = (file.open(fileName, &quot;w&quot;) != null) &amp;&amp; (...) || (...);</div><div class=\"line\">if (existed) &#123;</div><div class=\"line\">\t...</div><div class=\"line\">&#125;</div><div class=\"line\">反例：</div><div class=\"line\">if ((file.open(fileName, &quot;w&quot;) != null) &amp;&amp; (...) || (...)) &#123;</div><div class=\"line\">\t...</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的-try-catch-操作-（-这个-try-catch-是否可以移至循环体外-）-。\"><a href=\"#【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的-try-catch-操作-（-这个-try-catch-是否可以移至循环体外-）-。\" class=\"headerlink\" title=\"【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的 try - catch 操作 （ 这个 try - catch 是否可以移至循环体外 ） 。\"></a>【推荐】循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的 try - catch 操作 （ 这个 try - catch 是否可以移至循环体外 ） 。</h4><h4 id=\"【推荐】接口入参保护，这种场景常见的是用于做批量操作的接口。\"><a href=\"#【推荐】接口入参保护，这种场景常见的是用于做批量操作的接口。\" class=\"headerlink\" title=\"【推荐】接口入参保护，这种场景常见的是用于做批量操作的接口。\"></a>【推荐】接口入参保护，这种场景常见的是用于做批量操作的接口。</h4><h4 id=\"【参考】方法中需要进行参数校验的场景：\"><a href=\"#【参考】方法中需要进行参数校验的场景：\" class=\"headerlink\" title=\"【参考】方法中需要进行参数校验的场景：\"></a>【参考】方法中需要进行参数校验的场景：</h4><p>1 ） 调用频次低的方法。<br>2 ） 执行时间开销很大的方法，参数校验时间几乎可以忽略不计，但如果因为参数错误导致<br>中间执行回退，或者错误，那得不偿失。<br>3 ） 需要极高稳定性和可用性的方法。<br>4 ） 对外提供的开放接口，不管是 RPC / API / HTTP 接口。<br>5） 敏感权限入口。</p>\n<h4 id=\"【参考】方法中不需要参数校验的场景：\"><a href=\"#【参考】方法中不需要参数校验的场景：\" class=\"headerlink\" title=\"【参考】方法中不需要参数校验的场景：\"></a>【参考】方法中不需要参数校验的场景：</h4><p>1 ） 极有可能被循环调用的方法，不建议对参数进行校验。但在方法说明里必须注明外部参<br>数检查。<br>2 ） 底层的方法调用频度都比较高，一般不校验。毕竟是像纯净水过滤的最后一道，参数错<br>误不太可能到底层才会暴露问题。一般 DAO 层与 Service 层都在同一个应用中，部署在同一<br>台服务器中，所以 DAO 的参数校验，可以省略。<br>3 ） 被声明成 private 只会被自己代码所调用的方法，如果能够确定调用方法的代码传入参<br>数已经做过检查或者肯定不会有问题，此时可以不校验参数。</p>\n<h2 id=\"注释规约\"><a href=\"#注释规约\" class=\"headerlink\" title=\"注释规约\"></a>注释规约</h2><h4 id=\"【强制】类、类属性、类方法的注释必须使用-Javadoc-规范，使用-内容-格式，不得使用-xxx-方式。\"><a href=\"#【强制】类、类属性、类方法的注释必须使用-Javadoc-规范，使用-内容-格式，不得使用-xxx-方式。\" class=\"headerlink\" title=\"【强制】类、类属性、类方法的注释必须使用 Javadoc 规范，使用/*内容/格式，不得使用 // xxx 方式。\"></a>【强制】类、类属性、类方法的注释必须使用 Javadoc 规范，使用/<em>*内容</em>/格式，不得使用 // xxx 方式。</h4><p>说明：在 IDE 编辑窗口中， Javadoc 方式会提示相关注释，生成 Javadoc 可以正确输出相应注<br>释 ； 在 IDE 中，工程调用方法时，不进入方法即可悬浮提示方法、参数、返回值的意义，提高<br>阅读效率。</p>\n<h4 id=\"【强制】所有的抽象方法-（-包括接口中的方法-）-必须要用-Javadoc-注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。\"><a href=\"#【强制】所有的抽象方法-（-包括接口中的方法-）-必须要用-Javadoc-注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。\" class=\"headerlink\" title=\"【强制】所有的抽象方法 （ 包括接口中的方法 ） 必须要用 Javadoc 注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。\"></a>【强制】所有的抽象方法 （ 包括接口中的方法 ） 必须要用 Javadoc 注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。</h4><p>说明：对子类的实现要求，或者调用注意事项，请一并说明。</p>\n<h4 id=\"【强制】所有的枚举类型字段必须要有注释，说明每个数据项的用途。\"><a href=\"#【强制】所有的枚举类型字段必须要有注释，说明每个数据项的用途。\" class=\"headerlink\" title=\"【强制】所有的枚举类型字段必须要有注释，说明每个数据项的用途。\"></a>【强制】所有的枚举类型字段必须要有注释，说明每个数据项的用途。</h4><h4 id=\"【推荐】与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。\"><a href=\"#【推荐】与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。\" class=\"headerlink\" title=\"【推荐】与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。\"></a>【推荐】与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可。</h4><p>反例：“ TCP 连接超时”解释成“传输控制协议连接超时”，理解反而费脑筋。</p>\n<h4 id=\"【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。\"><a href=\"#【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。\" class=\"headerlink\" title=\"【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。\"></a>【推荐】代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改。</h4><p>说明：代码与注释更新不同步，就像路网与导航软件更新不同步一样，如果导航软件严重滞后，<br>就失去了导航的意义。</p>\n<h4 id=\"【参考】注释掉的代码尽量要配合说明，而不是简单的注释掉。\"><a href=\"#【参考】注释掉的代码尽量要配合说明，而不是简单的注释掉。\" class=\"headerlink\" title=\"【参考】注释掉的代码尽量要配合说明，而不是简单的注释掉。\"></a>【参考】注释掉的代码尽量要配合说明，而不是简单的注释掉。</h4><p>说明：代码被注释掉有两种可能性：1 ） 后续会恢复此段代码逻辑。2 ） 永久不用。前者如果没<br>有备注信息，难以知晓注释动机。后者建议直接删掉 （ 代码仓库保存了历史代码 ） 。<br>思考：这一点我需要反思，很多时候代码舍不得删，搞得很乱，所以应该勤提交，使用git来保存历史代码；</p>\n<h4 id=\"【参考】对于注释的要求：第一、能够准确反应设计思想和代码逻辑-；-第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。\"><a href=\"#【参考】对于注释的要求：第一、能够准确反应设计思想和代码逻辑-；-第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。\" class=\"headerlink\" title=\"【参考】对于注释的要求：第一、能够准确反应设计思想和代码逻辑 ； 第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。\"></a>【参考】对于注释的要求：第一、能够准确反应设计思想和代码逻辑 ； 第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息。</h4><p>完全没有注释的大段代码对于阅读者形同<br>天书，注释是给自己看的，即使隔很长时间，也能清晰理解当时的思路 ； 注释也是给继任者看<br>的，使其能够快速接替自己的工作。</p>\n<h4 id=\"【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。\"><a href=\"#【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。\" class=\"headerlink\" title=\"【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。\"></a>【参考】特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码。</h4><p>1 ） 待办事宜 （TODO） : （ 标记人，标记时间， [ 预计处理时间 ]）<br>表示需要实现，但目前还未实现的功能。这实际上是一个 Javadoc 的标签，目前的 Javadoc<br>还没有实现，但已经被广泛使用。只能应用于类，接口和方法 （ 因为它是一个 Javadoc 标签 ） 。<br>2 ） 错误，不能工作 （FIXME） : （ 标记人，标记时间， [ 预计处理时间 ]）<br>在注释中用 FIXME 标记某代码是错误的，而且不能工作，需要及时纠正的情况。</p>\n<h2 id=\"其它\"><a href=\"#其它\" class=\"headerlink\" title=\"其它\"></a>其它</h2><h4 id=\"【强制】在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。\"><a href=\"#【强制】在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。\" class=\"headerlink\" title=\"【强制】在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。\"></a>【强制】在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度。</h4><p>说明：不要在方法体内定义： Pattern pattern =  Pattern . compile( 规则 );</p>\n<h4 id=\"【强制】后台输送给页面的变量必须加-var-——中间的感叹号。\"><a href=\"#【强制】后台输送给页面的变量必须加-var-——中间的感叹号。\" class=\"headerlink\" title=\"【强制】后台输送给页面的变量必须加 $!{var} ——中间的感叹号。\"></a>【强制】后台输送给页面的变量必须加 $!{var} ——中间的感叹号。</h4><p>说明：如果 var = null 或者不存在，那么 ${var} 会直接显示在页面上。<br>思考：这里是 Velocity 模板引擎的一些内容，Velocity是一个基于Java的模板引擎，通过特定的语法，Velocity可以获取在java语言中定义的对象，从而实现界面和java代码的真正分离，这意味着可以使用velocity替代jsp的开发模式了。这使得前端开发人员可以和 Java 程序开发人员同步开发一个遵循 MVC 架构的 web 站点，在实际应用中，velocity还可以应用于很多其他的场景。，比如源代码生成、自动email和转换xml等。</p>\n<h4 id=\"【强制】注意-Math-random-这个方法返回是-double-类型，注意取值的范围-0≤-x-lt-1-（-能够取到零值，注意除零异常-）-，如果想获取整数类型的随机数，不要将-x-放大-10-的若干倍然后取整，直接使用-Random-对象的-nextInt-或者-nextLong-方法。\"><a href=\"#【强制】注意-Math-random-这个方法返回是-double-类型，注意取值的范围-0≤-x-lt-1-（-能够取到零值，注意除零异常-）-，如果想获取整数类型的随机数，不要将-x-放大-10-的若干倍然后取整，直接使用-Random-对象的-nextInt-或者-nextLong-方法。\" class=\"headerlink\" title=\"【强制】注意  Math . random() 这个方法返回是 double 类型，注意取值的范围 0≤ x &lt;1 （ 能够取到零值，注意除零异常 ） ，如果想获取整数类型的随机数，不要将 x 放大 10 的若干倍然后取整，直接使用 Random 对象的 nextInt 或者 nextLong 方法。\"></a>【强制】注意  Math . random() 这个方法返回是 double 类型，注意取值的范围 0≤ x &lt;1 （ 能够取到零值，注意除零异常 ） ，如果想获取整数类型的随机数，不要将 x 放大 10 的若干倍然后取整，直接使用 Random 对象的 nextInt 或者 nextLong 方法。</h4><h4 id=\"【强制】获取当前毫秒数-System-currentTimeMillis-而不是-new-Date-getTime\"><a href=\"#【强制】获取当前毫秒数-System-currentTimeMillis-而不是-new-Date-getTime\" class=\"headerlink\" title=\"【强制】获取当前毫秒数 System . currentTimeMillis(); 而不是 new Date() . getTime();\"></a>【强制】获取当前毫秒数 System . currentTimeMillis(); 而不是 new Date() . getTime();</h4><p>说明：如果想获取更加精确的纳秒级时间值，用 System . nanoTime() 。在 JDK 8 中，针对统计时间等场景，推荐使用 Instant类。</p>\n<h4 id=\"【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。\"><a href=\"#【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。\" class=\"headerlink\" title=\"【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。\"></a>【推荐】任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存。</h4><h4 id=\"【推荐】对于“明确停止使用的代码和配置”，如方法、变量、类、配置文件、动态配置属性等要坚决从程序中清理出去，避免造成过多垃圾。\"><a href=\"#【推荐】对于“明确停止使用的代码和配置”，如方法、变量、类、配置文件、动态配置属性等要坚决从程序中清理出去，避免造成过多垃圾。\" class=\"headerlink\" title=\"【推荐】对于“明确停止使用的代码和配置”，如方法、变量、类、配置文件、动态配置属性等要坚决从程序中清理出去，避免造成过多垃圾。\"></a>【推荐】对于“明确停止使用的代码和配置”，如方法、变量、类、配置文件、动态配置属性等要坚决从程序中清理出去，避免造成过多垃圾。</h4>"},{"title":"OpenApi之我浅薄见解","toc":true,"date":"2017-03-29T07:15:28.000Z","_content":"\n说到OpenAPI，顾名思义，主要包含两个内容：\n\n\n\nOpen：权限和安全\nAPI：对外提供统一的访问接口，我们将我们的能力封装成一系列API\n\n\n\n\n","source":"_posts/2017-03-29-OpenApi之我浅薄见解.md","raw":"---\ntitle: OpenApi之我浅薄见解\ntoc: true\ndate: 2017-03-29 15:15:28\ntags: \n- java\n- 架构\ncategories: java\n---\n\n说到OpenAPI，顾名思义，主要包含两个内容：\n\n\n\nOpen：权限和安全\nAPI：对外提供统一的访问接口，我们将我们的能力封装成一系列API\n\n\n\n\n","slug":"OpenApi之我浅薄见解","published":1,"updated":"2017-03-31T14:02:40.954Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfx9003apsgunfehs9ab","content":"<p>说到OpenAPI，顾名思义，主要包含两个内容：</p>\n<p>Open：权限和安全<br>API：对外提供统一的访问接口，我们将我们的能力封装成一系列API</p>\n","excerpt":"","more":"<p>说到OpenAPI，顾名思义，主要包含两个内容：</p>\n<p>Open：权限和安全<br>API：对外提供统一的访问接口，我们将我们的能力封装成一系列API</p>\n"},{"title":"阿里巴巴Java开发手册学习笔记（下）","toc":false,"date":"2017-03-24T11:57:27.000Z","_content":"\n# 异常日志\n\n## 异常处理\n#### 【强制】不要捕获 Java 类库中定义的继承自 RuntimeException 的运行时异常类，如：IndexOutOfBoundsException / NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。\n正例： if(obj != null) {...}\n反例： try { obj.method() } catch(NullPointerException e){...}\n\n#### 【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。\n思考：Scala中用模式匹配处理异常的效率是否会高？\n\n#### 【强制】对大段代码进行 try - catch ，这是不负责任的表现。 \ncatch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。\n\n#### 【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之；\n如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。\n思考：\n\n#### 【强制】有 try 块放到了事务代码中， catch 异常后，如果需要回滚事务，一定要注意手动回滚事务。\n\n#### 【强制】 finally 块必须对资源对象、流对象进行关闭，有异常也要做 try - catch 。\n说明：如果 JDK 7，可以使用 try - with - resources 方式。\n思考：finally中的语句，也根据情况要做try-cache\n\n#### 【强制】不能在 finally 块中使用 return ， finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。\n思考：finally中的return有效，但是就算有效，也没有意义，因为无论如何，都会返回同样的值\n\n#### 【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。\n说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。\n\n#### 推荐】方法的返回值可以为 null ，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回 null 值。调用方需要进行 null 判断防止 NPE 问题。\n说明：本规约明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败，运行时异常等场景返回 null 的情况\n思考：如果返回的是null，需要添加说明。\n\n#### 【推荐】防止 NPE ，是程序员的基本修养，注意 NPE 产生的场景：\n1 ） 返回类型为包装数据类型，有可能是 null ，返回 int 值时注意判空。\n反例： public int f() {  return Integer 对象}; 如果为 null ，自动解箱抛 NPE 。\n2 ） 数据库的查询结果可能为 null 。\n3 ） 集合里的元素即使 isNotEmpty ，取出的数据元素也可能为 null 。\n4 ） **远程调用返回对象，一律要求进行 NPE 判断。**\n5 ） 对于 Session 中获取的数据，建议 NPE 检查，避免空指针。\n6 ） **级联调用** obj . getA() . getB() . getC()； 一连串调用，易产生 NPE 。\n\n#### 【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的 http / api 开放接口必须使用“错误码” ； 而应用内部推荐异常抛出 ； 跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess 、“错误码”、“错误简短信息”。\n说明：关于 RPC 方法返回方式使用 Result 方式的理由：\n1 ） 使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。\n2 ） 如果不加栈信息，只是 new 自定义异常，加入自己的理解的 error message ，对于调用\n端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输\n的性能损耗也是问题。\n\n#### 【推荐】定义时区分 unchecked /  checked 异常，避免直接使用 RuntimeException 抛出，更不允许抛出 Exception 或者 Throwable ，应使用有业务含义的自定义异常。\n\n#### 【参考】避免出现重复的代码 （Don ’ t Repeat Yourself） ，即 DRY 原则。\n说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副\n本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是共用模块。\n正例：一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取：\nprivate boolean checkParam(DTO dto){...}\n\n\n## 日志规约\n\n#### 【强制】应用中不可直接使用日志系统 （Log 4 j 、 Logback） 中的 API ，而应依赖使用日志框架SLF4J中的 API ，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nprivate static final Logger logger = LoggerFactory.getLogger(Abc.class);\n思考：回头要注意，使用门面模式的日志框架\n\n#### 【强制】日志文件推荐至少保存 15 天，因为有些异常具备以“周”为频次发生的特点。\n\n#### 【强制】应用中的扩展日志 （ 如打点、临时监控、访问日志等 ） 命名方式：\nappName_logType_logName.log 。 \nlogType :日志类型，推荐分类有 stats / desc / monitor / visit 等 ；\nlogName :日志描述。这种命名的好处：通过文件名就可知道日志文件属于什么应用，什么类型，什么目的，也有利于归类查找。\n正例： mppserver 应用中单独监控时区转换异常，如：\nmppserver _ monitor _ timeZoneConvert . log\n说明：推荐对日志进行分类，错误日志和业务日志尽量分开存放，便于开发人员查看，也便于\n通过日志对系统进行及时监控。\n思考：原来我想到的别人已经想到了，一定践行；\n\n#### 【强制】对 trace / debug / info 级别的日志输出，必须使用条件输出形式或者使用占位符的方式。\n说明： logger . debug( \" Processing trade with id : \" +  id + \"  symbol : \" +  symbol);\n如果日志级别是 warn ，上述日志不会打印，但是会执行字符串拼接操作，如果 symbol 是对象，\n会执行 toString() 方法，浪费了系统资源，执行了上述操作，最终日志却没有打印。\n正例： （ 条件 ）\n``` java\nif (logger.isDebugEnabled()) {\n\tlogger.debug(\"Processing trade with id: \" + id + \" symbol: \" + symbol);\n}\n```\n正例： （ 占位符 ）\n``` java\nlogger.debug(\"Processing trade with id: {} symbol : {} \", id, symbol);\n```\n思考：这一点非常重要，学到了，既然这样，就直接统一使用占位符的方式吧；\n\n#### 【强制】避免重复打印日志，浪费磁盘空间，务必在 log4j.xml 中设置 additivity = false。\n正例： <logger name=\"com.taobao.dubbo.config\" additivity=\"false\"> \n\n#### 【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么往上抛。\n正例： logger.error(各类参数或者对象 toString + \"_\" + e.getMessage(), e);\n思考：一定要注意保留这两类信息：e.getMessage();\n\n#### 【推荐】可以使用 warn 日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。\n注意日志输出的级别， error 级别只记录系统逻辑出错、异常等重要的错误信息。如非必要，请不要在此场景打出 error 级别。\n\n#### 【推荐】谨慎地记录日志。生产环境禁止输出 debug 日志 ； 有选择地输出 info 日志 ； 如果使用 warn 来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。\n说明：大量地输出无效日志，不利于系统性能提升，也不利于快速定位错误点。记录日志时请\n思考：这些日志真的有人看吗？看到这条日志你能做什么？能不能给问题排查带来好处？\n\n# Mysql规约\n\n## 建表规约\n#### 【强制】表达是与否概念的字段，必须使用 is _ xxx 的方式命名，数据类型是 unsigned tinyint（1表示是，0表示否），此规则同样适用于 odps 建表。\n说明：任何字段如果为非负数，必须是 unsigned 。\n\n#### 【强制】表名、字段名必须使用小写字母或数字 ； 禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\n正例： getter _ admin ， task _ config ， level 3_ name\n反例： GetterAdmin ， taskConfig ， level _3_ name\n思考：因为以前遇到坑，不同数据库大小写规则不一样，所以都用小写\n\n#### 【强制】表名不使用复数名词。\n说明：表名应该仅仅表示表里面的实体内容，不应该表示实体数量，对应于 DO 类名也是单数形式，符合表达习惯。\n\n#### 【强制】唯一索引名为 uk _字段名 ； 普通索引名则为 idx _字段名。\n说明： uk _ 即  unique key；idx _ 即 index 的简称。\n\n#### 【强制】小数类型为 decimal ，禁止使用 float 和 double 。\n说明： float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到不\n正确的结果。\n如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储。\n\n#### 【强制】如果存储的字符串长度几乎相等，使用 char 定长字符串类型。\n\n#### 【强制】 varchar 是可变长字符串，不预先分配存储空间，长度不要超过 5000，如果存储长度大于此值，定义字段类型为 text ，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\n\n#### 【强制】表必备三字段： id ,  gmt_create,gmt_modified。\n说明：其中 id 必为主键，类型为 unsigned bigint 、单表时自增、步长为 1。 gmt_create,gmt_modified 的类型均为 date_time 类型。\n\n#### 【推荐】表的命名最好是加上“业务名称_表的作用”。\n正例： tiger_task/tiger_reader/mpp_config\n\n#### 【推荐】库名与应用名称尽量一致。\n\n#### 【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：\n1 ） 不是频繁修改的字段。\n2 ） 不是 varchar 超长字段，更不能是 text 字段。\n正例：商品类目名称使用频率高，字段长度短，名称基本一成不变，可在相关联的表中冗余存\n储类目名称，避免关联查询。\n\n#### 【推荐】单表行数超过 500 万行或者单表容量超过 2 GB ，才推荐进行分库分表。\n说明：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。\n\n## 索引规约\n这一块，现在理解不深，需要使用的时候再理解\n\n#### 【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。\n说明：不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明\n显的 ；\n\n#### 【强制】 超过三个表禁止 join 。需要 join 的字段，数据类型保持绝对一致 ； 多表关联查询时，保证被关联的字段需要有索引。\n说明：即使双表 join 也要注意表索引、 SQL 性能。\n\n#### 【强制】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\n说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会高达 90%以上，可以使用 count(distinct left( 列名, 索引长度 )) / count( * ) 的区分度来确定。\n\n#### 【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\n说明：索引文件具有 B - Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。\n\n####  【参考】创建索引时避免有如下极端误解：\n1 ） 误认为一个查询就需要建一个索引。\n2 ） 误认为索引会消耗空间、严重拖慢更新和新增速度。\n3 ） 误认为唯一索引一律需要在应用层通过“先查后插”方式解决。\n\n## SQL规约\n#### 【强制】不要使用count(列名)或count(常量)来替代count(*)，count(*)就是 SQL92定义的标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。\n说明： count(*)会统计值为NULL的行，而count(列名)不会统计此列为NULL值的行。\n\n#### 【强制】使用 ISNULL() 来判断是否为 NULL 值。注意： NULL 与任何值的直接比较都为 NULL。\n说明：\n1 ） NULL<>NULL 的返回结果是 NULL ，而不是 false 。\n2 ） NULL=NULL 的返回结果是 NULL ，而不是 true 。\n3 ） NULL<>1 的返回结果是 NULL ，而不是 true 。\n\n#### 【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\n\n#### 【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\n\n#### 【强制】数据订正时，删除和修改记录时，要先 select ，避免出现误删除，确认无误才能执行更新语句。\n\n#### 【推荐】 in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在 1000 个之内。\n\n#### 【参考】如果有全球化需要，所有的字符存储与表示，均以 utf -8 编码，那么字符计数方法\n说明：\n\tSELECT LENGTH( \"轻松工作\" )； 返回为 12\n\tSELECT CHARACTER_LENGTH( \"轻松工作\" )； 返回为 4\n\t如果要使用表情，那么使用 utfmb 4 来进行存储，注意它与 utf -8 编码的区别。\n\n#### 【参考】TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发 trigger ，有可能造成事故，故不建议在开发代码中使用此语句。\n说明： TRUNCATE TABLE 在功能上与不带  WHERE 子句的  DELETE 语句相同。\n\n## ORM规约\n\n#### 【强制】在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。\n说明：1 ） 增加查询分析器解析成本。2 ） 增减字段容易与 resultMap 配置不一致。\n思考：这一点我做得不好，一定需要注意；\n\n#### 【强制】 POJO 类的 boolean 属性不能加 is ，而数据库字段必须加 is _，要求在 resultMap中进行字段与属性之间的映射。\n说明：参见定义 POJO 类以及数据库字段定义规定，在 sql . xml 增加映射，是必须的。\n\n#### 【强制】不要用 resultClass 当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义 ； 反过来，每一个表也必然有一个与之对应。\n说明：配置映射关系，使字段与 DO 类解耦，方便维护。\n\n#### 【强制】 xml 配置中参数注意使用：#{}，# param # 不要使用${} 此种方式容易出现SQL注入。\n\n#### 【强制】不允许直接拿 HashMap 与 Hashtable 作为查询结果集的输出。\n思考：为啥？\n\n#### 【强制】更新数据表记录时，必须同时更新记录对应的 gmt _ modified 字段值为当前时间。\n\n#### 【推荐】不要写一个大而全的数据更新接口，传入为 POJO 类，不管是不是自己的目标更新字段，都进行 update table set c1=value1,c2=value2,c3=value3;  这是不对的。\n执行 SQL时，尽量不要更新无改动的字段，一是易出错 ； 二是效率低 ； 三是 binlog 增加存储。\n\n#### 【参考】@ Transactional 事务不要滥用。事务会影响数据库的 QPS ，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。\n\n#### 【参考】< isEqual >中的 compareValue 是与属性值对比的常量，一般是数字，表示相等时带上此条件 ； < isNotEmpty >表示不为空且不为 null 时执行 ； < isNotNull >表示不为 null 值时执行。\n\n## 工程规约\n\n#### 【强制】定义 GAV 遵从以下规则：\n1 ） GroupID 格式：com.{公司/BU}.业务线.[子业务线]，最多4级。\n说明：{公司/BU}例如：alibaba/taobao/tmall/aliexpress等BU一级；子业务线可选。\n正例：com.taobao.jstorm或com.alibaba.dubbo.register\n2）ArtifactID格式：产品线名-模块名。语义不重复不遗漏，先到仓库中心去查证一下。\n正例：dubbo-client/fastjson-api/jstorm-tool\n3）Version：详细规定参考下方。\n\n思考：com.ctcc.bigdata   openapi-storage\n\n#### 【强制】二方库版本号命名方式：主版本号.次版本号.修订号\n1 ） 主版本号 ：当做了不兼容的 API 修改，或者增加了能改变产品方向的新功能。\n2 ） 次版本号 ：当做了向下兼容的功能性新增 （ 新增类、接口等 ） 。\n3 ） 修订号 ：修复 bug ，没有修改方法签名的功能加强，保持  API 兼容性。\n说明：起始版本号必须为： 1.0.0 ，而不是 0.0.1\n\n#### 【强制】线上应用不要依赖 SNAPSHOT 版本 （ 安全包除外 ）； \n正式发布的类库必须使用 RELEASE版本号升级+1 的方式，且版本号不允许覆盖升级，必须去中央仓库进行查证。\n说明：不依赖 SNAPSHOT 版本是保证应用发布的幂等性。另外，也可以加快编译时的打包构建。\n\n#### 【强制】二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的 POJO 对象。\n\n#### 【强制】依赖于一个二方库群时，必须定义一个统一版本变量，避免版本号不一致。\n说明：依赖 springframework - core ,- context ,- beans ，它们都是同一个版本，可以定义一个变量来保存版本：${ spring . version }，定义依赖的时候，引用该版本。\n\n#### 【强制】禁止在子项目的 pom 依赖中出现相同的 GroupId ，相同的 ArtifactId ，但是不同的Version 。\n说明：在本地调试时会使用各子项目指定的版本号，但是合并成一个 war ，只能有一个版本号出现在最后的 lib 目录中。曾经出现过线下调试是正确的，发布到线上出故障的先例。\n\n## 服务器规约\n\n#### 【推荐】高并发服务器建议调小 TCP 协议的 time _ wait 超时时间。\n说明：操作系统默认 240 秒后，才会关闭处于 time _ wait 状态的连接，在高并发访问下，服务器端会因为处于 time _ wait 的连接数太多，可能无法建立新的连接，所以需要在服务器上调小此等待值。\n\n#### 【推荐】调大服务器所支持的最大文件句柄数 （File Descriptor ，简写为 fd） 。\n说明：主流操作系统的设计是将 TCP / UDP 连接采用与文件一样的方式去管理，即一个连接对应于一个 fd 。主流的 linux 服务器默认所支持最大 fd 数量为 1024，当并发连接数很大时很容易因为 fd 不足而出现“ open too many files ”错误，导致新的连接无法建立。 建议将 linux服务器所支持的最大句柄数调高数倍 （ 与服务器的内存数量相关 ） 。\n思考：kafka ，flume等中间件也要这么配\n\n#### 【推荐】给 JVM 设置- XX :+ HeapDumpOnOutOfMemoryError 参数，让 JVM 碰到 OOM 场景时输出dump 信息。\n说明： OOM 的发生是有概率的，甚至有规律地相隔数月才出现一例，出现时的现场信息对查错非常有价值。\n\n#### 【参考】服务器内部重定向使用 forward； 外部重定向地址使用 URL 拼装工具类来生成，否则会带来 URL 维护不一致的问题和潜在的安全风险。\n\n## 安全规约\n\n#### 【强制】隶属于用户个人的页面或者功能必须进行权限控制校验。\n说明：防止没有做水平权限校验就可随意访问、操作别人的数据，比如查看、修改别人的订单。\n\n#### 【强制】用户敏感数据禁止直接展示，必须对展示数据脱敏。\n说明：查看个人手机号码会显示成:158****9119，隐藏中间 4 位，防止隐私泄露。\n思考：在openAPI项目中一定要注意；\n\n#### 【强制】用户输入的 SQL 参数严格使用参数绑定或者 METADATA 字段值限定，防止 SQL 注入，\n禁止字符串拼接 SQL 访问数据库。\n\n#### 【强制】用户请求传入的任何参数必须做有效性验证。\n说明：忽略参数校验可能导致：\n  page size 过大导致内存溢出\n  恶意 order by 导致数据库慢查询\n  任意重定向\n  SQL 注入\n  反序列化注入\n  正则输入源串拒绝服务 ReDoS\n说明：Java 代码用正则来验证客户端的输入，有些正则写法验证普通用户输入没有问题，但是如果攻击人员使用的是特殊构造的字符串来验证，有可能导致死循环的效果。\n\n#### 【强制】禁止向 HTML 页面输出未经安全过滤或未正确转义的用户数据。\n\n#### 【强制】在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放限制，如数量限制、疲劳度控制、验证码校验，避免被滥刷、资损。\n说明：如注册时发送验证码到手机，如果没有限制次数和频率，那么可以利用此功能骚扰到其\n它用户，并造成短信平台资源浪费。\n\n# 思考\n这两天断断续续看完了全部，看得时候觉得受益匪浅，但是这种东西要消化成自己的知识，一定要践行，等编码到具体部分的时候再复习，写出优秀的代码。\n\n\n\n","source":"_posts/2017-03-24-阿里巴巴Java开发手册学习笔记2.md","raw":"---\ntitle: 阿里巴巴Java开发手册学习笔记（下）\ntoc: false\ndate: 2017-03-24 19:57:27\ntags: java\ncategories: java\n---\n\n# 异常日志\n\n## 异常处理\n#### 【强制】不要捕获 Java 类库中定义的继承自 RuntimeException 的运行时异常类，如：IndexOutOfBoundsException / NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。\n正例： if(obj != null) {...}\n反例： try { obj.method() } catch(NullPointerException e){...}\n\n#### 【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。\n思考：Scala中用模式匹配处理异常的效率是否会高？\n\n#### 【强制】对大段代码进行 try - catch ，这是不负责任的表现。 \ncatch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。\n\n#### 【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之；\n如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。\n思考：\n\n#### 【强制】有 try 块放到了事务代码中， catch 异常后，如果需要回滚事务，一定要注意手动回滚事务。\n\n#### 【强制】 finally 块必须对资源对象、流对象进行关闭，有异常也要做 try - catch 。\n说明：如果 JDK 7，可以使用 try - with - resources 方式。\n思考：finally中的语句，也根据情况要做try-cache\n\n#### 【强制】不能在 finally 块中使用 return ， finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。\n思考：finally中的return有效，但是就算有效，也没有意义，因为无论如何，都会返回同样的值\n\n#### 【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。\n说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。\n\n#### 推荐】方法的返回值可以为 null ，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回 null 值。调用方需要进行 null 判断防止 NPE 问题。\n说明：本规约明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败，运行时异常等场景返回 null 的情况\n思考：如果返回的是null，需要添加说明。\n\n#### 【推荐】防止 NPE ，是程序员的基本修养，注意 NPE 产生的场景：\n1 ） 返回类型为包装数据类型，有可能是 null ，返回 int 值时注意判空。\n反例： public int f() {  return Integer 对象}; 如果为 null ，自动解箱抛 NPE 。\n2 ） 数据库的查询结果可能为 null 。\n3 ） 集合里的元素即使 isNotEmpty ，取出的数据元素也可能为 null 。\n4 ） **远程调用返回对象，一律要求进行 NPE 判断。**\n5 ） 对于 Session 中获取的数据，建议 NPE 检查，避免空指针。\n6 ） **级联调用** obj . getA() . getB() . getC()； 一连串调用，易产生 NPE 。\n\n#### 【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的 http / api 开放接口必须使用“错误码” ； 而应用内部推荐异常抛出 ； 跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess 、“错误码”、“错误简短信息”。\n说明：关于 RPC 方法返回方式使用 Result 方式的理由：\n1 ） 使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。\n2 ） 如果不加栈信息，只是 new 自定义异常，加入自己的理解的 error message ，对于调用\n端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输\n的性能损耗也是问题。\n\n#### 【推荐】定义时区分 unchecked /  checked 异常，避免直接使用 RuntimeException 抛出，更不允许抛出 Exception 或者 Throwable ，应使用有业务含义的自定义异常。\n\n#### 【参考】避免出现重复的代码 （Don ’ t Repeat Yourself） ，即 DRY 原则。\n说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副\n本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是共用模块。\n正例：一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取：\nprivate boolean checkParam(DTO dto){...}\n\n\n## 日志规约\n\n#### 【强制】应用中不可直接使用日志系统 （Log 4 j 、 Logback） 中的 API ，而应依赖使用日志框架SLF4J中的 API ，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nprivate static final Logger logger = LoggerFactory.getLogger(Abc.class);\n思考：回头要注意，使用门面模式的日志框架\n\n#### 【强制】日志文件推荐至少保存 15 天，因为有些异常具备以“周”为频次发生的特点。\n\n#### 【强制】应用中的扩展日志 （ 如打点、临时监控、访问日志等 ） 命名方式：\nappName_logType_logName.log 。 \nlogType :日志类型，推荐分类有 stats / desc / monitor / visit 等 ；\nlogName :日志描述。这种命名的好处：通过文件名就可知道日志文件属于什么应用，什么类型，什么目的，也有利于归类查找。\n正例： mppserver 应用中单独监控时区转换异常，如：\nmppserver _ monitor _ timeZoneConvert . log\n说明：推荐对日志进行分类，错误日志和业务日志尽量分开存放，便于开发人员查看，也便于\n通过日志对系统进行及时监控。\n思考：原来我想到的别人已经想到了，一定践行；\n\n#### 【强制】对 trace / debug / info 级别的日志输出，必须使用条件输出形式或者使用占位符的方式。\n说明： logger . debug( \" Processing trade with id : \" +  id + \"  symbol : \" +  symbol);\n如果日志级别是 warn ，上述日志不会打印，但是会执行字符串拼接操作，如果 symbol 是对象，\n会执行 toString() 方法，浪费了系统资源，执行了上述操作，最终日志却没有打印。\n正例： （ 条件 ）\n``` java\nif (logger.isDebugEnabled()) {\n\tlogger.debug(\"Processing trade with id: \" + id + \" symbol: \" + symbol);\n}\n```\n正例： （ 占位符 ）\n``` java\nlogger.debug(\"Processing trade with id: {} symbol : {} \", id, symbol);\n```\n思考：这一点非常重要，学到了，既然这样，就直接统一使用占位符的方式吧；\n\n#### 【强制】避免重复打印日志，浪费磁盘空间，务必在 log4j.xml 中设置 additivity = false。\n正例： <logger name=\"com.taobao.dubbo.config\" additivity=\"false\"> \n\n#### 【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么往上抛。\n正例： logger.error(各类参数或者对象 toString + \"_\" + e.getMessage(), e);\n思考：一定要注意保留这两类信息：e.getMessage();\n\n#### 【推荐】可以使用 warn 日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。\n注意日志输出的级别， error 级别只记录系统逻辑出错、异常等重要的错误信息。如非必要，请不要在此场景打出 error 级别。\n\n#### 【推荐】谨慎地记录日志。生产环境禁止输出 debug 日志 ； 有选择地输出 info 日志 ； 如果使用 warn 来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。\n说明：大量地输出无效日志，不利于系统性能提升，也不利于快速定位错误点。记录日志时请\n思考：这些日志真的有人看吗？看到这条日志你能做什么？能不能给问题排查带来好处？\n\n# Mysql规约\n\n## 建表规约\n#### 【强制】表达是与否概念的字段，必须使用 is _ xxx 的方式命名，数据类型是 unsigned tinyint（1表示是，0表示否），此规则同样适用于 odps 建表。\n说明：任何字段如果为非负数，必须是 unsigned 。\n\n#### 【强制】表名、字段名必须使用小写字母或数字 ； 禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\n正例： getter _ admin ， task _ config ， level 3_ name\n反例： GetterAdmin ， taskConfig ， level _3_ name\n思考：因为以前遇到坑，不同数据库大小写规则不一样，所以都用小写\n\n#### 【强制】表名不使用复数名词。\n说明：表名应该仅仅表示表里面的实体内容，不应该表示实体数量，对应于 DO 类名也是单数形式，符合表达习惯。\n\n#### 【强制】唯一索引名为 uk _字段名 ； 普通索引名则为 idx _字段名。\n说明： uk _ 即  unique key；idx _ 即 index 的简称。\n\n#### 【强制】小数类型为 decimal ，禁止使用 float 和 double 。\n说明： float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到不\n正确的结果。\n如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储。\n\n#### 【强制】如果存储的字符串长度几乎相等，使用 char 定长字符串类型。\n\n#### 【强制】 varchar 是可变长字符串，不预先分配存储空间，长度不要超过 5000，如果存储长度大于此值，定义字段类型为 text ，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\n\n#### 【强制】表必备三字段： id ,  gmt_create,gmt_modified。\n说明：其中 id 必为主键，类型为 unsigned bigint 、单表时自增、步长为 1。 gmt_create,gmt_modified 的类型均为 date_time 类型。\n\n#### 【推荐】表的命名最好是加上“业务名称_表的作用”。\n正例： tiger_task/tiger_reader/mpp_config\n\n#### 【推荐】库名与应用名称尽量一致。\n\n#### 【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：\n1 ） 不是频繁修改的字段。\n2 ） 不是 varchar 超长字段，更不能是 text 字段。\n正例：商品类目名称使用频率高，字段长度短，名称基本一成不变，可在相关联的表中冗余存\n储类目名称，避免关联查询。\n\n#### 【推荐】单表行数超过 500 万行或者单表容量超过 2 GB ，才推荐进行分库分表。\n说明：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。\n\n## 索引规约\n这一块，现在理解不深，需要使用的时候再理解\n\n#### 【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。\n说明：不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明\n显的 ；\n\n#### 【强制】 超过三个表禁止 join 。需要 join 的字段，数据类型保持绝对一致 ； 多表关联查询时，保证被关联的字段需要有索引。\n说明：即使双表 join 也要注意表索引、 SQL 性能。\n\n#### 【强制】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\n说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会高达 90%以上，可以使用 count(distinct left( 列名, 索引长度 )) / count( * ) 的区分度来确定。\n\n#### 【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\n说明：索引文件具有 B - Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。\n\n####  【参考】创建索引时避免有如下极端误解：\n1 ） 误认为一个查询就需要建一个索引。\n2 ） 误认为索引会消耗空间、严重拖慢更新和新增速度。\n3 ） 误认为唯一索引一律需要在应用层通过“先查后插”方式解决。\n\n## SQL规约\n#### 【强制】不要使用count(列名)或count(常量)来替代count(*)，count(*)就是 SQL92定义的标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。\n说明： count(*)会统计值为NULL的行，而count(列名)不会统计此列为NULL值的行。\n\n#### 【强制】使用 ISNULL() 来判断是否为 NULL 值。注意： NULL 与任何值的直接比较都为 NULL。\n说明：\n1 ） NULL<>NULL 的返回结果是 NULL ，而不是 false 。\n2 ） NULL=NULL 的返回结果是 NULL ，而不是 true 。\n3 ） NULL<>1 的返回结果是 NULL ，而不是 true 。\n\n#### 【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\n\n#### 【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\n\n#### 【强制】数据订正时，删除和修改记录时，要先 select ，避免出现误删除，确认无误才能执行更新语句。\n\n#### 【推荐】 in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在 1000 个之内。\n\n#### 【参考】如果有全球化需要，所有的字符存储与表示，均以 utf -8 编码，那么字符计数方法\n说明：\n\tSELECT LENGTH( \"轻松工作\" )； 返回为 12\n\tSELECT CHARACTER_LENGTH( \"轻松工作\" )； 返回为 4\n\t如果要使用表情，那么使用 utfmb 4 来进行存储，注意它与 utf -8 编码的区别。\n\n#### 【参考】TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发 trigger ，有可能造成事故，故不建议在开发代码中使用此语句。\n说明： TRUNCATE TABLE 在功能上与不带  WHERE 子句的  DELETE 语句相同。\n\n## ORM规约\n\n#### 【强制】在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。\n说明：1 ） 增加查询分析器解析成本。2 ） 增减字段容易与 resultMap 配置不一致。\n思考：这一点我做得不好，一定需要注意；\n\n#### 【强制】 POJO 类的 boolean 属性不能加 is ，而数据库字段必须加 is _，要求在 resultMap中进行字段与属性之间的映射。\n说明：参见定义 POJO 类以及数据库字段定义规定，在 sql . xml 增加映射，是必须的。\n\n#### 【强制】不要用 resultClass 当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义 ； 反过来，每一个表也必然有一个与之对应。\n说明：配置映射关系，使字段与 DO 类解耦，方便维护。\n\n#### 【强制】 xml 配置中参数注意使用：#{}，# param # 不要使用${} 此种方式容易出现SQL注入。\n\n#### 【强制】不允许直接拿 HashMap 与 Hashtable 作为查询结果集的输出。\n思考：为啥？\n\n#### 【强制】更新数据表记录时，必须同时更新记录对应的 gmt _ modified 字段值为当前时间。\n\n#### 【推荐】不要写一个大而全的数据更新接口，传入为 POJO 类，不管是不是自己的目标更新字段，都进行 update table set c1=value1,c2=value2,c3=value3;  这是不对的。\n执行 SQL时，尽量不要更新无改动的字段，一是易出错 ； 二是效率低 ； 三是 binlog 增加存储。\n\n#### 【参考】@ Transactional 事务不要滥用。事务会影响数据库的 QPS ，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。\n\n#### 【参考】< isEqual >中的 compareValue 是与属性值对比的常量，一般是数字，表示相等时带上此条件 ； < isNotEmpty >表示不为空且不为 null 时执行 ； < isNotNull >表示不为 null 值时执行。\n\n## 工程规约\n\n#### 【强制】定义 GAV 遵从以下规则：\n1 ） GroupID 格式：com.{公司/BU}.业务线.[子业务线]，最多4级。\n说明：{公司/BU}例如：alibaba/taobao/tmall/aliexpress等BU一级；子业务线可选。\n正例：com.taobao.jstorm或com.alibaba.dubbo.register\n2）ArtifactID格式：产品线名-模块名。语义不重复不遗漏，先到仓库中心去查证一下。\n正例：dubbo-client/fastjson-api/jstorm-tool\n3）Version：详细规定参考下方。\n\n思考：com.ctcc.bigdata   openapi-storage\n\n#### 【强制】二方库版本号命名方式：主版本号.次版本号.修订号\n1 ） 主版本号 ：当做了不兼容的 API 修改，或者增加了能改变产品方向的新功能。\n2 ） 次版本号 ：当做了向下兼容的功能性新增 （ 新增类、接口等 ） 。\n3 ） 修订号 ：修复 bug ，没有修改方法签名的功能加强，保持  API 兼容性。\n说明：起始版本号必须为： 1.0.0 ，而不是 0.0.1\n\n#### 【强制】线上应用不要依赖 SNAPSHOT 版本 （ 安全包除外 ）； \n正式发布的类库必须使用 RELEASE版本号升级+1 的方式，且版本号不允许覆盖升级，必须去中央仓库进行查证。\n说明：不依赖 SNAPSHOT 版本是保证应用发布的幂等性。另外，也可以加快编译时的打包构建。\n\n#### 【强制】二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的 POJO 对象。\n\n#### 【强制】依赖于一个二方库群时，必须定义一个统一版本变量，避免版本号不一致。\n说明：依赖 springframework - core ,- context ,- beans ，它们都是同一个版本，可以定义一个变量来保存版本：${ spring . version }，定义依赖的时候，引用该版本。\n\n#### 【强制】禁止在子项目的 pom 依赖中出现相同的 GroupId ，相同的 ArtifactId ，但是不同的Version 。\n说明：在本地调试时会使用各子项目指定的版本号，但是合并成一个 war ，只能有一个版本号出现在最后的 lib 目录中。曾经出现过线下调试是正确的，发布到线上出故障的先例。\n\n## 服务器规约\n\n#### 【推荐】高并发服务器建议调小 TCP 协议的 time _ wait 超时时间。\n说明：操作系统默认 240 秒后，才会关闭处于 time _ wait 状态的连接，在高并发访问下，服务器端会因为处于 time _ wait 的连接数太多，可能无法建立新的连接，所以需要在服务器上调小此等待值。\n\n#### 【推荐】调大服务器所支持的最大文件句柄数 （File Descriptor ，简写为 fd） 。\n说明：主流操作系统的设计是将 TCP / UDP 连接采用与文件一样的方式去管理，即一个连接对应于一个 fd 。主流的 linux 服务器默认所支持最大 fd 数量为 1024，当并发连接数很大时很容易因为 fd 不足而出现“ open too many files ”错误，导致新的连接无法建立。 建议将 linux服务器所支持的最大句柄数调高数倍 （ 与服务器的内存数量相关 ） 。\n思考：kafka ，flume等中间件也要这么配\n\n#### 【推荐】给 JVM 设置- XX :+ HeapDumpOnOutOfMemoryError 参数，让 JVM 碰到 OOM 场景时输出dump 信息。\n说明： OOM 的发生是有概率的，甚至有规律地相隔数月才出现一例，出现时的现场信息对查错非常有价值。\n\n#### 【参考】服务器内部重定向使用 forward； 外部重定向地址使用 URL 拼装工具类来生成，否则会带来 URL 维护不一致的问题和潜在的安全风险。\n\n## 安全规约\n\n#### 【强制】隶属于用户个人的页面或者功能必须进行权限控制校验。\n说明：防止没有做水平权限校验就可随意访问、操作别人的数据，比如查看、修改别人的订单。\n\n#### 【强制】用户敏感数据禁止直接展示，必须对展示数据脱敏。\n说明：查看个人手机号码会显示成:158****9119，隐藏中间 4 位，防止隐私泄露。\n思考：在openAPI项目中一定要注意；\n\n#### 【强制】用户输入的 SQL 参数严格使用参数绑定或者 METADATA 字段值限定，防止 SQL 注入，\n禁止字符串拼接 SQL 访问数据库。\n\n#### 【强制】用户请求传入的任何参数必须做有效性验证。\n说明：忽略参数校验可能导致：\n  page size 过大导致内存溢出\n  恶意 order by 导致数据库慢查询\n  任意重定向\n  SQL 注入\n  反序列化注入\n  正则输入源串拒绝服务 ReDoS\n说明：Java 代码用正则来验证客户端的输入，有些正则写法验证普通用户输入没有问题，但是如果攻击人员使用的是特殊构造的字符串来验证，有可能导致死循环的效果。\n\n#### 【强制】禁止向 HTML 页面输出未经安全过滤或未正确转义的用户数据。\n\n#### 【强制】在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放限制，如数量限制、疲劳度控制、验证码校验，避免被滥刷、资损。\n说明：如注册时发送验证码到手机，如果没有限制次数和频率，那么可以利用此功能骚扰到其\n它用户，并造成短信平台资源浪费。\n\n# 思考\n这两天断断续续看完了全部，看得时候觉得受益匪浅，但是这种东西要消化成自己的知识，一定要践行，等编码到具体部分的时候再复习，写出优秀的代码。\n\n\n\n","slug":"阿里巴巴Java开发手册学习笔记2","published":1,"updated":"2017-03-24T09:07:15.878Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfxb003epsguyo29mh31","content":"<h1 id=\"异常日志\"><a href=\"#异常日志\" class=\"headerlink\" title=\"异常日志\"></a>异常日志</h1><h2 id=\"异常处理\"><a href=\"#异常处理\" class=\"headerlink\" title=\"异常处理\"></a>异常处理</h2><h4 id=\"【强制】不要捕获-Java-类库中定义的继承自-RuntimeException-的运行时异常类，如：IndexOutOfBoundsException-NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。\"><a href=\"#【强制】不要捕获-Java-类库中定义的继承自-RuntimeException-的运行时异常类，如：IndexOutOfBoundsException-NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。\" class=\"headerlink\" title=\"【强制】不要捕获 Java 类库中定义的继承自 RuntimeException 的运行时异常类，如：IndexOutOfBoundsException / NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。\"></a>【强制】不要捕获 Java 类库中定义的继承自 RuntimeException 的运行时异常类，如：IndexOutOfBoundsException / NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。</h4><p>正例： if(obj != null) {…}<br>反例： try { obj.method() } catch(NullPointerException e){…}</p>\n<h4 id=\"【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。\"><a href=\"#【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。\" class=\"headerlink\" title=\"【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。\"></a>【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。</h4><p>思考：Scala中用模式匹配处理异常的效率是否会高？</p>\n<h4 id=\"【强制】对大段代码进行-try-catch-，这是不负责任的表现。\"><a href=\"#【强制】对大段代码进行-try-catch-，这是不负责任的表现。\" class=\"headerlink\" title=\"【强制】对大段代码进行 try - catch ，这是不负责任的表现。\"></a>【强制】对大段代码进行 try - catch ，这是不负责任的表现。</h4><p>catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。</p>\n<h4 id=\"【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之；\"><a href=\"#【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之；\" class=\"headerlink\" title=\"【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之；\"></a>【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之；</h4><p>如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。<br>思考：</p>\n<h4 id=\"【强制】有-try-块放到了事务代码中，-catch-异常后，如果需要回滚事务，一定要注意手动回滚事务。\"><a href=\"#【强制】有-try-块放到了事务代码中，-catch-异常后，如果需要回滚事务，一定要注意手动回滚事务。\" class=\"headerlink\" title=\"【强制】有 try 块放到了事务代码中， catch 异常后，如果需要回滚事务，一定要注意手动回滚事务。\"></a>【强制】有 try 块放到了事务代码中， catch 异常后，如果需要回滚事务，一定要注意手动回滚事务。</h4><h4 id=\"【强制】-finally-块必须对资源对象、流对象进行关闭，有异常也要做-try-catch-。\"><a href=\"#【强制】-finally-块必须对资源对象、流对象进行关闭，有异常也要做-try-catch-。\" class=\"headerlink\" title=\"【强制】 finally 块必须对资源对象、流对象进行关闭，有异常也要做 try - catch 。\"></a>【强制】 finally 块必须对资源对象、流对象进行关闭，有异常也要做 try - catch 。</h4><p>说明：如果 JDK 7，可以使用 try - with - resources 方式。<br>思考：finally中的语句，也根据情况要做try-cache</p>\n<h4 id=\"【强制】不能在-finally-块中使用-return-，-finally-块中的-return-返回后方法结束执行，不会再执行-try-块中的-return-语句。\"><a href=\"#【强制】不能在-finally-块中使用-return-，-finally-块中的-return-返回后方法结束执行，不会再执行-try-块中的-return-语句。\" class=\"headerlink\" title=\"【强制】不能在 finally 块中使用 return ， finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。\"></a>【强制】不能在 finally 块中使用 return ， finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。</h4><p>思考：finally中的return有效，但是就算有效，也没有意义，因为无论如何，都会返回同样的值</p>\n<h4 id=\"【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。\"><a href=\"#【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。\" class=\"headerlink\" title=\"【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。\"></a>【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。</h4><p>说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。</p>\n<h4 id=\"推荐】方法的返回值可以为-null-，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回-null-值。调用方需要进行-null-判断防止-NPE-问题。\"><a href=\"#推荐】方法的返回值可以为-null-，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回-null-值。调用方需要进行-null-判断防止-NPE-问题。\" class=\"headerlink\" title=\"推荐】方法的返回值可以为 null ，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回 null 值。调用方需要进行 null 判断防止 NPE 问题。\"></a>推荐】方法的返回值可以为 null ，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回 null 值。调用方需要进行 null 判断防止 NPE 问题。</h4><p>说明：本规约明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败，运行时异常等场景返回 null 的情况<br>思考：如果返回的是null，需要添加说明。</p>\n<h4 id=\"【推荐】防止-NPE-，是程序员的基本修养，注意-NPE-产生的场景：\"><a href=\"#【推荐】防止-NPE-，是程序员的基本修养，注意-NPE-产生的场景：\" class=\"headerlink\" title=\"【推荐】防止 NPE ，是程序员的基本修养，注意 NPE 产生的场景：\"></a>【推荐】防止 NPE ，是程序员的基本修养，注意 NPE 产生的场景：</h4><p>1 ） 返回类型为包装数据类型，有可能是 null ，返回 int 值时注意判空。<br>反例： public int f() {  return Integer 对象}; 如果为 null ，自动解箱抛 NPE 。<br>2 ） 数据库的查询结果可能为 null 。<br>3 ） 集合里的元素即使 isNotEmpty ，取出的数据元素也可能为 null 。<br>4 ） <strong>远程调用返回对象，一律要求进行 NPE 判断。</strong><br>5 ） 对于 Session 中获取的数据，建议 NPE 检查，避免空指针。<br>6 ） <strong>级联调用</strong> obj . getA() . getB() . getC()； 一连串调用，易产生 NPE 。</p>\n<h4 id=\"【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的-http-api-开放接口必须使用“错误码”-；-而应用内部推荐异常抛出-；-跨应用间-RPC-调用优先考虑使用-Result-方式，封装-isSuccess-、“错误码”、“错误简短信息”。\"><a href=\"#【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的-http-api-开放接口必须使用“错误码”-；-而应用内部推荐异常抛出-；-跨应用间-RPC-调用优先考虑使用-Result-方式，封装-isSuccess-、“错误码”、“错误简短信息”。\" class=\"headerlink\" title=\"【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的 http / api 开放接口必须使用“错误码” ； 而应用内部推荐异常抛出 ； 跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess 、“错误码”、“错误简短信息”。\"></a>【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的 http / api 开放接口必须使用“错误码” ； 而应用内部推荐异常抛出 ； 跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess 、“错误码”、“错误简短信息”。</h4><p>说明：关于 RPC 方法返回方式使用 Result 方式的理由：<br>1 ） 使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。<br>2 ） 如果不加栈信息，只是 new 自定义异常，加入自己的理解的 error message ，对于调用<br>端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输<br>的性能损耗也是问题。</p>\n<h4 id=\"【推荐】定义时区分-unchecked-checked-异常，避免直接使用-RuntimeException-抛出，更不允许抛出-Exception-或者-Throwable-，应使用有业务含义的自定义异常。\"><a href=\"#【推荐】定义时区分-unchecked-checked-异常，避免直接使用-RuntimeException-抛出，更不允许抛出-Exception-或者-Throwable-，应使用有业务含义的自定义异常。\" class=\"headerlink\" title=\"【推荐】定义时区分 unchecked /  checked 异常，避免直接使用 RuntimeException 抛出，更不允许抛出 Exception 或者 Throwable ，应使用有业务含义的自定义异常。\"></a>【推荐】定义时区分 unchecked /  checked 异常，避免直接使用 RuntimeException 抛出，更不允许抛出 Exception 或者 Throwable ，应使用有业务含义的自定义异常。</h4><h4 id=\"【参考】避免出现重复的代码-（Don-’-t-Repeat-Yourself）-，即-DRY-原则。\"><a href=\"#【参考】避免出现重复的代码-（Don-’-t-Repeat-Yourself）-，即-DRY-原则。\" class=\"headerlink\" title=\"【参考】避免出现重复的代码 （Don ’ t Repeat Yourself） ，即 DRY 原则。\"></a>【参考】避免出现重复的代码 （Don ’ t Repeat Yourself） ，即 DRY 原则。</h4><p>说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副<br>本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是共用模块。<br>正例：一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取：<br>private boolean checkParam(DTO dto){…}</p>\n<h2 id=\"日志规约\"><a href=\"#日志规约\" class=\"headerlink\" title=\"日志规约\"></a>日志规约</h2><h4 id=\"【强制】应用中不可直接使用日志系统-（Log-4-j-、-Logback）-中的-API-，而应依赖使用日志框架SLF4J中的-API-，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。\"><a href=\"#【强制】应用中不可直接使用日志系统-（Log-4-j-、-Logback）-中的-API-，而应依赖使用日志框架SLF4J中的-API-，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。\" class=\"headerlink\" title=\"【强制】应用中不可直接使用日志系统 （Log 4 j 、 Logback） 中的 API ，而应依赖使用日志框架SLF4J中的 API ，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。\"></a>【强制】应用中不可直接使用日志系统 （Log 4 j 、 Logback） 中的 API ，而应依赖使用日志框架SLF4J中的 API ，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。</h4><p>import org.slf4j.Logger;<br>import org.slf4j.LoggerFactory;<br>private static final Logger logger = LoggerFactory.getLogger(Abc.class);<br>思考：回头要注意，使用门面模式的日志框架</p>\n<h4 id=\"【强制】日志文件推荐至少保存-15-天，因为有些异常具备以“周”为频次发生的特点。\"><a href=\"#【强制】日志文件推荐至少保存-15-天，因为有些异常具备以“周”为频次发生的特点。\" class=\"headerlink\" title=\"【强制】日志文件推荐至少保存 15 天，因为有些异常具备以“周”为频次发生的特点。\"></a>【强制】日志文件推荐至少保存 15 天，因为有些异常具备以“周”为频次发生的特点。</h4><h4 id=\"【强制】应用中的扩展日志-（-如打点、临时监控、访问日志等-）-命名方式：\"><a href=\"#【强制】应用中的扩展日志-（-如打点、临时监控、访问日志等-）-命名方式：\" class=\"headerlink\" title=\"【强制】应用中的扩展日志 （ 如打点、临时监控、访问日志等 ） 命名方式：\"></a>【强制】应用中的扩展日志 （ 如打点、临时监控、访问日志等 ） 命名方式：</h4><p>appName_logType<em>logName.log 。<br>logType :日志类型，推荐分类有 stats / desc / monitor / visit 等 ；<br>logName :日志描述。这种命名的好处：通过文件名就可知道日志文件属于什么应用，什么类型，什么目的，也有利于归类查找。<br>正例： mppserver 应用中单独监控时区转换异常，如：<br>mppserver </em> monitor _ timeZoneConvert . log<br>说明：推荐对日志进行分类，错误日志和业务日志尽量分开存放，便于开发人员查看，也便于<br>通过日志对系统进行及时监控。<br>思考：原来我想到的别人已经想到了，一定践行；</p>\n<h4 id=\"【强制】对-trace-debug-info-级别的日志输出，必须使用条件输出形式或者使用占位符的方式。\"><a href=\"#【强制】对-trace-debug-info-级别的日志输出，必须使用条件输出形式或者使用占位符的方式。\" class=\"headerlink\" title=\"【强制】对 trace / debug / info 级别的日志输出，必须使用条件输出形式或者使用占位符的方式。\"></a>【强制】对 trace / debug / info 级别的日志输出，必须使用条件输出形式或者使用占位符的方式。</h4><p>说明： logger . debug( “ Processing trade with id : “ +  id + “  symbol : “ +  symbol);<br>如果日志级别是 warn ，上述日志不会打印，但是会执行字符串拼接操作，如果 symbol 是对象，<br>会执行 toString() 方法，浪费了系统资源，执行了上述操作，最终日志却没有打印。<br>正例： （ 条件 ）<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (logger.isDebugEnabled()) &#123;</div><div class=\"line\">\tlogger.debug(<span class=\"string\">\"Processing trade with id: \"</span> + id + <span class=\"string\">\" symbol: \"</span> + symbol);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>正例： （ 占位符 ）<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">logger.debug(<span class=\"string\">\"Processing trade with id: &#123;&#125; symbol : &#123;&#125; \"</span>, id, symbol);</div></pre></td></tr></table></figure></p>\n<p>思考：这一点非常重要，学到了，既然这样，就直接统一使用占位符的方式吧；</p>\n<h4 id=\"【强制】避免重复打印日志，浪费磁盘空间，务必在-log4j-xml-中设置-additivity-false。\"><a href=\"#【强制】避免重复打印日志，浪费磁盘空间，务必在-log4j-xml-中设置-additivity-false。\" class=\"headerlink\" title=\"【强制】避免重复打印日志，浪费磁盘空间，务必在 log4j.xml 中设置 additivity = false。\"></a>【强制】避免重复打印日志，浪费磁盘空间，务必在 log4j.xml 中设置 additivity = false。</h4><p>正例： <logger name=\"com.taobao.dubbo.config\" additivity=\"false\"> </logger></p>\n<h4 id=\"【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么往上抛。\"><a href=\"#【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么往上抛。\" class=\"headerlink\" title=\"【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么往上抛。\"></a>【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么往上抛。</h4><p>正例： logger.error(各类参数或者对象 toString + “_” + e.getMessage(), e);<br>思考：一定要注意保留这两类信息：e.getMessage();</p>\n<h4 id=\"【推荐】可以使用-warn-日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。\"><a href=\"#【推荐】可以使用-warn-日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。\" class=\"headerlink\" title=\"【推荐】可以使用 warn 日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。\"></a>【推荐】可以使用 warn 日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。</h4><p>注意日志输出的级别， error 级别只记录系统逻辑出错、异常等重要的错误信息。如非必要，请不要在此场景打出 error 级别。</p>\n<h4 id=\"【推荐】谨慎地记录日志。生产环境禁止输出-debug-日志-；-有选择地输出-info-日志-；-如果使用-warn-来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。\"><a href=\"#【推荐】谨慎地记录日志。生产环境禁止输出-debug-日志-；-有选择地输出-info-日志-；-如果使用-warn-来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。\" class=\"headerlink\" title=\"【推荐】谨慎地记录日志。生产环境禁止输出 debug 日志 ； 有选择地输出 info 日志 ； 如果使用 warn 来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。\"></a>【推荐】谨慎地记录日志。生产环境禁止输出 debug 日志 ； 有选择地输出 info 日志 ； 如果使用 warn 来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。</h4><p>说明：大量地输出无效日志，不利于系统性能提升，也不利于快速定位错误点。记录日志时请<br>思考：这些日志真的有人看吗？看到这条日志你能做什么？能不能给问题排查带来好处？</p>\n<h1 id=\"Mysql规约\"><a href=\"#Mysql规约\" class=\"headerlink\" title=\"Mysql规约\"></a>Mysql规约</h1><h2 id=\"建表规约\"><a href=\"#建表规约\" class=\"headerlink\" title=\"建表规约\"></a>建表规约</h2><h4 id=\"【强制】表达是与否概念的字段，必须使用-is-xxx-的方式命名，数据类型是-unsigned-tinyint（1表示是，0表示否），此规则同样适用于-odps-建表。\"><a href=\"#【强制】表达是与否概念的字段，必须使用-is-xxx-的方式命名，数据类型是-unsigned-tinyint（1表示是，0表示否），此规则同样适用于-odps-建表。\" class=\"headerlink\" title=\"【强制】表达是与否概念的字段，必须使用 is _ xxx 的方式命名，数据类型是 unsigned tinyint（1表示是，0表示否），此规则同样适用于 odps 建表。\"></a>【强制】表达是与否概念的字段，必须使用 is _ xxx 的方式命名，数据类型是 unsigned tinyint（1表示是，0表示否），此规则同样适用于 odps 建表。</h4><p>说明：任何字段如果为非负数，必须是 unsigned 。</p>\n<h4 id=\"【强制】表名、字段名必须使用小写字母或数字-；-禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\"><a href=\"#【强制】表名、字段名必须使用小写字母或数字-；-禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\" class=\"headerlink\" title=\"【强制】表名、字段名必须使用小写字母或数字 ； 禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\"></a>【强制】表名、字段名必须使用小写字母或数字 ； 禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。</h4><p>正例： getter <em> admin ， task </em> config ， level 3_ name<br>反例： GetterAdmin ， taskConfig ， level <em>3</em> name<br>思考：因为以前遇到坑，不同数据库大小写规则不一样，所以都用小写</p>\n<h4 id=\"【强制】表名不使用复数名词。\"><a href=\"#【强制】表名不使用复数名词。\" class=\"headerlink\" title=\"【强制】表名不使用复数名词。\"></a>【强制】表名不使用复数名词。</h4><p>说明：表名应该仅仅表示表里面的实体内容，不应该表示实体数量，对应于 DO 类名也是单数形式，符合表达习惯。</p>\n<h4 id=\"【强制】唯一索引名为-uk-字段名-；-普通索引名则为-idx-字段名。\"><a href=\"#【强制】唯一索引名为-uk-字段名-；-普通索引名则为-idx-字段名。\" class=\"headerlink\" title=\"【强制】唯一索引名为 uk 字段名 ； 普通索引名则为 idx 字段名。\"></a>【强制】唯一索引名为 uk <em>字段名 ； 普通索引名则为 idx </em>字段名。</h4><p>说明： uk <em> 即  unique key；idx </em> 即 index 的简称。</p>\n<h4 id=\"【强制】小数类型为-decimal-，禁止使用-float-和-double-。\"><a href=\"#【强制】小数类型为-decimal-，禁止使用-float-和-double-。\" class=\"headerlink\" title=\"【强制】小数类型为 decimal ，禁止使用 float 和 double 。\"></a>【强制】小数类型为 decimal ，禁止使用 float 和 double 。</h4><p>说明： float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到不<br>正确的结果。<br>如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储。</p>\n<h4 id=\"【强制】如果存储的字符串长度几乎相等，使用-char-定长字符串类型。\"><a href=\"#【强制】如果存储的字符串长度几乎相等，使用-char-定长字符串类型。\" class=\"headerlink\" title=\"【强制】如果存储的字符串长度几乎相等，使用 char 定长字符串类型。\"></a>【强制】如果存储的字符串长度几乎相等，使用 char 定长字符串类型。</h4><h4 id=\"【强制】-varchar-是可变长字符串，不预先分配存储空间，长度不要超过-5000，如果存储长度大于此值，定义字段类型为-text-，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\"><a href=\"#【强制】-varchar-是可变长字符串，不预先分配存储空间，长度不要超过-5000，如果存储长度大于此值，定义字段类型为-text-，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\" class=\"headerlink\" title=\"【强制】 varchar 是可变长字符串，不预先分配存储空间，长度不要超过 5000，如果存储长度大于此值，定义字段类型为 text ，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\"></a>【强制】 varchar 是可变长字符串，不预先分配存储空间，长度不要超过 5000，如果存储长度大于此值，定义字段类型为 text ，独立出来一张表，用主键来对应，避免影响其它字段索引效率。</h4><h4 id=\"【强制】表必备三字段：-id-gmt-create-gmt-modified。\"><a href=\"#【强制】表必备三字段：-id-gmt-create-gmt-modified。\" class=\"headerlink\" title=\"【强制】表必备三字段： id ,  gmt_create,gmt_modified。\"></a>【强制】表必备三字段： id ,  gmt_create,gmt_modified。</h4><p>说明：其中 id 必为主键，类型为 unsigned bigint 、单表时自增、步长为 1。 gmt_create,gmt_modified 的类型均为 date_time 类型。</p>\n<h4 id=\"【推荐】表的命名最好是加上“业务名称-表的作用”。\"><a href=\"#【推荐】表的命名最好是加上“业务名称-表的作用”。\" class=\"headerlink\" title=\"【推荐】表的命名最好是加上“业务名称_表的作用”。\"></a>【推荐】表的命名最好是加上“业务名称_表的作用”。</h4><p>正例： tiger_task/tiger_reader/mpp_config</p>\n<h4 id=\"【推荐】库名与应用名称尽量一致。\"><a href=\"#【推荐】库名与应用名称尽量一致。\" class=\"headerlink\" title=\"【推荐】库名与应用名称尽量一致。\"></a>【推荐】库名与应用名称尽量一致。</h4><h4 id=\"【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：\"><a href=\"#【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：\" class=\"headerlink\" title=\"【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：\"></a>【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：</h4><p>1 ） 不是频繁修改的字段。<br>2 ） 不是 varchar 超长字段，更不能是 text 字段。<br>正例：商品类目名称使用频率高，字段长度短，名称基本一成不变，可在相关联的表中冗余存<br>储类目名称，避免关联查询。</p>\n<h4 id=\"【推荐】单表行数超过-500-万行或者单表容量超过-2-GB-，才推荐进行分库分表。\"><a href=\"#【推荐】单表行数超过-500-万行或者单表容量超过-2-GB-，才推荐进行分库分表。\" class=\"headerlink\" title=\"【推荐】单表行数超过 500 万行或者单表容量超过 2 GB ，才推荐进行分库分表。\"></a>【推荐】单表行数超过 500 万行或者单表容量超过 2 GB ，才推荐进行分库分表。</h4><p>说明：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。</p>\n<h2 id=\"索引规约\"><a href=\"#索引规约\" class=\"headerlink\" title=\"索引规约\"></a>索引规约</h2><p>这一块，现在理解不深，需要使用的时候再理解</p>\n<h4 id=\"【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。\"><a href=\"#【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。\" class=\"headerlink\" title=\"【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。\"></a>【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。</h4><p>说明：不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明<br>显的 ；</p>\n<h4 id=\"【强制】-超过三个表禁止-join-。需要-join-的字段，数据类型保持绝对一致-；-多表关联查询时，保证被关联的字段需要有索引。\"><a href=\"#【强制】-超过三个表禁止-join-。需要-join-的字段，数据类型保持绝对一致-；-多表关联查询时，保证被关联的字段需要有索引。\" class=\"headerlink\" title=\"【强制】 超过三个表禁止 join 。需要 join 的字段，数据类型保持绝对一致 ； 多表关联查询时，保证被关联的字段需要有索引。\"></a>【强制】 超过三个表禁止 join 。需要 join 的字段，数据类型保持绝对一致 ； 多表关联查询时，保证被关联的字段需要有索引。</h4><p>说明：即使双表 join 也要注意表索引、 SQL 性能。</p>\n<h4 id=\"【强制】在-varchar-字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\"><a href=\"#【强制】在-varchar-字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\" class=\"headerlink\" title=\"【强制】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\"></a>【强制】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。</h4><p>说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会高达 90%以上，可以使用 count(distinct left( 列名, 索引长度 )) / count( * ) 的区分度来确定。</p>\n<h4 id=\"【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\"><a href=\"#【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\" class=\"headerlink\" title=\"【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\"></a>【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。</h4><p>说明：索引文件具有 B - Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。</p>\n<h4 id=\"【参考】创建索引时避免有如下极端误解：\"><a href=\"#【参考】创建索引时避免有如下极端误解：\" class=\"headerlink\" title=\"【参考】创建索引时避免有如下极端误解：\"></a>【参考】创建索引时避免有如下极端误解：</h4><p>1 ） 误认为一个查询就需要建一个索引。<br>2 ） 误认为索引会消耗空间、严重拖慢更新和新增速度。<br>3 ） 误认为唯一索引一律需要在应用层通过“先查后插”方式解决。</p>\n<h2 id=\"SQL规约\"><a href=\"#SQL规约\" class=\"headerlink\" title=\"SQL规约\"></a>SQL规约</h2><h4 id=\"【强制】不要使用count-列名-或count-常量-来替代count-，count-就是-SQL92定义的标准统计行数的语法，跟数据库无关，跟-NULL-和非-NULL-无关。\"><a href=\"#【强制】不要使用count-列名-或count-常量-来替代count-，count-就是-SQL92定义的标准统计行数的语法，跟数据库无关，跟-NULL-和非-NULL-无关。\" class=\"headerlink\" title=\"【强制】不要使用count(列名)或count(常量)来替代count()，count()就是 SQL92定义的标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。\"></a>【强制】不要使用count(列名)或count(常量)来替代count(<em>)，count(</em>)就是 SQL92定义的标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。</h4><p>说明： count(*)会统计值为NULL的行，而count(列名)不会统计此列为NULL值的行。</p>\n<h4 id=\"【强制】使用-ISNULL-来判断是否为-NULL-值。注意：-NULL-与任何值的直接比较都为-NULL。\"><a href=\"#【强制】使用-ISNULL-来判断是否为-NULL-值。注意：-NULL-与任何值的直接比较都为-NULL。\" class=\"headerlink\" title=\"【强制】使用 ISNULL() 来判断是否为 NULL 值。注意： NULL 与任何值的直接比较都为 NULL。\"></a>【强制】使用 ISNULL() 来判断是否为 NULL 值。注意： NULL 与任何值的直接比较都为 NULL。</h4><p>说明：<br>1 ） NULL&lt;&gt;NULL 的返回结果是 NULL ，而不是 false 。<br>2 ） NULL=NULL 的返回结果是 NULL ，而不是 true 。<br>3 ） NULL&lt;&gt;1 的返回结果是 NULL ，而不是 true 。</p>\n<h4 id=\"【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\"><a href=\"#【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\" class=\"headerlink\" title=\"【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\"></a>【强制】不得使用外键与级联，一切外键概念必须在应用层解决。</h4><h4 id=\"【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\"><a href=\"#【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\" class=\"headerlink\" title=\"【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\"></a>【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。</h4><h4 id=\"【强制】数据订正时，删除和修改记录时，要先-select-，避免出现误删除，确认无误才能执行更新语句。\"><a href=\"#【强制】数据订正时，删除和修改记录时，要先-select-，避免出现误删除，确认无误才能执行更新语句。\" class=\"headerlink\" title=\"【强制】数据订正时，删除和修改记录时，要先 select ，避免出现误删除，确认无误才能执行更新语句。\"></a>【强制】数据订正时，删除和修改记录时，要先 select ，避免出现误删除，确认无误才能执行更新语句。</h4><h4 id=\"【推荐】-in-操作能避免则避免，若实在避免不了，需要仔细评估-in-后边的集合元素数量，控制在-1000-个之内。\"><a href=\"#【推荐】-in-操作能避免则避免，若实在避免不了，需要仔细评估-in-后边的集合元素数量，控制在-1000-个之内。\" class=\"headerlink\" title=\"【推荐】 in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在 1000 个之内。\"></a>【推荐】 in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在 1000 个之内。</h4><h4 id=\"【参考】如果有全球化需要，所有的字符存储与表示，均以-utf-8-编码，那么字符计数方法\"><a href=\"#【参考】如果有全球化需要，所有的字符存储与表示，均以-utf-8-编码，那么字符计数方法\" class=\"headerlink\" title=\"【参考】如果有全球化需要，所有的字符存储与表示，均以 utf -8 编码，那么字符计数方法\"></a>【参考】如果有全球化需要，所有的字符存储与表示，均以 utf -8 编码，那么字符计数方法</h4><p>说明：<br>    SELECT LENGTH( “轻松工作” )； 返回为 12<br>    SELECT CHARACTER_LENGTH( “轻松工作” )； 返回为 4<br>    如果要使用表情，那么使用 utfmb 4 来进行存储，注意它与 utf -8 编码的区别。</p>\n<h4 id=\"【参考】TRUNCATE-TABLE-比-DELETE-速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发-trigger-，有可能造成事故，故不建议在开发代码中使用此语句。\"><a href=\"#【参考】TRUNCATE-TABLE-比-DELETE-速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发-trigger-，有可能造成事故，故不建议在开发代码中使用此语句。\" class=\"headerlink\" title=\"【参考】TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发 trigger ，有可能造成事故，故不建议在开发代码中使用此语句。\"></a>【参考】TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发 trigger ，有可能造成事故，故不建议在开发代码中使用此语句。</h4><p>说明： TRUNCATE TABLE 在功能上与不带  WHERE 子句的  DELETE 语句相同。</p>\n<h2 id=\"ORM规约\"><a href=\"#ORM规约\" class=\"headerlink\" title=\"ORM规约\"></a>ORM规约</h2><h4 id=\"【强制】在表查询中，一律不要使用-作为查询的字段列表，需要哪些字段必须明确写明。\"><a href=\"#【强制】在表查询中，一律不要使用-作为查询的字段列表，需要哪些字段必须明确写明。\" class=\"headerlink\" title=\"【强制】在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。\"></a>【强制】在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。</h4><p>说明：1 ） 增加查询分析器解析成本。2 ） 增减字段容易与 resultMap 配置不一致。<br>思考：这一点我做得不好，一定需要注意；</p>\n<h4 id=\"【强制】-POJO-类的-boolean-属性不能加-is-，而数据库字段必须加-is-，要求在-resultMap中进行字段与属性之间的映射。\"><a href=\"#【强制】-POJO-类的-boolean-属性不能加-is-，而数据库字段必须加-is-，要求在-resultMap中进行字段与属性之间的映射。\" class=\"headerlink\" title=\"【强制】 POJO 类的 boolean 属性不能加 is ，而数据库字段必须加 is _，要求在 resultMap中进行字段与属性之间的映射。\"></a>【强制】 POJO 类的 boolean 属性不能加 is ，而数据库字段必须加 is _，要求在 resultMap中进行字段与属性之间的映射。</h4><p>说明：参见定义 POJO 类以及数据库字段定义规定，在 sql . xml 增加映射，是必须的。</p>\n<h4 id=\"【强制】不要用-resultClass-当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义-；-反过来，每一个表也必然有一个与之对应。\"><a href=\"#【强制】不要用-resultClass-当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义-；-反过来，每一个表也必然有一个与之对应。\" class=\"headerlink\" title=\"【强制】不要用 resultClass 当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义 ； 反过来，每一个表也必然有一个与之对应。\"></a>【强制】不要用 resultClass 当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义 ； 反过来，每一个表也必然有一个与之对应。</h4><p>说明：配置映射关系，使字段与 DO 类解耦，方便维护。</p>\n<h4 id=\"【强制】-xml-配置中参数注意使用：-，-param-不要使用-此种方式容易出现SQL注入。\"><a href=\"#【强制】-xml-配置中参数注意使用：-，-param-不要使用-此种方式容易出现SQL注入。\" class=\"headerlink\" title=\"【强制】 xml 配置中参数注意使用：#{}，# param # 不要使用${} 此种方式容易出现SQL注入。\"></a>【强制】 xml 配置中参数注意使用：#{}，# param # 不要使用${} 此种方式容易出现SQL注入。</h4><h4 id=\"【强制】不允许直接拿-HashMap-与-Hashtable-作为查询结果集的输出。\"><a href=\"#【强制】不允许直接拿-HashMap-与-Hashtable-作为查询结果集的输出。\" class=\"headerlink\" title=\"【强制】不允许直接拿 HashMap 与 Hashtable 作为查询结果集的输出。\"></a>【强制】不允许直接拿 HashMap 与 Hashtable 作为查询结果集的输出。</h4><p>思考：为啥？</p>\n<h4 id=\"【强制】更新数据表记录时，必须同时更新记录对应的-gmt-modified-字段值为当前时间。\"><a href=\"#【强制】更新数据表记录时，必须同时更新记录对应的-gmt-modified-字段值为当前时间。\" class=\"headerlink\" title=\"【强制】更新数据表记录时，必须同时更新记录对应的 gmt _ modified 字段值为当前时间。\"></a>【强制】更新数据表记录时，必须同时更新记录对应的 gmt _ modified 字段值为当前时间。</h4><h4 id=\"【推荐】不要写一个大而全的数据更新接口，传入为-POJO-类，不管是不是自己的目标更新字段，都进行-update-table-set-c1-value1-c2-value2-c3-value3-这是不对的。\"><a href=\"#【推荐】不要写一个大而全的数据更新接口，传入为-POJO-类，不管是不是自己的目标更新字段，都进行-update-table-set-c1-value1-c2-value2-c3-value3-这是不对的。\" class=\"headerlink\" title=\"【推荐】不要写一个大而全的数据更新接口，传入为 POJO 类，不管是不是自己的目标更新字段，都进行 update table set c1=value1,c2=value2,c3=value3;  这是不对的。\"></a>【推荐】不要写一个大而全的数据更新接口，传入为 POJO 类，不管是不是自己的目标更新字段，都进行 update table set c1=value1,c2=value2,c3=value3;  这是不对的。</h4><p>执行 SQL时，尽量不要更新无改动的字段，一是易出错 ； 二是效率低 ； 三是 binlog 增加存储。</p>\n<h4 id=\"【参考】-Transactional-事务不要滥用。事务会影响数据库的-QPS-，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。\"><a href=\"#【参考】-Transactional-事务不要滥用。事务会影响数据库的-QPS-，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。\" class=\"headerlink\" title=\"【参考】@ Transactional 事务不要滥用。事务会影响数据库的 QPS ，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。\"></a>【参考】@ Transactional 事务不要滥用。事务会影响数据库的 QPS ，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。</h4><h4 id=\"【参考】-lt-isEqual-gt-中的-compareValue-是与属性值对比的常量，一般是数字，表示相等时带上此条件-；-lt-isNotEmpty-gt-表示不为空且不为-null-时执行-；-lt-isNotNull-gt-表示不为-null-值时执行。\"><a href=\"#【参考】-lt-isEqual-gt-中的-compareValue-是与属性值对比的常量，一般是数字，表示相等时带上此条件-；-lt-isNotEmpty-gt-表示不为空且不为-null-时执行-；-lt-isNotNull-gt-表示不为-null-值时执行。\" class=\"headerlink\" title=\"【参考】&lt; isEqual &gt;中的 compareValue 是与属性值对比的常量，一般是数字，表示相等时带上此条件 ； &lt; isNotEmpty &gt;表示不为空且不为 null 时执行 ； &lt; isNotNull &gt;表示不为 null 值时执行。\"></a>【参考】&lt; isEqual &gt;中的 compareValue 是与属性值对比的常量，一般是数字，表示相等时带上此条件 ； &lt; isNotEmpty &gt;表示不为空且不为 null 时执行 ； &lt; isNotNull &gt;表示不为 null 值时执行。</h4><h2 id=\"工程规约\"><a href=\"#工程规约\" class=\"headerlink\" title=\"工程规约\"></a>工程规约</h2><h4 id=\"【强制】定义-GAV-遵从以下规则：\"><a href=\"#【强制】定义-GAV-遵从以下规则：\" class=\"headerlink\" title=\"【强制】定义 GAV 遵从以下规则：\"></a>【强制】定义 GAV 遵从以下规则：</h4><p>1 ） GroupID 格式：com.{公司/BU}.业务线.[子业务线]，最多4级。<br>说明：{公司/BU}例如：alibaba/taobao/tmall/aliexpress等BU一级；子业务线可选。<br>正例：com.taobao.jstorm或com.alibaba.dubbo.register<br>2）ArtifactID格式：产品线名-模块名。语义不重复不遗漏，先到仓库中心去查证一下。<br>正例：dubbo-client/fastjson-api/jstorm-tool<br>3）Version：详细规定参考下方。</p>\n<p>思考：com.ctcc.bigdata   openapi-storage</p>\n<h4 id=\"【强制】二方库版本号命名方式：主版本号-次版本号-修订号\"><a href=\"#【强制】二方库版本号命名方式：主版本号-次版本号-修订号\" class=\"headerlink\" title=\"【强制】二方库版本号命名方式：主版本号.次版本号.修订号\"></a>【强制】二方库版本号命名方式：主版本号.次版本号.修订号</h4><p>1 ） 主版本号 ：当做了不兼容的 API 修改，或者增加了能改变产品方向的新功能。<br>2 ） 次版本号 ：当做了向下兼容的功能性新增 （ 新增类、接口等 ） 。<br>3 ） 修订号 ：修复 bug ，没有修改方法签名的功能加强，保持  API 兼容性。<br>说明：起始版本号必须为： 1.0.0 ，而不是 0.0.1</p>\n<h4 id=\"【强制】线上应用不要依赖-SNAPSHOT-版本-（-安全包除外-）；\"><a href=\"#【强制】线上应用不要依赖-SNAPSHOT-版本-（-安全包除外-）；\" class=\"headerlink\" title=\"【强制】线上应用不要依赖 SNAPSHOT 版本 （ 安全包除外 ）；\"></a>【强制】线上应用不要依赖 SNAPSHOT 版本 （ 安全包除外 ）；</h4><p>正式发布的类库必须使用 RELEASE版本号升级+1 的方式，且版本号不允许覆盖升级，必须去中央仓库进行查证。<br>说明：不依赖 SNAPSHOT 版本是保证应用发布的幂等性。另外，也可以加快编译时的打包构建。</p>\n<h4 id=\"【强制】二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的-POJO-对象。\"><a href=\"#【强制】二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的-POJO-对象。\" class=\"headerlink\" title=\"【强制】二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的 POJO 对象。\"></a>【强制】二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的 POJO 对象。</h4><h4 id=\"【强制】依赖于一个二方库群时，必须定义一个统一版本变量，避免版本号不一致。\"><a href=\"#【强制】依赖于一个二方库群时，必须定义一个统一版本变量，避免版本号不一致。\" class=\"headerlink\" title=\"【强制】依赖于一个二方库群时，必须定义一个统一版本变量，避免版本号不一致。\"></a>【强制】依赖于一个二方库群时，必须定义一个统一版本变量，避免版本号不一致。</h4><p>说明：依赖 springframework - core ,- context ,- beans ，它们都是同一个版本，可以定义一个变量来保存版本：${ spring . version }，定义依赖的时候，引用该版本。</p>\n<h4 id=\"【强制】禁止在子项目的-pom-依赖中出现相同的-GroupId-，相同的-ArtifactId-，但是不同的Version-。\"><a href=\"#【强制】禁止在子项目的-pom-依赖中出现相同的-GroupId-，相同的-ArtifactId-，但是不同的Version-。\" class=\"headerlink\" title=\"【强制】禁止在子项目的 pom 依赖中出现相同的 GroupId ，相同的 ArtifactId ，但是不同的Version 。\"></a>【强制】禁止在子项目的 pom 依赖中出现相同的 GroupId ，相同的 ArtifactId ，但是不同的Version 。</h4><p>说明：在本地调试时会使用各子项目指定的版本号，但是合并成一个 war ，只能有一个版本号出现在最后的 lib 目录中。曾经出现过线下调试是正确的，发布到线上出故障的先例。</p>\n<h2 id=\"服务器规约\"><a href=\"#服务器规约\" class=\"headerlink\" title=\"服务器规约\"></a>服务器规约</h2><h4 id=\"【推荐】高并发服务器建议调小-TCP-协议的-time-wait-超时时间。\"><a href=\"#【推荐】高并发服务器建议调小-TCP-协议的-time-wait-超时时间。\" class=\"headerlink\" title=\"【推荐】高并发服务器建议调小 TCP 协议的 time _ wait 超时时间。\"></a>【推荐】高并发服务器建议调小 TCP 协议的 time _ wait 超时时间。</h4><p>说明：操作系统默认 240 秒后，才会关闭处于 time <em> wait 状态的连接，在高并发访问下，服务器端会因为处于 time </em> wait 的连接数太多，可能无法建立新的连接，所以需要在服务器上调小此等待值。</p>\n<h4 id=\"【推荐】调大服务器所支持的最大文件句柄数-（File-Descriptor-，简写为-fd）-。\"><a href=\"#【推荐】调大服务器所支持的最大文件句柄数-（File-Descriptor-，简写为-fd）-。\" class=\"headerlink\" title=\"【推荐】调大服务器所支持的最大文件句柄数 （File Descriptor ，简写为 fd） 。\"></a>【推荐】调大服务器所支持的最大文件句柄数 （File Descriptor ，简写为 fd） 。</h4><p>说明：主流操作系统的设计是将 TCP / UDP 连接采用与文件一样的方式去管理，即一个连接对应于一个 fd 。主流的 linux 服务器默认所支持最大 fd 数量为 1024，当并发连接数很大时很容易因为 fd 不足而出现“ open too many files ”错误，导致新的连接无法建立。 建议将 linux服务器所支持的最大句柄数调高数倍 （ 与服务器的内存数量相关 ） 。<br>思考：kafka ，flume等中间件也要这么配</p>\n<h4 id=\"【推荐】给-JVM-设置-XX-HeapDumpOnOutOfMemoryError-参数，让-JVM-碰到-OOM-场景时输出dump-信息。\"><a href=\"#【推荐】给-JVM-设置-XX-HeapDumpOnOutOfMemoryError-参数，让-JVM-碰到-OOM-场景时输出dump-信息。\" class=\"headerlink\" title=\"【推荐】给 JVM 设置- XX :+ HeapDumpOnOutOfMemoryError 参数，让 JVM 碰到 OOM 场景时输出dump 信息。\"></a>【推荐】给 JVM 设置- XX :+ HeapDumpOnOutOfMemoryError 参数，让 JVM 碰到 OOM 场景时输出dump 信息。</h4><p>说明： OOM 的发生是有概率的，甚至有规律地相隔数月才出现一例，出现时的现场信息对查错非常有价值。</p>\n<h4 id=\"【参考】服务器内部重定向使用-forward；-外部重定向地址使用-URL-拼装工具类来生成，否则会带来-URL-维护不一致的问题和潜在的安全风险。\"><a href=\"#【参考】服务器内部重定向使用-forward；-外部重定向地址使用-URL-拼装工具类来生成，否则会带来-URL-维护不一致的问题和潜在的安全风险。\" class=\"headerlink\" title=\"【参考】服务器内部重定向使用 forward； 外部重定向地址使用 URL 拼装工具类来生成，否则会带来 URL 维护不一致的问题和潜在的安全风险。\"></a>【参考】服务器内部重定向使用 forward； 外部重定向地址使用 URL 拼装工具类来生成，否则会带来 URL 维护不一致的问题和潜在的安全风险。</h4><h2 id=\"安全规约\"><a href=\"#安全规约\" class=\"headerlink\" title=\"安全规约\"></a>安全规约</h2><h4 id=\"【强制】隶属于用户个人的页面或者功能必须进行权限控制校验。\"><a href=\"#【强制】隶属于用户个人的页面或者功能必须进行权限控制校验。\" class=\"headerlink\" title=\"【强制】隶属于用户个人的页面或者功能必须进行权限控制校验。\"></a>【强制】隶属于用户个人的页面或者功能必须进行权限控制校验。</h4><p>说明：防止没有做水平权限校验就可随意访问、操作别人的数据，比如查看、修改别人的订单。</p>\n<h4 id=\"【强制】用户敏感数据禁止直接展示，必须对展示数据脱敏。\"><a href=\"#【强制】用户敏感数据禁止直接展示，必须对展示数据脱敏。\" class=\"headerlink\" title=\"【强制】用户敏感数据禁止直接展示，必须对展示数据脱敏。\"></a>【强制】用户敏感数据禁止直接展示，必须对展示数据脱敏。</h4><p>说明：查看个人手机号码会显示成:158<em>**</em>9119，隐藏中间 4 位，防止隐私泄露。<br>思考：在openAPI项目中一定要注意；</p>\n<h4 id=\"【强制】用户输入的-SQL-参数严格使用参数绑定或者-METADATA-字段值限定，防止-SQL-注入，\"><a href=\"#【强制】用户输入的-SQL-参数严格使用参数绑定或者-METADATA-字段值限定，防止-SQL-注入，\" class=\"headerlink\" title=\"【强制】用户输入的 SQL 参数严格使用参数绑定或者 METADATA 字段值限定，防止 SQL 注入，\"></a>【强制】用户输入的 SQL 参数严格使用参数绑定或者 METADATA 字段值限定，防止 SQL 注入，</h4><p>禁止字符串拼接 SQL 访问数据库。</p>\n<h4 id=\"【强制】用户请求传入的任何参数必须做有效性验证。\"><a href=\"#【强制】用户请求传入的任何参数必须做有效性验证。\" class=\"headerlink\" title=\"【强制】用户请求传入的任何参数必须做有效性验证。\"></a>【强制】用户请求传入的任何参数必须做有效性验证。</h4><p>说明：忽略参数校验可能导致：<br>  page size 过大导致内存溢出<br>  恶意 order by 导致数据库慢查询<br>  任意重定向<br>  SQL 注入<br>  反序列化注入<br>  正则输入源串拒绝服务 ReDoS<br>说明：Java 代码用正则来验证客户端的输入，有些正则写法验证普通用户输入没有问题，但是如果攻击人员使用的是特殊构造的字符串来验证，有可能导致死循环的效果。</p>\n<h4 id=\"【强制】禁止向-HTML-页面输出未经安全过滤或未正确转义的用户数据。\"><a href=\"#【强制】禁止向-HTML-页面输出未经安全过滤或未正确转义的用户数据。\" class=\"headerlink\" title=\"【强制】禁止向 HTML 页面输出未经安全过滤或未正确转义的用户数据。\"></a>【强制】禁止向 HTML 页面输出未经安全过滤或未正确转义的用户数据。</h4><h4 id=\"【强制】在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放限制，如数量限制、疲劳度控制、验证码校验，避免被滥刷、资损。\"><a href=\"#【强制】在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放限制，如数量限制、疲劳度控制、验证码校验，避免被滥刷、资损。\" class=\"headerlink\" title=\"【强制】在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放限制，如数量限制、疲劳度控制、验证码校验，避免被滥刷、资损。\"></a>【强制】在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放限制，如数量限制、疲劳度控制、验证码校验，避免被滥刷、资损。</h4><p>说明：如注册时发送验证码到手机，如果没有限制次数和频率，那么可以利用此功能骚扰到其<br>它用户，并造成短信平台资源浪费。</p>\n<h1 id=\"思考\"><a href=\"#思考\" class=\"headerlink\" title=\"思考\"></a>思考</h1><p>这两天断断续续看完了全部，看得时候觉得受益匪浅，但是这种东西要消化成自己的知识，一定要践行，等编码到具体部分的时候再复习，写出优秀的代码。</p>\n","excerpt":"","more":"<h1 id=\"异常日志\"><a href=\"#异常日志\" class=\"headerlink\" title=\"异常日志\"></a>异常日志</h1><h2 id=\"异常处理\"><a href=\"#异常处理\" class=\"headerlink\" title=\"异常处理\"></a>异常处理</h2><h4 id=\"【强制】不要捕获-Java-类库中定义的继承自-RuntimeException-的运行时异常类，如：IndexOutOfBoundsException-NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。\"><a href=\"#【强制】不要捕获-Java-类库中定义的继承自-RuntimeException-的运行时异常类，如：IndexOutOfBoundsException-NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。\" class=\"headerlink\" title=\"【强制】不要捕获 Java 类库中定义的继承自 RuntimeException 的运行时异常类，如：IndexOutOfBoundsException / NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。\"></a>【强制】不要捕获 Java 类库中定义的继承自 RuntimeException 的运行时异常类，如：IndexOutOfBoundsException / NullPointerException，这类异常由程序员预检查来规避，保证程序健壮性。</h4><p>正例： if(obj != null) {…}<br>反例： try { obj.method() } catch(NullPointerException e){…}</p>\n<h4 id=\"【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。\"><a href=\"#【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。\" class=\"headerlink\" title=\"【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。\"></a>【强制】异常不要用来做流程控制，条件控制，因为异常的处理效率比条件分支低。</h4><p>思考：Scala中用模式匹配处理异常的效率是否会高？</p>\n<h4 id=\"【强制】对大段代码进行-try-catch-，这是不负责任的表现。\"><a href=\"#【强制】对大段代码进行-try-catch-，这是不负责任的表现。\" class=\"headerlink\" title=\"【强制】对大段代码进行 try - catch ，这是不负责任的表现。\"></a>【强制】对大段代码进行 try - catch ，这是不负责任的表现。</h4><p>catch 时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的 catch 尽可能进行区分异常类型，再做对应的异常处理。</p>\n<h4 id=\"【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之；\"><a href=\"#【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之；\" class=\"headerlink\" title=\"【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之；\"></a>【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之；</h4><p>如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。<br>思考：</p>\n<h4 id=\"【强制】有-try-块放到了事务代码中，-catch-异常后，如果需要回滚事务，一定要注意手动回滚事务。\"><a href=\"#【强制】有-try-块放到了事务代码中，-catch-异常后，如果需要回滚事务，一定要注意手动回滚事务。\" class=\"headerlink\" title=\"【强制】有 try 块放到了事务代码中， catch 异常后，如果需要回滚事务，一定要注意手动回滚事务。\"></a>【强制】有 try 块放到了事务代码中， catch 异常后，如果需要回滚事务，一定要注意手动回滚事务。</h4><h4 id=\"【强制】-finally-块必须对资源对象、流对象进行关闭，有异常也要做-try-catch-。\"><a href=\"#【强制】-finally-块必须对资源对象、流对象进行关闭，有异常也要做-try-catch-。\" class=\"headerlink\" title=\"【强制】 finally 块必须对资源对象、流对象进行关闭，有异常也要做 try - catch 。\"></a>【强制】 finally 块必须对资源对象、流对象进行关闭，有异常也要做 try - catch 。</h4><p>说明：如果 JDK 7，可以使用 try - with - resources 方式。<br>思考：finally中的语句，也根据情况要做try-cache</p>\n<h4 id=\"【强制】不能在-finally-块中使用-return-，-finally-块中的-return-返回后方法结束执行，不会再执行-try-块中的-return-语句。\"><a href=\"#【强制】不能在-finally-块中使用-return-，-finally-块中的-return-返回后方法结束执行，不会再执行-try-块中的-return-语句。\" class=\"headerlink\" title=\"【强制】不能在 finally 块中使用 return ， finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。\"></a>【强制】不能在 finally 块中使用 return ， finally 块中的 return 返回后方法结束执行，不会再执行 try 块中的 return 语句。</h4><p>思考：finally中的return有效，但是就算有效，也没有意义，因为无论如何，都会返回同样的值</p>\n<h4 id=\"【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。\"><a href=\"#【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。\" class=\"headerlink\" title=\"【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。\"></a>【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。</h4><p>说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。</p>\n<h4 id=\"推荐】方法的返回值可以为-null-，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回-null-值。调用方需要进行-null-判断防止-NPE-问题。\"><a href=\"#推荐】方法的返回值可以为-null-，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回-null-值。调用方需要进行-null-判断防止-NPE-问题。\" class=\"headerlink\" title=\"推荐】方法的返回值可以为 null ，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回 null 值。调用方需要进行 null 判断防止 NPE 问题。\"></a>推荐】方法的返回值可以为 null ，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回 null 值。调用方需要进行 null 判断防止 NPE 问题。</h4><p>说明：本规约明确防止 NPE 是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败，运行时异常等场景返回 null 的情况<br>思考：如果返回的是null，需要添加说明。</p>\n<h4 id=\"【推荐】防止-NPE-，是程序员的基本修养，注意-NPE-产生的场景：\"><a href=\"#【推荐】防止-NPE-，是程序员的基本修养，注意-NPE-产生的场景：\" class=\"headerlink\" title=\"【推荐】防止 NPE ，是程序员的基本修养，注意 NPE 产生的场景：\"></a>【推荐】防止 NPE ，是程序员的基本修养，注意 NPE 产生的场景：</h4><p>1 ） 返回类型为包装数据类型，有可能是 null ，返回 int 值时注意判空。<br>反例： public int f() {  return Integer 对象}; 如果为 null ，自动解箱抛 NPE 。<br>2 ） 数据库的查询结果可能为 null 。<br>3 ） 集合里的元素即使 isNotEmpty ，取出的数据元素也可能为 null 。<br>4 ） <strong>远程调用返回对象，一律要求进行 NPE 判断。</strong><br>5 ） 对于 Session 中获取的数据，建议 NPE 检查，避免空指针。<br>6 ） <strong>级联调用</strong> obj . getA() . getB() . getC()； 一连串调用，易产生 NPE 。</p>\n<h4 id=\"【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的-http-api-开放接口必须使用“错误码”-；-而应用内部推荐异常抛出-；-跨应用间-RPC-调用优先考虑使用-Result-方式，封装-isSuccess-、“错误码”、“错误简短信息”。\"><a href=\"#【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的-http-api-开放接口必须使用“错误码”-；-而应用内部推荐异常抛出-；-跨应用间-RPC-调用优先考虑使用-Result-方式，封装-isSuccess-、“错误码”、“错误简短信息”。\" class=\"headerlink\" title=\"【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的 http / api 开放接口必须使用“错误码” ； 而应用内部推荐异常抛出 ； 跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess 、“错误码”、“错误简短信息”。\"></a>【推荐】在代码中使用“抛异常”还是“返回错误码”，对于公司外的 http / api 开放接口必须使用“错误码” ； 而应用内部推荐异常抛出 ； 跨应用间 RPC 调用优先考虑使用 Result 方式，封装 isSuccess 、“错误码”、“错误简短信息”。</h4><p>说明：关于 RPC 方法返回方式使用 Result 方式的理由：<br>1 ） 使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。<br>2 ） 如果不加栈信息，只是 new 自定义异常，加入自己的理解的 error message ，对于调用<br>端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输<br>的性能损耗也是问题。</p>\n<h4 id=\"【推荐】定义时区分-unchecked-checked-异常，避免直接使用-RuntimeException-抛出，更不允许抛出-Exception-或者-Throwable-，应使用有业务含义的自定义异常。\"><a href=\"#【推荐】定义时区分-unchecked-checked-异常，避免直接使用-RuntimeException-抛出，更不允许抛出-Exception-或者-Throwable-，应使用有业务含义的自定义异常。\" class=\"headerlink\" title=\"【推荐】定义时区分 unchecked /  checked 异常，避免直接使用 RuntimeException 抛出，更不允许抛出 Exception 或者 Throwable ，应使用有业务含义的自定义异常。\"></a>【推荐】定义时区分 unchecked /  checked 异常，避免直接使用 RuntimeException 抛出，更不允许抛出 Exception 或者 Throwable ，应使用有业务含义的自定义异常。</h4><h4 id=\"【参考】避免出现重复的代码-（Don-’-t-Repeat-Yourself）-，即-DRY-原则。\"><a href=\"#【参考】避免出现重复的代码-（Don-’-t-Repeat-Yourself）-，即-DRY-原则。\" class=\"headerlink\" title=\"【参考】避免出现重复的代码 （Don ’ t Repeat Yourself） ，即 DRY 原则。\"></a>【参考】避免出现重复的代码 （Don ’ t Repeat Yourself） ，即 DRY 原则。</h4><p>说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副<br>本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是共用模块。<br>正例：一个类中有多个 public 方法，都需要进行数行相同的参数校验操作，这个时候请抽取：<br>private boolean checkParam(DTO dto){…}</p>\n<h2 id=\"日志规约\"><a href=\"#日志规约\" class=\"headerlink\" title=\"日志规约\"></a>日志规约</h2><h4 id=\"【强制】应用中不可直接使用日志系统-（Log-4-j-、-Logback）-中的-API-，而应依赖使用日志框架SLF4J中的-API-，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。\"><a href=\"#【强制】应用中不可直接使用日志系统-（Log-4-j-、-Logback）-中的-API-，而应依赖使用日志框架SLF4J中的-API-，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。\" class=\"headerlink\" title=\"【强制】应用中不可直接使用日志系统 （Log 4 j 、 Logback） 中的 API ，而应依赖使用日志框架SLF4J中的 API ，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。\"></a>【强制】应用中不可直接使用日志系统 （Log 4 j 、 Logback） 中的 API ，而应依赖使用日志框架SLF4J中的 API ，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。</h4><p>import org.slf4j.Logger;<br>import org.slf4j.LoggerFactory;<br>private static final Logger logger = LoggerFactory.getLogger(Abc.class);<br>思考：回头要注意，使用门面模式的日志框架</p>\n<h4 id=\"【强制】日志文件推荐至少保存-15-天，因为有些异常具备以“周”为频次发生的特点。\"><a href=\"#【强制】日志文件推荐至少保存-15-天，因为有些异常具备以“周”为频次发生的特点。\" class=\"headerlink\" title=\"【强制】日志文件推荐至少保存 15 天，因为有些异常具备以“周”为频次发生的特点。\"></a>【强制】日志文件推荐至少保存 15 天，因为有些异常具备以“周”为频次发生的特点。</h4><h4 id=\"【强制】应用中的扩展日志-（-如打点、临时监控、访问日志等-）-命名方式：\"><a href=\"#【强制】应用中的扩展日志-（-如打点、临时监控、访问日志等-）-命名方式：\" class=\"headerlink\" title=\"【强制】应用中的扩展日志 （ 如打点、临时监控、访问日志等 ） 命名方式：\"></a>【强制】应用中的扩展日志 （ 如打点、临时监控、访问日志等 ） 命名方式：</h4><p>appName_logType<em>logName.log 。<br>logType :日志类型，推荐分类有 stats / desc / monitor / visit 等 ；<br>logName :日志描述。这种命名的好处：通过文件名就可知道日志文件属于什么应用，什么类型，什么目的，也有利于归类查找。<br>正例： mppserver 应用中单独监控时区转换异常，如：<br>mppserver </em> monitor _ timeZoneConvert . log<br>说明：推荐对日志进行分类，错误日志和业务日志尽量分开存放，便于开发人员查看，也便于<br>通过日志对系统进行及时监控。<br>思考：原来我想到的别人已经想到了，一定践行；</p>\n<h4 id=\"【强制】对-trace-debug-info-级别的日志输出，必须使用条件输出形式或者使用占位符的方式。\"><a href=\"#【强制】对-trace-debug-info-级别的日志输出，必须使用条件输出形式或者使用占位符的方式。\" class=\"headerlink\" title=\"【强制】对 trace / debug / info 级别的日志输出，必须使用条件输出形式或者使用占位符的方式。\"></a>【强制】对 trace / debug / info 级别的日志输出，必须使用条件输出形式或者使用占位符的方式。</h4><p>说明： logger . debug( “ Processing trade with id : “ +  id + “  symbol : “ +  symbol);<br>如果日志级别是 warn ，上述日志不会打印，但是会执行字符串拼接操作，如果 symbol 是对象，<br>会执行 toString() 方法，浪费了系统资源，执行了上述操作，最终日志却没有打印。<br>正例： （ 条件 ）<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">if</span> (logger.isDebugEnabled()) &#123;</div><div class=\"line\">\tlogger.debug(<span class=\"string\">\"Processing trade with id: \"</span> + id + <span class=\"string\">\" symbol: \"</span> + symbol);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>正例： （ 占位符 ）<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">logger.debug(<span class=\"string\">\"Processing trade with id: &#123;&#125; symbol : &#123;&#125; \"</span>, id, symbol);</div></pre></td></tr></table></figure></p>\n<p>思考：这一点非常重要，学到了，既然这样，就直接统一使用占位符的方式吧；</p>\n<h4 id=\"【强制】避免重复打印日志，浪费磁盘空间，务必在-log4j-xml-中设置-additivity-false。\"><a href=\"#【强制】避免重复打印日志，浪费磁盘空间，务必在-log4j-xml-中设置-additivity-false。\" class=\"headerlink\" title=\"【强制】避免重复打印日志，浪费磁盘空间，务必在 log4j.xml 中设置 additivity = false。\"></a>【强制】避免重复打印日志，浪费磁盘空间，务必在 log4j.xml 中设置 additivity = false。</h4><p>正例： <logger name=\"com.taobao.dubbo.config\" additivity=\"false\"> </p>\n<h4 id=\"【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么往上抛。\"><a href=\"#【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么往上抛。\" class=\"headerlink\" title=\"【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么往上抛。\"></a>【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么往上抛。</h4><p>正例： logger.error(各类参数或者对象 toString + “_” + e.getMessage(), e);<br>思考：一定要注意保留这两类信息：e.getMessage();</p>\n<h4 id=\"【推荐】可以使用-warn-日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。\"><a href=\"#【推荐】可以使用-warn-日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。\" class=\"headerlink\" title=\"【推荐】可以使用 warn 日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。\"></a>【推荐】可以使用 warn 日志级别来记录用户输入参数错误的情况，避免用户投诉时，无所适从。</h4><p>注意日志输出的级别， error 级别只记录系统逻辑出错、异常等重要的错误信息。如非必要，请不要在此场景打出 error 级别。</p>\n<h4 id=\"【推荐】谨慎地记录日志。生产环境禁止输出-debug-日志-；-有选择地输出-info-日志-；-如果使用-warn-来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。\"><a href=\"#【推荐】谨慎地记录日志。生产环境禁止输出-debug-日志-；-有选择地输出-info-日志-；-如果使用-warn-来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。\" class=\"headerlink\" title=\"【推荐】谨慎地记录日志。生产环境禁止输出 debug 日志 ； 有选择地输出 info 日志 ； 如果使用 warn 来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。\"></a>【推荐】谨慎地记录日志。生产环境禁止输出 debug 日志 ； 有选择地输出 info 日志 ； 如果使用 warn 来记录刚上线时的业务行为信息，一定要注意日志输出量的问题，避免把服务器磁盘撑爆，并记得及时删除这些观察日志。</h4><p>说明：大量地输出无效日志，不利于系统性能提升，也不利于快速定位错误点。记录日志时请<br>思考：这些日志真的有人看吗？看到这条日志你能做什么？能不能给问题排查带来好处？</p>\n<h1 id=\"Mysql规约\"><a href=\"#Mysql规约\" class=\"headerlink\" title=\"Mysql规约\"></a>Mysql规约</h1><h2 id=\"建表规约\"><a href=\"#建表规约\" class=\"headerlink\" title=\"建表规约\"></a>建表规约</h2><h4 id=\"【强制】表达是与否概念的字段，必须使用-is-xxx-的方式命名，数据类型是-unsigned-tinyint（1表示是，0表示否），此规则同样适用于-odps-建表。\"><a href=\"#【强制】表达是与否概念的字段，必须使用-is-xxx-的方式命名，数据类型是-unsigned-tinyint（1表示是，0表示否），此规则同样适用于-odps-建表。\" class=\"headerlink\" title=\"【强制】表达是与否概念的字段，必须使用 is _ xxx 的方式命名，数据类型是 unsigned tinyint（1表示是，0表示否），此规则同样适用于 odps 建表。\"></a>【强制】表达是与否概念的字段，必须使用 is _ xxx 的方式命名，数据类型是 unsigned tinyint（1表示是，0表示否），此规则同样适用于 odps 建表。</h4><p>说明：任何字段如果为非负数，必须是 unsigned 。</p>\n<h4 id=\"【强制】表名、字段名必须使用小写字母或数字-；-禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\"><a href=\"#【强制】表名、字段名必须使用小写字母或数字-；-禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\" class=\"headerlink\" title=\"【强制】表名、字段名必须使用小写字母或数字 ； 禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。\"></a>【强制】表名、字段名必须使用小写字母或数字 ； 禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。</h4><p>正例： getter <em> admin ， task </em> config ， level 3_ name<br>反例： GetterAdmin ， taskConfig ， level <em>3</em> name<br>思考：因为以前遇到坑，不同数据库大小写规则不一样，所以都用小写</p>\n<h4 id=\"【强制】表名不使用复数名词。\"><a href=\"#【强制】表名不使用复数名词。\" class=\"headerlink\" title=\"【强制】表名不使用复数名词。\"></a>【强制】表名不使用复数名词。</h4><p>说明：表名应该仅仅表示表里面的实体内容，不应该表示实体数量，对应于 DO 类名也是单数形式，符合表达习惯。</p>\n<h4 id=\"【强制】唯一索引名为-uk-字段名-；-普通索引名则为-idx-字段名。\"><a href=\"#【强制】唯一索引名为-uk-字段名-；-普通索引名则为-idx-字段名。\" class=\"headerlink\" title=\"【强制】唯一索引名为 uk 字段名 ； 普通索引名则为 idx 字段名。\"></a>【强制】唯一索引名为 uk <em>字段名 ； 普通索引名则为 idx </em>字段名。</h4><p>说明： uk <em> 即  unique key；idx </em> 即 index 的简称。</p>\n<h4 id=\"【强制】小数类型为-decimal-，禁止使用-float-和-double-。\"><a href=\"#【强制】小数类型为-decimal-，禁止使用-float-和-double-。\" class=\"headerlink\" title=\"【强制】小数类型为 decimal ，禁止使用 float 和 double 。\"></a>【强制】小数类型为 decimal ，禁止使用 float 和 double 。</h4><p>说明： float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到不<br>正确的结果。<br>如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储。</p>\n<h4 id=\"【强制】如果存储的字符串长度几乎相等，使用-char-定长字符串类型。\"><a href=\"#【强制】如果存储的字符串长度几乎相等，使用-char-定长字符串类型。\" class=\"headerlink\" title=\"【强制】如果存储的字符串长度几乎相等，使用 char 定长字符串类型。\"></a>【强制】如果存储的字符串长度几乎相等，使用 char 定长字符串类型。</h4><h4 id=\"【强制】-varchar-是可变长字符串，不预先分配存储空间，长度不要超过-5000，如果存储长度大于此值，定义字段类型为-text-，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\"><a href=\"#【强制】-varchar-是可变长字符串，不预先分配存储空间，长度不要超过-5000，如果存储长度大于此值，定义字段类型为-text-，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\" class=\"headerlink\" title=\"【强制】 varchar 是可变长字符串，不预先分配存储空间，长度不要超过 5000，如果存储长度大于此值，定义字段类型为 text ，独立出来一张表，用主键来对应，避免影响其它字段索引效率。\"></a>【强制】 varchar 是可变长字符串，不预先分配存储空间，长度不要超过 5000，如果存储长度大于此值，定义字段类型为 text ，独立出来一张表，用主键来对应，避免影响其它字段索引效率。</h4><h4 id=\"【强制】表必备三字段：-id-gmt-create-gmt-modified。\"><a href=\"#【强制】表必备三字段：-id-gmt-create-gmt-modified。\" class=\"headerlink\" title=\"【强制】表必备三字段： id ,  gmt_create,gmt_modified。\"></a>【强制】表必备三字段： id ,  gmt_create,gmt_modified。</h4><p>说明：其中 id 必为主键，类型为 unsigned bigint 、单表时自增、步长为 1。 gmt_create,gmt_modified 的类型均为 date_time 类型。</p>\n<h4 id=\"【推荐】表的命名最好是加上“业务名称-表的作用”。\"><a href=\"#【推荐】表的命名最好是加上“业务名称-表的作用”。\" class=\"headerlink\" title=\"【推荐】表的命名最好是加上“业务名称_表的作用”。\"></a>【推荐】表的命名最好是加上“业务名称_表的作用”。</h4><p>正例： tiger_task/tiger_reader/mpp_config</p>\n<h4 id=\"【推荐】库名与应用名称尽量一致。\"><a href=\"#【推荐】库名与应用名称尽量一致。\" class=\"headerlink\" title=\"【推荐】库名与应用名称尽量一致。\"></a>【推荐】库名与应用名称尽量一致。</h4><h4 id=\"【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：\"><a href=\"#【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：\" class=\"headerlink\" title=\"【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：\"></a>【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：</h4><p>1 ） 不是频繁修改的字段。<br>2 ） 不是 varchar 超长字段，更不能是 text 字段。<br>正例：商品类目名称使用频率高，字段长度短，名称基本一成不变，可在相关联的表中冗余存<br>储类目名称，避免关联查询。</p>\n<h4 id=\"【推荐】单表行数超过-500-万行或者单表容量超过-2-GB-，才推荐进行分库分表。\"><a href=\"#【推荐】单表行数超过-500-万行或者单表容量超过-2-GB-，才推荐进行分库分表。\" class=\"headerlink\" title=\"【推荐】单表行数超过 500 万行或者单表容量超过 2 GB ，才推荐进行分库分表。\"></a>【推荐】单表行数超过 500 万行或者单表容量超过 2 GB ，才推荐进行分库分表。</h4><p>说明：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。</p>\n<h2 id=\"索引规约\"><a href=\"#索引规约\" class=\"headerlink\" title=\"索引规约\"></a>索引规约</h2><p>这一块，现在理解不深，需要使用的时候再理解</p>\n<h4 id=\"【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。\"><a href=\"#【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。\" class=\"headerlink\" title=\"【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。\"></a>【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。</h4><p>说明：不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明<br>显的 ；</p>\n<h4 id=\"【强制】-超过三个表禁止-join-。需要-join-的字段，数据类型保持绝对一致-；-多表关联查询时，保证被关联的字段需要有索引。\"><a href=\"#【强制】-超过三个表禁止-join-。需要-join-的字段，数据类型保持绝对一致-；-多表关联查询时，保证被关联的字段需要有索引。\" class=\"headerlink\" title=\"【强制】 超过三个表禁止 join 。需要 join 的字段，数据类型保持绝对一致 ； 多表关联查询时，保证被关联的字段需要有索引。\"></a>【强制】 超过三个表禁止 join 。需要 join 的字段，数据类型保持绝对一致 ； 多表关联查询时，保证被关联的字段需要有索引。</h4><p>说明：即使双表 join 也要注意表索引、 SQL 性能。</p>\n<h4 id=\"【强制】在-varchar-字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\"><a href=\"#【强制】在-varchar-字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\" class=\"headerlink\" title=\"【强制】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。\"></a>【强制】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。</h4><p>说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会高达 90%以上，可以使用 count(distinct left( 列名, 索引长度 )) / count( * ) 的区分度来确定。</p>\n<h4 id=\"【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\"><a href=\"#【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\" class=\"headerlink\" title=\"【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\"></a>【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。</h4><p>说明：索引文件具有 B - Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。</p>\n<h4 id=\"【参考】创建索引时避免有如下极端误解：\"><a href=\"#【参考】创建索引时避免有如下极端误解：\" class=\"headerlink\" title=\"【参考】创建索引时避免有如下极端误解：\"></a>【参考】创建索引时避免有如下极端误解：</h4><p>1 ） 误认为一个查询就需要建一个索引。<br>2 ） 误认为索引会消耗空间、严重拖慢更新和新增速度。<br>3 ） 误认为唯一索引一律需要在应用层通过“先查后插”方式解决。</p>\n<h2 id=\"SQL规约\"><a href=\"#SQL规约\" class=\"headerlink\" title=\"SQL规约\"></a>SQL规约</h2><h4 id=\"【强制】不要使用count-列名-或count-常量-来替代count-，count-就是-SQL92定义的标准统计行数的语法，跟数据库无关，跟-NULL-和非-NULL-无关。\"><a href=\"#【强制】不要使用count-列名-或count-常量-来替代count-，count-就是-SQL92定义的标准统计行数的语法，跟数据库无关，跟-NULL-和非-NULL-无关。\" class=\"headerlink\" title=\"【强制】不要使用count(列名)或count(常量)来替代count()，count()就是 SQL92定义的标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。\"></a>【强制】不要使用count(列名)或count(常量)来替代count(<em>)，count(</em>)就是 SQL92定义的标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。</h4><p>说明： count(*)会统计值为NULL的行，而count(列名)不会统计此列为NULL值的行。</p>\n<h4 id=\"【强制】使用-ISNULL-来判断是否为-NULL-值。注意：-NULL-与任何值的直接比较都为-NULL。\"><a href=\"#【强制】使用-ISNULL-来判断是否为-NULL-值。注意：-NULL-与任何值的直接比较都为-NULL。\" class=\"headerlink\" title=\"【强制】使用 ISNULL() 来判断是否为 NULL 值。注意： NULL 与任何值的直接比较都为 NULL。\"></a>【强制】使用 ISNULL() 来判断是否为 NULL 值。注意： NULL 与任何值的直接比较都为 NULL。</h4><p>说明：<br>1 ） NULL&lt;&gt;NULL 的返回结果是 NULL ，而不是 false 。<br>2 ） NULL=NULL 的返回结果是 NULL ，而不是 true 。<br>3 ） NULL&lt;&gt;1 的返回结果是 NULL ，而不是 true 。</p>\n<h4 id=\"【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\"><a href=\"#【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\" class=\"headerlink\" title=\"【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\"></a>【强制】不得使用外键与级联，一切外键概念必须在应用层解决。</h4><h4 id=\"【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\"><a href=\"#【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\" class=\"headerlink\" title=\"【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。\"></a>【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。</h4><h4 id=\"【强制】数据订正时，删除和修改记录时，要先-select-，避免出现误删除，确认无误才能执行更新语句。\"><a href=\"#【强制】数据订正时，删除和修改记录时，要先-select-，避免出现误删除，确认无误才能执行更新语句。\" class=\"headerlink\" title=\"【强制】数据订正时，删除和修改记录时，要先 select ，避免出现误删除，确认无误才能执行更新语句。\"></a>【强制】数据订正时，删除和修改记录时，要先 select ，避免出现误删除，确认无误才能执行更新语句。</h4><h4 id=\"【推荐】-in-操作能避免则避免，若实在避免不了，需要仔细评估-in-后边的集合元素数量，控制在-1000-个之内。\"><a href=\"#【推荐】-in-操作能避免则避免，若实在避免不了，需要仔细评估-in-后边的集合元素数量，控制在-1000-个之内。\" class=\"headerlink\" title=\"【推荐】 in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在 1000 个之内。\"></a>【推荐】 in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在 1000 个之内。</h4><h4 id=\"【参考】如果有全球化需要，所有的字符存储与表示，均以-utf-8-编码，那么字符计数方法\"><a href=\"#【参考】如果有全球化需要，所有的字符存储与表示，均以-utf-8-编码，那么字符计数方法\" class=\"headerlink\" title=\"【参考】如果有全球化需要，所有的字符存储与表示，均以 utf -8 编码，那么字符计数方法\"></a>【参考】如果有全球化需要，所有的字符存储与表示，均以 utf -8 编码，那么字符计数方法</h4><p>说明：<br>    SELECT LENGTH( “轻松工作” )； 返回为 12<br>    SELECT CHARACTER_LENGTH( “轻松工作” )； 返回为 4<br>    如果要使用表情，那么使用 utfmb 4 来进行存储，注意它与 utf -8 编码的区别。</p>\n<h4 id=\"【参考】TRUNCATE-TABLE-比-DELETE-速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发-trigger-，有可能造成事故，故不建议在开发代码中使用此语句。\"><a href=\"#【参考】TRUNCATE-TABLE-比-DELETE-速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发-trigger-，有可能造成事故，故不建议在开发代码中使用此语句。\" class=\"headerlink\" title=\"【参考】TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发 trigger ，有可能造成事故，故不建议在开发代码中使用此语句。\"></a>【参考】TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但TRUNCATE无事务且不触发 trigger ，有可能造成事故，故不建议在开发代码中使用此语句。</h4><p>说明： TRUNCATE TABLE 在功能上与不带  WHERE 子句的  DELETE 语句相同。</p>\n<h2 id=\"ORM规约\"><a href=\"#ORM规约\" class=\"headerlink\" title=\"ORM规约\"></a>ORM规约</h2><h4 id=\"【强制】在表查询中，一律不要使用-作为查询的字段列表，需要哪些字段必须明确写明。\"><a href=\"#【强制】在表查询中，一律不要使用-作为查询的字段列表，需要哪些字段必须明确写明。\" class=\"headerlink\" title=\"【强制】在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。\"></a>【强制】在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。</h4><p>说明：1 ） 增加查询分析器解析成本。2 ） 增减字段容易与 resultMap 配置不一致。<br>思考：这一点我做得不好，一定需要注意；</p>\n<h4 id=\"【强制】-POJO-类的-boolean-属性不能加-is-，而数据库字段必须加-is-，要求在-resultMap中进行字段与属性之间的映射。\"><a href=\"#【强制】-POJO-类的-boolean-属性不能加-is-，而数据库字段必须加-is-，要求在-resultMap中进行字段与属性之间的映射。\" class=\"headerlink\" title=\"【强制】 POJO 类的 boolean 属性不能加 is ，而数据库字段必须加 is _，要求在 resultMap中进行字段与属性之间的映射。\"></a>【强制】 POJO 类的 boolean 属性不能加 is ，而数据库字段必须加 is _，要求在 resultMap中进行字段与属性之间的映射。</h4><p>说明：参见定义 POJO 类以及数据库字段定义规定，在 sql . xml 增加映射，是必须的。</p>\n<h4 id=\"【强制】不要用-resultClass-当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义-；-反过来，每一个表也必然有一个与之对应。\"><a href=\"#【强制】不要用-resultClass-当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义-；-反过来，每一个表也必然有一个与之对应。\" class=\"headerlink\" title=\"【强制】不要用 resultClass 当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义 ； 反过来，每一个表也必然有一个与之对应。\"></a>【强制】不要用 resultClass 当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义 ； 反过来，每一个表也必然有一个与之对应。</h4><p>说明：配置映射关系，使字段与 DO 类解耦，方便维护。</p>\n<h4 id=\"【强制】-xml-配置中参数注意使用：-，-param-不要使用-此种方式容易出现SQL注入。\"><a href=\"#【强制】-xml-配置中参数注意使用：-，-param-不要使用-此种方式容易出现SQL注入。\" class=\"headerlink\" title=\"【强制】 xml 配置中参数注意使用：#{}，# param # 不要使用${} 此种方式容易出现SQL注入。\"></a>【强制】 xml 配置中参数注意使用：#{}，# param # 不要使用${} 此种方式容易出现SQL注入。</h4><h4 id=\"【强制】不允许直接拿-HashMap-与-Hashtable-作为查询结果集的输出。\"><a href=\"#【强制】不允许直接拿-HashMap-与-Hashtable-作为查询结果集的输出。\" class=\"headerlink\" title=\"【强制】不允许直接拿 HashMap 与 Hashtable 作为查询结果集的输出。\"></a>【强制】不允许直接拿 HashMap 与 Hashtable 作为查询结果集的输出。</h4><p>思考：为啥？</p>\n<h4 id=\"【强制】更新数据表记录时，必须同时更新记录对应的-gmt-modified-字段值为当前时间。\"><a href=\"#【强制】更新数据表记录时，必须同时更新记录对应的-gmt-modified-字段值为当前时间。\" class=\"headerlink\" title=\"【强制】更新数据表记录时，必须同时更新记录对应的 gmt _ modified 字段值为当前时间。\"></a>【强制】更新数据表记录时，必须同时更新记录对应的 gmt _ modified 字段值为当前时间。</h4><h4 id=\"【推荐】不要写一个大而全的数据更新接口，传入为-POJO-类，不管是不是自己的目标更新字段，都进行-update-table-set-c1-value1-c2-value2-c3-value3-这是不对的。\"><a href=\"#【推荐】不要写一个大而全的数据更新接口，传入为-POJO-类，不管是不是自己的目标更新字段，都进行-update-table-set-c1-value1-c2-value2-c3-value3-这是不对的。\" class=\"headerlink\" title=\"【推荐】不要写一个大而全的数据更新接口，传入为 POJO 类，不管是不是自己的目标更新字段，都进行 update table set c1=value1,c2=value2,c3=value3;  这是不对的。\"></a>【推荐】不要写一个大而全的数据更新接口，传入为 POJO 类，不管是不是自己的目标更新字段，都进行 update table set c1=value1,c2=value2,c3=value3;  这是不对的。</h4><p>执行 SQL时，尽量不要更新无改动的字段，一是易出错 ； 二是效率低 ； 三是 binlog 增加存储。</p>\n<h4 id=\"【参考】-Transactional-事务不要滥用。事务会影响数据库的-QPS-，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。\"><a href=\"#【参考】-Transactional-事务不要滥用。事务会影响数据库的-QPS-，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。\" class=\"headerlink\" title=\"【参考】@ Transactional 事务不要滥用。事务会影响数据库的 QPS ，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。\"></a>【参考】@ Transactional 事务不要滥用。事务会影响数据库的 QPS ，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。</h4><h4 id=\"【参考】-lt-isEqual-gt-中的-compareValue-是与属性值对比的常量，一般是数字，表示相等时带上此条件-；-lt-isNotEmpty-gt-表示不为空且不为-null-时执行-；-lt-isNotNull-gt-表示不为-null-值时执行。\"><a href=\"#【参考】-lt-isEqual-gt-中的-compareValue-是与属性值对比的常量，一般是数字，表示相等时带上此条件-；-lt-isNotEmpty-gt-表示不为空且不为-null-时执行-；-lt-isNotNull-gt-表示不为-null-值时执行。\" class=\"headerlink\" title=\"【参考】&lt; isEqual &gt;中的 compareValue 是与属性值对比的常量，一般是数字，表示相等时带上此条件 ； &lt; isNotEmpty &gt;表示不为空且不为 null 时执行 ； &lt; isNotNull &gt;表示不为 null 值时执行。\"></a>【参考】&lt; isEqual &gt;中的 compareValue 是与属性值对比的常量，一般是数字，表示相等时带上此条件 ； &lt; isNotEmpty &gt;表示不为空且不为 null 时执行 ； &lt; isNotNull &gt;表示不为 null 值时执行。</h4><h2 id=\"工程规约\"><a href=\"#工程规约\" class=\"headerlink\" title=\"工程规约\"></a>工程规约</h2><h4 id=\"【强制】定义-GAV-遵从以下规则：\"><a href=\"#【强制】定义-GAV-遵从以下规则：\" class=\"headerlink\" title=\"【强制】定义 GAV 遵从以下规则：\"></a>【强制】定义 GAV 遵从以下规则：</h4><p>1 ） GroupID 格式：com.{公司/BU}.业务线.[子业务线]，最多4级。<br>说明：{公司/BU}例如：alibaba/taobao/tmall/aliexpress等BU一级；子业务线可选。<br>正例：com.taobao.jstorm或com.alibaba.dubbo.register<br>2）ArtifactID格式：产品线名-模块名。语义不重复不遗漏，先到仓库中心去查证一下。<br>正例：dubbo-client/fastjson-api/jstorm-tool<br>3）Version：详细规定参考下方。</p>\n<p>思考：com.ctcc.bigdata   openapi-storage</p>\n<h4 id=\"【强制】二方库版本号命名方式：主版本号-次版本号-修订号\"><a href=\"#【强制】二方库版本号命名方式：主版本号-次版本号-修订号\" class=\"headerlink\" title=\"【强制】二方库版本号命名方式：主版本号.次版本号.修订号\"></a>【强制】二方库版本号命名方式：主版本号.次版本号.修订号</h4><p>1 ） 主版本号 ：当做了不兼容的 API 修改，或者增加了能改变产品方向的新功能。<br>2 ） 次版本号 ：当做了向下兼容的功能性新增 （ 新增类、接口等 ） 。<br>3 ） 修订号 ：修复 bug ，没有修改方法签名的功能加强，保持  API 兼容性。<br>说明：起始版本号必须为： 1.0.0 ，而不是 0.0.1</p>\n<h4 id=\"【强制】线上应用不要依赖-SNAPSHOT-版本-（-安全包除外-）；\"><a href=\"#【强制】线上应用不要依赖-SNAPSHOT-版本-（-安全包除外-）；\" class=\"headerlink\" title=\"【强制】线上应用不要依赖 SNAPSHOT 版本 （ 安全包除外 ）；\"></a>【强制】线上应用不要依赖 SNAPSHOT 版本 （ 安全包除外 ）；</h4><p>正式发布的类库必须使用 RELEASE版本号升级+1 的方式，且版本号不允许覆盖升级，必须去中央仓库进行查证。<br>说明：不依赖 SNAPSHOT 版本是保证应用发布的幂等性。另外，也可以加快编译时的打包构建。</p>\n<h4 id=\"【强制】二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的-POJO-对象。\"><a href=\"#【强制】二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的-POJO-对象。\" class=\"headerlink\" title=\"【强制】二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的 POJO 对象。\"></a>【强制】二方库里可以定义枚举类型，参数可以使用枚举类型，但是接口返回值不允许使用枚举类型或者包含枚举类型的 POJO 对象。</h4><h4 id=\"【强制】依赖于一个二方库群时，必须定义一个统一版本变量，避免版本号不一致。\"><a href=\"#【强制】依赖于一个二方库群时，必须定义一个统一版本变量，避免版本号不一致。\" class=\"headerlink\" title=\"【强制】依赖于一个二方库群时，必须定义一个统一版本变量，避免版本号不一致。\"></a>【强制】依赖于一个二方库群时，必须定义一个统一版本变量，避免版本号不一致。</h4><p>说明：依赖 springframework - core ,- context ,- beans ，它们都是同一个版本，可以定义一个变量来保存版本：${ spring . version }，定义依赖的时候，引用该版本。</p>\n<h4 id=\"【强制】禁止在子项目的-pom-依赖中出现相同的-GroupId-，相同的-ArtifactId-，但是不同的Version-。\"><a href=\"#【强制】禁止在子项目的-pom-依赖中出现相同的-GroupId-，相同的-ArtifactId-，但是不同的Version-。\" class=\"headerlink\" title=\"【强制】禁止在子项目的 pom 依赖中出现相同的 GroupId ，相同的 ArtifactId ，但是不同的Version 。\"></a>【强制】禁止在子项目的 pom 依赖中出现相同的 GroupId ，相同的 ArtifactId ，但是不同的Version 。</h4><p>说明：在本地调试时会使用各子项目指定的版本号，但是合并成一个 war ，只能有一个版本号出现在最后的 lib 目录中。曾经出现过线下调试是正确的，发布到线上出故障的先例。</p>\n<h2 id=\"服务器规约\"><a href=\"#服务器规约\" class=\"headerlink\" title=\"服务器规约\"></a>服务器规约</h2><h4 id=\"【推荐】高并发服务器建议调小-TCP-协议的-time-wait-超时时间。\"><a href=\"#【推荐】高并发服务器建议调小-TCP-协议的-time-wait-超时时间。\" class=\"headerlink\" title=\"【推荐】高并发服务器建议调小 TCP 协议的 time _ wait 超时时间。\"></a>【推荐】高并发服务器建议调小 TCP 协议的 time _ wait 超时时间。</h4><p>说明：操作系统默认 240 秒后，才会关闭处于 time <em> wait 状态的连接，在高并发访问下，服务器端会因为处于 time </em> wait 的连接数太多，可能无法建立新的连接，所以需要在服务器上调小此等待值。</p>\n<h4 id=\"【推荐】调大服务器所支持的最大文件句柄数-（File-Descriptor-，简写为-fd）-。\"><a href=\"#【推荐】调大服务器所支持的最大文件句柄数-（File-Descriptor-，简写为-fd）-。\" class=\"headerlink\" title=\"【推荐】调大服务器所支持的最大文件句柄数 （File Descriptor ，简写为 fd） 。\"></a>【推荐】调大服务器所支持的最大文件句柄数 （File Descriptor ，简写为 fd） 。</h4><p>说明：主流操作系统的设计是将 TCP / UDP 连接采用与文件一样的方式去管理，即一个连接对应于一个 fd 。主流的 linux 服务器默认所支持最大 fd 数量为 1024，当并发连接数很大时很容易因为 fd 不足而出现“ open too many files ”错误，导致新的连接无法建立。 建议将 linux服务器所支持的最大句柄数调高数倍 （ 与服务器的内存数量相关 ） 。<br>思考：kafka ，flume等中间件也要这么配</p>\n<h4 id=\"【推荐】给-JVM-设置-XX-HeapDumpOnOutOfMemoryError-参数，让-JVM-碰到-OOM-场景时输出dump-信息。\"><a href=\"#【推荐】给-JVM-设置-XX-HeapDumpOnOutOfMemoryError-参数，让-JVM-碰到-OOM-场景时输出dump-信息。\" class=\"headerlink\" title=\"【推荐】给 JVM 设置- XX :+ HeapDumpOnOutOfMemoryError 参数，让 JVM 碰到 OOM 场景时输出dump 信息。\"></a>【推荐】给 JVM 设置- XX :+ HeapDumpOnOutOfMemoryError 参数，让 JVM 碰到 OOM 场景时输出dump 信息。</h4><p>说明： OOM 的发生是有概率的，甚至有规律地相隔数月才出现一例，出现时的现场信息对查错非常有价值。</p>\n<h4 id=\"【参考】服务器内部重定向使用-forward；-外部重定向地址使用-URL-拼装工具类来生成，否则会带来-URL-维护不一致的问题和潜在的安全风险。\"><a href=\"#【参考】服务器内部重定向使用-forward；-外部重定向地址使用-URL-拼装工具类来生成，否则会带来-URL-维护不一致的问题和潜在的安全风险。\" class=\"headerlink\" title=\"【参考】服务器内部重定向使用 forward； 外部重定向地址使用 URL 拼装工具类来生成，否则会带来 URL 维护不一致的问题和潜在的安全风险。\"></a>【参考】服务器内部重定向使用 forward； 外部重定向地址使用 URL 拼装工具类来生成，否则会带来 URL 维护不一致的问题和潜在的安全风险。</h4><h2 id=\"安全规约\"><a href=\"#安全规约\" class=\"headerlink\" title=\"安全规约\"></a>安全规约</h2><h4 id=\"【强制】隶属于用户个人的页面或者功能必须进行权限控制校验。\"><a href=\"#【强制】隶属于用户个人的页面或者功能必须进行权限控制校验。\" class=\"headerlink\" title=\"【强制】隶属于用户个人的页面或者功能必须进行权限控制校验。\"></a>【强制】隶属于用户个人的页面或者功能必须进行权限控制校验。</h4><p>说明：防止没有做水平权限校验就可随意访问、操作别人的数据，比如查看、修改别人的订单。</p>\n<h4 id=\"【强制】用户敏感数据禁止直接展示，必须对展示数据脱敏。\"><a href=\"#【强制】用户敏感数据禁止直接展示，必须对展示数据脱敏。\" class=\"headerlink\" title=\"【强制】用户敏感数据禁止直接展示，必须对展示数据脱敏。\"></a>【强制】用户敏感数据禁止直接展示，必须对展示数据脱敏。</h4><p>说明：查看个人手机号码会显示成:158<em>**</em>9119，隐藏中间 4 位，防止隐私泄露。<br>思考：在openAPI项目中一定要注意；</p>\n<h4 id=\"【强制】用户输入的-SQL-参数严格使用参数绑定或者-METADATA-字段值限定，防止-SQL-注入，\"><a href=\"#【强制】用户输入的-SQL-参数严格使用参数绑定或者-METADATA-字段值限定，防止-SQL-注入，\" class=\"headerlink\" title=\"【强制】用户输入的 SQL 参数严格使用参数绑定或者 METADATA 字段值限定，防止 SQL 注入，\"></a>【强制】用户输入的 SQL 参数严格使用参数绑定或者 METADATA 字段值限定，防止 SQL 注入，</h4><p>禁止字符串拼接 SQL 访问数据库。</p>\n<h4 id=\"【强制】用户请求传入的任何参数必须做有效性验证。\"><a href=\"#【强制】用户请求传入的任何参数必须做有效性验证。\" class=\"headerlink\" title=\"【强制】用户请求传入的任何参数必须做有效性验证。\"></a>【强制】用户请求传入的任何参数必须做有效性验证。</h4><p>说明：忽略参数校验可能导致：<br>  page size 过大导致内存溢出<br>  恶意 order by 导致数据库慢查询<br>  任意重定向<br>  SQL 注入<br>  反序列化注入<br>  正则输入源串拒绝服务 ReDoS<br>说明：Java 代码用正则来验证客户端的输入，有些正则写法验证普通用户输入没有问题，但是如果攻击人员使用的是特殊构造的字符串来验证，有可能导致死循环的效果。</p>\n<h4 id=\"【强制】禁止向-HTML-页面输出未经安全过滤或未正确转义的用户数据。\"><a href=\"#【强制】禁止向-HTML-页面输出未经安全过滤或未正确转义的用户数据。\" class=\"headerlink\" title=\"【强制】禁止向 HTML 页面输出未经安全过滤或未正确转义的用户数据。\"></a>【强制】禁止向 HTML 页面输出未经安全过滤或未正确转义的用户数据。</h4><h4 id=\"【强制】在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放限制，如数量限制、疲劳度控制、验证码校验，避免被滥刷、资损。\"><a href=\"#【强制】在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放限制，如数量限制、疲劳度控制、验证码校验，避免被滥刷、资损。\" class=\"headerlink\" title=\"【强制】在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放限制，如数量限制、疲劳度控制、验证码校验，避免被滥刷、资损。\"></a>【强制】在使用平台资源，譬如短信、邮件、电话、下单、支付，必须实现正确的防重放限制，如数量限制、疲劳度控制、验证码校验，避免被滥刷、资损。</h4><p>说明：如注册时发送验证码到手机，如果没有限制次数和频率，那么可以利用此功能骚扰到其<br>它用户，并造成短信平台资源浪费。</p>\n<h1 id=\"思考\"><a href=\"#思考\" class=\"headerlink\" title=\"思考\"></a>思考</h1><p>这两天断断续续看完了全部，看得时候觉得受益匪浅，但是这种东西要消化成自己的知识，一定要践行，等编码到具体部分的时候再复习，写出优秀的代码。</p>\n"},{"title":"OpenAPI微服务接入规范","toc":true,"date":"2017-05-22T07:15:52.000Z","_content":"\nOpenAPI采用了微服务的架构，基于Spring Cloud为微服务系统中相互依赖的服务提供了丰富的连接选项。\n\n### 接口规范\nOpenAPI采用了微服务的架构，每个微服务使用统一的对外接口规范，不需要考虑安全与负载均衡等其它因素，只需要关注自己的业务逻辑即可，参数与对外返回应该保持一致；\n\n#### 微服务统一命名\n每个微服务都有一个统一的名称作为服务唯一标识符，也作为服务注册发现，负载均衡，服务熔断等功能的标识，OpenAPI将为微服务提供统一的查询界面，通过唯一的微服务名称查询服务的功能、调用方式以及参数：\n![image](open-api1.png)\n样例：\n- openapi-service-business-wzfw 位置服务API\n- openapi-service-business-test 其它业务API\n- openapi-service-compute-adhoc  计算API-即席分析\n- openapi-service-storage-hbase  存储API-hbase\n\n#### 微服务端口规范\n接入的微服务端口号都在8000-9000之间，但不同类型的服务处在不同的区间，其中，存储API的端口默认为 8100-8200，业务API的端口默认为 8200-8300，计算API的端口默认为 8300-8400，其它类型微服务API的端口默认为 8400-8500\n\n#### 接口命名\n微服务开发的时候，不需要在自己的URL中指定URL前缀，URL前缀由网关层统一路由，只需要指定接口即可\n\n微服务API的接口应统一以小写字符串为接口名，不要用下划线，而用“-”，\n比如：GET /current-location\n\n注：网关层统一URL前缀路由方案：每个业务的URL前缀应该与微服务的统一命名有一致的对应关系，对应统一命名的后两段，比如：\n位置服务： 统一命名：openapi-service-business-wzfw 那它的URL前缀就是：/business/wzfw/\n\n#### 参数命名\n除了微服务自己的请求参数，每个微服务都会接收到一个可选的 OpenAPI的请求id openRequestId，**其它参数不能有相同的参数名称**。openRequestId 是由Open网关层生成的字符串类型，代表用户的这一次请求，便于问题追踪与故障排查以及其它需求；\n除此之外，没有其它限制\n\n#### 返回规范\n返回接口也均推荐采用RestFul API的风格，具体有以下几点约定：\n\n##### 响应码永远是200\n服务端在成功接收到客户端的请求之后，在能够处理和捕获的情况下，http头的响应码永远是200，具体成功与否及进一步的信息放入返回的内容。如果客户端获取到的返回码不是200，代表链路上某一个环节出了问题。\n\n##### 不允许抛出异常\n微服务的最外层不允许抛出异常，最好捕获所有指定的异常并主动返回，如果有未知异常发生，则建议捕获后将异常作为message信息返回；\n\n##### 统一返回格式\n所有返回都要有 code、openRequestId、data、message这四个参数，String方式返回以下形式：\n``` json\n{\n    code: int , # 返回码 正常为 2开头，比如200，错误情况尽量兼容RestFul的返回码，可以自定义\n    message: String # 如果出错的情况，则返回错误信息，如果不出错的话，则返回\"\"\n    openRequestId: String, # 这个openRequestId是open网关层传入的全局id，代表这一次请求\n    data: json, # 真正的数据，以json格式表示，一般会有 msisdn 字段\n}\n```\n注意，这里的code返回码，应该遵循统一的格式。\n\n比如：GET /business/wzfw/current-locationd的正确返回：\n``` json\n{\n  \"code\": 200,\n  \"message\": \"\",\n  \"openRequestId\": \"business_wzfw_work-location_000001\",\n  \"data\": {\n    \"msisdn\": \"17321452140\",\n    \"zipCode\": 21,\n    \"provId\": 831,\n    \"cityCode\": 83101,\n    \"location\": \"31.14570,121.54082\",\n    \"occurTime\": \"2017-05-23T09:52:07.000Z\"\n  }\n}\n```\n错误返回：\n``` json\n{\n  \"code\": 404001,\n  \"message\": \"该数据不存在.17322222222\",\n  \"openRequestId\": \"business_wzfw_work-location_000001\",\n  \"data\": null\n}\n```\n##### 统一错误码（持续更新）\n\n\n#### 统一日志、收集与分析\n为了对每个微服务进行日志的统一分析，我们使用了基于ELK技术栈的日志收集与分析的系统，为了更好的进行统一日志收集与分析，微服务需要遵循统一的约定来记录日志，各个微服务只需要将日志记录到本地文件中即可，我们会统一进行数据抓取，数据收集与数据分析；\n\n根据日志作用，将日志具体分成以下3类，分别放在指定目录的不同文件夹下：\n- 业务日志\n\tOpenApi业务调用日志，为后期日志分析，提供数据源。\n- 普通日志\n\t开发、调试相关日志，方便程序排除故障，程序调优等。\n- 组件日志\n\t系统内部相关组件日志，例如Spring日志，tomcat日志、eureka日志、ribbon日志、hystrix日志等\n\n以已经开发完成的位置服务为例：\n业务日志 ./logs/business/wzfw/stat/  使用统一的格式统计web层的请求，便于数据分析与统计\n普通日志 ./logs/business/wzfw/code/ 开发者在代码中用于debug的日志，便于排查\n组件日志 ./logs/business/wzfw/other/  Spring等其它引用框架自带的日志，作为留存\n其中业务日志是必须需要的；\n\n### 接入规范\n为了更好地进行微服务的开发，在遵循上述接口规范的基础上，在技术层面上会简要列出下述接入规范，供开发者开发以及已经开发好的项目接入进行参考；\n\n注：与基于Dubbo框架的微服务架构不同，基于spring-cloud的OpenAPI微服务架构做得的不仅仅是协议的转换，还提供了服务自动注册与发现，自动负载均衡，服务自动熔断，统一日志收集与分析以及监控告警等功能，在项目庞大的时候会引入统一配置管理。因此，为了更好更简单地使用这些额外的功能，推荐使用Java技术栈的spring-boot框架进行开发，会达到事半功倍的效果。当然，OpenAPI也支持其它语言与框架的接入。\n\n#### 使用spring-boot接入\nSpring Boot让我们的Spring应用变的更轻量化，有经验的Java程序员一天就可以上手，Java项目改造为spring-boot项目也很简单，spring-boot的主要优点如下：\n\n- 为所有Spring开发者更快的入门\n- 开箱即用，提供各种默认配置来简化项目配置\n- 内嵌式容器简化Web项目\n- 没有冗余代码生成和XML配置的要求\n\n#### 项目改造为spring-boot工程\n我们大部分的微服务都是以Java开发的，首先希望最好的接入方案是将项目改造为spring-boot工程，使用spring-boot进行开发，简单高效，只需要了解业务逻辑即可，这个是效果最好的接入方案，建议采用下列方式引入spring boot依赖\n``` xml\n<parent>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>1.4.5.RELEASE</version>\n    <relativePath /> <!-- lookup parent from repository -->\n</parent>\n```\n#### 微服务统一命名\n``` bash\nspring.application.name=openapi-service-business-wzfw\nserver.port=82XX\n```\n通过spring.application.name属性，我们可以指定微服务的名称后续在调用的时候只需要使用该名称就可以进行服务的访问。\n\n#### 接入服务发现与服务注册\n对于spring-boot来说，接入服务发现与服务注册功能和简单，只需要以下两步：\n1. pom.xml中增加依赖\n``` xml\n<!-- 添加Eureka的依赖 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-eureka</artifactId>\n\t\t</dependency>\n```\n2. 配置文件中增加一行eurake的配置\n``` bash\n// 指定注册中心的地址\neureka.client.serviceUrl.defaultZone=http://discovery:8761/eureka/\n```\n这样，简单的两步操作，就可以使你的服务被微服务统一管理了。\n\n#### 使用负载均衡功能\n有些微服务访问压力比较大，需要被多个负载均衡同时处理业务，需要负载均衡的能力，当你的服务注册到eurake中后，使用负载均衡功能非常简单，只需要换一个机器，将你的服务部署, 然后注册到OpenAPI就可以了，OpenAPI网关这里会自动通过 ribbon对你的服务进行负载均衡。\n\n负载均衡的默认策略是进行轮寻负载，如果有独特的负载均衡方式，请联系OpenAPI管理员进行其它的负载均衡方案。\n\n#### 使用断路器功能\n当某个服务单元发生故障（类似用电器发生短路）之后，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个错误响应，而不是长时间的等待。这样就不会使得线程因调用故障服务被长时间占用不释放，避免了故障在分布式系统中的蔓延。OpenAPI基于开源的Hystrix来实现断路器的功能；\n\n使用断路器功能，微服务开发者不需要修改任何代码，只需要为OpenAPI管理员提出需求，制定熔断策略，我们会对代码进行非侵入式的熔断管理，从而防止错误在整个系统中的蔓延；\n\n#### 使用统一日志收集与分析服务\n其中，如果使用Spring-boot开发的话，统一使用 slf4j + logback 的技术来进行日志记录，通过统一的logback配置文件可以极大地提高开发效率；业务日志统一使用面向切面编程技术（AOP）来记录日志。如果用python相关web框架实现微服务的话，也要做到统计日志格式的统一。\n\n具体日志格式以附件中 logback.xml 的配置文件为准。\n\n### Dubbo微服务接入\n王超\n\n### Python微服务接入\n黄泽实例\n\n### 其它微服务接入\n\n将 eurake的原理与API介绍给他们","source":"_posts/2017-05-22-OpenAPI微服务接入规范.md","raw":"---\ntitle: OpenAPI微服务接入规范\ntoc: true\ndate: 2017-05-22 15:15:52\ntags:\n- java\n- 架构\n- spring cloud\n- 微服务\ncategories: spring cloud\n---\n\nOpenAPI采用了微服务的架构，基于Spring Cloud为微服务系统中相互依赖的服务提供了丰富的连接选项。\n\n### 接口规范\nOpenAPI采用了微服务的架构，每个微服务使用统一的对外接口规范，不需要考虑安全与负载均衡等其它因素，只需要关注自己的业务逻辑即可，参数与对外返回应该保持一致；\n\n#### 微服务统一命名\n每个微服务都有一个统一的名称作为服务唯一标识符，也作为服务注册发现，负载均衡，服务熔断等功能的标识，OpenAPI将为微服务提供统一的查询界面，通过唯一的微服务名称查询服务的功能、调用方式以及参数：\n![image](open-api1.png)\n样例：\n- openapi-service-business-wzfw 位置服务API\n- openapi-service-business-test 其它业务API\n- openapi-service-compute-adhoc  计算API-即席分析\n- openapi-service-storage-hbase  存储API-hbase\n\n#### 微服务端口规范\n接入的微服务端口号都在8000-9000之间，但不同类型的服务处在不同的区间，其中，存储API的端口默认为 8100-8200，业务API的端口默认为 8200-8300，计算API的端口默认为 8300-8400，其它类型微服务API的端口默认为 8400-8500\n\n#### 接口命名\n微服务开发的时候，不需要在自己的URL中指定URL前缀，URL前缀由网关层统一路由，只需要指定接口即可\n\n微服务API的接口应统一以小写字符串为接口名，不要用下划线，而用“-”，\n比如：GET /current-location\n\n注：网关层统一URL前缀路由方案：每个业务的URL前缀应该与微服务的统一命名有一致的对应关系，对应统一命名的后两段，比如：\n位置服务： 统一命名：openapi-service-business-wzfw 那它的URL前缀就是：/business/wzfw/\n\n#### 参数命名\n除了微服务自己的请求参数，每个微服务都会接收到一个可选的 OpenAPI的请求id openRequestId，**其它参数不能有相同的参数名称**。openRequestId 是由Open网关层生成的字符串类型，代表用户的这一次请求，便于问题追踪与故障排查以及其它需求；\n除此之外，没有其它限制\n\n#### 返回规范\n返回接口也均推荐采用RestFul API的风格，具体有以下几点约定：\n\n##### 响应码永远是200\n服务端在成功接收到客户端的请求之后，在能够处理和捕获的情况下，http头的响应码永远是200，具体成功与否及进一步的信息放入返回的内容。如果客户端获取到的返回码不是200，代表链路上某一个环节出了问题。\n\n##### 不允许抛出异常\n微服务的最外层不允许抛出异常，最好捕获所有指定的异常并主动返回，如果有未知异常发生，则建议捕获后将异常作为message信息返回；\n\n##### 统一返回格式\n所有返回都要有 code、openRequestId、data、message这四个参数，String方式返回以下形式：\n``` json\n{\n    code: int , # 返回码 正常为 2开头，比如200，错误情况尽量兼容RestFul的返回码，可以自定义\n    message: String # 如果出错的情况，则返回错误信息，如果不出错的话，则返回\"\"\n    openRequestId: String, # 这个openRequestId是open网关层传入的全局id，代表这一次请求\n    data: json, # 真正的数据，以json格式表示，一般会有 msisdn 字段\n}\n```\n注意，这里的code返回码，应该遵循统一的格式。\n\n比如：GET /business/wzfw/current-locationd的正确返回：\n``` json\n{\n  \"code\": 200,\n  \"message\": \"\",\n  \"openRequestId\": \"business_wzfw_work-location_000001\",\n  \"data\": {\n    \"msisdn\": \"17321452140\",\n    \"zipCode\": 21,\n    \"provId\": 831,\n    \"cityCode\": 83101,\n    \"location\": \"31.14570,121.54082\",\n    \"occurTime\": \"2017-05-23T09:52:07.000Z\"\n  }\n}\n```\n错误返回：\n``` json\n{\n  \"code\": 404001,\n  \"message\": \"该数据不存在.17322222222\",\n  \"openRequestId\": \"business_wzfw_work-location_000001\",\n  \"data\": null\n}\n```\n##### 统一错误码（持续更新）\n\n\n#### 统一日志、收集与分析\n为了对每个微服务进行日志的统一分析，我们使用了基于ELK技术栈的日志收集与分析的系统，为了更好的进行统一日志收集与分析，微服务需要遵循统一的约定来记录日志，各个微服务只需要将日志记录到本地文件中即可，我们会统一进行数据抓取，数据收集与数据分析；\n\n根据日志作用，将日志具体分成以下3类，分别放在指定目录的不同文件夹下：\n- 业务日志\n\tOpenApi业务调用日志，为后期日志分析，提供数据源。\n- 普通日志\n\t开发、调试相关日志，方便程序排除故障，程序调优等。\n- 组件日志\n\t系统内部相关组件日志，例如Spring日志，tomcat日志、eureka日志、ribbon日志、hystrix日志等\n\n以已经开发完成的位置服务为例：\n业务日志 ./logs/business/wzfw/stat/  使用统一的格式统计web层的请求，便于数据分析与统计\n普通日志 ./logs/business/wzfw/code/ 开发者在代码中用于debug的日志，便于排查\n组件日志 ./logs/business/wzfw/other/  Spring等其它引用框架自带的日志，作为留存\n其中业务日志是必须需要的；\n\n### 接入规范\n为了更好地进行微服务的开发，在遵循上述接口规范的基础上，在技术层面上会简要列出下述接入规范，供开发者开发以及已经开发好的项目接入进行参考；\n\n注：与基于Dubbo框架的微服务架构不同，基于spring-cloud的OpenAPI微服务架构做得的不仅仅是协议的转换，还提供了服务自动注册与发现，自动负载均衡，服务自动熔断，统一日志收集与分析以及监控告警等功能，在项目庞大的时候会引入统一配置管理。因此，为了更好更简单地使用这些额外的功能，推荐使用Java技术栈的spring-boot框架进行开发，会达到事半功倍的效果。当然，OpenAPI也支持其它语言与框架的接入。\n\n#### 使用spring-boot接入\nSpring Boot让我们的Spring应用变的更轻量化，有经验的Java程序员一天就可以上手，Java项目改造为spring-boot项目也很简单，spring-boot的主要优点如下：\n\n- 为所有Spring开发者更快的入门\n- 开箱即用，提供各种默认配置来简化项目配置\n- 内嵌式容器简化Web项目\n- 没有冗余代码生成和XML配置的要求\n\n#### 项目改造为spring-boot工程\n我们大部分的微服务都是以Java开发的，首先希望最好的接入方案是将项目改造为spring-boot工程，使用spring-boot进行开发，简单高效，只需要了解业务逻辑即可，这个是效果最好的接入方案，建议采用下列方式引入spring boot依赖\n``` xml\n<parent>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>1.4.5.RELEASE</version>\n    <relativePath /> <!-- lookup parent from repository -->\n</parent>\n```\n#### 微服务统一命名\n``` bash\nspring.application.name=openapi-service-business-wzfw\nserver.port=82XX\n```\n通过spring.application.name属性，我们可以指定微服务的名称后续在调用的时候只需要使用该名称就可以进行服务的访问。\n\n#### 接入服务发现与服务注册\n对于spring-boot来说，接入服务发现与服务注册功能和简单，只需要以下两步：\n1. pom.xml中增加依赖\n``` xml\n<!-- 添加Eureka的依赖 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-eureka</artifactId>\n\t\t</dependency>\n```\n2. 配置文件中增加一行eurake的配置\n``` bash\n// 指定注册中心的地址\neureka.client.serviceUrl.defaultZone=http://discovery:8761/eureka/\n```\n这样，简单的两步操作，就可以使你的服务被微服务统一管理了。\n\n#### 使用负载均衡功能\n有些微服务访问压力比较大，需要被多个负载均衡同时处理业务，需要负载均衡的能力，当你的服务注册到eurake中后，使用负载均衡功能非常简单，只需要换一个机器，将你的服务部署, 然后注册到OpenAPI就可以了，OpenAPI网关这里会自动通过 ribbon对你的服务进行负载均衡。\n\n负载均衡的默认策略是进行轮寻负载，如果有独特的负载均衡方式，请联系OpenAPI管理员进行其它的负载均衡方案。\n\n#### 使用断路器功能\n当某个服务单元发生故障（类似用电器发生短路）之后，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个错误响应，而不是长时间的等待。这样就不会使得线程因调用故障服务被长时间占用不释放，避免了故障在分布式系统中的蔓延。OpenAPI基于开源的Hystrix来实现断路器的功能；\n\n使用断路器功能，微服务开发者不需要修改任何代码，只需要为OpenAPI管理员提出需求，制定熔断策略，我们会对代码进行非侵入式的熔断管理，从而防止错误在整个系统中的蔓延；\n\n#### 使用统一日志收集与分析服务\n其中，如果使用Spring-boot开发的话，统一使用 slf4j + logback 的技术来进行日志记录，通过统一的logback配置文件可以极大地提高开发效率；业务日志统一使用面向切面编程技术（AOP）来记录日志。如果用python相关web框架实现微服务的话，也要做到统计日志格式的统一。\n\n具体日志格式以附件中 logback.xml 的配置文件为准。\n\n### Dubbo微服务接入\n王超\n\n### Python微服务接入\n黄泽实例\n\n### 其它微服务接入\n\n将 eurake的原理与API介绍给他们","slug":"OpenAPI微服务接入规范","published":1,"updated":"2017-05-23T07:20:32.549Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfxe003ipsguo9a6olw3","content":"<p>OpenAPI采用了微服务的架构，基于Spring Cloud为微服务系统中相互依赖的服务提供了丰富的连接选项。</p>\n<h3 id=\"接口规范\"><a href=\"#接口规范\" class=\"headerlink\" title=\"接口规范\"></a>接口规范</h3><p>OpenAPI采用了微服务的架构，每个微服务使用统一的对外接口规范，不需要考虑安全与负载均衡等其它因素，只需要关注自己的业务逻辑即可，参数与对外返回应该保持一致；</p>\n<h4 id=\"微服务统一命名\"><a href=\"#微服务统一命名\" class=\"headerlink\" title=\"微服务统一命名\"></a>微服务统一命名</h4><p>每个微服务都有一个统一的名称作为服务唯一标识符，也作为服务注册发现，负载均衡，服务熔断等功能的标识，OpenAPI将为微服务提供统一的查询界面，通过唯一的微服务名称查询服务的功能、调用方式以及参数：<br><img src=\"open-api1.png\" alt=\"image\"><br>样例：</p>\n<ul>\n<li>openapi-service-business-wzfw 位置服务API</li>\n<li>openapi-service-business-test 其它业务API</li>\n<li>openapi-service-compute-adhoc  计算API-即席分析</li>\n<li>openapi-service-storage-hbase  存储API-hbase</li>\n</ul>\n<h4 id=\"微服务端口规范\"><a href=\"#微服务端口规范\" class=\"headerlink\" title=\"微服务端口规范\"></a>微服务端口规范</h4><p>接入的微服务端口号都在8000-9000之间，但不同类型的服务处在不同的区间，其中，存储API的端口默认为 8100-8200，业务API的端口默认为 8200-8300，计算API的端口默认为 8300-8400，其它类型微服务API的端口默认为 8400-8500</p>\n<h4 id=\"接口命名\"><a href=\"#接口命名\" class=\"headerlink\" title=\"接口命名\"></a>接口命名</h4><p>微服务开发的时候，不需要在自己的URL中指定URL前缀，URL前缀由网关层统一路由，只需要指定接口即可</p>\n<p>微服务API的接口应统一以小写字符串为接口名，不要用下划线，而用“-”，<br>比如：GET /current-location</p>\n<p>注：网关层统一URL前缀路由方案：每个业务的URL前缀应该与微服务的统一命名有一致的对应关系，对应统一命名的后两段，比如：<br>位置服务： 统一命名：openapi-service-business-wzfw 那它的URL前缀就是：/business/wzfw/</p>\n<h4 id=\"参数命名\"><a href=\"#参数命名\" class=\"headerlink\" title=\"参数命名\"></a>参数命名</h4><p>除了微服务自己的请求参数，每个微服务都会接收到一个可选的 OpenAPI的请求id openRequestId，<strong>其它参数不能有相同的参数名称</strong>。openRequestId 是由Open网关层生成的字符串类型，代表用户的这一次请求，便于问题追踪与故障排查以及其它需求；<br>除此之外，没有其它限制</p>\n<h4 id=\"返回规范\"><a href=\"#返回规范\" class=\"headerlink\" title=\"返回规范\"></a>返回规范</h4><p>返回接口也均推荐采用RestFul API的风格，具体有以下几点约定：</p>\n<h5 id=\"响应码永远是200\"><a href=\"#响应码永远是200\" class=\"headerlink\" title=\"响应码永远是200\"></a>响应码永远是200</h5><p>服务端在成功接收到客户端的请求之后，在能够处理和捕获的情况下，http头的响应码永远是200，具体成功与否及进一步的信息放入返回的内容。如果客户端获取到的返回码不是200，代表链路上某一个环节出了问题。</p>\n<h5 id=\"不允许抛出异常\"><a href=\"#不允许抛出异常\" class=\"headerlink\" title=\"不允许抛出异常\"></a>不允许抛出异常</h5><p>微服务的最外层不允许抛出异常，最好捕获所有指定的异常并主动返回，如果有未知异常发生，则建议捕获后将异常作为message信息返回；</p>\n<h5 id=\"统一返回格式\"><a href=\"#统一返回格式\" class=\"headerlink\" title=\"统一返回格式\"></a>统一返回格式</h5><p>所有返回都要有 code、openRequestId、data、message这四个参数，String方式返回以下形式：<br><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    code: int , # 返回码 正常为 2开头，比如200，错误情况尽量兼容RestFul的返回码，可以自定义</div><div class=\"line\">    message: String # 如果出错的情况，则返回错误信息，如果不出错的话，则返回\"\"</div><div class=\"line\">    openRequestId: String, # 这个openRequestId是open网关层传入的全局id，代表这一次请求</div><div class=\"line\">    data: json, # 真正的数据，以json格式表示，一般会有 msisdn 字段</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>注意，这里的code返回码，应该遵循统一的格式。</p>\n<p>比如：GET /business/wzfw/current-locationd的正确返回：<br><figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  <span class=\"attr\">\"code\"</span>: <span class=\"number\">200</span>,</div><div class=\"line\">  <span class=\"attr\">\"message\"</span>: <span class=\"string\">\"\"</span>,</div><div class=\"line\">  <span class=\"attr\">\"openRequestId\"</span>: <span class=\"string\">\"business_wzfw_work-location_000001\"</span>,</div><div class=\"line\">  <span class=\"attr\">\"data\"</span>: &#123;</div><div class=\"line\">    <span class=\"attr\">\"msisdn\"</span>: <span class=\"string\">\"17321452140\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"zipCode\"</span>: <span class=\"number\">21</span>,</div><div class=\"line\">    <span class=\"attr\">\"provId\"</span>: <span class=\"number\">831</span>,</div><div class=\"line\">    <span class=\"attr\">\"cityCode\"</span>: <span class=\"number\">83101</span>,</div><div class=\"line\">    <span class=\"attr\">\"location\"</span>: <span class=\"string\">\"31.14570,121.54082\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"occurTime\"</span>: <span class=\"string\">\"2017-05-23T09:52:07.000Z\"</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>错误返回：<br><figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  <span class=\"attr\">\"code\"</span>: <span class=\"number\">404001</span>,</div><div class=\"line\">  <span class=\"attr\">\"message\"</span>: <span class=\"string\">\"该数据不存在.17322222222\"</span>,</div><div class=\"line\">  <span class=\"attr\">\"openRequestId\"</span>: <span class=\"string\">\"business_wzfw_work-location_000001\"</span>,</div><div class=\"line\">  <span class=\"attr\">\"data\"</span>: <span class=\"literal\">null</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h5 id=\"统一错误码（持续更新）\"><a href=\"#统一错误码（持续更新）\" class=\"headerlink\" title=\"统一错误码（持续更新）\"></a>统一错误码（持续更新）</h5><h4 id=\"统一日志、收集与分析\"><a href=\"#统一日志、收集与分析\" class=\"headerlink\" title=\"统一日志、收集与分析\"></a>统一日志、收集与分析</h4><p>为了对每个微服务进行日志的统一分析，我们使用了基于ELK技术栈的日志收集与分析的系统，为了更好的进行统一日志收集与分析，微服务需要遵循统一的约定来记录日志，各个微服务只需要将日志记录到本地文件中即可，我们会统一进行数据抓取，数据收集与数据分析；</p>\n<p>根据日志作用，将日志具体分成以下3类，分别放在指定目录的不同文件夹下：</p>\n<ul>\n<li>业务日志<br>  OpenApi业务调用日志，为后期日志分析，提供数据源。</li>\n<li>普通日志<br>  开发、调试相关日志，方便程序排除故障，程序调优等。</li>\n<li>组件日志<br>  系统内部相关组件日志，例如Spring日志，tomcat日志、eureka日志、ribbon日志、hystrix日志等</li>\n</ul>\n<p>以已经开发完成的位置服务为例：<br>业务日志 ./logs/business/wzfw/stat/  使用统一的格式统计web层的请求，便于数据分析与统计<br>普通日志 ./logs/business/wzfw/code/ 开发者在代码中用于debug的日志，便于排查<br>组件日志 ./logs/business/wzfw/other/  Spring等其它引用框架自带的日志，作为留存<br>其中业务日志是必须需要的；</p>\n<h3 id=\"接入规范\"><a href=\"#接入规范\" class=\"headerlink\" title=\"接入规范\"></a>接入规范</h3><p>为了更好地进行微服务的开发，在遵循上述接口规范的基础上，在技术层面上会简要列出下述接入规范，供开发者开发以及已经开发好的项目接入进行参考；</p>\n<p>注：与基于Dubbo框架的微服务架构不同，基于spring-cloud的OpenAPI微服务架构做得的不仅仅是协议的转换，还提供了服务自动注册与发现，自动负载均衡，服务自动熔断，统一日志收集与分析以及监控告警等功能，在项目庞大的时候会引入统一配置管理。因此，为了更好更简单地使用这些额外的功能，推荐使用Java技术栈的spring-boot框架进行开发，会达到事半功倍的效果。当然，OpenAPI也支持其它语言与框架的接入。</p>\n<h4 id=\"使用spring-boot接入\"><a href=\"#使用spring-boot接入\" class=\"headerlink\" title=\"使用spring-boot接入\"></a>使用spring-boot接入</h4><p>Spring Boot让我们的Spring应用变的更轻量化，有经验的Java程序员一天就可以上手，Java项目改造为spring-boot项目也很简单，spring-boot的主要优点如下：</p>\n<ul>\n<li>为所有Spring开发者更快的入门</li>\n<li>开箱即用，提供各种默认配置来简化项目配置</li>\n<li>内嵌式容器简化Web项目</li>\n<li>没有冗余代码生成和XML配置的要求</li>\n</ul>\n<h4 id=\"项目改造为spring-boot工程\"><a href=\"#项目改造为spring-boot工程\" class=\"headerlink\" title=\"项目改造为spring-boot工程\"></a>项目改造为spring-boot工程</h4><p>我们大部分的微服务都是以Java开发的，首先希望最好的接入方案是将项目改造为spring-boot工程，使用spring-boot进行开发，简单高效，只需要了解业务逻辑即可，这个是效果最好的接入方案，建议采用下列方式引入spring boot依赖<br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">parent</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.springframework.boot<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>spring-boot-starter-parent<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>1.4.5.RELEASE<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">relativePath</span> /&gt;</span> <span class=\"comment\">&lt;!-- lookup parent from repository --&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">parent</span>&gt;</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"微服务统一命名-1\"><a href=\"#微服务统一命名-1\" class=\"headerlink\" title=\"微服务统一命名\"></a>微服务统一命名</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">spring.application.name=openapi-service-business-wzfw</div><div class=\"line\">server.port=82XX</div></pre></td></tr></table></figure>\n<p>通过spring.application.name属性，我们可以指定微服务的名称后续在调用的时候只需要使用该名称就可以进行服务的访问。</p>\n<h4 id=\"接入服务发现与服务注册\"><a href=\"#接入服务发现与服务注册\" class=\"headerlink\" title=\"接入服务发现与服务注册\"></a>接入服务发现与服务注册</h4><p>对于spring-boot来说，接入服务发现与服务注册功能和简单，只需要以下两步：</p>\n<ol>\n<li><p>pom.xml中增加依赖</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">&lt;!-- 添加Eureka的依赖 --&gt;</span></div><div class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></div><div class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.springframework.cloud<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></div><div class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>spring-cloud-starter-eureka<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></div><div class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></div></pre></td></tr></table></figure>\n</li>\n<li><p>配置文件中增加一行eurake的配置</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 指定注册中心的地址</div><div class=\"line\">eureka.client.serviceUrl.defaultZone=http://discovery:8761/eureka/</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>这样，简单的两步操作，就可以使你的服务被微服务统一管理了。</p>\n<h4 id=\"使用负载均衡功能\"><a href=\"#使用负载均衡功能\" class=\"headerlink\" title=\"使用负载均衡功能\"></a>使用负载均衡功能</h4><p>有些微服务访问压力比较大，需要被多个负载均衡同时处理业务，需要负载均衡的能力，当你的服务注册到eurake中后，使用负载均衡功能非常简单，只需要换一个机器，将你的服务部署, 然后注册到OpenAPI就可以了，OpenAPI网关这里会自动通过 ribbon对你的服务进行负载均衡。</p>\n<p>负载均衡的默认策略是进行轮寻负载，如果有独特的负载均衡方式，请联系OpenAPI管理员进行其它的负载均衡方案。</p>\n<h4 id=\"使用断路器功能\"><a href=\"#使用断路器功能\" class=\"headerlink\" title=\"使用断路器功能\"></a>使用断路器功能</h4><p>当某个服务单元发生故障（类似用电器发生短路）之后，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个错误响应，而不是长时间的等待。这样就不会使得线程因调用故障服务被长时间占用不释放，避免了故障在分布式系统中的蔓延。OpenAPI基于开源的Hystrix来实现断路器的功能；</p>\n<p>使用断路器功能，微服务开发者不需要修改任何代码，只需要为OpenAPI管理员提出需求，制定熔断策略，我们会对代码进行非侵入式的熔断管理，从而防止错误在整个系统中的蔓延；</p>\n<h4 id=\"使用统一日志收集与分析服务\"><a href=\"#使用统一日志收集与分析服务\" class=\"headerlink\" title=\"使用统一日志收集与分析服务\"></a>使用统一日志收集与分析服务</h4><p>其中，如果使用Spring-boot开发的话，统一使用 slf4j + logback 的技术来进行日志记录，通过统一的logback配置文件可以极大地提高开发效率；业务日志统一使用面向切面编程技术（AOP）来记录日志。如果用python相关web框架实现微服务的话，也要做到统计日志格式的统一。</p>\n<p>具体日志格式以附件中 logback.xml 的配置文件为准。</p>\n<h3 id=\"Dubbo微服务接入\"><a href=\"#Dubbo微服务接入\" class=\"headerlink\" title=\"Dubbo微服务接入\"></a>Dubbo微服务接入</h3><p>王超</p>\n<h3 id=\"Python微服务接入\"><a href=\"#Python微服务接入\" class=\"headerlink\" title=\"Python微服务接入\"></a>Python微服务接入</h3><p>黄泽实例</p>\n<h3 id=\"其它微服务接入\"><a href=\"#其它微服务接入\" class=\"headerlink\" title=\"其它微服务接入\"></a>其它微服务接入</h3><p>将 eurake的原理与API介绍给他们</p>\n","excerpt":"","more":"<p>OpenAPI采用了微服务的架构，基于Spring Cloud为微服务系统中相互依赖的服务提供了丰富的连接选项。</p>\n<h3 id=\"接口规范\"><a href=\"#接口规范\" class=\"headerlink\" title=\"接口规范\"></a>接口规范</h3><p>OpenAPI采用了微服务的架构，每个微服务使用统一的对外接口规范，不需要考虑安全与负载均衡等其它因素，只需要关注自己的业务逻辑即可，参数与对外返回应该保持一致；</p>\n<h4 id=\"微服务统一命名\"><a href=\"#微服务统一命名\" class=\"headerlink\" title=\"微服务统一命名\"></a>微服务统一命名</h4><p>每个微服务都有一个统一的名称作为服务唯一标识符，也作为服务注册发现，负载均衡，服务熔断等功能的标识，OpenAPI将为微服务提供统一的查询界面，通过唯一的微服务名称查询服务的功能、调用方式以及参数：<br><img src=\"open-api1.png\" alt=\"image\"><br>样例：</p>\n<ul>\n<li>openapi-service-business-wzfw 位置服务API</li>\n<li>openapi-service-business-test 其它业务API</li>\n<li>openapi-service-compute-adhoc  计算API-即席分析</li>\n<li>openapi-service-storage-hbase  存储API-hbase</li>\n</ul>\n<h4 id=\"微服务端口规范\"><a href=\"#微服务端口规范\" class=\"headerlink\" title=\"微服务端口规范\"></a>微服务端口规范</h4><p>接入的微服务端口号都在8000-9000之间，但不同类型的服务处在不同的区间，其中，存储API的端口默认为 8100-8200，业务API的端口默认为 8200-8300，计算API的端口默认为 8300-8400，其它类型微服务API的端口默认为 8400-8500</p>\n<h4 id=\"接口命名\"><a href=\"#接口命名\" class=\"headerlink\" title=\"接口命名\"></a>接口命名</h4><p>微服务开发的时候，不需要在自己的URL中指定URL前缀，URL前缀由网关层统一路由，只需要指定接口即可</p>\n<p>微服务API的接口应统一以小写字符串为接口名，不要用下划线，而用“-”，<br>比如：GET /current-location</p>\n<p>注：网关层统一URL前缀路由方案：每个业务的URL前缀应该与微服务的统一命名有一致的对应关系，对应统一命名的后两段，比如：<br>位置服务： 统一命名：openapi-service-business-wzfw 那它的URL前缀就是：/business/wzfw/</p>\n<h4 id=\"参数命名\"><a href=\"#参数命名\" class=\"headerlink\" title=\"参数命名\"></a>参数命名</h4><p>除了微服务自己的请求参数，每个微服务都会接收到一个可选的 OpenAPI的请求id openRequestId，<strong>其它参数不能有相同的参数名称</strong>。openRequestId 是由Open网关层生成的字符串类型，代表用户的这一次请求，便于问题追踪与故障排查以及其它需求；<br>除此之外，没有其它限制</p>\n<h4 id=\"返回规范\"><a href=\"#返回规范\" class=\"headerlink\" title=\"返回规范\"></a>返回规范</h4><p>返回接口也均推荐采用RestFul API的风格，具体有以下几点约定：</p>\n<h5 id=\"响应码永远是200\"><a href=\"#响应码永远是200\" class=\"headerlink\" title=\"响应码永远是200\"></a>响应码永远是200</h5><p>服务端在成功接收到客户端的请求之后，在能够处理和捕获的情况下，http头的响应码永远是200，具体成功与否及进一步的信息放入返回的内容。如果客户端获取到的返回码不是200，代表链路上某一个环节出了问题。</p>\n<h5 id=\"不允许抛出异常\"><a href=\"#不允许抛出异常\" class=\"headerlink\" title=\"不允许抛出异常\"></a>不允许抛出异常</h5><p>微服务的最外层不允许抛出异常，最好捕获所有指定的异常并主动返回，如果有未知异常发生，则建议捕获后将异常作为message信息返回；</p>\n<h5 id=\"统一返回格式\"><a href=\"#统一返回格式\" class=\"headerlink\" title=\"统一返回格式\"></a>统一返回格式</h5><p>所有返回都要有 code、openRequestId、data、message这四个参数，String方式返回以下形式：<br><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    code: int , # 返回码 正常为 2开头，比如200，错误情况尽量兼容RestFul的返回码，可以自定义</div><div class=\"line\">    message: String # 如果出错的情况，则返回错误信息，如果不出错的话，则返回\"\"</div><div class=\"line\">    openRequestId: String, # 这个openRequestId是open网关层传入的全局id，代表这一次请求</div><div class=\"line\">    data: json, # 真正的数据，以json格式表示，一般会有 msisdn 字段</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>注意，这里的code返回码，应该遵循统一的格式。</p>\n<p>比如：GET /business/wzfw/current-locationd的正确返回：<br><figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  <span class=\"attr\">\"code\"</span>: <span class=\"number\">200</span>,</div><div class=\"line\">  <span class=\"attr\">\"message\"</span>: <span class=\"string\">\"\"</span>,</div><div class=\"line\">  <span class=\"attr\">\"openRequestId\"</span>: <span class=\"string\">\"business_wzfw_work-location_000001\"</span>,</div><div class=\"line\">  <span class=\"attr\">\"data\"</span>: &#123;</div><div class=\"line\">    <span class=\"attr\">\"msisdn\"</span>: <span class=\"string\">\"17321452140\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"zipCode\"</span>: <span class=\"number\">21</span>,</div><div class=\"line\">    <span class=\"attr\">\"provId\"</span>: <span class=\"number\">831</span>,</div><div class=\"line\">    <span class=\"attr\">\"cityCode\"</span>: <span class=\"number\">83101</span>,</div><div class=\"line\">    <span class=\"attr\">\"location\"</span>: <span class=\"string\">\"31.14570,121.54082\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"occurTime\"</span>: <span class=\"string\">\"2017-05-23T09:52:07.000Z\"</span></div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>错误返回：<br><figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">  <span class=\"attr\">\"code\"</span>: <span class=\"number\">404001</span>,</div><div class=\"line\">  <span class=\"attr\">\"message\"</span>: <span class=\"string\">\"该数据不存在.17322222222\"</span>,</div><div class=\"line\">  <span class=\"attr\">\"openRequestId\"</span>: <span class=\"string\">\"business_wzfw_work-location_000001\"</span>,</div><div class=\"line\">  <span class=\"attr\">\"data\"</span>: <span class=\"literal\">null</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h5 id=\"统一错误码（持续更新）\"><a href=\"#统一错误码（持续更新）\" class=\"headerlink\" title=\"统一错误码（持续更新）\"></a>统一错误码（持续更新）</h5><h4 id=\"统一日志、收集与分析\"><a href=\"#统一日志、收集与分析\" class=\"headerlink\" title=\"统一日志、收集与分析\"></a>统一日志、收集与分析</h4><p>为了对每个微服务进行日志的统一分析，我们使用了基于ELK技术栈的日志收集与分析的系统，为了更好的进行统一日志收集与分析，微服务需要遵循统一的约定来记录日志，各个微服务只需要将日志记录到本地文件中即可，我们会统一进行数据抓取，数据收集与数据分析；</p>\n<p>根据日志作用，将日志具体分成以下3类，分别放在指定目录的不同文件夹下：</p>\n<ul>\n<li>业务日志<br>  OpenApi业务调用日志，为后期日志分析，提供数据源。</li>\n<li>普通日志<br>  开发、调试相关日志，方便程序排除故障，程序调优等。</li>\n<li>组件日志<br>  系统内部相关组件日志，例如Spring日志，tomcat日志、eureka日志、ribbon日志、hystrix日志等</li>\n</ul>\n<p>以已经开发完成的位置服务为例：<br>业务日志 ./logs/business/wzfw/stat/  使用统一的格式统计web层的请求，便于数据分析与统计<br>普通日志 ./logs/business/wzfw/code/ 开发者在代码中用于debug的日志，便于排查<br>组件日志 ./logs/business/wzfw/other/  Spring等其它引用框架自带的日志，作为留存<br>其中业务日志是必须需要的；</p>\n<h3 id=\"接入规范\"><a href=\"#接入规范\" class=\"headerlink\" title=\"接入规范\"></a>接入规范</h3><p>为了更好地进行微服务的开发，在遵循上述接口规范的基础上，在技术层面上会简要列出下述接入规范，供开发者开发以及已经开发好的项目接入进行参考；</p>\n<p>注：与基于Dubbo框架的微服务架构不同，基于spring-cloud的OpenAPI微服务架构做得的不仅仅是协议的转换，还提供了服务自动注册与发现，自动负载均衡，服务自动熔断，统一日志收集与分析以及监控告警等功能，在项目庞大的时候会引入统一配置管理。因此，为了更好更简单地使用这些额外的功能，推荐使用Java技术栈的spring-boot框架进行开发，会达到事半功倍的效果。当然，OpenAPI也支持其它语言与框架的接入。</p>\n<h4 id=\"使用spring-boot接入\"><a href=\"#使用spring-boot接入\" class=\"headerlink\" title=\"使用spring-boot接入\"></a>使用spring-boot接入</h4><p>Spring Boot让我们的Spring应用变的更轻量化，有经验的Java程序员一天就可以上手，Java项目改造为spring-boot项目也很简单，spring-boot的主要优点如下：</p>\n<ul>\n<li>为所有Spring开发者更快的入门</li>\n<li>开箱即用，提供各种默认配置来简化项目配置</li>\n<li>内嵌式容器简化Web项目</li>\n<li>没有冗余代码生成和XML配置的要求</li>\n</ul>\n<h4 id=\"项目改造为spring-boot工程\"><a href=\"#项目改造为spring-boot工程\" class=\"headerlink\" title=\"项目改造为spring-boot工程\"></a>项目改造为spring-boot工程</h4><p>我们大部分的微服务都是以Java开发的，首先希望最好的接入方案是将项目改造为spring-boot工程，使用spring-boot进行开发，简单高效，只需要了解业务逻辑即可，这个是效果最好的接入方案，建议采用下列方式引入spring boot依赖<br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">parent</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.springframework.boot<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>spring-boot-starter-parent<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>1.4.5.RELEASE<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">relativePath</span> /&gt;</span> <span class=\"comment\">&lt;!-- lookup parent from repository --&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">parent</span>&gt;</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"微服务统一命名-1\"><a href=\"#微服务统一命名-1\" class=\"headerlink\" title=\"微服务统一命名\"></a>微服务统一命名</h4><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">spring.application.name=openapi-service-business-wzfw</div><div class=\"line\">server.port=82XX</div></pre></td></tr></table></figure>\n<p>通过spring.application.name属性，我们可以指定微服务的名称后续在调用的时候只需要使用该名称就可以进行服务的访问。</p>\n<h4 id=\"接入服务发现与服务注册\"><a href=\"#接入服务发现与服务注册\" class=\"headerlink\" title=\"接入服务发现与服务注册\"></a>接入服务发现与服务注册</h4><p>对于spring-boot来说，接入服务发现与服务注册功能和简单，只需要以下两步：</p>\n<ol>\n<li><p>pom.xml中增加依赖</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">&lt;!-- 添加Eureka的依赖 --&gt;</span></div><div class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></div><div class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.springframework.cloud<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></div><div class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>spring-cloud-starter-eureka<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></div><div class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></div></pre></td></tr></table></figure>\n</li>\n<li><p>配置文件中增加一行eurake的配置</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 指定注册中心的地址</div><div class=\"line\">eureka.client.serviceUrl.defaultZone=http://discovery:8761/eureka/</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>这样，简单的两步操作，就可以使你的服务被微服务统一管理了。</p>\n<h4 id=\"使用负载均衡功能\"><a href=\"#使用负载均衡功能\" class=\"headerlink\" title=\"使用负载均衡功能\"></a>使用负载均衡功能</h4><p>有些微服务访问压力比较大，需要被多个负载均衡同时处理业务，需要负载均衡的能力，当你的服务注册到eurake中后，使用负载均衡功能非常简单，只需要换一个机器，将你的服务部署, 然后注册到OpenAPI就可以了，OpenAPI网关这里会自动通过 ribbon对你的服务进行负载均衡。</p>\n<p>负载均衡的默认策略是进行轮寻负载，如果有独特的负载均衡方式，请联系OpenAPI管理员进行其它的负载均衡方案。</p>\n<h4 id=\"使用断路器功能\"><a href=\"#使用断路器功能\" class=\"headerlink\" title=\"使用断路器功能\"></a>使用断路器功能</h4><p>当某个服务单元发生故障（类似用电器发生短路）之后，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个错误响应，而不是长时间的等待。这样就不会使得线程因调用故障服务被长时间占用不释放，避免了故障在分布式系统中的蔓延。OpenAPI基于开源的Hystrix来实现断路器的功能；</p>\n<p>使用断路器功能，微服务开发者不需要修改任何代码，只需要为OpenAPI管理员提出需求，制定熔断策略，我们会对代码进行非侵入式的熔断管理，从而防止错误在整个系统中的蔓延；</p>\n<h4 id=\"使用统一日志收集与分析服务\"><a href=\"#使用统一日志收集与分析服务\" class=\"headerlink\" title=\"使用统一日志收集与分析服务\"></a>使用统一日志收集与分析服务</h4><p>其中，如果使用Spring-boot开发的话，统一使用 slf4j + logback 的技术来进行日志记录，通过统一的logback配置文件可以极大地提高开发效率；业务日志统一使用面向切面编程技术（AOP）来记录日志。如果用python相关web框架实现微服务的话，也要做到统计日志格式的统一。</p>\n<p>具体日志格式以附件中 logback.xml 的配置文件为准。</p>\n<h3 id=\"Dubbo微服务接入\"><a href=\"#Dubbo微服务接入\" class=\"headerlink\" title=\"Dubbo微服务接入\"></a>Dubbo微服务接入</h3><p>王超</p>\n<h3 id=\"Python微服务接入\"><a href=\"#Python微服务接入\" class=\"headerlink\" title=\"Python微服务接入\"></a>Python微服务接入</h3><p>黄泽实例</p>\n<h3 id=\"其它微服务接入\"><a href=\"#其它微服务接入\" class=\"headerlink\" title=\"其它微服务接入\"></a>其它微服务接入</h3><p>将 eurake的原理与API介绍给他们</p>\n"},{"title":"spark奇技淫巧总结之强大的flatMap","toc":false,"date":"2017-06-22T11:00:17.000Z","_content":"\nspark RDD与DStream API支持很多好用的算子，最常用的莫过于map和filter了，顾名思义可知：\n**map**： 返回一个新的分布式数据集，其中每个元素都是由源RDD中一个元素经func转换得到的；\n**filter**： 返回一个新的数据集，其中包含的元素来自源RDD中元素经func过滤后（func返回true时才选中）的结果；\n\n举个例子：如下RDD a 的一个partition有10个元素，那么：\n\n``` scala\n\nval a: RDD[String] = sc.parallelize(Seq(\"1\", \"2\", \"1\", \"1\", \"4\", \"3\", \"3\", \"1\", \"5\", \"6\"))\n// 后的结果肯定有 10个元素\nval b: RDD[String] = a.map(func1()) \n// 后的结果肯定 <= 10个元素，并且元素的内容不会改变\nval c: RDD[String] = a.filter(func2()) \n\n```\n通过类似如上样例代码，可知，如果你要转化内容，通过map，10个元素变化后还是10个元素，如果你想过滤内容，c 通过a过滤后，虽然一些值被过滤掉了，**但是没被过滤掉的值依然没有变化**。\n\n先看flatMap的spark官网文档的官方解释： \n**类似于map，但每个输入元素可以映射到0到n个输出元素（所以要求func必须返回一个Seq而不是单个元素）。**\nRDD API的解释：\n**Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.**\n\n无论网上的一些例子还是解释，都给人一种flatMap只能把数据“压扁”的感觉。其实flatMap不仅仅能把数据压扁，还能把数据“拔高”，还能把数据“过滤”。flatMap不像map，只能1对1，也不像filter，只能1对1或0(还不能改变数据)，它是1对n(自然数)的。\n\n### 解决边处理边过滤的需求\n\n我在开发中遇到过很多次这种需求：**既要改变内容，同时不符合要求的数据需要过滤**，这种情况下该怎么办呢？\n\n#### 简单方案\n先filter，然后map，或者先map，然后filter，这样都可以完成这种需求，但是它有以下问题：\n1. 重复计算了，map和filter中很多类似的逻辑都要多算一遍，这在大量数据集下是不可容忍的；\n2. 对于有些判断，只可能判断一次，第二次计算结果会不一样，比如在transform中需要与外界redis等交互的判断，这种情况下，结果都是错误的；\n\n这种情况下，就可以用flatMap来解决，嘿嘿~\n\n直接说解决方案吧：对数据进行flatMap进行转换，如果不符合要求要过滤，则直接返回 None即可，样例代码如下：\n\n``` scala\n@RunWith(classOf[JUnitRunner])\nclass FlatMapTest extends FunSuite with Matchers {\n  /**\n    * 测试 rdd FlatMapTest 的过滤用法是否可行\n    */\n  test(\"FlatMapTest should work\") {\n    println(\"FlatMapTest  started\")\n    val conf = new SparkConf().setAppName(\"Simple Application\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val a: RDD[String] = sc.parallelize(Seq(\"1\", \"2\", \"1\", \"1\", \"4\", \"3\", \"3\", \"1\", \"5\", \"6\"))\n    val b: RDD[(String, String)] = a.map(f => (f, f + \"asd\"))\n    val c = b.mapPartitions {\n      eachPar => {\n        eachPar.flatMap(f =>\n          if (f._2.startsWith(\"3\")) {\n            Some(f._1, f._2 + \"--b\")\n          } else {\n            None\n          }\n        )\n      }\n    }\n    c.cache()\n    c.foreach(println(_))\n    println(c.count())\n    c.unpersist()\n    println(\"FlatMapTest ended\")\n  }\n}\n\n```\n上述的代码就可以做到处理的过程中进行过滤了~\n\n### 解决一条数据对应多条数据的需求\n\n我在开发中也遇到过这种需求：**RDD中一个元素处理后可能会变成多个元素**，比如一个用户可能会同时在多个景区存在，为了便于统计和输出，需要同时输出多个，这种情况下可以用flatMap来解决：\n\n首先看看RDD flatMap的定义：\n\n``` scala\n  /**\n   *  Return a new RDD by first applying a function to all elements of this\n   *  RDD, and then flattening the results.\n   */\n  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope {\n    val cleanF = sc.clean(f)\n    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.flatMap(cleanF))\n  }\n\n```\n它底层调用的是 Iterator 的flatMap\n``` scala\n  /** Creates a new iterator by applying a function to all values produced by this iterator\n   *  and concatenating the results.\n   *\n   *  @param f the function to apply on each element.\n   *  @return  the iterator resulting from applying the given iterator-valued function\n   *           `f` to each value produced by this iterator and concatenating the results.\n   *  @note    Reuse: $consumesAndProducesIterator\n   */\n  def flatMap[B](f: A => GenTraversableOnce[B]): Iterator[B] = new AbstractIterator[B] {\n    private var cur: Iterator[B] = empty\n    def hasNext: Boolean =\n      cur.hasNext || self.hasNext && { cur = f(self.next).toIterator; hasNext }\n    def next(): B = (if (hasNext) cur else empty).next()\n  }\n```\n\n可以看到，flatMap的返回也是 Iterator[B]，所以，只要我们以 Iterator 的方式返回，就可以返回多条数据了，在Scala中，只要你返回的格式是某种可以Iterator的，就满足要求了，在Scala中，所有的集合Iterable都是 trait Iterator的一种扩展，无论是 seq set 还是map，这就很方便了，样例代码如下：\n\n``` scala\n  test(\"FlatMapTest should work\") {\n    println(\"FlatMapTest  started\")\n    val conf = new SparkConf().setAppName(\"Simple Application\").setMaster(\"local[3]\")\n    val sc = new SparkContext(conf)\n    val a: RDD[String] = sc.parallelize(Seq(\"1\", \"2\", \"1\", \"1\", \"4\", \"3\", \"3\", \"1\", \"5\", \"6\", \"5\"))\n    val b: RDD[(String, String)] = a.map(f => (f, f + \"asd\"))\n    val c = b.mapPartitions {\n      eachPar => {\n        eachPar.flatMap(f => {\n          val returnSeq = ArrayBuffer.empty[(String, String)]\n          if (f._2.startsWith(\"3\")) {\n            returnSeq += ((f._1, f._2 + \"--b\"))\n            returnSeq += ((f._1, f._2 + \"-4c\"))\n            returnSeq += ((f._2, f._1 + \"-6d\"))\n//            Seq(Some(f._1, f._2 + \"--b\"), Some(f._1, f._2 + \"--c\"), Some(f._2, f._1 + \"--d\"))\n          } else {\n          }\n          returnSeq\n        }\n        )\n      }\n    }\n    c.cache()\n    c.foreach(println(_))\n    println(c.count())\n    c.unpersist()\n    println(\"FlatMapTest ended\")\n  }\n```\n\n当然，实际处理比这复杂多了，你可以在flatMap中随意发挥，进行各种对外的连接查询操作。","source":"_posts/2017-06-22-spark奇技淫巧总结之flatMap.md","raw":"---\ntitle: spark奇技淫巧总结之强大的flatMap\ntoc: false\ndate: 2017-06-22 19:00:17\ntags: spark开发\ncategories: spark开发\n---\n\nspark RDD与DStream API支持很多好用的算子，最常用的莫过于map和filter了，顾名思义可知：\n**map**： 返回一个新的分布式数据集，其中每个元素都是由源RDD中一个元素经func转换得到的；\n**filter**： 返回一个新的数据集，其中包含的元素来自源RDD中元素经func过滤后（func返回true时才选中）的结果；\n\n举个例子：如下RDD a 的一个partition有10个元素，那么：\n\n``` scala\n\nval a: RDD[String] = sc.parallelize(Seq(\"1\", \"2\", \"1\", \"1\", \"4\", \"3\", \"3\", \"1\", \"5\", \"6\"))\n// 后的结果肯定有 10个元素\nval b: RDD[String] = a.map(func1()) \n// 后的结果肯定 <= 10个元素，并且元素的内容不会改变\nval c: RDD[String] = a.filter(func2()) \n\n```\n通过类似如上样例代码，可知，如果你要转化内容，通过map，10个元素变化后还是10个元素，如果你想过滤内容，c 通过a过滤后，虽然一些值被过滤掉了，**但是没被过滤掉的值依然没有变化**。\n\n先看flatMap的spark官网文档的官方解释： \n**类似于map，但每个输入元素可以映射到0到n个输出元素（所以要求func必须返回一个Seq而不是单个元素）。**\nRDD API的解释：\n**Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.**\n\n无论网上的一些例子还是解释，都给人一种flatMap只能把数据“压扁”的感觉。其实flatMap不仅仅能把数据压扁，还能把数据“拔高”，还能把数据“过滤”。flatMap不像map，只能1对1，也不像filter，只能1对1或0(还不能改变数据)，它是1对n(自然数)的。\n\n### 解决边处理边过滤的需求\n\n我在开发中遇到过很多次这种需求：**既要改变内容，同时不符合要求的数据需要过滤**，这种情况下该怎么办呢？\n\n#### 简单方案\n先filter，然后map，或者先map，然后filter，这样都可以完成这种需求，但是它有以下问题：\n1. 重复计算了，map和filter中很多类似的逻辑都要多算一遍，这在大量数据集下是不可容忍的；\n2. 对于有些判断，只可能判断一次，第二次计算结果会不一样，比如在transform中需要与外界redis等交互的判断，这种情况下，结果都是错误的；\n\n这种情况下，就可以用flatMap来解决，嘿嘿~\n\n直接说解决方案吧：对数据进行flatMap进行转换，如果不符合要求要过滤，则直接返回 None即可，样例代码如下：\n\n``` scala\n@RunWith(classOf[JUnitRunner])\nclass FlatMapTest extends FunSuite with Matchers {\n  /**\n    * 测试 rdd FlatMapTest 的过滤用法是否可行\n    */\n  test(\"FlatMapTest should work\") {\n    println(\"FlatMapTest  started\")\n    val conf = new SparkConf().setAppName(\"Simple Application\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val a: RDD[String] = sc.parallelize(Seq(\"1\", \"2\", \"1\", \"1\", \"4\", \"3\", \"3\", \"1\", \"5\", \"6\"))\n    val b: RDD[(String, String)] = a.map(f => (f, f + \"asd\"))\n    val c = b.mapPartitions {\n      eachPar => {\n        eachPar.flatMap(f =>\n          if (f._2.startsWith(\"3\")) {\n            Some(f._1, f._2 + \"--b\")\n          } else {\n            None\n          }\n        )\n      }\n    }\n    c.cache()\n    c.foreach(println(_))\n    println(c.count())\n    c.unpersist()\n    println(\"FlatMapTest ended\")\n  }\n}\n\n```\n上述的代码就可以做到处理的过程中进行过滤了~\n\n### 解决一条数据对应多条数据的需求\n\n我在开发中也遇到过这种需求：**RDD中一个元素处理后可能会变成多个元素**，比如一个用户可能会同时在多个景区存在，为了便于统计和输出，需要同时输出多个，这种情况下可以用flatMap来解决：\n\n首先看看RDD flatMap的定义：\n\n``` scala\n  /**\n   *  Return a new RDD by first applying a function to all elements of this\n   *  RDD, and then flattening the results.\n   */\n  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope {\n    val cleanF = sc.clean(f)\n    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.flatMap(cleanF))\n  }\n\n```\n它底层调用的是 Iterator 的flatMap\n``` scala\n  /** Creates a new iterator by applying a function to all values produced by this iterator\n   *  and concatenating the results.\n   *\n   *  @param f the function to apply on each element.\n   *  @return  the iterator resulting from applying the given iterator-valued function\n   *           `f` to each value produced by this iterator and concatenating the results.\n   *  @note    Reuse: $consumesAndProducesIterator\n   */\n  def flatMap[B](f: A => GenTraversableOnce[B]): Iterator[B] = new AbstractIterator[B] {\n    private var cur: Iterator[B] = empty\n    def hasNext: Boolean =\n      cur.hasNext || self.hasNext && { cur = f(self.next).toIterator; hasNext }\n    def next(): B = (if (hasNext) cur else empty).next()\n  }\n```\n\n可以看到，flatMap的返回也是 Iterator[B]，所以，只要我们以 Iterator 的方式返回，就可以返回多条数据了，在Scala中，只要你返回的格式是某种可以Iterator的，就满足要求了，在Scala中，所有的集合Iterable都是 trait Iterator的一种扩展，无论是 seq set 还是map，这就很方便了，样例代码如下：\n\n``` scala\n  test(\"FlatMapTest should work\") {\n    println(\"FlatMapTest  started\")\n    val conf = new SparkConf().setAppName(\"Simple Application\").setMaster(\"local[3]\")\n    val sc = new SparkContext(conf)\n    val a: RDD[String] = sc.parallelize(Seq(\"1\", \"2\", \"1\", \"1\", \"4\", \"3\", \"3\", \"1\", \"5\", \"6\", \"5\"))\n    val b: RDD[(String, String)] = a.map(f => (f, f + \"asd\"))\n    val c = b.mapPartitions {\n      eachPar => {\n        eachPar.flatMap(f => {\n          val returnSeq = ArrayBuffer.empty[(String, String)]\n          if (f._2.startsWith(\"3\")) {\n            returnSeq += ((f._1, f._2 + \"--b\"))\n            returnSeq += ((f._1, f._2 + \"-4c\"))\n            returnSeq += ((f._2, f._1 + \"-6d\"))\n//            Seq(Some(f._1, f._2 + \"--b\"), Some(f._1, f._2 + \"--c\"), Some(f._2, f._1 + \"--d\"))\n          } else {\n          }\n          returnSeq\n        }\n        )\n      }\n    }\n    c.cache()\n    c.foreach(println(_))\n    println(c.count())\n    c.unpersist()\n    println(\"FlatMapTest ended\")\n  }\n```\n\n当然，实际处理比这复杂多了，你可以在flatMap中随意发挥，进行各种对外的连接查询操作。","slug":"spark奇技淫巧总结之flatMap","published":1,"updated":"2017-06-28T07:50:07.299Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfxi003mpsgucthh1dc2","content":"<p>spark RDD与DStream API支持很多好用的算子，最常用的莫过于map和filter了，顾名思义可知：<br><strong>map</strong>： 返回一个新的分布式数据集，其中每个元素都是由源RDD中一个元素经func转换得到的；<br><strong>filter</strong>： 返回一个新的数据集，其中包含的元素来自源RDD中元素经func过滤后（func返回true时才选中）的结果；</p>\n<p>举个例子：如下RDD a 的一个partition有10个元素，那么：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> a: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.parallelize(<span class=\"type\">Seq</span>(<span class=\"string\">\"1\"</span>, <span class=\"string\">\"2\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"4\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"5\"</span>, <span class=\"string\">\"6\"</span>))</div><div class=\"line\"><span class=\"comment\">// 后的结果肯定有 10个元素</span></div><div class=\"line\"><span class=\"keyword\">val</span> b: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = a.map(func1()) </div><div class=\"line\"><span class=\"comment\">// 后的结果肯定 &lt;= 10个元素，并且元素的内容不会改变</span></div><div class=\"line\"><span class=\"keyword\">val</span> c: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = a.filter(func2())</div></pre></td></tr></table></figure>\n<p>通过类似如上样例代码，可知，如果你要转化内容，通过map，10个元素变化后还是10个元素，如果你想过滤内容，c 通过a过滤后，虽然一些值被过滤掉了，<strong>但是没被过滤掉的值依然没有变化</strong>。</p>\n<p>先看flatMap的spark官网文档的官方解释：<br><strong>类似于map，但每个输入元素可以映射到0到n个输出元素（所以要求func必须返回一个Seq而不是单个元素）。</strong><br>RDD API的解释：<br><strong>Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.</strong></p>\n<p>无论网上的一些例子还是解释，都给人一种flatMap只能把数据“压扁”的感觉。其实flatMap不仅仅能把数据压扁，还能把数据“拔高”，还能把数据“过滤”。flatMap不像map，只能1对1，也不像filter，只能1对1或0(还不能改变数据)，它是1对n(自然数)的。</p>\n<h3 id=\"解决边处理边过滤的需求\"><a href=\"#解决边处理边过滤的需求\" class=\"headerlink\" title=\"解决边处理边过滤的需求\"></a>解决边处理边过滤的需求</h3><p>我在开发中遇到过很多次这种需求：<strong>既要改变内容，同时不符合要求的数据需要过滤</strong>，这种情况下该怎么办呢？</p>\n<h4 id=\"简单方案\"><a href=\"#简单方案\" class=\"headerlink\" title=\"简单方案\"></a>简单方案</h4><p>先filter，然后map，或者先map，然后filter，这样都可以完成这种需求，但是它有以下问题：</p>\n<ol>\n<li>重复计算了，map和filter中很多类似的逻辑都要多算一遍，这在大量数据集下是不可容忍的；</li>\n<li>对于有些判断，只可能判断一次，第二次计算结果会不一样，比如在transform中需要与外界redis等交互的判断，这种情况下，结果都是错误的；</li>\n</ol>\n<p>这种情况下，就可以用flatMap来解决，嘿嘿~</p>\n<p>直接说解决方案吧：对数据进行flatMap进行转换，如果不符合要求要过滤，则直接返回 None即可，样例代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@RunWith</span>(classOf[<span class=\"type\">JUnitRunner</span>])</div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FlatMapTest</span> <span class=\"keyword\">extends</span> <span class=\"title\">FunSuite</span> <span class=\"keyword\">with</span> <span class=\"title\">Matchers</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">/**</span></div><div class=\"line\">    * 测试 rdd FlatMapTest 的过滤用法是否可行</div><div class=\"line\">    */</div><div class=\"line\">  test(<span class=\"string\">\"FlatMapTest should work\"</span>) &#123;</div><div class=\"line\">    println(<span class=\"string\">\"FlatMapTest  started\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">\"Simple Application\"</span>).setMaster(<span class=\"string\">\"local\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(conf)</div><div class=\"line\">    <span class=\"keyword\">val</span> a: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.parallelize(<span class=\"type\">Seq</span>(<span class=\"string\">\"1\"</span>, <span class=\"string\">\"2\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"4\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"5\"</span>, <span class=\"string\">\"6\"</span>))</div><div class=\"line\">    <span class=\"keyword\">val</span> b: <span class=\"type\">RDD</span>[(<span class=\"type\">String</span>, <span class=\"type\">String</span>)] = a.map(f =&gt; (f, f + <span class=\"string\">\"asd\"</span>))</div><div class=\"line\">    <span class=\"keyword\">val</span> c = b.mapPartitions &#123;</div><div class=\"line\">      eachPar =&gt; &#123;</div><div class=\"line\">        eachPar.flatMap(f =&gt;</div><div class=\"line\">          <span class=\"keyword\">if</span> (f._2.startsWith(<span class=\"string\">\"3\"</span>)) &#123;</div><div class=\"line\">            <span class=\"type\">Some</span>(f._1, f._2 + <span class=\"string\">\"--b\"</span>)</div><div class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">            <span class=\"type\">None</span></div><div class=\"line\">          &#125;</div><div class=\"line\">        )</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    c.cache()</div><div class=\"line\">    c.foreach(println(_))</div><div class=\"line\">    println(c.count())</div><div class=\"line\">    c.unpersist()</div><div class=\"line\">    println(<span class=\"string\">\"FlatMapTest ended\"</span>)</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>上述的代码就可以做到处理的过程中进行过滤了~</p>\n<h3 id=\"解决一条数据对应多条数据的需求\"><a href=\"#解决一条数据对应多条数据的需求\" class=\"headerlink\" title=\"解决一条数据对应多条数据的需求\"></a>解决一条数据对应多条数据的需求</h3><p>我在开发中也遇到过这种需求：<strong>RDD中一个元素处理后可能会变成多个元素</strong>，比如一个用户可能会同时在多个景区存在，为了便于统计和输出，需要同时输出多个，这种情况下可以用flatMap来解决：</p>\n<p>首先看看RDD flatMap的定义：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"> *  Return a new RDD by first applying a function to all elements of this</div><div class=\"line\"> *  RDD, and then flattening the results.</div><div class=\"line\"> */</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flatMap</span></span>[<span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>](f: <span class=\"type\">T</span> =&gt; <span class=\"type\">TraversableOnce</span>[<span class=\"type\">U</span>]): <span class=\"type\">RDD</span>[<span class=\"type\">U</span>] = withScope &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> cleanF = sc.clean(f)</div><div class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">MapPartitionsRDD</span>[<span class=\"type\">U</span>, <span class=\"type\">T</span>](<span class=\"keyword\">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>它底层调用的是 Iterator 的flatMap<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/** Creates a new iterator by applying a function to all values produced by this iterator</span></div><div class=\"line\"> *  and concatenating the results.</div><div class=\"line\"> *</div><div class=\"line\"> *  @param f the function to apply on each element.</div><div class=\"line\"> *  @return  the iterator resulting from applying the given iterator-valued function</div><div class=\"line\"> *           `f` to each value produced by this iterator and concatenating the results.</div><div class=\"line\"> *  @note    Reuse: $consumesAndProducesIterator</div><div class=\"line\"> */</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flatMap</span></span>[<span class=\"type\">B</span>](f: <span class=\"type\">A</span> =&gt; <span class=\"type\">GenTraversableOnce</span>[<span class=\"type\">B</span>]): <span class=\"type\">Iterator</span>[<span class=\"type\">B</span>] = <span class=\"keyword\">new</span> <span class=\"type\">AbstractIterator</span>[<span class=\"type\">B</span>] &#123;</div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> cur: <span class=\"type\">Iterator</span>[<span class=\"type\">B</span>] = empty</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> =</div><div class=\"line\">    cur.hasNext || self.hasNext &amp;&amp; &#123; cur = f(self.next).toIterator; hasNext &#125;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">B</span> = (<span class=\"keyword\">if</span> (hasNext) cur <span class=\"keyword\">else</span> empty).next()</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>可以看到，flatMap的返回也是 Iterator[B]，所以，只要我们以 Iterator 的方式返回，就可以返回多条数据了，在Scala中，只要你返回的格式是某种可以Iterator的，就满足要求了，在Scala中，所有的集合Iterable都是 trait Iterator的一种扩展，无论是 seq set 还是map，这就很方便了，样例代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">  test(<span class=\"string\">\"FlatMapTest should work\"</span>) &#123;</div><div class=\"line\">    println(<span class=\"string\">\"FlatMapTest  started\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">\"Simple Application\"</span>).setMaster(<span class=\"string\">\"local[3]\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(conf)</div><div class=\"line\">    <span class=\"keyword\">val</span> a: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.parallelize(<span class=\"type\">Seq</span>(<span class=\"string\">\"1\"</span>, <span class=\"string\">\"2\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"4\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"5\"</span>, <span class=\"string\">\"6\"</span>, <span class=\"string\">\"5\"</span>))</div><div class=\"line\">    <span class=\"keyword\">val</span> b: <span class=\"type\">RDD</span>[(<span class=\"type\">String</span>, <span class=\"type\">String</span>)] = a.map(f =&gt; (f, f + <span class=\"string\">\"asd\"</span>))</div><div class=\"line\">    <span class=\"keyword\">val</span> c = b.mapPartitions &#123;</div><div class=\"line\">      eachPar =&gt; &#123;</div><div class=\"line\">        eachPar.flatMap(f =&gt; &#123;</div><div class=\"line\">          <span class=\"keyword\">val</span> returnSeq = <span class=\"type\">ArrayBuffer</span>.empty[(<span class=\"type\">String</span>, <span class=\"type\">String</span>)]</div><div class=\"line\">          <span class=\"keyword\">if</span> (f._2.startsWith(<span class=\"string\">\"3\"</span>)) &#123;</div><div class=\"line\">            returnSeq += ((f._1, f._2 + <span class=\"string\">\"--b\"</span>))</div><div class=\"line\">            returnSeq += ((f._1, f._2 + <span class=\"string\">\"-4c\"</span>))</div><div class=\"line\">            returnSeq += ((f._2, f._1 + <span class=\"string\">\"-6d\"</span>))</div><div class=\"line\"><span class=\"comment\">//            Seq(Some(f._1, f._2 + \"--b\"), Some(f._1, f._2 + \"--c\"), Some(f._2, f._1 + \"--d\"))</span></div><div class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">          &#125;</div><div class=\"line\">          returnSeq</div><div class=\"line\">        &#125;</div><div class=\"line\">        )</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    c.cache()</div><div class=\"line\">    c.foreach(println(_))</div><div class=\"line\">    println(c.count())</div><div class=\"line\">    c.unpersist()</div><div class=\"line\">    println(<span class=\"string\">\"FlatMapTest ended\"</span>)</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<p>当然，实际处理比这复杂多了，你可以在flatMap中随意发挥，进行各种对外的连接查询操作。</p>\n","excerpt":"","more":"<p>spark RDD与DStream API支持很多好用的算子，最常用的莫过于map和filter了，顾名思义可知：<br><strong>map</strong>： 返回一个新的分布式数据集，其中每个元素都是由源RDD中一个元素经func转换得到的；<br><strong>filter</strong>： 返回一个新的数据集，其中包含的元素来自源RDD中元素经func过滤后（func返回true时才选中）的结果；</p>\n<p>举个例子：如下RDD a 的一个partition有10个元素，那么：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> a: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.parallelize(<span class=\"type\">Seq</span>(<span class=\"string\">\"1\"</span>, <span class=\"string\">\"2\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"4\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"5\"</span>, <span class=\"string\">\"6\"</span>))</div><div class=\"line\"><span class=\"comment\">// 后的结果肯定有 10个元素</span></div><div class=\"line\"><span class=\"keyword\">val</span> b: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = a.map(func1()) </div><div class=\"line\"><span class=\"comment\">// 后的结果肯定 &lt;= 10个元素，并且元素的内容不会改变</span></div><div class=\"line\"><span class=\"keyword\">val</span> c: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = a.filter(func2())</div></pre></td></tr></table></figure>\n<p>通过类似如上样例代码，可知，如果你要转化内容，通过map，10个元素变化后还是10个元素，如果你想过滤内容，c 通过a过滤后，虽然一些值被过滤掉了，<strong>但是没被过滤掉的值依然没有变化</strong>。</p>\n<p>先看flatMap的spark官网文档的官方解释：<br><strong>类似于map，但每个输入元素可以映射到0到n个输出元素（所以要求func必须返回一个Seq而不是单个元素）。</strong><br>RDD API的解释：<br><strong>Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.</strong></p>\n<p>无论网上的一些例子还是解释，都给人一种flatMap只能把数据“压扁”的感觉。其实flatMap不仅仅能把数据压扁，还能把数据“拔高”，还能把数据“过滤”。flatMap不像map，只能1对1，也不像filter，只能1对1或0(还不能改变数据)，它是1对n(自然数)的。</p>\n<h3 id=\"解决边处理边过滤的需求\"><a href=\"#解决边处理边过滤的需求\" class=\"headerlink\" title=\"解决边处理边过滤的需求\"></a>解决边处理边过滤的需求</h3><p>我在开发中遇到过很多次这种需求：<strong>既要改变内容，同时不符合要求的数据需要过滤</strong>，这种情况下该怎么办呢？</p>\n<h4 id=\"简单方案\"><a href=\"#简单方案\" class=\"headerlink\" title=\"简单方案\"></a>简单方案</h4><p>先filter，然后map，或者先map，然后filter，这样都可以完成这种需求，但是它有以下问题：</p>\n<ol>\n<li>重复计算了，map和filter中很多类似的逻辑都要多算一遍，这在大量数据集下是不可容忍的；</li>\n<li>对于有些判断，只可能判断一次，第二次计算结果会不一样，比如在transform中需要与外界redis等交互的判断，这种情况下，结果都是错误的；</li>\n</ol>\n<p>这种情况下，就可以用flatMap来解决，嘿嘿~</p>\n<p>直接说解决方案吧：对数据进行flatMap进行转换，如果不符合要求要过滤，则直接返回 None即可，样例代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">@RunWith</span>(classOf[<span class=\"type\">JUnitRunner</span>])</div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FlatMapTest</span> <span class=\"keyword\">extends</span> <span class=\"title\">FunSuite</span> <span class=\"keyword\">with</span> <span class=\"title\">Matchers</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">/**</div><div class=\"line\">    * 测试 rdd FlatMapTest 的过滤用法是否可行</div><div class=\"line\">    */</span></div><div class=\"line\">  test(<span class=\"string\">\"FlatMapTest should work\"</span>) &#123;</div><div class=\"line\">    println(<span class=\"string\">\"FlatMapTest  started\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">\"Simple Application\"</span>).setMaster(<span class=\"string\">\"local\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(conf)</div><div class=\"line\">    <span class=\"keyword\">val</span> a: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.parallelize(<span class=\"type\">Seq</span>(<span class=\"string\">\"1\"</span>, <span class=\"string\">\"2\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"4\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"5\"</span>, <span class=\"string\">\"6\"</span>))</div><div class=\"line\">    <span class=\"keyword\">val</span> b: <span class=\"type\">RDD</span>[(<span class=\"type\">String</span>, <span class=\"type\">String</span>)] = a.map(f =&gt; (f, f + <span class=\"string\">\"asd\"</span>))</div><div class=\"line\">    <span class=\"keyword\">val</span> c = b.mapPartitions &#123;</div><div class=\"line\">      eachPar =&gt; &#123;</div><div class=\"line\">        eachPar.flatMap(f =&gt;</div><div class=\"line\">          <span class=\"keyword\">if</span> (f._2.startsWith(<span class=\"string\">\"3\"</span>)) &#123;</div><div class=\"line\">            <span class=\"type\">Some</span>(f._1, f._2 + <span class=\"string\">\"--b\"</span>)</div><div class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">            <span class=\"type\">None</span></div><div class=\"line\">          &#125;</div><div class=\"line\">        )</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    c.cache()</div><div class=\"line\">    c.foreach(println(_))</div><div class=\"line\">    println(c.count())</div><div class=\"line\">    c.unpersist()</div><div class=\"line\">    println(<span class=\"string\">\"FlatMapTest ended\"</span>)</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>上述的代码就可以做到处理的过程中进行过滤了~</p>\n<h3 id=\"解决一条数据对应多条数据的需求\"><a href=\"#解决一条数据对应多条数据的需求\" class=\"headerlink\" title=\"解决一条数据对应多条数据的需求\"></a>解决一条数据对应多条数据的需求</h3><p>我在开发中也遇到过这种需求：<strong>RDD中一个元素处理后可能会变成多个元素</strong>，比如一个用户可能会同时在多个景区存在，为了便于统计和输出，需要同时输出多个，这种情况下可以用flatMap来解决：</p>\n<p>首先看看RDD flatMap的定义：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\"> *  Return a new RDD by first applying a function to all elements of this</div><div class=\"line\"> *  RDD, and then flattening the results.</div><div class=\"line\"> */</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flatMap</span></span>[<span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>](f: <span class=\"type\">T</span> =&gt; <span class=\"type\">TraversableOnce</span>[<span class=\"type\">U</span>]): <span class=\"type\">RDD</span>[<span class=\"type\">U</span>] = withScope &#123;</div><div class=\"line\">  <span class=\"keyword\">val</span> cleanF = sc.clean(f)</div><div class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">MapPartitionsRDD</span>[<span class=\"type\">U</span>, <span class=\"type\">T</span>](<span class=\"keyword\">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>它底层调用的是 Iterator 的flatMap<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">/** Creates a new iterator by applying a function to all values produced by this iterator</div><div class=\"line\"> *  and concatenating the results.</div><div class=\"line\"> *</div><div class=\"line\"> *  @param f the function to apply on each element.</div><div class=\"line\"> *  @return  the iterator resulting from applying the given iterator-valued function</div><div class=\"line\"> *           `f` to each value produced by this iterator and concatenating the results.</div><div class=\"line\"> *  @note    Reuse: $consumesAndProducesIterator</div><div class=\"line\"> */</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flatMap</span></span>[<span class=\"type\">B</span>](f: <span class=\"type\">A</span> =&gt; <span class=\"type\">GenTraversableOnce</span>[<span class=\"type\">B</span>]): <span class=\"type\">Iterator</span>[<span class=\"type\">B</span>] = <span class=\"keyword\">new</span> <span class=\"type\">AbstractIterator</span>[<span class=\"type\">B</span>] &#123;</div><div class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">var</span> cur: <span class=\"type\">Iterator</span>[<span class=\"type\">B</span>] = empty</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> =</div><div class=\"line\">    cur.hasNext || self.hasNext &amp;&amp; &#123; cur = f(self.next).toIterator; hasNext &#125;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">B</span> = (<span class=\"keyword\">if</span> (hasNext) cur <span class=\"keyword\">else</span> empty).next()</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>可以看到，flatMap的返回也是 Iterator[B]，所以，只要我们以 Iterator 的方式返回，就可以返回多条数据了，在Scala中，只要你返回的格式是某种可以Iterator的，就满足要求了，在Scala中，所有的集合Iterable都是 trait Iterator的一种扩展，无论是 seq set 还是map，这就很方便了，样例代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">  test(<span class=\"string\">\"FlatMapTest should work\"</span>) &#123;</div><div class=\"line\">    println(<span class=\"string\">\"FlatMapTest  started\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">\"Simple Application\"</span>).setMaster(<span class=\"string\">\"local[3]\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(conf)</div><div class=\"line\">    <span class=\"keyword\">val</span> a: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.parallelize(<span class=\"type\">Seq</span>(<span class=\"string\">\"1\"</span>, <span class=\"string\">\"2\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"4\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"3\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"5\"</span>, <span class=\"string\">\"6\"</span>, <span class=\"string\">\"5\"</span>))</div><div class=\"line\">    <span class=\"keyword\">val</span> b: <span class=\"type\">RDD</span>[(<span class=\"type\">String</span>, <span class=\"type\">String</span>)] = a.map(f =&gt; (f, f + <span class=\"string\">\"asd\"</span>))</div><div class=\"line\">    <span class=\"keyword\">val</span> c = b.mapPartitions &#123;</div><div class=\"line\">      eachPar =&gt; &#123;</div><div class=\"line\">        eachPar.flatMap(f =&gt; &#123;</div><div class=\"line\">          <span class=\"keyword\">val</span> returnSeq = <span class=\"type\">ArrayBuffer</span>.empty[(<span class=\"type\">String</span>, <span class=\"type\">String</span>)]</div><div class=\"line\">          <span class=\"keyword\">if</span> (f._2.startsWith(<span class=\"string\">\"3\"</span>)) &#123;</div><div class=\"line\">            returnSeq += ((f._1, f._2 + <span class=\"string\">\"--b\"</span>))</div><div class=\"line\">            returnSeq += ((f._1, f._2 + <span class=\"string\">\"-4c\"</span>))</div><div class=\"line\">            returnSeq += ((f._2, f._1 + <span class=\"string\">\"-6d\"</span>))</div><div class=\"line\"><span class=\"comment\">//            Seq(Some(f._1, f._2 + \"--b\"), Some(f._1, f._2 + \"--c\"), Some(f._2, f._1 + \"--d\"))</span></div><div class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">          &#125;</div><div class=\"line\">          returnSeq</div><div class=\"line\">        &#125;</div><div class=\"line\">        )</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    c.cache()</div><div class=\"line\">    c.foreach(println(_))</div><div class=\"line\">    println(c.count())</div><div class=\"line\">    c.unpersist()</div><div class=\"line\">    println(<span class=\"string\">\"FlatMapTest ended\"</span>)</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<p>当然，实际处理比这复杂多了，你可以在flatMap中随意发挥，进行各种对外的连接查询操作。</p>\n"},{"title":"位置服务开发上线总结————实时数据推送","toc":true,"date":"2017-06-28T07:52:05.000Z","_content":"\n#### 天作棋盘星作子，谁人能下？ 地为琵琶路为弦，哪个敢弹！\n\n位置服务项目不止一期，也断断续续写了一年，不知从哪里谈起，在此着重记录一些问题以及一些解决方案，重要的是在解决问题过程中的思考。\n\n# 背景\n## 需求\n需求比较复杂，细节比较多，在这里简明扼要说一下：\n1. 用户A(spId)在中国地图上画了100个圆圈(locationId)，想让我们实时告诉他这些圈内新来的人和他的位置信息，哪些是游客哪些是土著；\n2. A的100个圆圈会随时修改，增大，变小，或者再多加100个圈圈，我们要不停程序，秒级给他这些数据；\n3. A觉得这样推送的数据量太大，因为有的人本来就住这个圈里，所以对每个不同的圈，里面的人在 7天（可调）之内不重复；\n4. 用户B也想使用这个服务，在中国地图上画了100个圈，B的圈和A的圈可能重复可能不重复，需求是一样的；\n5. 用户C也想使用这个服务，不过他觉得画个圈不符合他的要求，他要在地图上随便画100个什么图形，然后给他他想要的数据；\n6. 用户D也想使用这个服务，不过他不想要知道这个图形里的人的细节，而是只想实时知道这个图形当下有多少人。\n\n上面是位置服务二期的需求的简单总结，一期太简单不值一提，三期主要是数据的存储，以后再谈，这里只提二期。\n\n\n最终，我们采用spark streaming来实现了上述需求，使用一个Spark Streaming程序支持多用户随时订阅随时修改，在yarn持续稳定运行超过一千个小时。\n\n## 数据输入\n用户的位置数据是实时得通过kafka传过来的，数据量大概为几十万条每秒，以一个电话号码msisdn表示一个用户，每条数据都代表一个用户当前所在的经纬度位置，以及一些省市相关的信息；\n每个用户的数据，平均每隔半个多小时会上传一条，当然实际情况不是这样简单；\n\n# 实现思路\n\n整体架构如下图，我们借助kafka作为数据源的数据中转，redis作为数据缓存，mysql来实现实时订阅，flume来作为数据管道，Spring写的订阅服务以及基于SpringCloud的微服务来实现一些查询功能和监控功能，zabbix主要用户监控和报警，后期引入ES实现了数据多样化数据的存储；\n{% asset_img wzfw2_1.png %}\n\n整体记录意义不大，这里着重介绍遇到的一些问题：\n\n## 1. spark Streaming程序在yarn上稳定运行的问题\n这个问题的解决需要多方面的支持：\n1. 首先我们是基于Kerberos的，在hdp2.6上如果nn有主备的话，有一个大坑，导致超过两天就不行，需要hadoop打个patch或者spark打个patch，传送门：[Spark踩坑之Streaming在Kerberos的hadoop中renew失败](http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E5%9C%A8Kerberos%E7%9A%84hadoop%E4%B8%ADrenew%E5%A4%B1%E8%B4%A5/)\n2. spark Streaming的稳定运行需要yarn的稳定，如果你依赖kafka的话，需要kafka稳定，我们遇到多次kafka找不到broker的错误，导致spark崩溃，所以要做好监控\n3. spark Streaming开发连接redis与mysql的时候也踩了一些坑，请看踩坑集锦；\n4. 如果你的日志是INFO模式的话，请改成WARN模式，过多的日志会让程序变得不稳定，当时忘记把日志模式改成WARN，导致程序跑了两个月的日志是几百G。。。\n5. 无论如何，由于程序依赖的组件较多，不敢说spark Streaming永远稳定，所以要做好程序的监控与失败后的拉起以及数据的恢复，我们这边写了一个通用的脚本，通过yarn的API实时监控Streaming程序的状态，然后发送zabbix记录与报警，如果发现挂了，就自动拉起，以防万一；\n\n## 2. 判断一个用户是否在该区域的算法\n圆形：只需要判断圆形中心点和用户所在的点的半径是否小于圆形的半径即可。\n任意规则图形：根据PNPoly 算法实现。\n都是现成的算法用Scala简单实现了而已。[传送门](http://flume.cn/2016/11/24/%E9%80%9A%E8%BF%87%E7%BB%8F%E7%BA%AC%E5%BA%A6%E8%AE%A1%E7%AE%97%E8%B7%9D%E7%A6%BB%E7%AE%97%E6%B3%95%E7%9A%84scala%E5%AE%9E%E7%8E%B0/)\n\n值得一提的是，如果全国的每条数据都对每个区域来判断，这个计算量太大了，我们在进行区域判断之前做了一个优化，优化方案如下：\n订阅者订阅的每个区域都会有一个它所属的城市列表(如果在多个城市之间，则它属于多个城市)，这样我们会通过实时扫描数据库，统一实时得去维护一个 HashMap，它的数据结构如下：\nutil.HashMap[Int, util.LinkedList[PositionSubData]]，这里以城市id为key，以订阅景点的数据结构 PositionSubData 的列表为value，代表这个城市里的所有区域信息。\n这样的话，每次有新数据过来，首先判断一下它在不在订阅到的城市，如果在该城市，再只要判断它所属的那几个景区就好了。\n\n## 3. spark开发过程中的一些踩坑经验\n网上有很多经验，这里总结网上没有的，或者不常发现的：\n1. 通过RDD的flatMap方法，可以实现map方法和filter方法效果的结合，也可以实现每一条输入数据，可以输出多条数据的情况；[传送门](http://flume.cn/2017/06/22/spark%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7%E6%80%BB%E7%BB%93%E4%B9%8BflatMap/)\n2. 不是所有的方法都用foreachRDD就可以的，因为有监控的需求，必须要在transform方法中实现一些复杂的逻辑，我们知道spark是lazy的，所以在编写过程中(尤其与外部redis或者mysql进行交互)要注意很多细节；这里不详细展开，看我其它相关总结；\n3. 如果有多个action操作的话，一定要cache，否则前面会执行多次，不仅仅会浪费计算，更会导致结果是错误的（如果依赖redis或者mysql类似的外部数据的话）；\n4. 很多时候会觉得，这个需求在流处理里是根本无法实现的感觉，但到最后发现没有什么需求是不能实现的，不过总会有比你的方案更优的方案；\n5. 前期组件的性能调研非常重要，预估需要的性能非常重要，写到后面大数据量上来了后才发现，组件扛不住，就坑了。。。\n\n## 4. 返回区域实时人数的思路与总结\n\n[传送门](http://flume.cn/2017/06/29/%E8%BF%94%E5%9B%9E%E5%8C%BA%E5%9F%9F%E5%AE%9E%E6%97%B6%E4%BA%BA%E6%95%B0%E7%9A%84%E6%80%9D%E8%B7%AF%E4%B8%8E%E6%80%BB%E7%BB%93/)\n\n# 思考与未来\nSpark Streaming基本上可以实现大部分的业务需求，但是一些时间窗口相关的需求难以搞定，需要借助外部存储机制来完善，接下来主要做以下几件事：\n1. 实时程序的部署监控报警的统一化；现在从kafka导数据到其它组件还没有做到自动化，需要人肉运维，并且flume的配置也是基于命令行的，这些都需要统一；\n2. 调研与应用structured Streaming；\n3. 根据需求，完成流式计算平台，将简单的逻辑傻瓜化，用户只需要输入简单的一些指令或者sql语句，就能实现程序的调试，部署与上线；\n4. 跟着Apache Beam的发展，逐渐将业务逻辑与处理引擎分开，这是大势所趋。\n","source":"_posts/2017-06-28-位置服务开发上线总结.md","raw":"---\ntitle: 位置服务开发上线总结————实时数据推送\ntoc: true\ndate: 2017-06-28 15:52:05\ntags: spark开发\ncategories: spark开发\n---\n\n#### 天作棋盘星作子，谁人能下？ 地为琵琶路为弦，哪个敢弹！\n\n位置服务项目不止一期，也断断续续写了一年，不知从哪里谈起，在此着重记录一些问题以及一些解决方案，重要的是在解决问题过程中的思考。\n\n# 背景\n## 需求\n需求比较复杂，细节比较多，在这里简明扼要说一下：\n1. 用户A(spId)在中国地图上画了100个圆圈(locationId)，想让我们实时告诉他这些圈内新来的人和他的位置信息，哪些是游客哪些是土著；\n2. A的100个圆圈会随时修改，增大，变小，或者再多加100个圈圈，我们要不停程序，秒级给他这些数据；\n3. A觉得这样推送的数据量太大，因为有的人本来就住这个圈里，所以对每个不同的圈，里面的人在 7天（可调）之内不重复；\n4. 用户B也想使用这个服务，在中国地图上画了100个圈，B的圈和A的圈可能重复可能不重复，需求是一样的；\n5. 用户C也想使用这个服务，不过他觉得画个圈不符合他的要求，他要在地图上随便画100个什么图形，然后给他他想要的数据；\n6. 用户D也想使用这个服务，不过他不想要知道这个图形里的人的细节，而是只想实时知道这个图形当下有多少人。\n\n上面是位置服务二期的需求的简单总结，一期太简单不值一提，三期主要是数据的存储，以后再谈，这里只提二期。\n\n\n最终，我们采用spark streaming来实现了上述需求，使用一个Spark Streaming程序支持多用户随时订阅随时修改，在yarn持续稳定运行超过一千个小时。\n\n## 数据输入\n用户的位置数据是实时得通过kafka传过来的，数据量大概为几十万条每秒，以一个电话号码msisdn表示一个用户，每条数据都代表一个用户当前所在的经纬度位置，以及一些省市相关的信息；\n每个用户的数据，平均每隔半个多小时会上传一条，当然实际情况不是这样简单；\n\n# 实现思路\n\n整体架构如下图，我们借助kafka作为数据源的数据中转，redis作为数据缓存，mysql来实现实时订阅，flume来作为数据管道，Spring写的订阅服务以及基于SpringCloud的微服务来实现一些查询功能和监控功能，zabbix主要用户监控和报警，后期引入ES实现了数据多样化数据的存储；\n{% asset_img wzfw2_1.png %}\n\n整体记录意义不大，这里着重介绍遇到的一些问题：\n\n## 1. spark Streaming程序在yarn上稳定运行的问题\n这个问题的解决需要多方面的支持：\n1. 首先我们是基于Kerberos的，在hdp2.6上如果nn有主备的话，有一个大坑，导致超过两天就不行，需要hadoop打个patch或者spark打个patch，传送门：[Spark踩坑之Streaming在Kerberos的hadoop中renew失败](http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E5%9C%A8Kerberos%E7%9A%84hadoop%E4%B8%ADrenew%E5%A4%B1%E8%B4%A5/)\n2. spark Streaming的稳定运行需要yarn的稳定，如果你依赖kafka的话，需要kafka稳定，我们遇到多次kafka找不到broker的错误，导致spark崩溃，所以要做好监控\n3. spark Streaming开发连接redis与mysql的时候也踩了一些坑，请看踩坑集锦；\n4. 如果你的日志是INFO模式的话，请改成WARN模式，过多的日志会让程序变得不稳定，当时忘记把日志模式改成WARN，导致程序跑了两个月的日志是几百G。。。\n5. 无论如何，由于程序依赖的组件较多，不敢说spark Streaming永远稳定，所以要做好程序的监控与失败后的拉起以及数据的恢复，我们这边写了一个通用的脚本，通过yarn的API实时监控Streaming程序的状态，然后发送zabbix记录与报警，如果发现挂了，就自动拉起，以防万一；\n\n## 2. 判断一个用户是否在该区域的算法\n圆形：只需要判断圆形中心点和用户所在的点的半径是否小于圆形的半径即可。\n任意规则图形：根据PNPoly 算法实现。\n都是现成的算法用Scala简单实现了而已。[传送门](http://flume.cn/2016/11/24/%E9%80%9A%E8%BF%87%E7%BB%8F%E7%BA%AC%E5%BA%A6%E8%AE%A1%E7%AE%97%E8%B7%9D%E7%A6%BB%E7%AE%97%E6%B3%95%E7%9A%84scala%E5%AE%9E%E7%8E%B0/)\n\n值得一提的是，如果全国的每条数据都对每个区域来判断，这个计算量太大了，我们在进行区域判断之前做了一个优化，优化方案如下：\n订阅者订阅的每个区域都会有一个它所属的城市列表(如果在多个城市之间，则它属于多个城市)，这样我们会通过实时扫描数据库，统一实时得去维护一个 HashMap，它的数据结构如下：\nutil.HashMap[Int, util.LinkedList[PositionSubData]]，这里以城市id为key，以订阅景点的数据结构 PositionSubData 的列表为value，代表这个城市里的所有区域信息。\n这样的话，每次有新数据过来，首先判断一下它在不在订阅到的城市，如果在该城市，再只要判断它所属的那几个景区就好了。\n\n## 3. spark开发过程中的一些踩坑经验\n网上有很多经验，这里总结网上没有的，或者不常发现的：\n1. 通过RDD的flatMap方法，可以实现map方法和filter方法效果的结合，也可以实现每一条输入数据，可以输出多条数据的情况；[传送门](http://flume.cn/2017/06/22/spark%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7%E6%80%BB%E7%BB%93%E4%B9%8BflatMap/)\n2. 不是所有的方法都用foreachRDD就可以的，因为有监控的需求，必须要在transform方法中实现一些复杂的逻辑，我们知道spark是lazy的，所以在编写过程中(尤其与外部redis或者mysql进行交互)要注意很多细节；这里不详细展开，看我其它相关总结；\n3. 如果有多个action操作的话，一定要cache，否则前面会执行多次，不仅仅会浪费计算，更会导致结果是错误的（如果依赖redis或者mysql类似的外部数据的话）；\n4. 很多时候会觉得，这个需求在流处理里是根本无法实现的感觉，但到最后发现没有什么需求是不能实现的，不过总会有比你的方案更优的方案；\n5. 前期组件的性能调研非常重要，预估需要的性能非常重要，写到后面大数据量上来了后才发现，组件扛不住，就坑了。。。\n\n## 4. 返回区域实时人数的思路与总结\n\n[传送门](http://flume.cn/2017/06/29/%E8%BF%94%E5%9B%9E%E5%8C%BA%E5%9F%9F%E5%AE%9E%E6%97%B6%E4%BA%BA%E6%95%B0%E7%9A%84%E6%80%9D%E8%B7%AF%E4%B8%8E%E6%80%BB%E7%BB%93/)\n\n# 思考与未来\nSpark Streaming基本上可以实现大部分的业务需求，但是一些时间窗口相关的需求难以搞定，需要借助外部存储机制来完善，接下来主要做以下几件事：\n1. 实时程序的部署监控报警的统一化；现在从kafka导数据到其它组件还没有做到自动化，需要人肉运维，并且flume的配置也是基于命令行的，这些都需要统一；\n2. 调研与应用structured Streaming；\n3. 根据需求，完成流式计算平台，将简单的逻辑傻瓜化，用户只需要输入简单的一些指令或者sql语句，就能实现程序的调试，部署与上线；\n4. 跟着Apache Beam的发展，逐渐将业务逻辑与处理引擎分开，这是大势所趋。\n","slug":"位置服务开发上线总结","published":1,"updated":"2017-06-29T11:38:22.421Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfxn003qpsgu1psmy1ta","content":"<h4 id=\"天作棋盘星作子，谁人能下？-地为琵琶路为弦，哪个敢弹！\"><a href=\"#天作棋盘星作子，谁人能下？-地为琵琶路为弦，哪个敢弹！\" class=\"headerlink\" title=\"天作棋盘星作子，谁人能下？ 地为琵琶路为弦，哪个敢弹！\"></a>天作棋盘星作子，谁人能下？ 地为琵琶路为弦，哪个敢弹！</h4><p>位置服务项目不止一期，也断断续续写了一年，不知从哪里谈起，在此着重记录一些问题以及一些解决方案，重要的是在解决问题过程中的思考。</p>\n<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><h2 id=\"需求\"><a href=\"#需求\" class=\"headerlink\" title=\"需求\"></a>需求</h2><p>需求比较复杂，细节比较多，在这里简明扼要说一下：</p>\n<ol>\n<li>用户A(spId)在中国地图上画了100个圆圈(locationId)，想让我们实时告诉他这些圈内新来的人和他的位置信息，哪些是游客哪些是土著；</li>\n<li>A的100个圆圈会随时修改，增大，变小，或者再多加100个圈圈，我们要不停程序，秒级给他这些数据；</li>\n<li>A觉得这样推送的数据量太大，因为有的人本来就住这个圈里，所以对每个不同的圈，里面的人在 7天（可调）之内不重复；</li>\n<li>用户B也想使用这个服务，在中国地图上画了100个圈，B的圈和A的圈可能重复可能不重复，需求是一样的；</li>\n<li>用户C也想使用这个服务，不过他觉得画个圈不符合他的要求，他要在地图上随便画100个什么图形，然后给他他想要的数据；</li>\n<li>用户D也想使用这个服务，不过他不想要知道这个图形里的人的细节，而是只想实时知道这个图形当下有多少人。</li>\n</ol>\n<p>上面是位置服务二期的需求的简单总结，一期太简单不值一提，三期主要是数据的存储，以后再谈，这里只提二期。</p>\n<p>最终，我们采用spark streaming来实现了上述需求，使用一个Spark Streaming程序支持多用户随时订阅随时修改，在yarn持续稳定运行超过一千个小时。</p>\n<h2 id=\"数据输入\"><a href=\"#数据输入\" class=\"headerlink\" title=\"数据输入\"></a>数据输入</h2><p>用户的位置数据是实时得通过kafka传过来的，数据量大概为几十万条每秒，以一个电话号码msisdn表示一个用户，每条数据都代表一个用户当前所在的经纬度位置，以及一些省市相关的信息；<br>每个用户的数据，平均每隔半个多小时会上传一条，当然实际情况不是这样简单；</p>\n<h1 id=\"实现思路\"><a href=\"#实现思路\" class=\"headerlink\" title=\"实现思路\"></a>实现思路</h1><p>整体架构如下图，我们借助kafka作为数据源的数据中转，redis作为数据缓存，mysql来实现实时订阅，flume来作为数据管道，Spring写的订阅服务以及基于SpringCloud的微服务来实现一些查询功能和监控功能，zabbix主要用户监控和报警，后期引入ES实现了数据多样化数据的存储；<br><img src=\"/2017/06/28/位置服务开发上线总结/wzfw2_1.png\" alt=\"wzfw2_1.png\" title=\"\"></p>\n<p>整体记录意义不大，这里着重介绍遇到的一些问题：</p>\n<h2 id=\"1-spark-Streaming程序在yarn上稳定运行的问题\"><a href=\"#1-spark-Streaming程序在yarn上稳定运行的问题\" class=\"headerlink\" title=\"1. spark Streaming程序在yarn上稳定运行的问题\"></a>1. spark Streaming程序在yarn上稳定运行的问题</h2><p>这个问题的解决需要多方面的支持：</p>\n<ol>\n<li>首先我们是基于Kerberos的，在hdp2.6上如果nn有主备的话，有一个大坑，导致超过两天就不行，需要hadoop打个patch或者spark打个patch，传送门：<a href=\"http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E5%9C%A8Kerberos%E7%9A%84hadoop%E4%B8%ADrenew%E5%A4%B1%E8%B4%A5/\">Spark踩坑之Streaming在Kerberos的hadoop中renew失败</a></li>\n<li>spark Streaming的稳定运行需要yarn的稳定，如果你依赖kafka的话，需要kafka稳定，我们遇到多次kafka找不到broker的错误，导致spark崩溃，所以要做好监控</li>\n<li>spark Streaming开发连接redis与mysql的时候也踩了一些坑，请看踩坑集锦；</li>\n<li>如果你的日志是INFO模式的话，请改成WARN模式，过多的日志会让程序变得不稳定，当时忘记把日志模式改成WARN，导致程序跑了两个月的日志是几百G。。。</li>\n<li>无论如何，由于程序依赖的组件较多，不敢说spark Streaming永远稳定，所以要做好程序的监控与失败后的拉起以及数据的恢复，我们这边写了一个通用的脚本，通过yarn的API实时监控Streaming程序的状态，然后发送zabbix记录与报警，如果发现挂了，就自动拉起，以防万一；</li>\n</ol>\n<h2 id=\"2-判断一个用户是否在该区域的算法\"><a href=\"#2-判断一个用户是否在该区域的算法\" class=\"headerlink\" title=\"2. 判断一个用户是否在该区域的算法\"></a>2. 判断一个用户是否在该区域的算法</h2><p>圆形：只需要判断圆形中心点和用户所在的点的半径是否小于圆形的半径即可。<br>任意规则图形：根据PNPoly 算法实现。<br>都是现成的算法用Scala简单实现了而已。<a href=\"http://flume.cn/2016/11/24/%E9%80%9A%E8%BF%87%E7%BB%8F%E7%BA%AC%E5%BA%A6%E8%AE%A1%E7%AE%97%E8%B7%9D%E7%A6%BB%E7%AE%97%E6%B3%95%E7%9A%84scala%E5%AE%9E%E7%8E%B0/\">传送门</a></p>\n<p>值得一提的是，如果全国的每条数据都对每个区域来判断，这个计算量太大了，我们在进行区域判断之前做了一个优化，优化方案如下：<br>订阅者订阅的每个区域都会有一个它所属的城市列表(如果在多个城市之间，则它属于多个城市)，这样我们会通过实时扫描数据库，统一实时得去维护一个 HashMap，它的数据结构如下：<br>util.HashMap[Int, util.LinkedList[PositionSubData]]，这里以城市id为key，以订阅景点的数据结构 PositionSubData 的列表为value，代表这个城市里的所有区域信息。<br>这样的话，每次有新数据过来，首先判断一下它在不在订阅到的城市，如果在该城市，再只要判断它所属的那几个景区就好了。</p>\n<h2 id=\"3-spark开发过程中的一些踩坑经验\"><a href=\"#3-spark开发过程中的一些踩坑经验\" class=\"headerlink\" title=\"3. spark开发过程中的一些踩坑经验\"></a>3. spark开发过程中的一些踩坑经验</h2><p>网上有很多经验，这里总结网上没有的，或者不常发现的：</p>\n<ol>\n<li>通过RDD的flatMap方法，可以实现map方法和filter方法效果的结合，也可以实现每一条输入数据，可以输出多条数据的情况；<a href=\"http://flume.cn/2017/06/22/spark%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7%E6%80%BB%E7%BB%93%E4%B9%8BflatMap/\">传送门</a></li>\n<li>不是所有的方法都用foreachRDD就可以的，因为有监控的需求，必须要在transform方法中实现一些复杂的逻辑，我们知道spark是lazy的，所以在编写过程中(尤其与外部redis或者mysql进行交互)要注意很多细节；这里不详细展开，看我其它相关总结；</li>\n<li>如果有多个action操作的话，一定要cache，否则前面会执行多次，不仅仅会浪费计算，更会导致结果是错误的（如果依赖redis或者mysql类似的外部数据的话）；</li>\n<li>很多时候会觉得，这个需求在流处理里是根本无法实现的感觉，但到最后发现没有什么需求是不能实现的，不过总会有比你的方案更优的方案；</li>\n<li>前期组件的性能调研非常重要，预估需要的性能非常重要，写到后面大数据量上来了后才发现，组件扛不住，就坑了。。。</li>\n</ol>\n<h2 id=\"4-返回区域实时人数的思路与总结\"><a href=\"#4-返回区域实时人数的思路与总结\" class=\"headerlink\" title=\"4. 返回区域实时人数的思路与总结\"></a>4. 返回区域实时人数的思路与总结</h2><p><a href=\"http://flume.cn/2017/06/29/%E8%BF%94%E5%9B%9E%E5%8C%BA%E5%9F%9F%E5%AE%9E%E6%97%B6%E4%BA%BA%E6%95%B0%E7%9A%84%E6%80%9D%E8%B7%AF%E4%B8%8E%E6%80%BB%E7%BB%93/\">传送门</a></p>\n<h1 id=\"思考与未来\"><a href=\"#思考与未来\" class=\"headerlink\" title=\"思考与未来\"></a>思考与未来</h1><p>Spark Streaming基本上可以实现大部分的业务需求，但是一些时间窗口相关的需求难以搞定，需要借助外部存储机制来完善，接下来主要做以下几件事：</p>\n<ol>\n<li>实时程序的部署监控报警的统一化；现在从kafka导数据到其它组件还没有做到自动化，需要人肉运维，并且flume的配置也是基于命令行的，这些都需要统一；</li>\n<li>调研与应用structured Streaming；</li>\n<li>根据需求，完成流式计算平台，将简单的逻辑傻瓜化，用户只需要输入简单的一些指令或者sql语句，就能实现程序的调试，部署与上线；</li>\n<li>跟着Apache Beam的发展，逐渐将业务逻辑与处理引擎分开，这是大势所趋。</li>\n</ol>\n","excerpt":"","more":"<h4 id=\"天作棋盘星作子，谁人能下？-地为琵琶路为弦，哪个敢弹！\"><a href=\"#天作棋盘星作子，谁人能下？-地为琵琶路为弦，哪个敢弹！\" class=\"headerlink\" title=\"天作棋盘星作子，谁人能下？ 地为琵琶路为弦，哪个敢弹！\"></a>天作棋盘星作子，谁人能下？ 地为琵琶路为弦，哪个敢弹！</h4><p>位置服务项目不止一期，也断断续续写了一年，不知从哪里谈起，在此着重记录一些问题以及一些解决方案，重要的是在解决问题过程中的思考。</p>\n<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><h2 id=\"需求\"><a href=\"#需求\" class=\"headerlink\" title=\"需求\"></a>需求</h2><p>需求比较复杂，细节比较多，在这里简明扼要说一下：</p>\n<ol>\n<li>用户A(spId)在中国地图上画了100个圆圈(locationId)，想让我们实时告诉他这些圈内新来的人和他的位置信息，哪些是游客哪些是土著；</li>\n<li>A的100个圆圈会随时修改，增大，变小，或者再多加100个圈圈，我们要不停程序，秒级给他这些数据；</li>\n<li>A觉得这样推送的数据量太大，因为有的人本来就住这个圈里，所以对每个不同的圈，里面的人在 7天（可调）之内不重复；</li>\n<li>用户B也想使用这个服务，在中国地图上画了100个圈，B的圈和A的圈可能重复可能不重复，需求是一样的；</li>\n<li>用户C也想使用这个服务，不过他觉得画个圈不符合他的要求，他要在地图上随便画100个什么图形，然后给他他想要的数据；</li>\n<li>用户D也想使用这个服务，不过他不想要知道这个图形里的人的细节，而是只想实时知道这个图形当下有多少人。</li>\n</ol>\n<p>上面是位置服务二期的需求的简单总结，一期太简单不值一提，三期主要是数据的存储，以后再谈，这里只提二期。</p>\n<p>最终，我们采用spark streaming来实现了上述需求，使用一个Spark Streaming程序支持多用户随时订阅随时修改，在yarn持续稳定运行超过一千个小时。</p>\n<h2 id=\"数据输入\"><a href=\"#数据输入\" class=\"headerlink\" title=\"数据输入\"></a>数据输入</h2><p>用户的位置数据是实时得通过kafka传过来的，数据量大概为几十万条每秒，以一个电话号码msisdn表示一个用户，每条数据都代表一个用户当前所在的经纬度位置，以及一些省市相关的信息；<br>每个用户的数据，平均每隔半个多小时会上传一条，当然实际情况不是这样简单；</p>\n<h1 id=\"实现思路\"><a href=\"#实现思路\" class=\"headerlink\" title=\"实现思路\"></a>实现思路</h1><p>整体架构如下图，我们借助kafka作为数据源的数据中转，redis作为数据缓存，mysql来实现实时订阅，flume来作为数据管道，Spring写的订阅服务以及基于SpringCloud的微服务来实现一些查询功能和监控功能，zabbix主要用户监控和报警，后期引入ES实现了数据多样化数据的存储；<br><img src=\"/2017/06/28/位置服务开发上线总结/wzfw2_1.png\" alt=\"wzfw2_1.png\" title=\"\"></p>\n<p>整体记录意义不大，这里着重介绍遇到的一些问题：</p>\n<h2 id=\"1-spark-Streaming程序在yarn上稳定运行的问题\"><a href=\"#1-spark-Streaming程序在yarn上稳定运行的问题\" class=\"headerlink\" title=\"1. spark Streaming程序在yarn上稳定运行的问题\"></a>1. spark Streaming程序在yarn上稳定运行的问题</h2><p>这个问题的解决需要多方面的支持：</p>\n<ol>\n<li>首先我们是基于Kerberos的，在hdp2.6上如果nn有主备的话，有一个大坑，导致超过两天就不行，需要hadoop打个patch或者spark打个patch，传送门：<a href=\"http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E5%9C%A8Kerberos%E7%9A%84hadoop%E4%B8%ADrenew%E5%A4%B1%E8%B4%A5/\">Spark踩坑之Streaming在Kerberos的hadoop中renew失败</a></li>\n<li>spark Streaming的稳定运行需要yarn的稳定，如果你依赖kafka的话，需要kafka稳定，我们遇到多次kafka找不到broker的错误，导致spark崩溃，所以要做好监控</li>\n<li>spark Streaming开发连接redis与mysql的时候也踩了一些坑，请看踩坑集锦；</li>\n<li>如果你的日志是INFO模式的话，请改成WARN模式，过多的日志会让程序变得不稳定，当时忘记把日志模式改成WARN，导致程序跑了两个月的日志是几百G。。。</li>\n<li>无论如何，由于程序依赖的组件较多，不敢说spark Streaming永远稳定，所以要做好程序的监控与失败后的拉起以及数据的恢复，我们这边写了一个通用的脚本，通过yarn的API实时监控Streaming程序的状态，然后发送zabbix记录与报警，如果发现挂了，就自动拉起，以防万一；</li>\n</ol>\n<h2 id=\"2-判断一个用户是否在该区域的算法\"><a href=\"#2-判断一个用户是否在该区域的算法\" class=\"headerlink\" title=\"2. 判断一个用户是否在该区域的算法\"></a>2. 判断一个用户是否在该区域的算法</h2><p>圆形：只需要判断圆形中心点和用户所在的点的半径是否小于圆形的半径即可。<br>任意规则图形：根据PNPoly 算法实现。<br>都是现成的算法用Scala简单实现了而已。<a href=\"http://flume.cn/2016/11/24/%E9%80%9A%E8%BF%87%E7%BB%8F%E7%BA%AC%E5%BA%A6%E8%AE%A1%E7%AE%97%E8%B7%9D%E7%A6%BB%E7%AE%97%E6%B3%95%E7%9A%84scala%E5%AE%9E%E7%8E%B0/\">传送门</a></p>\n<p>值得一提的是，如果全国的每条数据都对每个区域来判断，这个计算量太大了，我们在进行区域判断之前做了一个优化，优化方案如下：<br>订阅者订阅的每个区域都会有一个它所属的城市列表(如果在多个城市之间，则它属于多个城市)，这样我们会通过实时扫描数据库，统一实时得去维护一个 HashMap，它的数据结构如下：<br>util.HashMap[Int, util.LinkedList[PositionSubData]]，这里以城市id为key，以订阅景点的数据结构 PositionSubData 的列表为value，代表这个城市里的所有区域信息。<br>这样的话，每次有新数据过来，首先判断一下它在不在订阅到的城市，如果在该城市，再只要判断它所属的那几个景区就好了。</p>\n<h2 id=\"3-spark开发过程中的一些踩坑经验\"><a href=\"#3-spark开发过程中的一些踩坑经验\" class=\"headerlink\" title=\"3. spark开发过程中的一些踩坑经验\"></a>3. spark开发过程中的一些踩坑经验</h2><p>网上有很多经验，这里总结网上没有的，或者不常发现的：</p>\n<ol>\n<li>通过RDD的flatMap方法，可以实现map方法和filter方法效果的结合，也可以实现每一条输入数据，可以输出多条数据的情况；<a href=\"http://flume.cn/2017/06/22/spark%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7%E6%80%BB%E7%BB%93%E4%B9%8BflatMap/\">传送门</a></li>\n<li>不是所有的方法都用foreachRDD就可以的，因为有监控的需求，必须要在transform方法中实现一些复杂的逻辑，我们知道spark是lazy的，所以在编写过程中(尤其与外部redis或者mysql进行交互)要注意很多细节；这里不详细展开，看我其它相关总结；</li>\n<li>如果有多个action操作的话，一定要cache，否则前面会执行多次，不仅仅会浪费计算，更会导致结果是错误的（如果依赖redis或者mysql类似的外部数据的话）；</li>\n<li>很多时候会觉得，这个需求在流处理里是根本无法实现的感觉，但到最后发现没有什么需求是不能实现的，不过总会有比你的方案更优的方案；</li>\n<li>前期组件的性能调研非常重要，预估需要的性能非常重要，写到后面大数据量上来了后才发现，组件扛不住，就坑了。。。</li>\n</ol>\n<h2 id=\"4-返回区域实时人数的思路与总结\"><a href=\"#4-返回区域实时人数的思路与总结\" class=\"headerlink\" title=\"4. 返回区域实时人数的思路与总结\"></a>4. 返回区域实时人数的思路与总结</h2><p><a href=\"http://flume.cn/2017/06/29/%E8%BF%94%E5%9B%9E%E5%8C%BA%E5%9F%9F%E5%AE%9E%E6%97%B6%E4%BA%BA%E6%95%B0%E7%9A%84%E6%80%9D%E8%B7%AF%E4%B8%8E%E6%80%BB%E7%BB%93/\">传送门</a></p>\n<h1 id=\"思考与未来\"><a href=\"#思考与未来\" class=\"headerlink\" title=\"思考与未来\"></a>思考与未来</h1><p>Spark Streaming基本上可以实现大部分的业务需求，但是一些时间窗口相关的需求难以搞定，需要借助外部存储机制来完善，接下来主要做以下几件事：</p>\n<ol>\n<li>实时程序的部署监控报警的统一化；现在从kafka导数据到其它组件还没有做到自动化，需要人肉运维，并且flume的配置也是基于命令行的，这些都需要统一；</li>\n<li>调研与应用structured Streaming；</li>\n<li>根据需求，完成流式计算平台，将简单的逻辑傻瓜化，用户只需要输入简单的一些指令或者sql语句，就能实现程序的调试，部署与上线；</li>\n<li>跟着Apache Beam的发展，逐渐将业务逻辑与处理引擎分开，这是大势所趋。</li>\n</ol>\n"},{"title":"返回区域实时人数的思路与总结","toc":true,"date":"2017-06-29T10:45:21.000Z","_content":"\n## 问题背景\n先说一下整个位置服务的背景，需求比较复杂，细节比较多，在这里简明扼要说一下：\n1. 用户A(spId)在中国地图上画了100个圆圈(locationId)，想让我们实时告诉他这些圈内新来的人和他的位置信息，哪些是游客哪些是土著；\n2. A的100个圆圈会随时修改，增大，变小，或者再多加100个圈圈，我们要不停程序，秒级给他这些数据；\n3. A觉得这样推送的数据量太大，因为有的人本来就住这个圈里，所以对每个不同的圈，里面的人在 7天（可调）之内不重复；\n4. 用户B也想使用这个服务，在中国地图上画了100个圈，B的圈和A的圈可能重复可能不重复，需求是一样的；\n5. 用户C也想使用这个服务，不过他觉得画个圈不符合他的要求，他要在地图上随便画100个什么图形，然后给他他想要的数据；\n6. 用户D也想使用这个服务，不过他不想要知道这个图形里的人的细节，而是只想实时知道这个图形当下有多少人。\n\n**本文只关注返回区域实时人数的需求：**\n\n需求分为两大类，第一类是返回刚进入这个区域的人（返回明细），这个实现起来比较简单，因为它没有窗口的概念，只需要有数据过来，通过redis和mysql，经过一系列计算，判断它在景区并且是新来的人的话，直接推送到下游就好了；\n第二个需求是实时返回这个区域的当前人数（返回人数）。这个听起来好像挺简单，比如当前上海迪士尼有56000个人，就给订阅者 56000这个数字就好了，但仔细思考一下就会发现较难实现，主要原因如下：\n\n1. 实时上传过来的数据并不是全量数据，如果以5s为一批，这批上传的数据可能只占全国总量2亿用户中的100W条，这样就导致无法在一批数据中就得到结果；\n2. 用户上传数据的频率也是不一致的，有的可能一分钟会发多条，有的可能四五十分钟才会上传一条，有的干脆好久都不会上传；\n3. 多个订阅者同时订阅多个景区，每个订阅者的数据都要是对的；\n4. 数据量太大，导致难以用时间窗口把它装住，数据还是得流进来，五秒内就得流出去；\n\n## 一些简单可能的方案\n为了满足上述需求，之前提出过一些方案：\n1. 每次处理一个小时的数据，因为大概一个小时以内，所有的用户基本都会上传一遍数据，这样通过去重就能知道上个小时该区域的人数的，但这是相当于批处理，没有实时性，是最差的选择了；\n2. 使用时间窗口，维护所有用户当前的位置信息，每隔一段时间计算，然后输出（类型与Apache Beam的定义模式），这个由于数据量太大，难以实现，因为有其它方案，所以也没有尝试；\n3. 使用外部的一个数据库，维护所有用户的当前位置信息，然后每次计算，输出；如果用户量在千万级别以下的话，觉得用Mysql就可以了，但是2亿的用户，Mysql难以胜任，如果用Hbase或redis保存所有人状态的话，的话，每次统计计算多个区域的人数信息需要的计算量也很大，后来也没有尝试；\n\n## 基于双层redis的方案\n\n### 方案思路\n首先引入两个状态：用户这次是否在景区 thisTime 和 用户上次是否在景区 lastTime，基于这两个状态，可以得到如下表格\n\n| 用户位置判断 | 本次是否在该区域 | 上次是否在该区域 | 处理逻辑           |\n| ------ | -------- | -------- | -------------- |\n| 情况1    | 0        | 0        | 不处理            |\n| 情况2    | 0        | 1        | 原子删除（一级总表和二级表） |\n| 情况3    | 1        | 0        | 原子增加（一级总表和二级表） |\n| 情况4    | 1        | 1        | 不处理            |\n\n如上表格所示，每个人都维护这次与上次两个状态，如果这个人这次不在该区域，上次也不在该区域，说明什么都不需要做，如果这个人这次不在该区域，上次在，说明这个人离开了这个区域，则就要把它从该区域中剔除；同理可得，如果这次在，上次不在，就把该用户加到该区域。这样一来，我们就可以对每个区域的人了如指掌，而且在理论上，这是目前位置最精确的判断方法。\n\n### 可行性\n这套方案在这个问题上是可行的，虽然用户的位置轨迹类似于用户登陆网页的轨迹，但是又有极大的不同，因为每个人虽然都可离开这个网站，但是每个人都不会跳出地球！如果是网站的话，我只知道你访问了我这个网页所做的操作，但我不知道你什么时候没有访问我的网页，比如你从baidu.com跑到了google，baidu的数据里是不知道的，但你从这个区域跑到另外一个区域，我们的数据里是知道的，这就让该方案可行。\n\n### 实现方案\n\n如下图右侧是实现方案的简化版，我们使用了双层redis的方法实现了这个思路，第一层redis的数据结构为hash，订阅者id(spId)命名(当然实际名称多了些前缀)，key是电话号码msisdn，value是它对应的区域id(locationId)，第二层redis的数据结构为Set，以区域id(locationId)命名，里面直接是 msisdn。\n{% asset_img wzfw2_2.png %}\n判断本次是否在景区，直接根据位置算法进行计算就可以知道了，判断上次是否在景区，只需要循环去第一层redis中查有没有这个msisdn，如果有就返回它的locationId，如果没有，就说明该用户不在任何景区。如果要往该景区增加这个人，则需要同时往一级redis和二级对应的某一个redis中同时增加，同理，删除的话同时删除。值得注意的是，redis本身支持的事务并不支持回滚，所以还是要自己去控制它，通过返回值来进行原子删除和增加。上述操作的时间复杂度均为O(1)，道理上讲还是很快的。然后每个set都代表一个景区，想知道它的人数，so easy，O(1)的时间复杂度就能搞定了，连具体是谁都能列出来。\n\n当然具体的代码实现就繁杂很多了：\n1. 因为是不同的订阅者订阅好多景区，一个用户可能同时在多个订阅者订阅的好多个景区，这就要循环判断很多，会存在RDD的一条输入，输出是多条的情况；\n2. 除了上述四种情况，还有几种情况需要考虑到，比如这个人从这个区域跑到同一个订阅者订阅的另一个区域的话，就需要使用 redis set的move操作，同时改变一级hash表的值。如果跑到另一个订阅者的另一个区域的话，又会有它相应的逻辑。\n但大部分情况下还是以上四种逻辑，毕竟中国很大，订阅的区域间距还是比较远的。\n3. 为了防止各种原因导致redis中脏数据的产生，在存入的时候会加一个时间戳，如果时间过长，就会将其中的数据删除。\n实现逻辑大概代码流程图如下：\n{% asset_img wzfw2_3.png %}\n\n为什么要以双层redis的方法呢？单层的不行嘛？由于我们的数据量非常大，如果仅仅是单层的hash的话，每次去计算某个景区的人数，又要把所有的数据全部遍历一遍，这个性能消耗是非常大的，这里采用空间换时间的方法，用极少的空间，节省了大量的时间。说白了，如果数据量小的话，用mysql之类的关系型数据库最好了，查的时候只要类似于`select count(*) from spId where locationId = locationA`之类的语句就可以了。使用Hbase能不能达到这个要求呢？个人没有进行过更深入的调研，浅薄地认为因为hbase的数据本身是存在hdfs的，如果是全量数据的话，它缓存命中的概率是比较低的，这样延迟会比较大，那么大的数据，难以秒级全部响应，所以就再也没有深入调研。\n\n### 问题与提高\n这个方案，理论上的结果是很精确的，但是它有一个小问题，就是在判断用户上次是否在该区域的时候，“在判断该用户上次是否在该区域之前，我们是不知道他上次是否在某个区域的”，这话听起来比较拗口，换一种方式表达：对于全国每一个用户的每一条数据，我们都要判断它上次是否在某个区域，这代表20W/s的redis读！当然我这边会做一些优化，先根据城市id进行过滤，仅仅读相关城市的数据，不过这个数据量也是比较大的，也到了1W/s的级别。\n\n实际生产压测过程中，当全量数据上来后，由于redis的每次连接导致网络IO过多，响应会变慢，导致程序会逐渐产生延迟，也影响到了稳定运行的明细数据需求，所以存在问题！\n\n由于我们的redis是与其它需求共用的，并且当时为了稳定使用的sentinel模式，也难以在有限的几天内就申请到cluster模式的redis集群，也难以使用pipeline来优化，还有几天就要上线了。所以提出了下面的基于数据过期的模糊方案来进行过渡。\n\n\n## 基于数据过期的模糊方案\n提出模糊方案的一个前提是，用户上传的数据本来就是模糊的，如果一个用户半个小时上传一次他的位置数据，我们无法得知他在这半个小时之间的位置，也无法得知他下半个小时在哪里\n\n### 方案思路\n用户每次进入某个区域后，就将它存入该区域集合中，不用考虑其它，区域中的数据是有时效的，超过30min(可调)后，区域中的数据就会被直接删除。\n\n### 实现方案\n对于每一个位置 locationId，我们使用redis的 sorted set数据结构，里面保存的值是msisdn，里面的score值是当前时间戳精确到分钟。它插入(zadd)的时间复杂度是O(log(N))，移除(ZREMRANGEBYSCORE)的时间复杂度是 O(log(N)+M)。每次有符合条件的数据，我们就直接插入到相应的locationId，然后异步地通过外部程序，对其中过期超过30min(可调)的数据进行批量删除，实际中这个删除是在spark Streaming的driver中实现每10s删除一次。\n\n该方案虽然在插入时时间复杂度有所增加，但是极大得简化了redis的连接次数，ZREMRANGEBYSCORE操作也只是统一每隔10s执行一次，最终程序稳定运行在了生产。\n\n### 问题与提高\n该方案的优点是简单易行，问题是它并不像上一个方案一样是理论上精确的，但由于原始数据并不精确，所以有它存在的道理。\n关于超时删除时间，这个时间是可以调优的，调优方案很简单，就一个变量而已，通过批处理的方式对区域的数据进行精确的统计，然后对比两者的结果，最终找出一个最靠谱的超时删除时间。\n当然，区域的订阅面积越大，redis的响应就越慢，所以当订阅面积大到一定程度，redis肯定也是扛不住的。\n\n\n## 总结与思考\n\n1. 判断区域人数是一个比较通用的需求，比如判断当前网站在线用户数，当前频道观众，但是具体实现起来的确觉得不那么简单，我目前能想到的算法就这两个，总觉得会有更好的方法，不同的问题背景产生不同的方法；\n2. 我们设计与实现双层redis方法的时候，没有考虑到redis网络IO过多导致延迟的问题，以后一定要评估好性能瓶颈，提前申请机器；\n3. 有时候追求完美不是好事，完美得向现实妥协，最优的算法并不是最适合的，要在满足需求的基础上，以最少的代码完成这个事情，增加鲁棒性；\n4. 之前对时间窗口相关的不太深入，接下来深入理解一下structured Streaming与Apache Beam的API的逻辑；","source":"_posts/2017-06-29-返回区域实时人数的思路与总结.md","raw":"---\ntitle: 返回区域实时人数的思路与总结\ntoc: true\ndate: 2017-06-29 18:45:21\ntags: \n- spark开发\n- redis\ncategories: spark开发\n---\n\n## 问题背景\n先说一下整个位置服务的背景，需求比较复杂，细节比较多，在这里简明扼要说一下：\n1. 用户A(spId)在中国地图上画了100个圆圈(locationId)，想让我们实时告诉他这些圈内新来的人和他的位置信息，哪些是游客哪些是土著；\n2. A的100个圆圈会随时修改，增大，变小，或者再多加100个圈圈，我们要不停程序，秒级给他这些数据；\n3. A觉得这样推送的数据量太大，因为有的人本来就住这个圈里，所以对每个不同的圈，里面的人在 7天（可调）之内不重复；\n4. 用户B也想使用这个服务，在中国地图上画了100个圈，B的圈和A的圈可能重复可能不重复，需求是一样的；\n5. 用户C也想使用这个服务，不过他觉得画个圈不符合他的要求，他要在地图上随便画100个什么图形，然后给他他想要的数据；\n6. 用户D也想使用这个服务，不过他不想要知道这个图形里的人的细节，而是只想实时知道这个图形当下有多少人。\n\n**本文只关注返回区域实时人数的需求：**\n\n需求分为两大类，第一类是返回刚进入这个区域的人（返回明细），这个实现起来比较简单，因为它没有窗口的概念，只需要有数据过来，通过redis和mysql，经过一系列计算，判断它在景区并且是新来的人的话，直接推送到下游就好了；\n第二个需求是实时返回这个区域的当前人数（返回人数）。这个听起来好像挺简单，比如当前上海迪士尼有56000个人，就给订阅者 56000这个数字就好了，但仔细思考一下就会发现较难实现，主要原因如下：\n\n1. 实时上传过来的数据并不是全量数据，如果以5s为一批，这批上传的数据可能只占全国总量2亿用户中的100W条，这样就导致无法在一批数据中就得到结果；\n2. 用户上传数据的频率也是不一致的，有的可能一分钟会发多条，有的可能四五十分钟才会上传一条，有的干脆好久都不会上传；\n3. 多个订阅者同时订阅多个景区，每个订阅者的数据都要是对的；\n4. 数据量太大，导致难以用时间窗口把它装住，数据还是得流进来，五秒内就得流出去；\n\n## 一些简单可能的方案\n为了满足上述需求，之前提出过一些方案：\n1. 每次处理一个小时的数据，因为大概一个小时以内，所有的用户基本都会上传一遍数据，这样通过去重就能知道上个小时该区域的人数的，但这是相当于批处理，没有实时性，是最差的选择了；\n2. 使用时间窗口，维护所有用户当前的位置信息，每隔一段时间计算，然后输出（类型与Apache Beam的定义模式），这个由于数据量太大，难以实现，因为有其它方案，所以也没有尝试；\n3. 使用外部的一个数据库，维护所有用户的当前位置信息，然后每次计算，输出；如果用户量在千万级别以下的话，觉得用Mysql就可以了，但是2亿的用户，Mysql难以胜任，如果用Hbase或redis保存所有人状态的话，的话，每次统计计算多个区域的人数信息需要的计算量也很大，后来也没有尝试；\n\n## 基于双层redis的方案\n\n### 方案思路\n首先引入两个状态：用户这次是否在景区 thisTime 和 用户上次是否在景区 lastTime，基于这两个状态，可以得到如下表格\n\n| 用户位置判断 | 本次是否在该区域 | 上次是否在该区域 | 处理逻辑           |\n| ------ | -------- | -------- | -------------- |\n| 情况1    | 0        | 0        | 不处理            |\n| 情况2    | 0        | 1        | 原子删除（一级总表和二级表） |\n| 情况3    | 1        | 0        | 原子增加（一级总表和二级表） |\n| 情况4    | 1        | 1        | 不处理            |\n\n如上表格所示，每个人都维护这次与上次两个状态，如果这个人这次不在该区域，上次也不在该区域，说明什么都不需要做，如果这个人这次不在该区域，上次在，说明这个人离开了这个区域，则就要把它从该区域中剔除；同理可得，如果这次在，上次不在，就把该用户加到该区域。这样一来，我们就可以对每个区域的人了如指掌，而且在理论上，这是目前位置最精确的判断方法。\n\n### 可行性\n这套方案在这个问题上是可行的，虽然用户的位置轨迹类似于用户登陆网页的轨迹，但是又有极大的不同，因为每个人虽然都可离开这个网站，但是每个人都不会跳出地球！如果是网站的话，我只知道你访问了我这个网页所做的操作，但我不知道你什么时候没有访问我的网页，比如你从baidu.com跑到了google，baidu的数据里是不知道的，但你从这个区域跑到另外一个区域，我们的数据里是知道的，这就让该方案可行。\n\n### 实现方案\n\n如下图右侧是实现方案的简化版，我们使用了双层redis的方法实现了这个思路，第一层redis的数据结构为hash，订阅者id(spId)命名(当然实际名称多了些前缀)，key是电话号码msisdn，value是它对应的区域id(locationId)，第二层redis的数据结构为Set，以区域id(locationId)命名，里面直接是 msisdn。\n{% asset_img wzfw2_2.png %}\n判断本次是否在景区，直接根据位置算法进行计算就可以知道了，判断上次是否在景区，只需要循环去第一层redis中查有没有这个msisdn，如果有就返回它的locationId，如果没有，就说明该用户不在任何景区。如果要往该景区增加这个人，则需要同时往一级redis和二级对应的某一个redis中同时增加，同理，删除的话同时删除。值得注意的是，redis本身支持的事务并不支持回滚，所以还是要自己去控制它，通过返回值来进行原子删除和增加。上述操作的时间复杂度均为O(1)，道理上讲还是很快的。然后每个set都代表一个景区，想知道它的人数，so easy，O(1)的时间复杂度就能搞定了，连具体是谁都能列出来。\n\n当然具体的代码实现就繁杂很多了：\n1. 因为是不同的订阅者订阅好多景区，一个用户可能同时在多个订阅者订阅的好多个景区，这就要循环判断很多，会存在RDD的一条输入，输出是多条的情况；\n2. 除了上述四种情况，还有几种情况需要考虑到，比如这个人从这个区域跑到同一个订阅者订阅的另一个区域的话，就需要使用 redis set的move操作，同时改变一级hash表的值。如果跑到另一个订阅者的另一个区域的话，又会有它相应的逻辑。\n但大部分情况下还是以上四种逻辑，毕竟中国很大，订阅的区域间距还是比较远的。\n3. 为了防止各种原因导致redis中脏数据的产生，在存入的时候会加一个时间戳，如果时间过长，就会将其中的数据删除。\n实现逻辑大概代码流程图如下：\n{% asset_img wzfw2_3.png %}\n\n为什么要以双层redis的方法呢？单层的不行嘛？由于我们的数据量非常大，如果仅仅是单层的hash的话，每次去计算某个景区的人数，又要把所有的数据全部遍历一遍，这个性能消耗是非常大的，这里采用空间换时间的方法，用极少的空间，节省了大量的时间。说白了，如果数据量小的话，用mysql之类的关系型数据库最好了，查的时候只要类似于`select count(*) from spId where locationId = locationA`之类的语句就可以了。使用Hbase能不能达到这个要求呢？个人没有进行过更深入的调研，浅薄地认为因为hbase的数据本身是存在hdfs的，如果是全量数据的话，它缓存命中的概率是比较低的，这样延迟会比较大，那么大的数据，难以秒级全部响应，所以就再也没有深入调研。\n\n### 问题与提高\n这个方案，理论上的结果是很精确的，但是它有一个小问题，就是在判断用户上次是否在该区域的时候，“在判断该用户上次是否在该区域之前，我们是不知道他上次是否在某个区域的”，这话听起来比较拗口，换一种方式表达：对于全国每一个用户的每一条数据，我们都要判断它上次是否在某个区域，这代表20W/s的redis读！当然我这边会做一些优化，先根据城市id进行过滤，仅仅读相关城市的数据，不过这个数据量也是比较大的，也到了1W/s的级别。\n\n实际生产压测过程中，当全量数据上来后，由于redis的每次连接导致网络IO过多，响应会变慢，导致程序会逐渐产生延迟，也影响到了稳定运行的明细数据需求，所以存在问题！\n\n由于我们的redis是与其它需求共用的，并且当时为了稳定使用的sentinel模式，也难以在有限的几天内就申请到cluster模式的redis集群，也难以使用pipeline来优化，还有几天就要上线了。所以提出了下面的基于数据过期的模糊方案来进行过渡。\n\n\n## 基于数据过期的模糊方案\n提出模糊方案的一个前提是，用户上传的数据本来就是模糊的，如果一个用户半个小时上传一次他的位置数据，我们无法得知他在这半个小时之间的位置，也无法得知他下半个小时在哪里\n\n### 方案思路\n用户每次进入某个区域后，就将它存入该区域集合中，不用考虑其它，区域中的数据是有时效的，超过30min(可调)后，区域中的数据就会被直接删除。\n\n### 实现方案\n对于每一个位置 locationId，我们使用redis的 sorted set数据结构，里面保存的值是msisdn，里面的score值是当前时间戳精确到分钟。它插入(zadd)的时间复杂度是O(log(N))，移除(ZREMRANGEBYSCORE)的时间复杂度是 O(log(N)+M)。每次有符合条件的数据，我们就直接插入到相应的locationId，然后异步地通过外部程序，对其中过期超过30min(可调)的数据进行批量删除，实际中这个删除是在spark Streaming的driver中实现每10s删除一次。\n\n该方案虽然在插入时时间复杂度有所增加，但是极大得简化了redis的连接次数，ZREMRANGEBYSCORE操作也只是统一每隔10s执行一次，最终程序稳定运行在了生产。\n\n### 问题与提高\n该方案的优点是简单易行，问题是它并不像上一个方案一样是理论上精确的，但由于原始数据并不精确，所以有它存在的道理。\n关于超时删除时间，这个时间是可以调优的，调优方案很简单，就一个变量而已，通过批处理的方式对区域的数据进行精确的统计，然后对比两者的结果，最终找出一个最靠谱的超时删除时间。\n当然，区域的订阅面积越大，redis的响应就越慢，所以当订阅面积大到一定程度，redis肯定也是扛不住的。\n\n\n## 总结与思考\n\n1. 判断区域人数是一个比较通用的需求，比如判断当前网站在线用户数，当前频道观众，但是具体实现起来的确觉得不那么简单，我目前能想到的算法就这两个，总觉得会有更好的方法，不同的问题背景产生不同的方法；\n2. 我们设计与实现双层redis方法的时候，没有考虑到redis网络IO过多导致延迟的问题，以后一定要评估好性能瓶颈，提前申请机器；\n3. 有时候追求完美不是好事，完美得向现实妥协，最优的算法并不是最适合的，要在满足需求的基础上，以最少的代码完成这个事情，增加鲁棒性；\n4. 之前对时间窗口相关的不太深入，接下来深入理解一下structured Streaming与Apache Beam的API的逻辑；","slug":"返回区域实时人数的思路与总结","published":1,"updated":"2017-06-29T11:37:29.146Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfxq003tpsgugf2kbpa4","content":"<h2 id=\"问题背景\"><a href=\"#问题背景\" class=\"headerlink\" title=\"问题背景\"></a>问题背景</h2><p>先说一下整个位置服务的背景，需求比较复杂，细节比较多，在这里简明扼要说一下：</p>\n<ol>\n<li>用户A(spId)在中国地图上画了100个圆圈(locationId)，想让我们实时告诉他这些圈内新来的人和他的位置信息，哪些是游客哪些是土著；</li>\n<li>A的100个圆圈会随时修改，增大，变小，或者再多加100个圈圈，我们要不停程序，秒级给他这些数据；</li>\n<li>A觉得这样推送的数据量太大，因为有的人本来就住这个圈里，所以对每个不同的圈，里面的人在 7天（可调）之内不重复；</li>\n<li>用户B也想使用这个服务，在中国地图上画了100个圈，B的圈和A的圈可能重复可能不重复，需求是一样的；</li>\n<li>用户C也想使用这个服务，不过他觉得画个圈不符合他的要求，他要在地图上随便画100个什么图形，然后给他他想要的数据；</li>\n<li>用户D也想使用这个服务，不过他不想要知道这个图形里的人的细节，而是只想实时知道这个图形当下有多少人。</li>\n</ol>\n<p><strong>本文只关注返回区域实时人数的需求：</strong></p>\n<p>需求分为两大类，第一类是返回刚进入这个区域的人（返回明细），这个实现起来比较简单，因为它没有窗口的概念，只需要有数据过来，通过redis和mysql，经过一系列计算，判断它在景区并且是新来的人的话，直接推送到下游就好了；<br>第二个需求是实时返回这个区域的当前人数（返回人数）。这个听起来好像挺简单，比如当前上海迪士尼有56000个人，就给订阅者 56000这个数字就好了，但仔细思考一下就会发现较难实现，主要原因如下：</p>\n<ol>\n<li>实时上传过来的数据并不是全量数据，如果以5s为一批，这批上传的数据可能只占全国总量2亿用户中的100W条，这样就导致无法在一批数据中就得到结果；</li>\n<li>用户上传数据的频率也是不一致的，有的可能一分钟会发多条，有的可能四五十分钟才会上传一条，有的干脆好久都不会上传；</li>\n<li>多个订阅者同时订阅多个景区，每个订阅者的数据都要是对的；</li>\n<li>数据量太大，导致难以用时间窗口把它装住，数据还是得流进来，五秒内就得流出去；</li>\n</ol>\n<h2 id=\"一些简单可能的方案\"><a href=\"#一些简单可能的方案\" class=\"headerlink\" title=\"一些简单可能的方案\"></a>一些简单可能的方案</h2><p>为了满足上述需求，之前提出过一些方案：</p>\n<ol>\n<li>每次处理一个小时的数据，因为大概一个小时以内，所有的用户基本都会上传一遍数据，这样通过去重就能知道上个小时该区域的人数的，但这是相当于批处理，没有实时性，是最差的选择了；</li>\n<li>使用时间窗口，维护所有用户当前的位置信息，每隔一段时间计算，然后输出（类型与Apache Beam的定义模式），这个由于数据量太大，难以实现，因为有其它方案，所以也没有尝试；</li>\n<li>使用外部的一个数据库，维护所有用户的当前位置信息，然后每次计算，输出；如果用户量在千万级别以下的话，觉得用Mysql就可以了，但是2亿的用户，Mysql难以胜任，如果用Hbase或redis保存所有人状态的话，的话，每次统计计算多个区域的人数信息需要的计算量也很大，后来也没有尝试；</li>\n</ol>\n<h2 id=\"基于双层redis的方案\"><a href=\"#基于双层redis的方案\" class=\"headerlink\" title=\"基于双层redis的方案\"></a>基于双层redis的方案</h2><h3 id=\"方案思路\"><a href=\"#方案思路\" class=\"headerlink\" title=\"方案思路\"></a>方案思路</h3><p>首先引入两个状态：用户这次是否在景区 thisTime 和 用户上次是否在景区 lastTime，基于这两个状态，可以得到如下表格</p>\n<table>\n<thead>\n<tr>\n<th>用户位置判断</th>\n<th>本次是否在该区域</th>\n<th>上次是否在该区域</th>\n<th>处理逻辑</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>情况1</td>\n<td>0</td>\n<td>0</td>\n<td>不处理</td>\n</tr>\n<tr>\n<td>情况2</td>\n<td>0</td>\n<td>1</td>\n<td>原子删除（一级总表和二级表）</td>\n</tr>\n<tr>\n<td>情况3</td>\n<td>1</td>\n<td>0</td>\n<td>原子增加（一级总表和二级表）</td>\n</tr>\n<tr>\n<td>情况4</td>\n<td>1</td>\n<td>1</td>\n<td>不处理</td>\n</tr>\n</tbody>\n</table>\n<p>如上表格所示，每个人都维护这次与上次两个状态，如果这个人这次不在该区域，上次也不在该区域，说明什么都不需要做，如果这个人这次不在该区域，上次在，说明这个人离开了这个区域，则就要把它从该区域中剔除；同理可得，如果这次在，上次不在，就把该用户加到该区域。这样一来，我们就可以对每个区域的人了如指掌，而且在理论上，这是目前位置最精确的判断方法。</p>\n<h3 id=\"可行性\"><a href=\"#可行性\" class=\"headerlink\" title=\"可行性\"></a>可行性</h3><p>这套方案在这个问题上是可行的，虽然用户的位置轨迹类似于用户登陆网页的轨迹，但是又有极大的不同，因为每个人虽然都可离开这个网站，但是每个人都不会跳出地球！如果是网站的话，我只知道你访问了我这个网页所做的操作，但我不知道你什么时候没有访问我的网页，比如你从baidu.com跑到了google，baidu的数据里是不知道的，但你从这个区域跑到另外一个区域，我们的数据里是知道的，这就让该方案可行。</p>\n<h3 id=\"实现方案\"><a href=\"#实现方案\" class=\"headerlink\" title=\"实现方案\"></a>实现方案</h3><p>如下图右侧是实现方案的简化版，我们使用了双层redis的方法实现了这个思路，第一层redis的数据结构为hash，订阅者id(spId)命名(当然实际名称多了些前缀)，key是电话号码msisdn，value是它对应的区域id(locationId)，第二层redis的数据结构为Set，以区域id(locationId)命名，里面直接是 msisdn。<br><img src=\"/2017/06/29/返回区域实时人数的思路与总结/wzfw2_2.png\" alt=\"wzfw2_2.png\" title=\"\"><br>判断本次是否在景区，直接根据位置算法进行计算就可以知道了，判断上次是否在景区，只需要循环去第一层redis中查有没有这个msisdn，如果有就返回它的locationId，如果没有，就说明该用户不在任何景区。如果要往该景区增加这个人，则需要同时往一级redis和二级对应的某一个redis中同时增加，同理，删除的话同时删除。值得注意的是，redis本身支持的事务并不支持回滚，所以还是要自己去控制它，通过返回值来进行原子删除和增加。上述操作的时间复杂度均为O(1)，道理上讲还是很快的。然后每个set都代表一个景区，想知道它的人数，so easy，O(1)的时间复杂度就能搞定了，连具体是谁都能列出来。</p>\n<p>当然具体的代码实现就繁杂很多了：</p>\n<ol>\n<li>因为是不同的订阅者订阅好多景区，一个用户可能同时在多个订阅者订阅的好多个景区，这就要循环判断很多，会存在RDD的一条输入，输出是多条的情况；</li>\n<li>除了上述四种情况，还有几种情况需要考虑到，比如这个人从这个区域跑到同一个订阅者订阅的另一个区域的话，就需要使用 redis set的move操作，同时改变一级hash表的值。如果跑到另一个订阅者的另一个区域的话，又会有它相应的逻辑。<br>但大部分情况下还是以上四种逻辑，毕竟中国很大，订阅的区域间距还是比较远的。</li>\n<li>为了防止各种原因导致redis中脏数据的产生，在存入的时候会加一个时间戳，如果时间过长，就会将其中的数据删除。<br>实现逻辑大概代码流程图如下：<img src=\"/2017/06/29/返回区域实时人数的思路与总结/wzfw2_3.png\" alt=\"wzfw2_3.png\" title=\"\">\n</li>\n</ol>\n<p>为什么要以双层redis的方法呢？单层的不行嘛？由于我们的数据量非常大，如果仅仅是单层的hash的话，每次去计算某个景区的人数，又要把所有的数据全部遍历一遍，这个性能消耗是非常大的，这里采用空间换时间的方法，用极少的空间，节省了大量的时间。说白了，如果数据量小的话，用mysql之类的关系型数据库最好了，查的时候只要类似于<code>select count(*) from spId where locationId = locationA</code>之类的语句就可以了。使用Hbase能不能达到这个要求呢？个人没有进行过更深入的调研，浅薄地认为因为hbase的数据本身是存在hdfs的，如果是全量数据的话，它缓存命中的概率是比较低的，这样延迟会比较大，那么大的数据，难以秒级全部响应，所以就再也没有深入调研。</p>\n<h3 id=\"问题与提高\"><a href=\"#问题与提高\" class=\"headerlink\" title=\"问题与提高\"></a>问题与提高</h3><p>这个方案，理论上的结果是很精确的，但是它有一个小问题，就是在判断用户上次是否在该区域的时候，“在判断该用户上次是否在该区域之前，我们是不知道他上次是否在某个区域的”，这话听起来比较拗口，换一种方式表达：对于全国每一个用户的每一条数据，我们都要判断它上次是否在某个区域，这代表20W/s的redis读！当然我这边会做一些优化，先根据城市id进行过滤，仅仅读相关城市的数据，不过这个数据量也是比较大的，也到了1W/s的级别。</p>\n<p>实际生产压测过程中，当全量数据上来后，由于redis的每次连接导致网络IO过多，响应会变慢，导致程序会逐渐产生延迟，也影响到了稳定运行的明细数据需求，所以存在问题！</p>\n<p>由于我们的redis是与其它需求共用的，并且当时为了稳定使用的sentinel模式，也难以在有限的几天内就申请到cluster模式的redis集群，也难以使用pipeline来优化，还有几天就要上线了。所以提出了下面的基于数据过期的模糊方案来进行过渡。</p>\n<h2 id=\"基于数据过期的模糊方案\"><a href=\"#基于数据过期的模糊方案\" class=\"headerlink\" title=\"基于数据过期的模糊方案\"></a>基于数据过期的模糊方案</h2><p>提出模糊方案的一个前提是，用户上传的数据本来就是模糊的，如果一个用户半个小时上传一次他的位置数据，我们无法得知他在这半个小时之间的位置，也无法得知他下半个小时在哪里</p>\n<h3 id=\"方案思路-1\"><a href=\"#方案思路-1\" class=\"headerlink\" title=\"方案思路\"></a>方案思路</h3><p>用户每次进入某个区域后，就将它存入该区域集合中，不用考虑其它，区域中的数据是有时效的，超过30min(可调)后，区域中的数据就会被直接删除。</p>\n<h3 id=\"实现方案-1\"><a href=\"#实现方案-1\" class=\"headerlink\" title=\"实现方案\"></a>实现方案</h3><p>对于每一个位置 locationId，我们使用redis的 sorted set数据结构，里面保存的值是msisdn，里面的score值是当前时间戳精确到分钟。它插入(zadd)的时间复杂度是O(log(N))，移除(ZREMRANGEBYSCORE)的时间复杂度是 O(log(N)+M)。每次有符合条件的数据，我们就直接插入到相应的locationId，然后异步地通过外部程序，对其中过期超过30min(可调)的数据进行批量删除，实际中这个删除是在spark Streaming的driver中实现每10s删除一次。</p>\n<p>该方案虽然在插入时时间复杂度有所增加，但是极大得简化了redis的连接次数，ZREMRANGEBYSCORE操作也只是统一每隔10s执行一次，最终程序稳定运行在了生产。</p>\n<h3 id=\"问题与提高-1\"><a href=\"#问题与提高-1\" class=\"headerlink\" title=\"问题与提高\"></a>问题与提高</h3><p>该方案的优点是简单易行，问题是它并不像上一个方案一样是理论上精确的，但由于原始数据并不精确，所以有它存在的道理。<br>关于超时删除时间，这个时间是可以调优的，调优方案很简单，就一个变量而已，通过批处理的方式对区域的数据进行精确的统计，然后对比两者的结果，最终找出一个最靠谱的超时删除时间。<br>当然，区域的订阅面积越大，redis的响应就越慢，所以当订阅面积大到一定程度，redis肯定也是扛不住的。</p>\n<h2 id=\"总结与思考\"><a href=\"#总结与思考\" class=\"headerlink\" title=\"总结与思考\"></a>总结与思考</h2><ol>\n<li>判断区域人数是一个比较通用的需求，比如判断当前网站在线用户数，当前频道观众，但是具体实现起来的确觉得不那么简单，我目前能想到的算法就这两个，总觉得会有更好的方法，不同的问题背景产生不同的方法；</li>\n<li>我们设计与实现双层redis方法的时候，没有考虑到redis网络IO过多导致延迟的问题，以后一定要评估好性能瓶颈，提前申请机器；</li>\n<li>有时候追求完美不是好事，完美得向现实妥协，最优的算法并不是最适合的，要在满足需求的基础上，以最少的代码完成这个事情，增加鲁棒性；</li>\n<li>之前对时间窗口相关的不太深入，接下来深入理解一下structured Streaming与Apache Beam的API的逻辑；</li>\n</ol>\n","excerpt":"","more":"<h2 id=\"问题背景\"><a href=\"#问题背景\" class=\"headerlink\" title=\"问题背景\"></a>问题背景</h2><p>先说一下整个位置服务的背景，需求比较复杂，细节比较多，在这里简明扼要说一下：</p>\n<ol>\n<li>用户A(spId)在中国地图上画了100个圆圈(locationId)，想让我们实时告诉他这些圈内新来的人和他的位置信息，哪些是游客哪些是土著；</li>\n<li>A的100个圆圈会随时修改，增大，变小，或者再多加100个圈圈，我们要不停程序，秒级给他这些数据；</li>\n<li>A觉得这样推送的数据量太大，因为有的人本来就住这个圈里，所以对每个不同的圈，里面的人在 7天（可调）之内不重复；</li>\n<li>用户B也想使用这个服务，在中国地图上画了100个圈，B的圈和A的圈可能重复可能不重复，需求是一样的；</li>\n<li>用户C也想使用这个服务，不过他觉得画个圈不符合他的要求，他要在地图上随便画100个什么图形，然后给他他想要的数据；</li>\n<li>用户D也想使用这个服务，不过他不想要知道这个图形里的人的细节，而是只想实时知道这个图形当下有多少人。</li>\n</ol>\n<p><strong>本文只关注返回区域实时人数的需求：</strong></p>\n<p>需求分为两大类，第一类是返回刚进入这个区域的人（返回明细），这个实现起来比较简单，因为它没有窗口的概念，只需要有数据过来，通过redis和mysql，经过一系列计算，判断它在景区并且是新来的人的话，直接推送到下游就好了；<br>第二个需求是实时返回这个区域的当前人数（返回人数）。这个听起来好像挺简单，比如当前上海迪士尼有56000个人，就给订阅者 56000这个数字就好了，但仔细思考一下就会发现较难实现，主要原因如下：</p>\n<ol>\n<li>实时上传过来的数据并不是全量数据，如果以5s为一批，这批上传的数据可能只占全国总量2亿用户中的100W条，这样就导致无法在一批数据中就得到结果；</li>\n<li>用户上传数据的频率也是不一致的，有的可能一分钟会发多条，有的可能四五十分钟才会上传一条，有的干脆好久都不会上传；</li>\n<li>多个订阅者同时订阅多个景区，每个订阅者的数据都要是对的；</li>\n<li>数据量太大，导致难以用时间窗口把它装住，数据还是得流进来，五秒内就得流出去；</li>\n</ol>\n<h2 id=\"一些简单可能的方案\"><a href=\"#一些简单可能的方案\" class=\"headerlink\" title=\"一些简单可能的方案\"></a>一些简单可能的方案</h2><p>为了满足上述需求，之前提出过一些方案：</p>\n<ol>\n<li>每次处理一个小时的数据，因为大概一个小时以内，所有的用户基本都会上传一遍数据，这样通过去重就能知道上个小时该区域的人数的，但这是相当于批处理，没有实时性，是最差的选择了；</li>\n<li>使用时间窗口，维护所有用户当前的位置信息，每隔一段时间计算，然后输出（类型与Apache Beam的定义模式），这个由于数据量太大，难以实现，因为有其它方案，所以也没有尝试；</li>\n<li>使用外部的一个数据库，维护所有用户的当前位置信息，然后每次计算，输出；如果用户量在千万级别以下的话，觉得用Mysql就可以了，但是2亿的用户，Mysql难以胜任，如果用Hbase或redis保存所有人状态的话，的话，每次统计计算多个区域的人数信息需要的计算量也很大，后来也没有尝试；</li>\n</ol>\n<h2 id=\"基于双层redis的方案\"><a href=\"#基于双层redis的方案\" class=\"headerlink\" title=\"基于双层redis的方案\"></a>基于双层redis的方案</h2><h3 id=\"方案思路\"><a href=\"#方案思路\" class=\"headerlink\" title=\"方案思路\"></a>方案思路</h3><p>首先引入两个状态：用户这次是否在景区 thisTime 和 用户上次是否在景区 lastTime，基于这两个状态，可以得到如下表格</p>\n<table>\n<thead>\n<tr>\n<th>用户位置判断</th>\n<th>本次是否在该区域</th>\n<th>上次是否在该区域</th>\n<th>处理逻辑</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>情况1</td>\n<td>0</td>\n<td>0</td>\n<td>不处理</td>\n</tr>\n<tr>\n<td>情况2</td>\n<td>0</td>\n<td>1</td>\n<td>原子删除（一级总表和二级表）</td>\n</tr>\n<tr>\n<td>情况3</td>\n<td>1</td>\n<td>0</td>\n<td>原子增加（一级总表和二级表）</td>\n</tr>\n<tr>\n<td>情况4</td>\n<td>1</td>\n<td>1</td>\n<td>不处理</td>\n</tr>\n</tbody>\n</table>\n<p>如上表格所示，每个人都维护这次与上次两个状态，如果这个人这次不在该区域，上次也不在该区域，说明什么都不需要做，如果这个人这次不在该区域，上次在，说明这个人离开了这个区域，则就要把它从该区域中剔除；同理可得，如果这次在，上次不在，就把该用户加到该区域。这样一来，我们就可以对每个区域的人了如指掌，而且在理论上，这是目前位置最精确的判断方法。</p>\n<h3 id=\"可行性\"><a href=\"#可行性\" class=\"headerlink\" title=\"可行性\"></a>可行性</h3><p>这套方案在这个问题上是可行的，虽然用户的位置轨迹类似于用户登陆网页的轨迹，但是又有极大的不同，因为每个人虽然都可离开这个网站，但是每个人都不会跳出地球！如果是网站的话，我只知道你访问了我这个网页所做的操作，但我不知道你什么时候没有访问我的网页，比如你从baidu.com跑到了google，baidu的数据里是不知道的，但你从这个区域跑到另外一个区域，我们的数据里是知道的，这就让该方案可行。</p>\n<h3 id=\"实现方案\"><a href=\"#实现方案\" class=\"headerlink\" title=\"实现方案\"></a>实现方案</h3><p>如下图右侧是实现方案的简化版，我们使用了双层redis的方法实现了这个思路，第一层redis的数据结构为hash，订阅者id(spId)命名(当然实际名称多了些前缀)，key是电话号码msisdn，value是它对应的区域id(locationId)，第二层redis的数据结构为Set，以区域id(locationId)命名，里面直接是 msisdn。<br><img src=\"/2017/06/29/返回区域实时人数的思路与总结/wzfw2_2.png\" alt=\"wzfw2_2.png\" title=\"\"><br>判断本次是否在景区，直接根据位置算法进行计算就可以知道了，判断上次是否在景区，只需要循环去第一层redis中查有没有这个msisdn，如果有就返回它的locationId，如果没有，就说明该用户不在任何景区。如果要往该景区增加这个人，则需要同时往一级redis和二级对应的某一个redis中同时增加，同理，删除的话同时删除。值得注意的是，redis本身支持的事务并不支持回滚，所以还是要自己去控制它，通过返回值来进行原子删除和增加。上述操作的时间复杂度均为O(1)，道理上讲还是很快的。然后每个set都代表一个景区，想知道它的人数，so easy，O(1)的时间复杂度就能搞定了，连具体是谁都能列出来。</p>\n<p>当然具体的代码实现就繁杂很多了：</p>\n<ol>\n<li>因为是不同的订阅者订阅好多景区，一个用户可能同时在多个订阅者订阅的好多个景区，这就要循环判断很多，会存在RDD的一条输入，输出是多条的情况；</li>\n<li>除了上述四种情况，还有几种情况需要考虑到，比如这个人从这个区域跑到同一个订阅者订阅的另一个区域的话，就需要使用 redis set的move操作，同时改变一级hash表的值。如果跑到另一个订阅者的另一个区域的话，又会有它相应的逻辑。<br>但大部分情况下还是以上四种逻辑，毕竟中国很大，订阅的区域间距还是比较远的。</li>\n<li>为了防止各种原因导致redis中脏数据的产生，在存入的时候会加一个时间戳，如果时间过长，就会将其中的数据删除。<br>实现逻辑大概代码流程图如下：<img src=\"/2017/06/29/返回区域实时人数的思路与总结/wzfw2_3.png\" alt=\"wzfw2_3.png\" title=\"\">\n</li>\n</ol>\n<p>为什么要以双层redis的方法呢？单层的不行嘛？由于我们的数据量非常大，如果仅仅是单层的hash的话，每次去计算某个景区的人数，又要把所有的数据全部遍历一遍，这个性能消耗是非常大的，这里采用空间换时间的方法，用极少的空间，节省了大量的时间。说白了，如果数据量小的话，用mysql之类的关系型数据库最好了，查的时候只要类似于<code>select count(*) from spId where locationId = locationA</code>之类的语句就可以了。使用Hbase能不能达到这个要求呢？个人没有进行过更深入的调研，浅薄地认为因为hbase的数据本身是存在hdfs的，如果是全量数据的话，它缓存命中的概率是比较低的，这样延迟会比较大，那么大的数据，难以秒级全部响应，所以就再也没有深入调研。</p>\n<h3 id=\"问题与提高\"><a href=\"#问题与提高\" class=\"headerlink\" title=\"问题与提高\"></a>问题与提高</h3><p>这个方案，理论上的结果是很精确的，但是它有一个小问题，就是在判断用户上次是否在该区域的时候，“在判断该用户上次是否在该区域之前，我们是不知道他上次是否在某个区域的”，这话听起来比较拗口，换一种方式表达：对于全国每一个用户的每一条数据，我们都要判断它上次是否在某个区域，这代表20W/s的redis读！当然我这边会做一些优化，先根据城市id进行过滤，仅仅读相关城市的数据，不过这个数据量也是比较大的，也到了1W/s的级别。</p>\n<p>实际生产压测过程中，当全量数据上来后，由于redis的每次连接导致网络IO过多，响应会变慢，导致程序会逐渐产生延迟，也影响到了稳定运行的明细数据需求，所以存在问题！</p>\n<p>由于我们的redis是与其它需求共用的，并且当时为了稳定使用的sentinel模式，也难以在有限的几天内就申请到cluster模式的redis集群，也难以使用pipeline来优化，还有几天就要上线了。所以提出了下面的基于数据过期的模糊方案来进行过渡。</p>\n<h2 id=\"基于数据过期的模糊方案\"><a href=\"#基于数据过期的模糊方案\" class=\"headerlink\" title=\"基于数据过期的模糊方案\"></a>基于数据过期的模糊方案</h2><p>提出模糊方案的一个前提是，用户上传的数据本来就是模糊的，如果一个用户半个小时上传一次他的位置数据，我们无法得知他在这半个小时之间的位置，也无法得知他下半个小时在哪里</p>\n<h3 id=\"方案思路-1\"><a href=\"#方案思路-1\" class=\"headerlink\" title=\"方案思路\"></a>方案思路</h3><p>用户每次进入某个区域后，就将它存入该区域集合中，不用考虑其它，区域中的数据是有时效的，超过30min(可调)后，区域中的数据就会被直接删除。</p>\n<h3 id=\"实现方案-1\"><a href=\"#实现方案-1\" class=\"headerlink\" title=\"实现方案\"></a>实现方案</h3><p>对于每一个位置 locationId，我们使用redis的 sorted set数据结构，里面保存的值是msisdn，里面的score值是当前时间戳精确到分钟。它插入(zadd)的时间复杂度是O(log(N))，移除(ZREMRANGEBYSCORE)的时间复杂度是 O(log(N)+M)。每次有符合条件的数据，我们就直接插入到相应的locationId，然后异步地通过外部程序，对其中过期超过30min(可调)的数据进行批量删除，实际中这个删除是在spark Streaming的driver中实现每10s删除一次。</p>\n<p>该方案虽然在插入时时间复杂度有所增加，但是极大得简化了redis的连接次数，ZREMRANGEBYSCORE操作也只是统一每隔10s执行一次，最终程序稳定运行在了生产。</p>\n<h3 id=\"问题与提高-1\"><a href=\"#问题与提高-1\" class=\"headerlink\" title=\"问题与提高\"></a>问题与提高</h3><p>该方案的优点是简单易行，问题是它并不像上一个方案一样是理论上精确的，但由于原始数据并不精确，所以有它存在的道理。<br>关于超时删除时间，这个时间是可以调优的，调优方案很简单，就一个变量而已，通过批处理的方式对区域的数据进行精确的统计，然后对比两者的结果，最终找出一个最靠谱的超时删除时间。<br>当然，区域的订阅面积越大，redis的响应就越慢，所以当订阅面积大到一定程度，redis肯定也是扛不住的。</p>\n<h2 id=\"总结与思考\"><a href=\"#总结与思考\" class=\"headerlink\" title=\"总结与思考\"></a>总结与思考</h2><ol>\n<li>判断区域人数是一个比较通用的需求，比如判断当前网站在线用户数，当前频道观众，但是具体实现起来的确觉得不那么简单，我目前能想到的算法就这两个，总觉得会有更好的方法，不同的问题背景产生不同的方法；</li>\n<li>我们设计与实现双层redis方法的时候，没有考虑到redis网络IO过多导致延迟的问题，以后一定要评估好性能瓶颈，提前申请机器；</li>\n<li>有时候追求完美不是好事，完美得向现实妥协，最优的算法并不是最适合的，要在满足需求的基础上，以最少的代码完成这个事情，增加鲁棒性；</li>\n<li>之前对时间窗口相关的不太深入，接下来深入理解一下structured Streaming与Apache Beam的API的逻辑；</li>\n</ol>\n"},{"title":"菜鸟如何单排上王者——背景","toc":true,"date":"2018-08-17T09:06:37.000Z","_content":"\n佛为心，道为骨，儒为表，大度看队友；\n技在手，能在胸，思在脑，从容上王者。\n\n# 背景\n我是一个菜鸟，一开始接触这个游戏主要是跟朋友一起娱乐，认真玩是三个赛季前，在星耀段位沉浮了两个赛季后，上赛季单排上了王者。本S12赛季初的半个月后，我平均每天两三把，花了半个多月的世界，用了79场以65%的胜率上了王者，我觉得这是一个还不错的成绩了，相信如果我继续认真玩的话，在王者低分局和巅峰赛也会顺利上分，遂把心得记录一下，希望能给各位朋友一点帮助，也算是这段时间的一个总结。\n{% asset_img wangzhe_1.png %}\n对于90初的我来说，早已经过了游戏的巅峰期，平常有工作和其它事情，也没有时间去刻意练习，铭文只拼凑了一套百穿和半套法师和半套肉铭文，只是当时为了跟同事组件战队花了6块钱，之所以说这么多，只是想给你打气，相信你不用充钱，也不用做大量的练习，看了我的总结，只要勤于思考，也会单排上王者，甚至超越我。\n{% asset_img wangzhe_2.png %}\n## 游戏不是洪水猛兽\n我周围那波人都是从小玩到大的，从最早的掌机到小霸王，再到街机，各种电脑游戏，再到手机游戏，现在我们都成年了，也没有比不玩游戏的人差到哪里去，当然我们也一起玩沙包，打球踢球，一起听歌，下棋打牌啥的，有时候重要的不仅仅是游戏的乐趣，而是跟你一起玩的人。\n现在一些人对游戏存在偏见，好像洪水猛兽，避之不及，就像用惯了马车的人害怕火车一样。其实只要有节制，玩玩游戏还是挺好的，而且王者打排位是一个烧脑的游戏，玩太多了反而不利于上分。\n\n## 玩排位的目的\n我觉得这个是你接下来看我攻略的前提，我玩排位的目的很简单：“赢”！我所有的快感来自于赢，输了比赛，就算拿了败方MVP，我也觉得不爽，而且我很多时候总觉得我们会翻盘，我从来不会点投降，说得最多的一句话就是“稳住，我们能赢”；可能我从小就比较要强，无论打篮球、踢足球亦或下棋，我总是对胜利充满了渴望，我觉得这很多来自于天性。如果想娱乐或者练英雄，我一般会匹配；\n\n如果你玩游戏的第一目的不是赢，更在意表现自己，如果你对胜利没有那么大的渴望，那这篇文章的很多内容并不适合你；\n\n## 这是一个智力游戏\n首先，玩排位很烧脑，它是一个获取各种信息并做出决策的过程，需要专注，我无法以轻松娱乐的心态去玩排位，如果我今天一直在做需要思考的事情，比如看算法和编程，我就不会马上打排位了，如果很累很困，我也不会在这个时候打排位，这个时候可以看看视频啊，或者吃鸡，我反应很慢，玩吃鸡很菜，喜欢开车开船瞎晃，还能看看游戏里的大海和晚霞；\n\n在好的状态，好的网络下玩，真得很重要，就我自己来说，好的状态下如果有70%的胜率的话，差状态下有时候胜率都不足50%；如果状态好，可以玩一些风险高收益大的英雄，比如打野，突进以及需要走位的C位，如果状态差，就优先玩辅助、肉以及后面猥琐丢技能的甄姬姜子牙啥的；\n\n其次，与魔兽争霸需要多线程操作不同，比如打对战的时候，我明明想多路包抄跑狼骑，但操作不过来，总发现外面打架，家里基地忘了升级，这是属于手速跟不上想法，dota有时候也会有类似的情况，用鸡买装备一不小心就把鸡拉到野外了。。。由于王者对操作的要求不高，每个人基本上只要想到就能做到，所以对思路的要求更加纯碎；\n\n我从小喜欢下棋，我觉得王者荣耀的博弈跟古代的下棋有点类似，只不过王者里的变数更多，更实时，更加形象，需要一定的操作，我认为王者就是现代版的博弈，是5V5的博弈；\n\n最后一点，因为这是个智力游戏，所以不要让机械记忆占据意识的上风，今天我打了一把王者局，补位辅助，我们家的猴子前期被我养得很肥，拿了很多人头，但是能看出来是很明显的机械记忆玩家，总是脑子一发热冲上去一打五，拉都拉不住，随后我们就被翻盘了，我觉得这位玩家不是在用脑子打游戏，而是用肌肉在玩游戏，如果我当时能提醒他一下，他如果能冷静下来，锁定目标后进场，就不会输了，回头看了一下他的资料，胜率不足50%；\n\n## 这个游戏不简单\n王者荣耀刚出来的15年我就下载了，不过玩了两天就删了，当时觉得太简单了，没魔兽和dota好玩。以前我们玩魔兽争霸，觉得控制一支军队挺有意思，后来类似真三和dota出来的时候，感觉好简单，和魔兽相比，只需要控制一个英雄就好了，但是玩了一段时间后发现其实内容蛮多的，后来出现了LOL，更简单了，连反补都没有了，回城不需要TP，买东西不需要鸡，感觉少了很多真实性，所以没人玩，我现在很多朋友，也只玩dota不玩王者，都觉得因为简单所以少了很多乐趣；\n然而，这个游戏并不简单。我总结了以下6个阶段，你在哪个阶段呢：\n\n### 1.好简单（萌新：好好玩） \n据我观察，周围以前玩过Dota/LOL的玩家，在刚进入这个游戏的时候，可以轻松上到黄金铂金段位，可能会说：“很简单”，而刚进入Moba类游戏世界的萌新，会感觉好好玩，新手都会在这个阶段熟悉这个游戏规则，任何人都会经过这个阶段而达到阶段2，这个阶段一般集中在白银和黄金段位；\n\n### 2.坑队友 \n当你多打几把后，你的水平没法carry了，“坑队友”成为了主要矛盾，队友也不太靠谱，所以你认为是队友坑了你，导致你段位上不去，如果天美团队能统计一下的话，相信包含“坑队友”ID的人在这个段位是最频繁的，“坑队友”阶段的人一般在黄金铂金段位，赛季末可能会上钻石；\n\n难道真的都是别人坑你吗？非也，其实大家都有比较熟悉的英雄，但是依然在互坑，对面的也在互坑，造成这个问题的主要原因是你不理解你队友，雪上加霜的是，因为你被坑多了所以更加不相信队友，导致明明该赢的局没法使尽全力输了。\n\n其实除了打野位，我在这个段位胜率也不高，而之所以打野位胜率高，是因为我基本上会把对面的某个位置针对到他心态不稳（可能这时候对面吵起来了），很多时候，在我们崩之前，对面比我们先崩了。\n\n而要早日突破这个阶段，应该从“道”上下功夫，做到理解队友的行为，最好是五个位置都玩过，你就能理解很多队友的行为了，多跟队友沟通，让队友舒服；\n\n举个例子：射手拿了一红，一般打野会心态不好，其实你可以反向思维：射手敢拿红，说明在他之前的经验里，拿了红会猛很多，他的射手肯定是不坑的，你就打字：“相信你”。如果你是射手，就不拿一红，让打野不坑，并打字，“打野拿红带我飞”；\n\n### 3.傻逼排位机制 \n当你在阶段2做到了理解队友以后，你的操作已经很好了，也知道每个位置该干什么，你上了钻石，在钻石和星耀之间徘徊，由于周围的人也都会玩了，你会偶尔遇到坑，偶尔也会遇到大神带躺，单排胜率一直在50%左右，靠着勇者积分上分，这已经不是技术问题了；\n\n这时候你在想，是不是排位机制故意针对我？非也，大家都一样，我反问一句：如果五个你在一起，怎么选人？你能打过对面吗？是不是就是你在坑你？\n\n### 4.55%胜率上王者\n你已经很厉害了，有一个位置挺厉害，会根据阵容选人，基本上补的位置也不差，单排上了王者，但是就是容错率比较低，偶尔该赢的局输了，该输的局还是输了，是不是一个赛季能打三四百场甚至更多？你只喜欢打排位，一有时间就打，状态差也打，有时候会连胜，有时候会连败，所以胜率还是在55%以下；\n\n### 5.轻松上王者\n这是我们的目标，你有3、4阶段的技术以及对游戏的理解，你的队友基本上处于阶段3和阶段4，自己从各方面做到位，不是自己carry就是他们带你carry，选出的阵容容错率比较高，赢下该赢得局，拼了该拼的局，可能你打得并不多，但65%胜率以上上王者还是可以的；\n\n### 6.大神单排虐菜\n我觉得这是一些有名主播和职业选手的水平，他们普遍心态很好，需要一定的操作、天赋和长时间的训练，是专业和业余的差距，我等常人是难以企及了，其它阶段我们都可以达到。当然职业战队更厉害了，是整体的战术素养层面，这里只讲单排；\n\n相信大部分玩家在2.3阶段，其实3 4 5阶段的人技术都差不多，主要差距在思路，看了我的思路，轻松上王者，不是梦。\n\n我曾经就是这样的，轻松上到铂金，熟悉了甄姬和庄周就上到了钻石，再认真一点，拿庄周，狄仁杰、甄姬、白起、姜子牙这种英雄，磕磕绊绊上到星耀，但是接下来在星耀二三徘徊了两个赛季，赢一场，输一场，胜率一直无法突破55%，这中间我先后熟悉了哪吒、牛魔、雅典娜、兰陵王。直到上赛季末和这赛季有了思路以后，才轻松上了王者，本来有望70%胜率上去的，可是在星耀一心态有点急躁，导致多输了几场，心态平复后才一鼓作气上了去，在这期间我一度打到上海浦东第1兰陵王，以及上海市第5兰陵王的称号，由于我打得不多，上王者后没啥追求，胜率后来掉下来了，如果我常打的话，某个熟练的英雄保持省级前十应该是可以的；\n\n因为没有暴击铭文，金币也不多，很多英雄一开始就没玩，手速也不够快，所以玩得都是不难的英雄，但是至少每个位置都有两三个拿的出手的英雄，相信这些英雄你也会玩得很好；\n\n\n\n前面讲了这么多背景有点啰嗦，（我还做了删减 /笑哭），但我个人认为背景很重要，有了前提的铺垫，才能有后面的结论。下面分别从心态、道（玄学）、游戏素质、游戏技术、游戏战略以及细节思路六个方面来阐述，总结为一句对联就是：“佛为心，道为骨，儒为表，大度看队友；技在手，能在胸，思在脑，从容上王者。”，其中，每个都很重要，直接关系着能否上王者；我认为技术是一切的基础，你首先要会玩，相信大部分玩家是有技术的，除了技术的基础地位，其它几个方面如果非要排序的话，我个人认为按照重要性优先级排序如下： 技术是基础：佛为心 > 道为骨 > 儒为表 > 能在胸 > 思在脑；\n\n# 佛为心\n心态真的很重要，说多了大家感觉是废话，人人都知道","source":"_posts/2018-08-17-菜鸟如何单排上王者.md","raw":"---\ntitle: 菜鸟如何单排上王者——背景\ntoc: true\ndate: 2018-08-17 17:06:37\ntags: game\ncategories:\n---\n\n佛为心，道为骨，儒为表，大度看队友；\n技在手，能在胸，思在脑，从容上王者。\n\n# 背景\n我是一个菜鸟，一开始接触这个游戏主要是跟朋友一起娱乐，认真玩是三个赛季前，在星耀段位沉浮了两个赛季后，上赛季单排上了王者。本S12赛季初的半个月后，我平均每天两三把，花了半个多月的世界，用了79场以65%的胜率上了王者，我觉得这是一个还不错的成绩了，相信如果我继续认真玩的话，在王者低分局和巅峰赛也会顺利上分，遂把心得记录一下，希望能给各位朋友一点帮助，也算是这段时间的一个总结。\n{% asset_img wangzhe_1.png %}\n对于90初的我来说，早已经过了游戏的巅峰期，平常有工作和其它事情，也没有时间去刻意练习，铭文只拼凑了一套百穿和半套法师和半套肉铭文，只是当时为了跟同事组件战队花了6块钱，之所以说这么多，只是想给你打气，相信你不用充钱，也不用做大量的练习，看了我的总结，只要勤于思考，也会单排上王者，甚至超越我。\n{% asset_img wangzhe_2.png %}\n## 游戏不是洪水猛兽\n我周围那波人都是从小玩到大的，从最早的掌机到小霸王，再到街机，各种电脑游戏，再到手机游戏，现在我们都成年了，也没有比不玩游戏的人差到哪里去，当然我们也一起玩沙包，打球踢球，一起听歌，下棋打牌啥的，有时候重要的不仅仅是游戏的乐趣，而是跟你一起玩的人。\n现在一些人对游戏存在偏见，好像洪水猛兽，避之不及，就像用惯了马车的人害怕火车一样。其实只要有节制，玩玩游戏还是挺好的，而且王者打排位是一个烧脑的游戏，玩太多了反而不利于上分。\n\n## 玩排位的目的\n我觉得这个是你接下来看我攻略的前提，我玩排位的目的很简单：“赢”！我所有的快感来自于赢，输了比赛，就算拿了败方MVP，我也觉得不爽，而且我很多时候总觉得我们会翻盘，我从来不会点投降，说得最多的一句话就是“稳住，我们能赢”；可能我从小就比较要强，无论打篮球、踢足球亦或下棋，我总是对胜利充满了渴望，我觉得这很多来自于天性。如果想娱乐或者练英雄，我一般会匹配；\n\n如果你玩游戏的第一目的不是赢，更在意表现自己，如果你对胜利没有那么大的渴望，那这篇文章的很多内容并不适合你；\n\n## 这是一个智力游戏\n首先，玩排位很烧脑，它是一个获取各种信息并做出决策的过程，需要专注，我无法以轻松娱乐的心态去玩排位，如果我今天一直在做需要思考的事情，比如看算法和编程，我就不会马上打排位了，如果很累很困，我也不会在这个时候打排位，这个时候可以看看视频啊，或者吃鸡，我反应很慢，玩吃鸡很菜，喜欢开车开船瞎晃，还能看看游戏里的大海和晚霞；\n\n在好的状态，好的网络下玩，真得很重要，就我自己来说，好的状态下如果有70%的胜率的话，差状态下有时候胜率都不足50%；如果状态好，可以玩一些风险高收益大的英雄，比如打野，突进以及需要走位的C位，如果状态差，就优先玩辅助、肉以及后面猥琐丢技能的甄姬姜子牙啥的；\n\n其次，与魔兽争霸需要多线程操作不同，比如打对战的时候，我明明想多路包抄跑狼骑，但操作不过来，总发现外面打架，家里基地忘了升级，这是属于手速跟不上想法，dota有时候也会有类似的情况，用鸡买装备一不小心就把鸡拉到野外了。。。由于王者对操作的要求不高，每个人基本上只要想到就能做到，所以对思路的要求更加纯碎；\n\n我从小喜欢下棋，我觉得王者荣耀的博弈跟古代的下棋有点类似，只不过王者里的变数更多，更实时，更加形象，需要一定的操作，我认为王者就是现代版的博弈，是5V5的博弈；\n\n最后一点，因为这是个智力游戏，所以不要让机械记忆占据意识的上风，今天我打了一把王者局，补位辅助，我们家的猴子前期被我养得很肥，拿了很多人头，但是能看出来是很明显的机械记忆玩家，总是脑子一发热冲上去一打五，拉都拉不住，随后我们就被翻盘了，我觉得这位玩家不是在用脑子打游戏，而是用肌肉在玩游戏，如果我当时能提醒他一下，他如果能冷静下来，锁定目标后进场，就不会输了，回头看了一下他的资料，胜率不足50%；\n\n## 这个游戏不简单\n王者荣耀刚出来的15年我就下载了，不过玩了两天就删了，当时觉得太简单了，没魔兽和dota好玩。以前我们玩魔兽争霸，觉得控制一支军队挺有意思，后来类似真三和dota出来的时候，感觉好简单，和魔兽相比，只需要控制一个英雄就好了，但是玩了一段时间后发现其实内容蛮多的，后来出现了LOL，更简单了，连反补都没有了，回城不需要TP，买东西不需要鸡，感觉少了很多真实性，所以没人玩，我现在很多朋友，也只玩dota不玩王者，都觉得因为简单所以少了很多乐趣；\n然而，这个游戏并不简单。我总结了以下6个阶段，你在哪个阶段呢：\n\n### 1.好简单（萌新：好好玩） \n据我观察，周围以前玩过Dota/LOL的玩家，在刚进入这个游戏的时候，可以轻松上到黄金铂金段位，可能会说：“很简单”，而刚进入Moba类游戏世界的萌新，会感觉好好玩，新手都会在这个阶段熟悉这个游戏规则，任何人都会经过这个阶段而达到阶段2，这个阶段一般集中在白银和黄金段位；\n\n### 2.坑队友 \n当你多打几把后，你的水平没法carry了，“坑队友”成为了主要矛盾，队友也不太靠谱，所以你认为是队友坑了你，导致你段位上不去，如果天美团队能统计一下的话，相信包含“坑队友”ID的人在这个段位是最频繁的，“坑队友”阶段的人一般在黄金铂金段位，赛季末可能会上钻石；\n\n难道真的都是别人坑你吗？非也，其实大家都有比较熟悉的英雄，但是依然在互坑，对面的也在互坑，造成这个问题的主要原因是你不理解你队友，雪上加霜的是，因为你被坑多了所以更加不相信队友，导致明明该赢的局没法使尽全力输了。\n\n其实除了打野位，我在这个段位胜率也不高，而之所以打野位胜率高，是因为我基本上会把对面的某个位置针对到他心态不稳（可能这时候对面吵起来了），很多时候，在我们崩之前，对面比我们先崩了。\n\n而要早日突破这个阶段，应该从“道”上下功夫，做到理解队友的行为，最好是五个位置都玩过，你就能理解很多队友的行为了，多跟队友沟通，让队友舒服；\n\n举个例子：射手拿了一红，一般打野会心态不好，其实你可以反向思维：射手敢拿红，说明在他之前的经验里，拿了红会猛很多，他的射手肯定是不坑的，你就打字：“相信你”。如果你是射手，就不拿一红，让打野不坑，并打字，“打野拿红带我飞”；\n\n### 3.傻逼排位机制 \n当你在阶段2做到了理解队友以后，你的操作已经很好了，也知道每个位置该干什么，你上了钻石，在钻石和星耀之间徘徊，由于周围的人也都会玩了，你会偶尔遇到坑，偶尔也会遇到大神带躺，单排胜率一直在50%左右，靠着勇者积分上分，这已经不是技术问题了；\n\n这时候你在想，是不是排位机制故意针对我？非也，大家都一样，我反问一句：如果五个你在一起，怎么选人？你能打过对面吗？是不是就是你在坑你？\n\n### 4.55%胜率上王者\n你已经很厉害了，有一个位置挺厉害，会根据阵容选人，基本上补的位置也不差，单排上了王者，但是就是容错率比较低，偶尔该赢的局输了，该输的局还是输了，是不是一个赛季能打三四百场甚至更多？你只喜欢打排位，一有时间就打，状态差也打，有时候会连胜，有时候会连败，所以胜率还是在55%以下；\n\n### 5.轻松上王者\n这是我们的目标，你有3、4阶段的技术以及对游戏的理解，你的队友基本上处于阶段3和阶段4，自己从各方面做到位，不是自己carry就是他们带你carry，选出的阵容容错率比较高，赢下该赢得局，拼了该拼的局，可能你打得并不多，但65%胜率以上上王者还是可以的；\n\n### 6.大神单排虐菜\n我觉得这是一些有名主播和职业选手的水平，他们普遍心态很好，需要一定的操作、天赋和长时间的训练，是专业和业余的差距，我等常人是难以企及了，其它阶段我们都可以达到。当然职业战队更厉害了，是整体的战术素养层面，这里只讲单排；\n\n相信大部分玩家在2.3阶段，其实3 4 5阶段的人技术都差不多，主要差距在思路，看了我的思路，轻松上王者，不是梦。\n\n我曾经就是这样的，轻松上到铂金，熟悉了甄姬和庄周就上到了钻石，再认真一点，拿庄周，狄仁杰、甄姬、白起、姜子牙这种英雄，磕磕绊绊上到星耀，但是接下来在星耀二三徘徊了两个赛季，赢一场，输一场，胜率一直无法突破55%，这中间我先后熟悉了哪吒、牛魔、雅典娜、兰陵王。直到上赛季末和这赛季有了思路以后，才轻松上了王者，本来有望70%胜率上去的，可是在星耀一心态有点急躁，导致多输了几场，心态平复后才一鼓作气上了去，在这期间我一度打到上海浦东第1兰陵王，以及上海市第5兰陵王的称号，由于我打得不多，上王者后没啥追求，胜率后来掉下来了，如果我常打的话，某个熟练的英雄保持省级前十应该是可以的；\n\n因为没有暴击铭文，金币也不多，很多英雄一开始就没玩，手速也不够快，所以玩得都是不难的英雄，但是至少每个位置都有两三个拿的出手的英雄，相信这些英雄你也会玩得很好；\n\n\n\n前面讲了这么多背景有点啰嗦，（我还做了删减 /笑哭），但我个人认为背景很重要，有了前提的铺垫，才能有后面的结论。下面分别从心态、道（玄学）、游戏素质、游戏技术、游戏战略以及细节思路六个方面来阐述，总结为一句对联就是：“佛为心，道为骨，儒为表，大度看队友；技在手，能在胸，思在脑，从容上王者。”，其中，每个都很重要，直接关系着能否上王者；我认为技术是一切的基础，你首先要会玩，相信大部分玩家是有技术的，除了技术的基础地位，其它几个方面如果非要排序的话，我个人认为按照重要性优先级排序如下： 技术是基础：佛为心 > 道为骨 > 儒为表 > 能在胸 > 思在脑；\n\n# 佛为心\n心态真的很重要，说多了大家感觉是废话，人人都知道","slug":"菜鸟如何单排上王者","published":1,"updated":"2018-08-28T10:36:20.222Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfxs003wpsgux6l2n88f","content":"<p>佛为心，道为骨，儒为表，大度看队友；<br>技在手，能在胸，思在脑，从容上王者。</p>\n<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><p>我是一个菜鸟，一开始接触这个游戏主要是跟朋友一起娱乐，认真玩是三个赛季前，在星耀段位沉浮了两个赛季后，上赛季单排上了王者。本S12赛季初的半个月后，我平均每天两三把，花了半个多月的世界，用了79场以65%的胜率上了王者，我觉得这是一个还不错的成绩了，相信如果我继续认真玩的话，在王者低分局和巅峰赛也会顺利上分，遂把心得记录一下，希望能给各位朋友一点帮助，也算是这段时间的一个总结。<br><img src=\"/2018/08/17/菜鸟如何单排上王者/wangzhe_1.png\" alt=\"wangzhe_1.png\" title=\"\"><br>对于90初的我来说，早已经过了游戏的巅峰期，平常有工作和其它事情，也没有时间去刻意练习，铭文只拼凑了一套百穿和半套法师和半套肉铭文，只是当时为了跟同事组件战队花了6块钱，之所以说这么多，只是想给你打气，相信你不用充钱，也不用做大量的练习，看了我的总结，只要勤于思考，也会单排上王者，甚至超越我。<br><img src=\"/2018/08/17/菜鸟如何单排上王者/wangzhe_2.png\" alt=\"wangzhe_2.png\" title=\"\"></p>\n<h2 id=\"游戏不是洪水猛兽\"><a href=\"#游戏不是洪水猛兽\" class=\"headerlink\" title=\"游戏不是洪水猛兽\"></a>游戏不是洪水猛兽</h2><p>我周围那波人都是从小玩到大的，从最早的掌机到小霸王，再到街机，各种电脑游戏，再到手机游戏，现在我们都成年了，也没有比不玩游戏的人差到哪里去，当然我们也一起玩沙包，打球踢球，一起听歌，下棋打牌啥的，有时候重要的不仅仅是游戏的乐趣，而是跟你一起玩的人。<br>现在一些人对游戏存在偏见，好像洪水猛兽，避之不及，就像用惯了马车的人害怕火车一样。其实只要有节制，玩玩游戏还是挺好的，而且王者打排位是一个烧脑的游戏，玩太多了反而不利于上分。</p>\n<h2 id=\"玩排位的目的\"><a href=\"#玩排位的目的\" class=\"headerlink\" title=\"玩排位的目的\"></a>玩排位的目的</h2><p>我觉得这个是你接下来看我攻略的前提，我玩排位的目的很简单：“赢”！我所有的快感来自于赢，输了比赛，就算拿了败方MVP，我也觉得不爽，而且我很多时候总觉得我们会翻盘，我从来不会点投降，说得最多的一句话就是“稳住，我们能赢”；可能我从小就比较要强，无论打篮球、踢足球亦或下棋，我总是对胜利充满了渴望，我觉得这很多来自于天性。如果想娱乐或者练英雄，我一般会匹配；</p>\n<p>如果你玩游戏的第一目的不是赢，更在意表现自己，如果你对胜利没有那么大的渴望，那这篇文章的很多内容并不适合你；</p>\n<h2 id=\"这是一个智力游戏\"><a href=\"#这是一个智力游戏\" class=\"headerlink\" title=\"这是一个智力游戏\"></a>这是一个智力游戏</h2><p>首先，玩排位很烧脑，它是一个获取各种信息并做出决策的过程，需要专注，我无法以轻松娱乐的心态去玩排位，如果我今天一直在做需要思考的事情，比如看算法和编程，我就不会马上打排位了，如果很累很困，我也不会在这个时候打排位，这个时候可以看看视频啊，或者吃鸡，我反应很慢，玩吃鸡很菜，喜欢开车开船瞎晃，还能看看游戏里的大海和晚霞；</p>\n<p>在好的状态，好的网络下玩，真得很重要，就我自己来说，好的状态下如果有70%的胜率的话，差状态下有时候胜率都不足50%；如果状态好，可以玩一些风险高收益大的英雄，比如打野，突进以及需要走位的C位，如果状态差，就优先玩辅助、肉以及后面猥琐丢技能的甄姬姜子牙啥的；</p>\n<p>其次，与魔兽争霸需要多线程操作不同，比如打对战的时候，我明明想多路包抄跑狼骑，但操作不过来，总发现外面打架，家里基地忘了升级，这是属于手速跟不上想法，dota有时候也会有类似的情况，用鸡买装备一不小心就把鸡拉到野外了。。。由于王者对操作的要求不高，每个人基本上只要想到就能做到，所以对思路的要求更加纯碎；</p>\n<p>我从小喜欢下棋，我觉得王者荣耀的博弈跟古代的下棋有点类似，只不过王者里的变数更多，更实时，更加形象，需要一定的操作，我认为王者就是现代版的博弈，是5V5的博弈；</p>\n<p>最后一点，因为这是个智力游戏，所以不要让机械记忆占据意识的上风，今天我打了一把王者局，补位辅助，我们家的猴子前期被我养得很肥，拿了很多人头，但是能看出来是很明显的机械记忆玩家，总是脑子一发热冲上去一打五，拉都拉不住，随后我们就被翻盘了，我觉得这位玩家不是在用脑子打游戏，而是用肌肉在玩游戏，如果我当时能提醒他一下，他如果能冷静下来，锁定目标后进场，就不会输了，回头看了一下他的资料，胜率不足50%；</p>\n<h2 id=\"这个游戏不简单\"><a href=\"#这个游戏不简单\" class=\"headerlink\" title=\"这个游戏不简单\"></a>这个游戏不简单</h2><p>王者荣耀刚出来的15年我就下载了，不过玩了两天就删了，当时觉得太简单了，没魔兽和dota好玩。以前我们玩魔兽争霸，觉得控制一支军队挺有意思，后来类似真三和dota出来的时候，感觉好简单，和魔兽相比，只需要控制一个英雄就好了，但是玩了一段时间后发现其实内容蛮多的，后来出现了LOL，更简单了，连反补都没有了，回城不需要TP，买东西不需要鸡，感觉少了很多真实性，所以没人玩，我现在很多朋友，也只玩dota不玩王者，都觉得因为简单所以少了很多乐趣；<br>然而，这个游戏并不简单。我总结了以下6个阶段，你在哪个阶段呢：</p>\n<h3 id=\"1-好简单（萌新：好好玩）\"><a href=\"#1-好简单（萌新：好好玩）\" class=\"headerlink\" title=\"1.好简单（萌新：好好玩）\"></a>1.好简单（萌新：好好玩）</h3><p>据我观察，周围以前玩过Dota/LOL的玩家，在刚进入这个游戏的时候，可以轻松上到黄金铂金段位，可能会说：“很简单”，而刚进入Moba类游戏世界的萌新，会感觉好好玩，新手都会在这个阶段熟悉这个游戏规则，任何人都会经过这个阶段而达到阶段2，这个阶段一般集中在白银和黄金段位；</p>\n<h3 id=\"2-坑队友\"><a href=\"#2-坑队友\" class=\"headerlink\" title=\"2.坑队友\"></a>2.坑队友</h3><p>当你多打几把后，你的水平没法carry了，“坑队友”成为了主要矛盾，队友也不太靠谱，所以你认为是队友坑了你，导致你段位上不去，如果天美团队能统计一下的话，相信包含“坑队友”ID的人在这个段位是最频繁的，“坑队友”阶段的人一般在黄金铂金段位，赛季末可能会上钻石；</p>\n<p>难道真的都是别人坑你吗？非也，其实大家都有比较熟悉的英雄，但是依然在互坑，对面的也在互坑，造成这个问题的主要原因是你不理解你队友，雪上加霜的是，因为你被坑多了所以更加不相信队友，导致明明该赢的局没法使尽全力输了。</p>\n<p>其实除了打野位，我在这个段位胜率也不高，而之所以打野位胜率高，是因为我基本上会把对面的某个位置针对到他心态不稳（可能这时候对面吵起来了），很多时候，在我们崩之前，对面比我们先崩了。</p>\n<p>而要早日突破这个阶段，应该从“道”上下功夫，做到理解队友的行为，最好是五个位置都玩过，你就能理解很多队友的行为了，多跟队友沟通，让队友舒服；</p>\n<p>举个例子：射手拿了一红，一般打野会心态不好，其实你可以反向思维：射手敢拿红，说明在他之前的经验里，拿了红会猛很多，他的射手肯定是不坑的，你就打字：“相信你”。如果你是射手，就不拿一红，让打野不坑，并打字，“打野拿红带我飞”；</p>\n<h3 id=\"3-傻逼排位机制\"><a href=\"#3-傻逼排位机制\" class=\"headerlink\" title=\"3.傻逼排位机制\"></a>3.傻逼排位机制</h3><p>当你在阶段2做到了理解队友以后，你的操作已经很好了，也知道每个位置该干什么，你上了钻石，在钻石和星耀之间徘徊，由于周围的人也都会玩了，你会偶尔遇到坑，偶尔也会遇到大神带躺，单排胜率一直在50%左右，靠着勇者积分上分，这已经不是技术问题了；</p>\n<p>这时候你在想，是不是排位机制故意针对我？非也，大家都一样，我反问一句：如果五个你在一起，怎么选人？你能打过对面吗？是不是就是你在坑你？</p>\n<h3 id=\"4-55-胜率上王者\"><a href=\"#4-55-胜率上王者\" class=\"headerlink\" title=\"4.55%胜率上王者\"></a>4.55%胜率上王者</h3><p>你已经很厉害了，有一个位置挺厉害，会根据阵容选人，基本上补的位置也不差，单排上了王者，但是就是容错率比较低，偶尔该赢的局输了，该输的局还是输了，是不是一个赛季能打三四百场甚至更多？你只喜欢打排位，一有时间就打，状态差也打，有时候会连胜，有时候会连败，所以胜率还是在55%以下；</p>\n<h3 id=\"5-轻松上王者\"><a href=\"#5-轻松上王者\" class=\"headerlink\" title=\"5.轻松上王者\"></a>5.轻松上王者</h3><p>这是我们的目标，你有3、4阶段的技术以及对游戏的理解，你的队友基本上处于阶段3和阶段4，自己从各方面做到位，不是自己carry就是他们带你carry，选出的阵容容错率比较高，赢下该赢得局，拼了该拼的局，可能你打得并不多，但65%胜率以上上王者还是可以的；</p>\n<h3 id=\"6-大神单排虐菜\"><a href=\"#6-大神单排虐菜\" class=\"headerlink\" title=\"6.大神单排虐菜\"></a>6.大神单排虐菜</h3><p>我觉得这是一些有名主播和职业选手的水平，他们普遍心态很好，需要一定的操作、天赋和长时间的训练，是专业和业余的差距，我等常人是难以企及了，其它阶段我们都可以达到。当然职业战队更厉害了，是整体的战术素养层面，这里只讲单排；</p>\n<p>相信大部分玩家在2.3阶段，其实3 4 5阶段的人技术都差不多，主要差距在思路，看了我的思路，轻松上王者，不是梦。</p>\n<p>我曾经就是这样的，轻松上到铂金，熟悉了甄姬和庄周就上到了钻石，再认真一点，拿庄周，狄仁杰、甄姬、白起、姜子牙这种英雄，磕磕绊绊上到星耀，但是接下来在星耀二三徘徊了两个赛季，赢一场，输一场，胜率一直无法突破55%，这中间我先后熟悉了哪吒、牛魔、雅典娜、兰陵王。直到上赛季末和这赛季有了思路以后，才轻松上了王者，本来有望70%胜率上去的，可是在星耀一心态有点急躁，导致多输了几场，心态平复后才一鼓作气上了去，在这期间我一度打到上海浦东第1兰陵王，以及上海市第5兰陵王的称号，由于我打得不多，上王者后没啥追求，胜率后来掉下来了，如果我常打的话，某个熟练的英雄保持省级前十应该是可以的；</p>\n<p>因为没有暴击铭文，金币也不多，很多英雄一开始就没玩，手速也不够快，所以玩得都是不难的英雄，但是至少每个位置都有两三个拿的出手的英雄，相信这些英雄你也会玩得很好；</p>\n<p>前面讲了这么多背景有点啰嗦，（我还做了删减 /笑哭），但我个人认为背景很重要，有了前提的铺垫，才能有后面的结论。下面分别从心态、道（玄学）、游戏素质、游戏技术、游戏战略以及细节思路六个方面来阐述，总结为一句对联就是：“佛为心，道为骨，儒为表，大度看队友；技在手，能在胸，思在脑，从容上王者。”，其中，每个都很重要，直接关系着能否上王者；我认为技术是一切的基础，你首先要会玩，相信大部分玩家是有技术的，除了技术的基础地位，其它几个方面如果非要排序的话，我个人认为按照重要性优先级排序如下： 技术是基础：佛为心 &gt; 道为骨 &gt; 儒为表 &gt; 能在胸 &gt; 思在脑；</p>\n<h1 id=\"佛为心\"><a href=\"#佛为心\" class=\"headerlink\" title=\"佛为心\"></a>佛为心</h1><p>心态真的很重要，说多了大家感觉是废话，人人都知道</p>\n","excerpt":"","more":"<p>佛为心，道为骨，儒为表，大度看队友；<br>技在手，能在胸，思在脑，从容上王者。</p>\n<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><p>我是一个菜鸟，一开始接触这个游戏主要是跟朋友一起娱乐，认真玩是三个赛季前，在星耀段位沉浮了两个赛季后，上赛季单排上了王者。本S12赛季初的半个月后，我平均每天两三把，花了半个多月的世界，用了79场以65%的胜率上了王者，我觉得这是一个还不错的成绩了，相信如果我继续认真玩的话，在王者低分局和巅峰赛也会顺利上分，遂把心得记录一下，希望能给各位朋友一点帮助，也算是这段时间的一个总结。<br><img src=\"/2018/08/17/菜鸟如何单排上王者/wangzhe_1.png\" alt=\"wangzhe_1.png\" title=\"\"><br>对于90初的我来说，早已经过了游戏的巅峰期，平常有工作和其它事情，也没有时间去刻意练习，铭文只拼凑了一套百穿和半套法师和半套肉铭文，只是当时为了跟同事组件战队花了6块钱，之所以说这么多，只是想给你打气，相信你不用充钱，也不用做大量的练习，看了我的总结，只要勤于思考，也会单排上王者，甚至超越我。<br><img src=\"/2018/08/17/菜鸟如何单排上王者/wangzhe_2.png\" alt=\"wangzhe_2.png\" title=\"\"></p>\n<h2 id=\"游戏不是洪水猛兽\"><a href=\"#游戏不是洪水猛兽\" class=\"headerlink\" title=\"游戏不是洪水猛兽\"></a>游戏不是洪水猛兽</h2><p>我周围那波人都是从小玩到大的，从最早的掌机到小霸王，再到街机，各种电脑游戏，再到手机游戏，现在我们都成年了，也没有比不玩游戏的人差到哪里去，当然我们也一起玩沙包，打球踢球，一起听歌，下棋打牌啥的，有时候重要的不仅仅是游戏的乐趣，而是跟你一起玩的人。<br>现在一些人对游戏存在偏见，好像洪水猛兽，避之不及，就像用惯了马车的人害怕火车一样。其实只要有节制，玩玩游戏还是挺好的，而且王者打排位是一个烧脑的游戏，玩太多了反而不利于上分。</p>\n<h2 id=\"玩排位的目的\"><a href=\"#玩排位的目的\" class=\"headerlink\" title=\"玩排位的目的\"></a>玩排位的目的</h2><p>我觉得这个是你接下来看我攻略的前提，我玩排位的目的很简单：“赢”！我所有的快感来自于赢，输了比赛，就算拿了败方MVP，我也觉得不爽，而且我很多时候总觉得我们会翻盘，我从来不会点投降，说得最多的一句话就是“稳住，我们能赢”；可能我从小就比较要强，无论打篮球、踢足球亦或下棋，我总是对胜利充满了渴望，我觉得这很多来自于天性。如果想娱乐或者练英雄，我一般会匹配；</p>\n<p>如果你玩游戏的第一目的不是赢，更在意表现自己，如果你对胜利没有那么大的渴望，那这篇文章的很多内容并不适合你；</p>\n<h2 id=\"这是一个智力游戏\"><a href=\"#这是一个智力游戏\" class=\"headerlink\" title=\"这是一个智力游戏\"></a>这是一个智力游戏</h2><p>首先，玩排位很烧脑，它是一个获取各种信息并做出决策的过程，需要专注，我无法以轻松娱乐的心态去玩排位，如果我今天一直在做需要思考的事情，比如看算法和编程，我就不会马上打排位了，如果很累很困，我也不会在这个时候打排位，这个时候可以看看视频啊，或者吃鸡，我反应很慢，玩吃鸡很菜，喜欢开车开船瞎晃，还能看看游戏里的大海和晚霞；</p>\n<p>在好的状态，好的网络下玩，真得很重要，就我自己来说，好的状态下如果有70%的胜率的话，差状态下有时候胜率都不足50%；如果状态好，可以玩一些风险高收益大的英雄，比如打野，突进以及需要走位的C位，如果状态差，就优先玩辅助、肉以及后面猥琐丢技能的甄姬姜子牙啥的；</p>\n<p>其次，与魔兽争霸需要多线程操作不同，比如打对战的时候，我明明想多路包抄跑狼骑，但操作不过来，总发现外面打架，家里基地忘了升级，这是属于手速跟不上想法，dota有时候也会有类似的情况，用鸡买装备一不小心就把鸡拉到野外了。。。由于王者对操作的要求不高，每个人基本上只要想到就能做到，所以对思路的要求更加纯碎；</p>\n<p>我从小喜欢下棋，我觉得王者荣耀的博弈跟古代的下棋有点类似，只不过王者里的变数更多，更实时，更加形象，需要一定的操作，我认为王者就是现代版的博弈，是5V5的博弈；</p>\n<p>最后一点，因为这是个智力游戏，所以不要让机械记忆占据意识的上风，今天我打了一把王者局，补位辅助，我们家的猴子前期被我养得很肥，拿了很多人头，但是能看出来是很明显的机械记忆玩家，总是脑子一发热冲上去一打五，拉都拉不住，随后我们就被翻盘了，我觉得这位玩家不是在用脑子打游戏，而是用肌肉在玩游戏，如果我当时能提醒他一下，他如果能冷静下来，锁定目标后进场，就不会输了，回头看了一下他的资料，胜率不足50%；</p>\n<h2 id=\"这个游戏不简单\"><a href=\"#这个游戏不简单\" class=\"headerlink\" title=\"这个游戏不简单\"></a>这个游戏不简单</h2><p>王者荣耀刚出来的15年我就下载了，不过玩了两天就删了，当时觉得太简单了，没魔兽和dota好玩。以前我们玩魔兽争霸，觉得控制一支军队挺有意思，后来类似真三和dota出来的时候，感觉好简单，和魔兽相比，只需要控制一个英雄就好了，但是玩了一段时间后发现其实内容蛮多的，后来出现了LOL，更简单了，连反补都没有了，回城不需要TP，买东西不需要鸡，感觉少了很多真实性，所以没人玩，我现在很多朋友，也只玩dota不玩王者，都觉得因为简单所以少了很多乐趣；<br>然而，这个游戏并不简单。我总结了以下6个阶段，你在哪个阶段呢：</p>\n<h3 id=\"1-好简单（萌新：好好玩）\"><a href=\"#1-好简单（萌新：好好玩）\" class=\"headerlink\" title=\"1.好简单（萌新：好好玩）\"></a>1.好简单（萌新：好好玩）</h3><p>据我观察，周围以前玩过Dota/LOL的玩家，在刚进入这个游戏的时候，可以轻松上到黄金铂金段位，可能会说：“很简单”，而刚进入Moba类游戏世界的萌新，会感觉好好玩，新手都会在这个阶段熟悉这个游戏规则，任何人都会经过这个阶段而达到阶段2，这个阶段一般集中在白银和黄金段位；</p>\n<h3 id=\"2-坑队友\"><a href=\"#2-坑队友\" class=\"headerlink\" title=\"2.坑队友\"></a>2.坑队友</h3><p>当你多打几把后，你的水平没法carry了，“坑队友”成为了主要矛盾，队友也不太靠谱，所以你认为是队友坑了你，导致你段位上不去，如果天美团队能统计一下的话，相信包含“坑队友”ID的人在这个段位是最频繁的，“坑队友”阶段的人一般在黄金铂金段位，赛季末可能会上钻石；</p>\n<p>难道真的都是别人坑你吗？非也，其实大家都有比较熟悉的英雄，但是依然在互坑，对面的也在互坑，造成这个问题的主要原因是你不理解你队友，雪上加霜的是，因为你被坑多了所以更加不相信队友，导致明明该赢的局没法使尽全力输了。</p>\n<p>其实除了打野位，我在这个段位胜率也不高，而之所以打野位胜率高，是因为我基本上会把对面的某个位置针对到他心态不稳（可能这时候对面吵起来了），很多时候，在我们崩之前，对面比我们先崩了。</p>\n<p>而要早日突破这个阶段，应该从“道”上下功夫，做到理解队友的行为，最好是五个位置都玩过，你就能理解很多队友的行为了，多跟队友沟通，让队友舒服；</p>\n<p>举个例子：射手拿了一红，一般打野会心态不好，其实你可以反向思维：射手敢拿红，说明在他之前的经验里，拿了红会猛很多，他的射手肯定是不坑的，你就打字：“相信你”。如果你是射手，就不拿一红，让打野不坑，并打字，“打野拿红带我飞”；</p>\n<h3 id=\"3-傻逼排位机制\"><a href=\"#3-傻逼排位机制\" class=\"headerlink\" title=\"3.傻逼排位机制\"></a>3.傻逼排位机制</h3><p>当你在阶段2做到了理解队友以后，你的操作已经很好了，也知道每个位置该干什么，你上了钻石，在钻石和星耀之间徘徊，由于周围的人也都会玩了，你会偶尔遇到坑，偶尔也会遇到大神带躺，单排胜率一直在50%左右，靠着勇者积分上分，这已经不是技术问题了；</p>\n<p>这时候你在想，是不是排位机制故意针对我？非也，大家都一样，我反问一句：如果五个你在一起，怎么选人？你能打过对面吗？是不是就是你在坑你？</p>\n<h3 id=\"4-55-胜率上王者\"><a href=\"#4-55-胜率上王者\" class=\"headerlink\" title=\"4.55%胜率上王者\"></a>4.55%胜率上王者</h3><p>你已经很厉害了，有一个位置挺厉害，会根据阵容选人，基本上补的位置也不差，单排上了王者，但是就是容错率比较低，偶尔该赢的局输了，该输的局还是输了，是不是一个赛季能打三四百场甚至更多？你只喜欢打排位，一有时间就打，状态差也打，有时候会连胜，有时候会连败，所以胜率还是在55%以下；</p>\n<h3 id=\"5-轻松上王者\"><a href=\"#5-轻松上王者\" class=\"headerlink\" title=\"5.轻松上王者\"></a>5.轻松上王者</h3><p>这是我们的目标，你有3、4阶段的技术以及对游戏的理解，你的队友基本上处于阶段3和阶段4，自己从各方面做到位，不是自己carry就是他们带你carry，选出的阵容容错率比较高，赢下该赢得局，拼了该拼的局，可能你打得并不多，但65%胜率以上上王者还是可以的；</p>\n<h3 id=\"6-大神单排虐菜\"><a href=\"#6-大神单排虐菜\" class=\"headerlink\" title=\"6.大神单排虐菜\"></a>6.大神单排虐菜</h3><p>我觉得这是一些有名主播和职业选手的水平，他们普遍心态很好，需要一定的操作、天赋和长时间的训练，是专业和业余的差距，我等常人是难以企及了，其它阶段我们都可以达到。当然职业战队更厉害了，是整体的战术素养层面，这里只讲单排；</p>\n<p>相信大部分玩家在2.3阶段，其实3 4 5阶段的人技术都差不多，主要差距在思路，看了我的思路，轻松上王者，不是梦。</p>\n<p>我曾经就是这样的，轻松上到铂金，熟悉了甄姬和庄周就上到了钻石，再认真一点，拿庄周，狄仁杰、甄姬、白起、姜子牙这种英雄，磕磕绊绊上到星耀，但是接下来在星耀二三徘徊了两个赛季，赢一场，输一场，胜率一直无法突破55%，这中间我先后熟悉了哪吒、牛魔、雅典娜、兰陵王。直到上赛季末和这赛季有了思路以后，才轻松上了王者，本来有望70%胜率上去的，可是在星耀一心态有点急躁，导致多输了几场，心态平复后才一鼓作气上了去，在这期间我一度打到上海浦东第1兰陵王，以及上海市第5兰陵王的称号，由于我打得不多，上王者后没啥追求，胜率后来掉下来了，如果我常打的话，某个熟练的英雄保持省级前十应该是可以的；</p>\n<p>因为没有暴击铭文，金币也不多，很多英雄一开始就没玩，手速也不够快，所以玩得都是不难的英雄，但是至少每个位置都有两三个拿的出手的英雄，相信这些英雄你也会玩得很好；</p>\n<p>前面讲了这么多背景有点啰嗦，（我还做了删减 /笑哭），但我个人认为背景很重要，有了前提的铺垫，才能有后面的结论。下面分别从心态、道（玄学）、游戏素质、游戏技术、游戏战略以及细节思路六个方面来阐述，总结为一句对联就是：“佛为心，道为骨，儒为表，大度看队友；技在手，能在胸，思在脑，从容上王者。”，其中，每个都很重要，直接关系着能否上王者；我认为技术是一切的基础，你首先要会玩，相信大部分玩家是有技术的，除了技术的基础地位，其它几个方面如果非要排序的话，我个人认为按照重要性优先级排序如下： 技术是基础：佛为心 &gt; 道为骨 &gt; 儒为表 &gt; 能在胸 &gt; 思在脑；</p>\n<h1 id=\"佛为心\"><a href=\"#佛为心\" class=\"headerlink\" title=\"佛为心\"></a>佛为心</h1><p>心态真的很重要，说多了大家感觉是废话，人人都知道</p>\n"},{"title":"spark往ES中写入数据的方法","toc":true,"date":"2017-07-17T10:41:17.000Z","_content":"\n他们之前把数据导入ES是通过单机的程序导的，或者通过logstash从kafka往ES导，但当数据量很大的时候就会变得很低效，我这两天调研了一下把数据从hdfs直接通过spark导入ES的方法，当然，也适合spark Streaming程序；\n这里指出版本号是有必要的，spark版本：1.6.2 ES版本：5.2.1，由于ES的API变动比较频繁，因此最好参考官网文档。\n\n### 连接ES的方法列举\n\n1. ES官网中给出了一个与spark连接的方法：https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html，是通过RDD可以直接调用 saveToEs 方法实现的；\n2. 如果数据量不大的话，可以参考ES提供的RestFulAPI来实现，https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html ；\n3. 本文主要说明我使用的方法，通过 TransportClient 和 bulk 批处理操作来实现，这种方法比较适合数据量很大的情况，又可以灵活处理。\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html\n\n### 使用TransportClient往ES批量导入的方法\n样例代码如下：\n\n``` scala\nimport java.net.InetAddress\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.elasticsearch.action.bulk.{BulkRequestBuilder, BulkResponse}\nimport org.elasticsearch.client.transport.TransportClient\nimport org.elasticsearch.common.settings.Settings\nimport org.elasticsearch.common.transport.InetSocketTransportAddress\nimport org.elasticsearch.transport.client.PreBuiltTransportClient\n\n/**\n  * Author: wangxiaogang\n  * Date: 2017/7/11\n  * Email: Adamyuanyuan@gmail.com\n  * hdfs 中的数据根据格式写到ES中\n  */\nobject HdfsToEs {\n\n  def main(args: Array[String]) {\n    if (args.length < 5) {\n      System.err.println(\"Usage: HdfsToEs <file> <esIndex> <esType> <partition>\")\n      System.exit(1)\n    }\n    val hdfsInputPath: String = args(0)\n    println(\"hdfsInputPath: \" + hdfsInputPath)\n\n    val conf = new SparkConf().setAppName(\"HdfsToEs\")\n    val sc = new SparkContext(conf)\n\n    //插入相关，索引 类型 id相关  以args方式提供接口。\n    val esIndex: String = args(1)\n    val esType: String = args(2)\n    val partition: Int = args(3).toInt\n    val bulkNum: Int = args(4).toInt\n\n    val hdfsRdd: RDD[String] = sc.textFile(hdfsInputPath, partition)\n    val startTime: Long = System.currentTimeMillis\n\n    println(\"hdfsRDD partition: \" + hdfsRdd.getNumPartitions + \" setted partition: \" + partition)\n\n    hdfsRdd.foreachPartition {\n      eachPa => {\n\n        //        生产环境\n        val settings: Settings = Settings.builder.put(\"cluster.name\", \"production-es\").put(\"client.transport.sniff\", true)\n          .put(\"transport.type\", \"netty3\").put(\"http.type\", \"netty3\").build\n        val client: TransportClient = new PreBuiltTransportClient(settings)\n          .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"----\"), 8300))\n          .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"----\"), 8300))\n          .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"----\"), 8300))\n          .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"----\"), 8300))\n          .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"----\"), 8300))\n\n        var bulkRequest: BulkRequestBuilder = null\n        var flag = true\n        var lineNum = 0\n\n        for (eachLine <- eachPa) {\n          // 每个bulk是10-15M为宜，数据封装为bulk后会较原来的数据略有增大，如果每行数据约为 1.5KB，则每 10000 行为一个bulk\n          if (flag) {\n            bulkRequest = client.prepareBulk\n            flag = false\n          }\n          val strArray: Array[String] = eachLine.split(\"###\")\n          if (strArray.length != 25) {\n            // 表示这行数据又问题，为了不影响整体，则跳过\n            println(\"ERROR: strArray.length != 25: \" + strArray.length + \" lineNum: \" + lineNum + \" strArray(0): \" + strArray(0))\n          } else {\n          \t// LinkedHashMap让ES中的数据变得有序\n            val esDataMap: java.util.Map[String, String] = new java.util.LinkedHashMap[String, String]\n            val id: String = strArray(0) \n            esDataMap.put(\"msisdn\", id)\n            // 数据合并后的格式为： msisdn###w0的前三###w1的前三###如果为空的话就是null...###w23的前三，共25列\n            for (i <- 1 to 24) {\n              val locTimesListStr = strArray(i)\n              val esDataKey = \"w\" + (i - 1)\n              if (locTimesListStr == null || locTimesListStr.isEmpty || locTimesListStr.equals(\"null\")) {\n                esDataMap.put(esDataKey, \"\")\n              } else {\n                esDataMap.put(esDataKey, locTimesListStr)\n              }\n            }\n            bulkRequest.add(client.prepareIndex(esIndex, esType, id).setSource(esDataMap))\n            lineNum += 1\n            if (lineNum % bulkNum == 0) {\n              val endTime: Long = System.currentTimeMillis\n              println(\"bulk push， current lineNum: \" + lineNum + \", currentTime s: \" + ((endTime - startTime) / 1000))\n              val bbq: BulkResponse = bulkRequest.execute.actionGet()\n              flag = true\n              if (bbq.hasFailures) {\n                println(\"bbq.hasFailures: \" + bbq.toString)\n                bulkRequest.execute.actionGet\n              }\n            }\n          }\n        }\n        if (bulkRequest != null) {\n          bulkRequest.execute().actionGet()\n        }\n\n        client.close()\n        val endTime: Long = System.currentTimeMillis\n        println(\"ths time is: \" + (endTime - startTime) / 1000 + \"s \")\n      }\n    }\n    sc.stop()\n  }\n\n}\n```\n\n踩坑说明：在编写代码中踩了如下坑：\n\n1. 依赖冲突的问题： ES5.2与Spark1.6有如下包会产生依赖： `netty-all:io.netty`，`com.fasterxml.jackson.core:jackson-core`, `org.apache.logging.log4j:log4j-core`.\n解决方案：\n通过 `mvn dependency:tree -Dverbose -Dincludes=com.fasterxml.jackson.core` 命令查出依赖原因，然后在pom.xml中增加所需的相关依赖的最高版本；\n\n2. 每个bulk的大小，根据网上的经验是10M-15M为宜，大概计算一下就好了；\n\n3. 后来在单机测试通过，但在集群模式中还是会出现 netty4的依赖冲突：\n```\n17/07/17 10:21:57 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[elasticsearch[_client_][management][T#1],5,main]\njava.lang.NoSuchMethodError: io.netty.buffer.CompositeByteBuf.addComponents(ZLjava/lang/Iterable;)Lio/netty/buffer/CompositeByteBuf;\n        at org.elasticsearch.transport.netty4.Netty4Utils.toByteBuf(Netty4Utils.java:78)\n        at org.elasticsearch.transport.netty4.Netty4Transport.sendMessage(Netty4Transport.java:422)\n        at org.elasticsearch.transport.netty4.Netty4Transport.sendMessage(Netty4Transport.java:93)\n        at org.elasticsearch.transport.TcpTransport.internalSendMessage(TcpTransport.java:1058)\n        at org.elasticsearch.transport.TcpTransport.sendRequestToChannel(TcpTransport.java:1040)\n        at org.elasticsearch.transport.TcpTransport.executeHandshake(TcpTransport.java:1555)\n        at org.elasticsearch.transport.TcpTransport.openConnection(TcpTransport.java:502)\n        at org.elasticsearch.transport.TcpTransport.connectToNode(TcpTransport.java:460)\n        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:318)\n        at org.elasticsearch.client.transport.TransportClientNodesService$SniffNodesSampler$1.run(TransportClientNodesService.java:488)\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:527)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```\n有一种解决方案我没有尝试成功，就是在pom中将冲突的依赖包exclusions掉，各位感兴趣可以尝试，成功了麻烦告知我一下。参考链接：https://www.elastic.co/blog/to-shade-or-not-to-shade, 使用 maven-shade-plugin 工具打包。\n\n上个方法我尝试几次不成功后，使用了比较暴力的方法，直接将ES的netty参数由netty4改成了netty3，\n```\n.put(\"transport.type\", \"netty3\").put(\"http.type\", \"netty3\").build\n```\n好了，打包好之后，程序就可以完美运行了。\n\n### ES中创建索引\n就算如果ES中是自动创建索引的，也希望你能手动创建索引和字段属性，因为默认的字段属性是Text，ES会自动对它进行分词相关的操作，如果ES中存的字符串你不想让它被分隔的话，就用keyword替代为Text类型，命令如下：\n\n```\nPUT  /weekend-20170718\n{\n  \"settings\" : {\n    \"index\" : {\n      \"number_of_shards\" : 5, \n      \"number_of_replicas\" : 1,\n      \"refresh_interval\" : \"60s\"\n    },\n  \"index.routing.allocation.include.zone\": \"light\"\n  },\n  \"mappings\": {\n    \"offline\": {\n      \"properties\": {\n        \"msisdn\": {\n          \"type\": \"keyword\" \n        },\"w0\": {\n          \"type\": \"keyword\" \n        } ...后面省略\n      }\n    }\n  }\n}\n```\n\n创建好索引后检查一下：\n\n```\nGET /weekend-20170718/_mapping\n```\n\n### 集群中运行\n这个比较简单，只需要注意以下几点就好了：\n1. 使用jdk1.8版本；\n2. 注意内存的申请，可能会出现跑了一段时间后，内存不够用导致程序退出的情况；\n3. 观测好ES集群的状态，一段时间后，ES机器的GC比较高\n4. 最好别一下子跑所有数据，分几批跑，这样就算出问题，只需要重跑那一部分就好了\n\n数据：通过观察，导入的速度随着时间的增长呈下降趋势，整体来说，ES集群隔离的小集群共有五台物理机，共2.23亿条，751G的数据导入用了约4.5小时，平均速度为 45M/s, 1.38W条/s。\n\n### 参考\n感谢 技术交流群里 @叫我老陈行了吧，@岑玉海 等大神网友；\n\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html","source":"_posts/2017-07-17-spark往ES中写入数据的方法.md","raw":"---\ntitle: spark往ES中写入数据的方法\ntoc: true\ndate: 2017-07-17 18:41:17\ntags:\n- spark\n- es\ncategories: spark开发\n---\n\n他们之前把数据导入ES是通过单机的程序导的，或者通过logstash从kafka往ES导，但当数据量很大的时候就会变得很低效，我这两天调研了一下把数据从hdfs直接通过spark导入ES的方法，当然，也适合spark Streaming程序；\n这里指出版本号是有必要的，spark版本：1.6.2 ES版本：5.2.1，由于ES的API变动比较频繁，因此最好参考官网文档。\n\n### 连接ES的方法列举\n\n1. ES官网中给出了一个与spark连接的方法：https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html，是通过RDD可以直接调用 saveToEs 方法实现的；\n2. 如果数据量不大的话，可以参考ES提供的RestFulAPI来实现，https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html ；\n3. 本文主要说明我使用的方法，通过 TransportClient 和 bulk 批处理操作来实现，这种方法比较适合数据量很大的情况，又可以灵活处理。\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html\n\n### 使用TransportClient往ES批量导入的方法\n样例代码如下：\n\n``` scala\nimport java.net.InetAddress\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.elasticsearch.action.bulk.{BulkRequestBuilder, BulkResponse}\nimport org.elasticsearch.client.transport.TransportClient\nimport org.elasticsearch.common.settings.Settings\nimport org.elasticsearch.common.transport.InetSocketTransportAddress\nimport org.elasticsearch.transport.client.PreBuiltTransportClient\n\n/**\n  * Author: wangxiaogang\n  * Date: 2017/7/11\n  * Email: Adamyuanyuan@gmail.com\n  * hdfs 中的数据根据格式写到ES中\n  */\nobject HdfsToEs {\n\n  def main(args: Array[String]) {\n    if (args.length < 5) {\n      System.err.println(\"Usage: HdfsToEs <file> <esIndex> <esType> <partition>\")\n      System.exit(1)\n    }\n    val hdfsInputPath: String = args(0)\n    println(\"hdfsInputPath: \" + hdfsInputPath)\n\n    val conf = new SparkConf().setAppName(\"HdfsToEs\")\n    val sc = new SparkContext(conf)\n\n    //插入相关，索引 类型 id相关  以args方式提供接口。\n    val esIndex: String = args(1)\n    val esType: String = args(2)\n    val partition: Int = args(3).toInt\n    val bulkNum: Int = args(4).toInt\n\n    val hdfsRdd: RDD[String] = sc.textFile(hdfsInputPath, partition)\n    val startTime: Long = System.currentTimeMillis\n\n    println(\"hdfsRDD partition: \" + hdfsRdd.getNumPartitions + \" setted partition: \" + partition)\n\n    hdfsRdd.foreachPartition {\n      eachPa => {\n\n        //        生产环境\n        val settings: Settings = Settings.builder.put(\"cluster.name\", \"production-es\").put(\"client.transport.sniff\", true)\n          .put(\"transport.type\", \"netty3\").put(\"http.type\", \"netty3\").build\n        val client: TransportClient = new PreBuiltTransportClient(settings)\n          .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"----\"), 8300))\n          .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"----\"), 8300))\n          .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"----\"), 8300))\n          .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"----\"), 8300))\n          .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"----\"), 8300))\n\n        var bulkRequest: BulkRequestBuilder = null\n        var flag = true\n        var lineNum = 0\n\n        for (eachLine <- eachPa) {\n          // 每个bulk是10-15M为宜，数据封装为bulk后会较原来的数据略有增大，如果每行数据约为 1.5KB，则每 10000 行为一个bulk\n          if (flag) {\n            bulkRequest = client.prepareBulk\n            flag = false\n          }\n          val strArray: Array[String] = eachLine.split(\"###\")\n          if (strArray.length != 25) {\n            // 表示这行数据又问题，为了不影响整体，则跳过\n            println(\"ERROR: strArray.length != 25: \" + strArray.length + \" lineNum: \" + lineNum + \" strArray(0): \" + strArray(0))\n          } else {\n          \t// LinkedHashMap让ES中的数据变得有序\n            val esDataMap: java.util.Map[String, String] = new java.util.LinkedHashMap[String, String]\n            val id: String = strArray(0) \n            esDataMap.put(\"msisdn\", id)\n            // 数据合并后的格式为： msisdn###w0的前三###w1的前三###如果为空的话就是null...###w23的前三，共25列\n            for (i <- 1 to 24) {\n              val locTimesListStr = strArray(i)\n              val esDataKey = \"w\" + (i - 1)\n              if (locTimesListStr == null || locTimesListStr.isEmpty || locTimesListStr.equals(\"null\")) {\n                esDataMap.put(esDataKey, \"\")\n              } else {\n                esDataMap.put(esDataKey, locTimesListStr)\n              }\n            }\n            bulkRequest.add(client.prepareIndex(esIndex, esType, id).setSource(esDataMap))\n            lineNum += 1\n            if (lineNum % bulkNum == 0) {\n              val endTime: Long = System.currentTimeMillis\n              println(\"bulk push， current lineNum: \" + lineNum + \", currentTime s: \" + ((endTime - startTime) / 1000))\n              val bbq: BulkResponse = bulkRequest.execute.actionGet()\n              flag = true\n              if (bbq.hasFailures) {\n                println(\"bbq.hasFailures: \" + bbq.toString)\n                bulkRequest.execute.actionGet\n              }\n            }\n          }\n        }\n        if (bulkRequest != null) {\n          bulkRequest.execute().actionGet()\n        }\n\n        client.close()\n        val endTime: Long = System.currentTimeMillis\n        println(\"ths time is: \" + (endTime - startTime) / 1000 + \"s \")\n      }\n    }\n    sc.stop()\n  }\n\n}\n```\n\n踩坑说明：在编写代码中踩了如下坑：\n\n1. 依赖冲突的问题： ES5.2与Spark1.6有如下包会产生依赖： `netty-all:io.netty`，`com.fasterxml.jackson.core:jackson-core`, `org.apache.logging.log4j:log4j-core`.\n解决方案：\n通过 `mvn dependency:tree -Dverbose -Dincludes=com.fasterxml.jackson.core` 命令查出依赖原因，然后在pom.xml中增加所需的相关依赖的最高版本；\n\n2. 每个bulk的大小，根据网上的经验是10M-15M为宜，大概计算一下就好了；\n\n3. 后来在单机测试通过，但在集群模式中还是会出现 netty4的依赖冲突：\n```\n17/07/17 10:21:57 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[elasticsearch[_client_][management][T#1],5,main]\njava.lang.NoSuchMethodError: io.netty.buffer.CompositeByteBuf.addComponents(ZLjava/lang/Iterable;)Lio/netty/buffer/CompositeByteBuf;\n        at org.elasticsearch.transport.netty4.Netty4Utils.toByteBuf(Netty4Utils.java:78)\n        at org.elasticsearch.transport.netty4.Netty4Transport.sendMessage(Netty4Transport.java:422)\n        at org.elasticsearch.transport.netty4.Netty4Transport.sendMessage(Netty4Transport.java:93)\n        at org.elasticsearch.transport.TcpTransport.internalSendMessage(TcpTransport.java:1058)\n        at org.elasticsearch.transport.TcpTransport.sendRequestToChannel(TcpTransport.java:1040)\n        at org.elasticsearch.transport.TcpTransport.executeHandshake(TcpTransport.java:1555)\n        at org.elasticsearch.transport.TcpTransport.openConnection(TcpTransport.java:502)\n        at org.elasticsearch.transport.TcpTransport.connectToNode(TcpTransport.java:460)\n        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:318)\n        at org.elasticsearch.client.transport.TransportClientNodesService$SniffNodesSampler$1.run(TransportClientNodesService.java:488)\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:527)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```\n有一种解决方案我没有尝试成功，就是在pom中将冲突的依赖包exclusions掉，各位感兴趣可以尝试，成功了麻烦告知我一下。参考链接：https://www.elastic.co/blog/to-shade-or-not-to-shade, 使用 maven-shade-plugin 工具打包。\n\n上个方法我尝试几次不成功后，使用了比较暴力的方法，直接将ES的netty参数由netty4改成了netty3，\n```\n.put(\"transport.type\", \"netty3\").put(\"http.type\", \"netty3\").build\n```\n好了，打包好之后，程序就可以完美运行了。\n\n### ES中创建索引\n就算如果ES中是自动创建索引的，也希望你能手动创建索引和字段属性，因为默认的字段属性是Text，ES会自动对它进行分词相关的操作，如果ES中存的字符串你不想让它被分隔的话，就用keyword替代为Text类型，命令如下：\n\n```\nPUT  /weekend-20170718\n{\n  \"settings\" : {\n    \"index\" : {\n      \"number_of_shards\" : 5, \n      \"number_of_replicas\" : 1,\n      \"refresh_interval\" : \"60s\"\n    },\n  \"index.routing.allocation.include.zone\": \"light\"\n  },\n  \"mappings\": {\n    \"offline\": {\n      \"properties\": {\n        \"msisdn\": {\n          \"type\": \"keyword\" \n        },\"w0\": {\n          \"type\": \"keyword\" \n        } ...后面省略\n      }\n    }\n  }\n}\n```\n\n创建好索引后检查一下：\n\n```\nGET /weekend-20170718/_mapping\n```\n\n### 集群中运行\n这个比较简单，只需要注意以下几点就好了：\n1. 使用jdk1.8版本；\n2. 注意内存的申请，可能会出现跑了一段时间后，内存不够用导致程序退出的情况；\n3. 观测好ES集群的状态，一段时间后，ES机器的GC比较高\n4. 最好别一下子跑所有数据，分几批跑，这样就算出问题，只需要重跑那一部分就好了\n\n数据：通过观察，导入的速度随着时间的增长呈下降趋势，整体来说，ES集群隔离的小集群共有五台物理机，共2.23亿条，751G的数据导入用了约4.5小时，平均速度为 45M/s, 1.38W条/s。\n\n### 参考\n感谢 技术交流群里 @叫我老陈行了吧，@岑玉海 等大神网友；\n\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html","slug":"spark往ES中写入数据的方法","published":1,"updated":"2017-07-18T08:11:29.391Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfxv003zpsguuezusctq","content":"<p>他们之前把数据导入ES是通过单机的程序导的，或者通过logstash从kafka往ES导，但当数据量很大的时候就会变得很低效，我这两天调研了一下把数据从hdfs直接通过spark导入ES的方法，当然，也适合spark Streaming程序；<br>这里指出版本号是有必要的，spark版本：1.6.2 ES版本：5.2.1，由于ES的API变动比较频繁，因此最好参考官网文档。</p>\n<h3 id=\"连接ES的方法列举\"><a href=\"#连接ES的方法列举\" class=\"headerlink\" title=\"连接ES的方法列举\"></a>连接ES的方法列举</h3><ol>\n<li>ES官网中给出了一个与spark连接的方法：<a href=\"https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html，是通过RDD可以直接调用\" target=\"_blank\" rel=\"external\">https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html，是通过RDD可以直接调用</a> saveToEs 方法实现的；</li>\n<li>如果数据量不大的话，可以参考ES提供的RestFulAPI来实现，<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html\" target=\"_blank\" rel=\"external\">https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html</a> ；</li>\n<li>本文主要说明我使用的方法，通过 TransportClient 和 bulk 批处理操作来实现，这种方法比较适合数据量很大的情况，又可以灵活处理。<br><a href=\"https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html\" target=\"_blank\" rel=\"external\">https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html</a><br><a href=\"https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html\" target=\"_blank\" rel=\"external\">https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html</a></li>\n</ol>\n<h3 id=\"使用TransportClient往ES批量导入的方法\"><a href=\"#使用TransportClient往ES批量导入的方法\" class=\"headerlink\" title=\"使用TransportClient往ES批量导入的方法\"></a>使用TransportClient往ES批量导入的方法</h3><p>样例代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> java.net.<span class=\"type\">InetAddress</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.rdd.<span class=\"type\">RDD</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.&#123;<span class=\"type\">SparkConf</span>, <span class=\"type\">SparkContext</span>&#125;</div><div class=\"line\"><span class=\"keyword\">import</span> org.elasticsearch.action.bulk.&#123;<span class=\"type\">BulkRequestBuilder</span>, <span class=\"type\">BulkResponse</span>&#125;</div><div class=\"line\"><span class=\"keyword\">import</span> org.elasticsearch.client.transport.<span class=\"type\">TransportClient</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.elasticsearch.common.settings.<span class=\"type\">Settings</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.elasticsearch.common.transport.<span class=\"type\">InetSocketTransportAddress</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.elasticsearch.transport.client.<span class=\"type\">PreBuiltTransportClient</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\">  * Author: wangxiaogang</div><div class=\"line\">  * Date: 2017/7/11</div><div class=\"line\">  * Email: Adamyuanyuan@gmail.com</div><div class=\"line\">  * hdfs 中的数据根据格式写到ES中</div><div class=\"line\">  */</div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">HdfsToEs</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (args.length &lt; <span class=\"number\">5</span>) &#123;</div><div class=\"line\">      <span class=\"type\">System</span>.err.println(<span class=\"string\">\"Usage: HdfsToEs &lt;file&gt; &lt;esIndex&gt; &lt;esType&gt; &lt;partition&gt;\"</span>)</div><div class=\"line\">      <span class=\"type\">System</span>.exit(<span class=\"number\">1</span>)</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">val</span> hdfsInputPath: <span class=\"type\">String</span> = args(<span class=\"number\">0</span>)</div><div class=\"line\">    println(<span class=\"string\">\"hdfsInputPath: \"</span> + hdfsInputPath)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">\"HdfsToEs\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(conf)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">//插入相关，索引 类型 id相关  以args方式提供接口。</span></div><div class=\"line\">    <span class=\"keyword\">val</span> esIndex: <span class=\"type\">String</span> = args(<span class=\"number\">1</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> esType: <span class=\"type\">String</span> = args(<span class=\"number\">2</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> partition: <span class=\"type\">Int</span> = args(<span class=\"number\">3</span>).toInt</div><div class=\"line\">    <span class=\"keyword\">val</span> bulkNum: <span class=\"type\">Int</span> = args(<span class=\"number\">4</span>).toInt</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> hdfsRdd: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.textFile(hdfsInputPath, partition)</div><div class=\"line\">    <span class=\"keyword\">val</span> startTime: <span class=\"type\">Long</span> = <span class=\"type\">System</span>.currentTimeMillis</div><div class=\"line\"></div><div class=\"line\">    println(<span class=\"string\">\"hdfsRDD partition: \"</span> + hdfsRdd.getNumPartitions + <span class=\"string\">\" setted partition: \"</span> + partition)</div><div class=\"line\"></div><div class=\"line\">    hdfsRdd.foreachPartition &#123;</div><div class=\"line\">      eachPa =&gt; &#123;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">//        生产环境</span></div><div class=\"line\">        <span class=\"keyword\">val</span> settings: <span class=\"type\">Settings</span> = <span class=\"type\">Settings</span>.builder.put(<span class=\"string\">\"cluster.name\"</span>, <span class=\"string\">\"production-es\"</span>).put(<span class=\"string\">\"client.transport.sniff\"</span>, <span class=\"literal\">true</span>)</div><div class=\"line\">          .put(<span class=\"string\">\"transport.type\"</span>, <span class=\"string\">\"netty3\"</span>).put(<span class=\"string\">\"http.type\"</span>, <span class=\"string\">\"netty3\"</span>).build</div><div class=\"line\">        <span class=\"keyword\">val</span> client: <span class=\"type\">TransportClient</span> = <span class=\"keyword\">new</span> <span class=\"type\">PreBuiltTransportClient</span>(settings)</div><div class=\"line\">          .addTransportAddress(<span class=\"keyword\">new</span> <span class=\"type\">InetSocketTransportAddress</span>(<span class=\"type\">InetAddress</span>.getByName(<span class=\"string\">\"----\"</span>), <span class=\"number\">8300</span>))</div><div class=\"line\">          .addTransportAddress(<span class=\"keyword\">new</span> <span class=\"type\">InetSocketTransportAddress</span>(<span class=\"type\">InetAddress</span>.getByName(<span class=\"string\">\"----\"</span>), <span class=\"number\">8300</span>))</div><div class=\"line\">          .addTransportAddress(<span class=\"keyword\">new</span> <span class=\"type\">InetSocketTransportAddress</span>(<span class=\"type\">InetAddress</span>.getByName(<span class=\"string\">\"----\"</span>), <span class=\"number\">8300</span>))</div><div class=\"line\">          .addTransportAddress(<span class=\"keyword\">new</span> <span class=\"type\">InetSocketTransportAddress</span>(<span class=\"type\">InetAddress</span>.getByName(<span class=\"string\">\"----\"</span>), <span class=\"number\">8300</span>))</div><div class=\"line\">          .addTransportAddress(<span class=\"keyword\">new</span> <span class=\"type\">InetSocketTransportAddress</span>(<span class=\"type\">InetAddress</span>.getByName(<span class=\"string\">\"----\"</span>), <span class=\"number\">8300</span>))</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">var</span> bulkRequest: <span class=\"type\">BulkRequestBuilder</span> = <span class=\"literal\">null</span></div><div class=\"line\">        <span class=\"keyword\">var</span> flag = <span class=\"literal\">true</span></div><div class=\"line\">        <span class=\"keyword\">var</span> lineNum = <span class=\"number\">0</span></div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">for</span> (eachLine &lt;- eachPa) &#123;</div><div class=\"line\">          <span class=\"comment\">// 每个bulk是10-15M为宜，数据封装为bulk后会较原来的数据略有增大，如果每行数据约为 1.5KB，则每 10000 行为一个bulk</span></div><div class=\"line\">          <span class=\"keyword\">if</span> (flag) &#123;</div><div class=\"line\">            bulkRequest = client.prepareBulk</div><div class=\"line\">            flag = <span class=\"literal\">false</span></div><div class=\"line\">          &#125;</div><div class=\"line\">          <span class=\"keyword\">val</span> strArray: <span class=\"type\">Array</span>[<span class=\"type\">String</span>] = eachLine.split(<span class=\"string\">\"###\"</span>)</div><div class=\"line\">          <span class=\"keyword\">if</span> (strArray.length != <span class=\"number\">25</span>) &#123;</div><div class=\"line\">            <span class=\"comment\">// 表示这行数据又问题，为了不影响整体，则跳过</span></div><div class=\"line\">            println(<span class=\"string\">\"ERROR: strArray.length != 25: \"</span> + strArray.length + <span class=\"string\">\" lineNum: \"</span> + lineNum + <span class=\"string\">\" strArray(0): \"</span> + strArray(<span class=\"number\">0</span>))</div><div class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">          \t<span class=\"comment\">// LinkedHashMap让ES中的数据变得有序</span></div><div class=\"line\">            <span class=\"keyword\">val</span> esDataMap: java.util.<span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"keyword\">new</span> java.util.<span class=\"type\">LinkedHashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]</div><div class=\"line\">            <span class=\"keyword\">val</span> id: <span class=\"type\">String</span> = strArray(<span class=\"number\">0</span>) </div><div class=\"line\">            esDataMap.put(<span class=\"string\">\"msisdn\"</span>, id)</div><div class=\"line\">            <span class=\"comment\">// 数据合并后的格式为： msisdn###w0的前三###w1的前三###如果为空的话就是null...###w23的前三，共25列</span></div><div class=\"line\">            <span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">1</span> to <span class=\"number\">24</span>) &#123;</div><div class=\"line\">              <span class=\"keyword\">val</span> locTimesListStr = strArray(i)</div><div class=\"line\">              <span class=\"keyword\">val</span> esDataKey = <span class=\"string\">\"w\"</span> + (i - <span class=\"number\">1</span>)</div><div class=\"line\">              <span class=\"keyword\">if</span> (locTimesListStr == <span class=\"literal\">null</span> || locTimesListStr.isEmpty || locTimesListStr.equals(<span class=\"string\">\"null\"</span>)) &#123;</div><div class=\"line\">                esDataMap.put(esDataKey, <span class=\"string\">\"\"</span>)</div><div class=\"line\">              &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">                esDataMap.put(esDataKey, locTimesListStr)</div><div class=\"line\">              &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            bulkRequest.add(client.prepareIndex(esIndex, esType, id).setSource(esDataMap))</div><div class=\"line\">            lineNum += <span class=\"number\">1</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (lineNum % bulkNum == <span class=\"number\">0</span>) &#123;</div><div class=\"line\">              <span class=\"keyword\">val</span> endTime: <span class=\"type\">Long</span> = <span class=\"type\">System</span>.currentTimeMillis</div><div class=\"line\">              println(<span class=\"string\">\"bulk push， current lineNum: \"</span> + lineNum + <span class=\"string\">\", currentTime s: \"</span> + ((endTime - startTime) / <span class=\"number\">1000</span>))</div><div class=\"line\">              <span class=\"keyword\">val</span> bbq: <span class=\"type\">BulkResponse</span> = bulkRequest.execute.actionGet()</div><div class=\"line\">              flag = <span class=\"literal\">true</span></div><div class=\"line\">              <span class=\"keyword\">if</span> (bbq.hasFailures) &#123;</div><div class=\"line\">                println(<span class=\"string\">\"bbq.hasFailures: \"</span> + bbq.toString)</div><div class=\"line\">                bulkRequest.execute.actionGet</div><div class=\"line\">              &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">          &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">if</span> (bulkRequest != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">          bulkRequest.execute().actionGet()</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        client.close()</div><div class=\"line\">        <span class=\"keyword\">val</span> endTime: <span class=\"type\">Long</span> = <span class=\"type\">System</span>.currentTimeMillis</div><div class=\"line\">        println(<span class=\"string\">\"ths time is: \"</span> + (endTime - startTime) / <span class=\"number\">1000</span> + <span class=\"string\">\"s \"</span>)</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    sc.stop()</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>踩坑说明：在编写代码中踩了如下坑：</p>\n<ol>\n<li><p>依赖冲突的问题： ES5.2与Spark1.6有如下包会产生依赖： <code>netty-all:io.netty</code>，<code>com.fasterxml.jackson.core:jackson-core</code>, <code>org.apache.logging.log4j:log4j-core</code>.<br>解决方案：<br>通过 <code>mvn dependency:tree -Dverbose -Dincludes=com.fasterxml.jackson.core</code> 命令查出依赖原因，然后在pom.xml中增加所需的相关依赖的最高版本；</p>\n</li>\n<li><p>每个bulk的大小，根据网上的经验是10M-15M为宜，大概计算一下就好了；</p>\n</li>\n<li><p>后来在单机测试通过，但在集群模式中还是会出现 netty4的依赖冲突：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">17/07/17 10:21:57 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[elasticsearch[_client_][management][T#1],5,main]</div><div class=\"line\">java.lang.NoSuchMethodError: io.netty.buffer.CompositeByteBuf.addComponents(ZLjava/lang/Iterable;)Lio/netty/buffer/CompositeByteBuf;</div><div class=\"line\">        at org.elasticsearch.transport.netty4.Netty4Utils.toByteBuf(Netty4Utils.java:78)</div><div class=\"line\">        at org.elasticsearch.transport.netty4.Netty4Transport.sendMessage(Netty4Transport.java:422)</div><div class=\"line\">        at org.elasticsearch.transport.netty4.Netty4Transport.sendMessage(Netty4Transport.java:93)</div><div class=\"line\">        at org.elasticsearch.transport.TcpTransport.internalSendMessage(TcpTransport.java:1058)</div><div class=\"line\">        at org.elasticsearch.transport.TcpTransport.sendRequestToChannel(TcpTransport.java:1040)</div><div class=\"line\">        at org.elasticsearch.transport.TcpTransport.executeHandshake(TcpTransport.java:1555)</div><div class=\"line\">        at org.elasticsearch.transport.TcpTransport.openConnection(TcpTransport.java:502)</div><div class=\"line\">        at org.elasticsearch.transport.TcpTransport.connectToNode(TcpTransport.java:460)</div><div class=\"line\">        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:318)</div><div class=\"line\">        at org.elasticsearch.client.transport.TransportClientNodesService$SniffNodesSampler$1.run(TransportClientNodesService.java:488)</div><div class=\"line\">        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:527)</div><div class=\"line\">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</div><div class=\"line\">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</div><div class=\"line\">        at java.lang.Thread.run(Thread.java:745)</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>有一种解决方案我没有尝试成功，就是在pom中将冲突的依赖包exclusions掉，各位感兴趣可以尝试，成功了麻烦告知我一下。参考链接：<a href=\"https://www.elastic.co/blog/to-shade-or-not-to-shade\" target=\"_blank\" rel=\"external\">https://www.elastic.co/blog/to-shade-or-not-to-shade</a>, 使用 maven-shade-plugin 工具打包。</p>\n<p>上个方法我尝试几次不成功后，使用了比较暴力的方法，直接将ES的netty参数由netty4改成了netty3，<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">.put(&quot;transport.type&quot;, &quot;netty3&quot;).put(&quot;http.type&quot;, &quot;netty3&quot;).build</div></pre></td></tr></table></figure></p>\n<p>好了，打包好之后，程序就可以完美运行了。</p>\n<h3 id=\"ES中创建索引\"><a href=\"#ES中创建索引\" class=\"headerlink\" title=\"ES中创建索引\"></a>ES中创建索引</h3><p>就算如果ES中是自动创建索引的，也希望你能手动创建索引和字段属性，因为默认的字段属性是Text，ES会自动对它进行分词相关的操作，如果ES中存的字符串你不想让它被分隔的话，就用keyword替代为Text类型，命令如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">PUT  /weekend-20170718</div><div class=\"line\">&#123;</div><div class=\"line\">  &quot;settings&quot; : &#123;</div><div class=\"line\">    &quot;index&quot; : &#123;</div><div class=\"line\">      &quot;number_of_shards&quot; : 5, </div><div class=\"line\">      &quot;number_of_replicas&quot; : 1,</div><div class=\"line\">      &quot;refresh_interval&quot; : &quot;60s&quot;</div><div class=\"line\">    &#125;,</div><div class=\"line\">  &quot;index.routing.allocation.include.zone&quot;: &quot;light&quot;</div><div class=\"line\">  &#125;,</div><div class=\"line\">  &quot;mappings&quot;: &#123;</div><div class=\"line\">    &quot;offline&quot;: &#123;</div><div class=\"line\">      &quot;properties&quot;: &#123;</div><div class=\"line\">        &quot;msisdn&quot;: &#123;</div><div class=\"line\">          &quot;type&quot;: &quot;keyword&quot; </div><div class=\"line\">        &#125;,&quot;w0&quot;: &#123;</div><div class=\"line\">          &quot;type&quot;: &quot;keyword&quot; </div><div class=\"line\">        &#125; ...后面省略</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>创建好索引后检查一下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">GET /weekend-20170718/_mapping</div></pre></td></tr></table></figure>\n<h3 id=\"集群中运行\"><a href=\"#集群中运行\" class=\"headerlink\" title=\"集群中运行\"></a>集群中运行</h3><p>这个比较简单，只需要注意以下几点就好了：</p>\n<ol>\n<li>使用jdk1.8版本；</li>\n<li>注意内存的申请，可能会出现跑了一段时间后，内存不够用导致程序退出的情况；</li>\n<li>观测好ES集群的状态，一段时间后，ES机器的GC比较高</li>\n<li>最好别一下子跑所有数据，分几批跑，这样就算出问题，只需要重跑那一部分就好了</li>\n</ol>\n<p>数据：通过观察，导入的速度随着时间的增长呈下降趋势，整体来说，ES集群隔离的小集群共有五台物理机，共2.23亿条，751G的数据导入用了约4.5小时，平均速度为 45M/s, 1.38W条/s。</p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>感谢 技术交流群里 @叫我老陈行了吧，@岑玉海 等大神网友；</p>\n<p><a href=\"https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html\" target=\"_blank\" rel=\"external\">https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html</a><br><a href=\"https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html\" target=\"_blank\" rel=\"external\">https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html</a></p>\n","excerpt":"","more":"<p>他们之前把数据导入ES是通过单机的程序导的，或者通过logstash从kafka往ES导，但当数据量很大的时候就会变得很低效，我这两天调研了一下把数据从hdfs直接通过spark导入ES的方法，当然，也适合spark Streaming程序；<br>这里指出版本号是有必要的，spark版本：1.6.2 ES版本：5.2.1，由于ES的API变动比较频繁，因此最好参考官网文档。</p>\n<h3 id=\"连接ES的方法列举\"><a href=\"#连接ES的方法列举\" class=\"headerlink\" title=\"连接ES的方法列举\"></a>连接ES的方法列举</h3><ol>\n<li>ES官网中给出了一个与spark连接的方法：<a href=\"https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html，是通过RDD可以直接调用\">https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html，是通过RDD可以直接调用</a> saveToEs 方法实现的；</li>\n<li>如果数据量不大的话，可以参考ES提供的RestFulAPI来实现，<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html\">https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html</a> ；</li>\n<li>本文主要说明我使用的方法，通过 TransportClient 和 bulk 批处理操作来实现，这种方法比较适合数据量很大的情况，又可以灵活处理。<br><a href=\"https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html\">https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html</a><br><a href=\"https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html\">https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html</a></li>\n</ol>\n<h3 id=\"使用TransportClient往ES批量导入的方法\"><a href=\"#使用TransportClient往ES批量导入的方法\" class=\"headerlink\" title=\"使用TransportClient往ES批量导入的方法\"></a>使用TransportClient往ES批量导入的方法</h3><p>样例代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> java.net.<span class=\"type\">InetAddress</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.rdd.<span class=\"type\">RDD</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.&#123;<span class=\"type\">SparkConf</span>, <span class=\"type\">SparkContext</span>&#125;</div><div class=\"line\"><span class=\"keyword\">import</span> org.elasticsearch.action.bulk.&#123;<span class=\"type\">BulkRequestBuilder</span>, <span class=\"type\">BulkResponse</span>&#125;</div><div class=\"line\"><span class=\"keyword\">import</span> org.elasticsearch.client.transport.<span class=\"type\">TransportClient</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.elasticsearch.common.settings.<span class=\"type\">Settings</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.elasticsearch.common.transport.<span class=\"type\">InetSocketTransportAddress</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.elasticsearch.transport.client.<span class=\"type\">PreBuiltTransportClient</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</div><div class=\"line\">  * Author: wangxiaogang</div><div class=\"line\">  * Date: 2017/7/11</div><div class=\"line\">  * Email: Adamyuanyuan@gmail.com</div><div class=\"line\">  * hdfs 中的数据根据格式写到ES中</div><div class=\"line\">  */</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">HdfsToEs</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (args.length &lt; <span class=\"number\">5</span>) &#123;</div><div class=\"line\">      <span class=\"type\">System</span>.err.println(<span class=\"string\">\"Usage: HdfsToEs &lt;file&gt; &lt;esIndex&gt; &lt;esType&gt; &lt;partition&gt;\"</span>)</div><div class=\"line\">      <span class=\"type\">System</span>.exit(<span class=\"number\">1</span>)</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">val</span> hdfsInputPath: <span class=\"type\">String</span> = args(<span class=\"number\">0</span>)</div><div class=\"line\">    println(<span class=\"string\">\"hdfsInputPath: \"</span> + hdfsInputPath)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">\"HdfsToEs\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(conf)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">//插入相关，索引 类型 id相关  以args方式提供接口。</span></div><div class=\"line\">    <span class=\"keyword\">val</span> esIndex: <span class=\"type\">String</span> = args(<span class=\"number\">1</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> esType: <span class=\"type\">String</span> = args(<span class=\"number\">2</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> partition: <span class=\"type\">Int</span> = args(<span class=\"number\">3</span>).toInt</div><div class=\"line\">    <span class=\"keyword\">val</span> bulkNum: <span class=\"type\">Int</span> = args(<span class=\"number\">4</span>).toInt</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> hdfsRdd: <span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = sc.textFile(hdfsInputPath, partition)</div><div class=\"line\">    <span class=\"keyword\">val</span> startTime: <span class=\"type\">Long</span> = <span class=\"type\">System</span>.currentTimeMillis</div><div class=\"line\"></div><div class=\"line\">    println(<span class=\"string\">\"hdfsRDD partition: \"</span> + hdfsRdd.getNumPartitions + <span class=\"string\">\" setted partition: \"</span> + partition)</div><div class=\"line\"></div><div class=\"line\">    hdfsRdd.foreachPartition &#123;</div><div class=\"line\">      eachPa =&gt; &#123;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">//        生产环境</span></div><div class=\"line\">        <span class=\"keyword\">val</span> settings: <span class=\"type\">Settings</span> = <span class=\"type\">Settings</span>.builder.put(<span class=\"string\">\"cluster.name\"</span>, <span class=\"string\">\"production-es\"</span>).put(<span class=\"string\">\"client.transport.sniff\"</span>, <span class=\"literal\">true</span>)</div><div class=\"line\">          .put(<span class=\"string\">\"transport.type\"</span>, <span class=\"string\">\"netty3\"</span>).put(<span class=\"string\">\"http.type\"</span>, <span class=\"string\">\"netty3\"</span>).build</div><div class=\"line\">        <span class=\"keyword\">val</span> client: <span class=\"type\">TransportClient</span> = <span class=\"keyword\">new</span> <span class=\"type\">PreBuiltTransportClient</span>(settings)</div><div class=\"line\">          .addTransportAddress(<span class=\"keyword\">new</span> <span class=\"type\">InetSocketTransportAddress</span>(<span class=\"type\">InetAddress</span>.getByName(<span class=\"string\">\"----\"</span>), <span class=\"number\">8300</span>))</div><div class=\"line\">          .addTransportAddress(<span class=\"keyword\">new</span> <span class=\"type\">InetSocketTransportAddress</span>(<span class=\"type\">InetAddress</span>.getByName(<span class=\"string\">\"----\"</span>), <span class=\"number\">8300</span>))</div><div class=\"line\">          .addTransportAddress(<span class=\"keyword\">new</span> <span class=\"type\">InetSocketTransportAddress</span>(<span class=\"type\">InetAddress</span>.getByName(<span class=\"string\">\"----\"</span>), <span class=\"number\">8300</span>))</div><div class=\"line\">          .addTransportAddress(<span class=\"keyword\">new</span> <span class=\"type\">InetSocketTransportAddress</span>(<span class=\"type\">InetAddress</span>.getByName(<span class=\"string\">\"----\"</span>), <span class=\"number\">8300</span>))</div><div class=\"line\">          .addTransportAddress(<span class=\"keyword\">new</span> <span class=\"type\">InetSocketTransportAddress</span>(<span class=\"type\">InetAddress</span>.getByName(<span class=\"string\">\"----\"</span>), <span class=\"number\">8300</span>))</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">var</span> bulkRequest: <span class=\"type\">BulkRequestBuilder</span> = <span class=\"literal\">null</span></div><div class=\"line\">        <span class=\"keyword\">var</span> flag = <span class=\"literal\">true</span></div><div class=\"line\">        <span class=\"keyword\">var</span> lineNum = <span class=\"number\">0</span></div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">for</span> (eachLine &lt;- eachPa) &#123;</div><div class=\"line\">          <span class=\"comment\">// 每个bulk是10-15M为宜，数据封装为bulk后会较原来的数据略有增大，如果每行数据约为 1.5KB，则每 10000 行为一个bulk</span></div><div class=\"line\">          <span class=\"keyword\">if</span> (flag) &#123;</div><div class=\"line\">            bulkRequest = client.prepareBulk</div><div class=\"line\">            flag = <span class=\"literal\">false</span></div><div class=\"line\">          &#125;</div><div class=\"line\">          <span class=\"keyword\">val</span> strArray: <span class=\"type\">Array</span>[<span class=\"type\">String</span>] = eachLine.split(<span class=\"string\">\"###\"</span>)</div><div class=\"line\">          <span class=\"keyword\">if</span> (strArray.length != <span class=\"number\">25</span>) &#123;</div><div class=\"line\">            <span class=\"comment\">// 表示这行数据又问题，为了不影响整体，则跳过</span></div><div class=\"line\">            println(<span class=\"string\">\"ERROR: strArray.length != 25: \"</span> + strArray.length + <span class=\"string\">\" lineNum: \"</span> + lineNum + <span class=\"string\">\" strArray(0): \"</span> + strArray(<span class=\"number\">0</span>))</div><div class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">          \t<span class=\"comment\">// LinkedHashMap让ES中的数据变得有序</span></div><div class=\"line\">            <span class=\"keyword\">val</span> esDataMap: java.util.<span class=\"type\">Map</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>] = <span class=\"keyword\">new</span> java.util.<span class=\"type\">LinkedHashMap</span>[<span class=\"type\">String</span>, <span class=\"type\">String</span>]</div><div class=\"line\">            <span class=\"keyword\">val</span> id: <span class=\"type\">String</span> = strArray(<span class=\"number\">0</span>) </div><div class=\"line\">            esDataMap.put(<span class=\"string\">\"msisdn\"</span>, id)</div><div class=\"line\">            <span class=\"comment\">// 数据合并后的格式为： msisdn###w0的前三###w1的前三###如果为空的话就是null...###w23的前三，共25列</span></div><div class=\"line\">            <span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">1</span> to <span class=\"number\">24</span>) &#123;</div><div class=\"line\">              <span class=\"keyword\">val</span> locTimesListStr = strArray(i)</div><div class=\"line\">              <span class=\"keyword\">val</span> esDataKey = <span class=\"string\">\"w\"</span> + (i - <span class=\"number\">1</span>)</div><div class=\"line\">              <span class=\"keyword\">if</span> (locTimesListStr == <span class=\"literal\">null</span> || locTimesListStr.isEmpty || locTimesListStr.equals(<span class=\"string\">\"null\"</span>)) &#123;</div><div class=\"line\">                esDataMap.put(esDataKey, <span class=\"string\">\"\"</span>)</div><div class=\"line\">              &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">                esDataMap.put(esDataKey, locTimesListStr)</div><div class=\"line\">              &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            bulkRequest.add(client.prepareIndex(esIndex, esType, id).setSource(esDataMap))</div><div class=\"line\">            lineNum += <span class=\"number\">1</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (lineNum % bulkNum == <span class=\"number\">0</span>) &#123;</div><div class=\"line\">              <span class=\"keyword\">val</span> endTime: <span class=\"type\">Long</span> = <span class=\"type\">System</span>.currentTimeMillis</div><div class=\"line\">              println(<span class=\"string\">\"bulk push， current lineNum: \"</span> + lineNum + <span class=\"string\">\", currentTime s: \"</span> + ((endTime - startTime) / <span class=\"number\">1000</span>))</div><div class=\"line\">              <span class=\"keyword\">val</span> bbq: <span class=\"type\">BulkResponse</span> = bulkRequest.execute.actionGet()</div><div class=\"line\">              flag = <span class=\"literal\">true</span></div><div class=\"line\">              <span class=\"keyword\">if</span> (bbq.hasFailures) &#123;</div><div class=\"line\">                println(<span class=\"string\">\"bbq.hasFailures: \"</span> + bbq.toString)</div><div class=\"line\">                bulkRequest.execute.actionGet</div><div class=\"line\">              &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">          &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">if</span> (bulkRequest != <span class=\"literal\">null</span>) &#123;</div><div class=\"line\">          bulkRequest.execute().actionGet()</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        client.close()</div><div class=\"line\">        <span class=\"keyword\">val</span> endTime: <span class=\"type\">Long</span> = <span class=\"type\">System</span>.currentTimeMillis</div><div class=\"line\">        println(<span class=\"string\">\"ths time is: \"</span> + (endTime - startTime) / <span class=\"number\">1000</span> + <span class=\"string\">\"s \"</span>)</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    sc.stop()</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>踩坑说明：在编写代码中踩了如下坑：</p>\n<ol>\n<li><p>依赖冲突的问题： ES5.2与Spark1.6有如下包会产生依赖： <code>netty-all:io.netty</code>，<code>com.fasterxml.jackson.core:jackson-core</code>, <code>org.apache.logging.log4j:log4j-core</code>.<br>解决方案：<br>通过 <code>mvn dependency:tree -Dverbose -Dincludes=com.fasterxml.jackson.core</code> 命令查出依赖原因，然后在pom.xml中增加所需的相关依赖的最高版本；</p>\n</li>\n<li><p>每个bulk的大小，根据网上的经验是10M-15M为宜，大概计算一下就好了；</p>\n</li>\n<li><p>后来在单机测试通过，但在集群模式中还是会出现 netty4的依赖冲突：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">17/07/17 10:21:57 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[elasticsearch[_client_][management][T#1],5,main]</div><div class=\"line\">java.lang.NoSuchMethodError: io.netty.buffer.CompositeByteBuf.addComponents(ZLjava/lang/Iterable;)Lio/netty/buffer/CompositeByteBuf;</div><div class=\"line\">        at org.elasticsearch.transport.netty4.Netty4Utils.toByteBuf(Netty4Utils.java:78)</div><div class=\"line\">        at org.elasticsearch.transport.netty4.Netty4Transport.sendMessage(Netty4Transport.java:422)</div><div class=\"line\">        at org.elasticsearch.transport.netty4.Netty4Transport.sendMessage(Netty4Transport.java:93)</div><div class=\"line\">        at org.elasticsearch.transport.TcpTransport.internalSendMessage(TcpTransport.java:1058)</div><div class=\"line\">        at org.elasticsearch.transport.TcpTransport.sendRequestToChannel(TcpTransport.java:1040)</div><div class=\"line\">        at org.elasticsearch.transport.TcpTransport.executeHandshake(TcpTransport.java:1555)</div><div class=\"line\">        at org.elasticsearch.transport.TcpTransport.openConnection(TcpTransport.java:502)</div><div class=\"line\">        at org.elasticsearch.transport.TcpTransport.connectToNode(TcpTransport.java:460)</div><div class=\"line\">        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:318)</div><div class=\"line\">        at org.elasticsearch.client.transport.TransportClientNodesService$SniffNodesSampler$1.run(TransportClientNodesService.java:488)</div><div class=\"line\">        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:527)</div><div class=\"line\">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</div><div class=\"line\">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</div><div class=\"line\">        at java.lang.Thread.run(Thread.java:745)</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>有一种解决方案我没有尝试成功，就是在pom中将冲突的依赖包exclusions掉，各位感兴趣可以尝试，成功了麻烦告知我一下。参考链接：<a href=\"https://www.elastic.co/blog/to-shade-or-not-to-shade\">https://www.elastic.co/blog/to-shade-or-not-to-shade</a>, 使用 maven-shade-plugin 工具打包。</p>\n<p>上个方法我尝试几次不成功后，使用了比较暴力的方法，直接将ES的netty参数由netty4改成了netty3，<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">.put(&quot;transport.type&quot;, &quot;netty3&quot;).put(&quot;http.type&quot;, &quot;netty3&quot;).build</div></pre></td></tr></table></figure></p>\n<p>好了，打包好之后，程序就可以完美运行了。</p>\n<h3 id=\"ES中创建索引\"><a href=\"#ES中创建索引\" class=\"headerlink\" title=\"ES中创建索引\"></a>ES中创建索引</h3><p>就算如果ES中是自动创建索引的，也希望你能手动创建索引和字段属性，因为默认的字段属性是Text，ES会自动对它进行分词相关的操作，如果ES中存的字符串你不想让它被分隔的话，就用keyword替代为Text类型，命令如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">PUT  /weekend-20170718</div><div class=\"line\">&#123;</div><div class=\"line\">  &quot;settings&quot; : &#123;</div><div class=\"line\">    &quot;index&quot; : &#123;</div><div class=\"line\">      &quot;number_of_shards&quot; : 5, </div><div class=\"line\">      &quot;number_of_replicas&quot; : 1,</div><div class=\"line\">      &quot;refresh_interval&quot; : &quot;60s&quot;</div><div class=\"line\">    &#125;,</div><div class=\"line\">  &quot;index.routing.allocation.include.zone&quot;: &quot;light&quot;</div><div class=\"line\">  &#125;,</div><div class=\"line\">  &quot;mappings&quot;: &#123;</div><div class=\"line\">    &quot;offline&quot;: &#123;</div><div class=\"line\">      &quot;properties&quot;: &#123;</div><div class=\"line\">        &quot;msisdn&quot;: &#123;</div><div class=\"line\">          &quot;type&quot;: &quot;keyword&quot; </div><div class=\"line\">        &#125;,&quot;w0&quot;: &#123;</div><div class=\"line\">          &quot;type&quot;: &quot;keyword&quot; </div><div class=\"line\">        &#125; ...后面省略</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>创建好索引后检查一下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">GET /weekend-20170718/_mapping</div></pre></td></tr></table></figure>\n<h3 id=\"集群中运行\"><a href=\"#集群中运行\" class=\"headerlink\" title=\"集群中运行\"></a>集群中运行</h3><p>这个比较简单，只需要注意以下几点就好了：</p>\n<ol>\n<li>使用jdk1.8版本；</li>\n<li>注意内存的申请，可能会出现跑了一段时间后，内存不够用导致程序退出的情况；</li>\n<li>观测好ES集群的状态，一段时间后，ES机器的GC比较高</li>\n<li>最好别一下子跑所有数据，分几批跑，这样就算出问题，只需要重跑那一部分就好了</li>\n</ol>\n<p>数据：通过观察，导入的速度随着时间的增长呈下降趋势，整体来说，ES集群隔离的小集群共有五台物理机，共2.23亿条，751G的数据导入用了约4.5小时，平均速度为 45M/s, 1.38W条/s。</p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>感谢 技术交流群里 @叫我老陈行了吧，@岑玉海 等大神网友；</p>\n<p><a href=\"https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html\">https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html</a><br><a href=\"https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html\">https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk.html</a></p>\n"},{"title":"spark在yarn中运行jdk8","toc":false,"date":"2017-07-17T10:00:59.000Z","_content":"\n我们hadoop集群使用jdk版本为1.7，由于往5.X的ES中写数据必须要使用jdk1.8，该怎么办呢？\n首先，把hadoop集群升级到jdk1.8是肯定可以的，但是这样代价太大。\n我们通过如下两步操作，就可以在不升级集群的基础上，在yarn上运行用jdk1.8编译的spark程序。\n\n1. yarn集群的每台nodeManager都需要安装jdk1.8，比如我们这边的安装路径是 `/usr/local/jdk1.8.0_111`\n2. spark作业提交的时候，增加如下参数：\n```\n--conf \"spark.yarn.appMasterEnv.JAVA_HOME=/usr/local/jdk1.8.0_111\" --conf \"spark.executorEnv.JAVA_HOME=/usr/local/jdk1.8.0_111\"\n```\n这样就能使用jdk1.8啦，亲测可用","source":"_posts/2017-07-17-spark指定java版本向yarn提交程序.md","raw":"---\ntitle: spark在yarn中运行jdk8\ntoc: false\ndate: 2017-07-17 18:00:59\ntags: spark开发\ncategories: spark开发\n---\n\n我们hadoop集群使用jdk版本为1.7，由于往5.X的ES中写数据必须要使用jdk1.8，该怎么办呢？\n首先，把hadoop集群升级到jdk1.8是肯定可以的，但是这样代价太大。\n我们通过如下两步操作，就可以在不升级集群的基础上，在yarn上运行用jdk1.8编译的spark程序。\n\n1. yarn集群的每台nodeManager都需要安装jdk1.8，比如我们这边的安装路径是 `/usr/local/jdk1.8.0_111`\n2. spark作业提交的时候，增加如下参数：\n```\n--conf \"spark.yarn.appMasterEnv.JAVA_HOME=/usr/local/jdk1.8.0_111\" --conf \"spark.executorEnv.JAVA_HOME=/usr/local/jdk1.8.0_111\"\n```\n这样就能使用jdk1.8啦，亲测可用","slug":"spark指定java版本向yarn提交程序","published":1,"updated":"2017-07-18T05:49:00.494Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfxy0042psgump2ter1o","content":"<p>我们hadoop集群使用jdk版本为1.7，由于往5.X的ES中写数据必须要使用jdk1.8，该怎么办呢？<br>首先，把hadoop集群升级到jdk1.8是肯定可以的，但是这样代价太大。<br>我们通过如下两步操作，就可以在不升级集群的基础上，在yarn上运行用jdk1.8编译的spark程序。</p>\n<ol>\n<li>yarn集群的每台nodeManager都需要安装jdk1.8，比如我们这边的安装路径是 <code>/usr/local/jdk1.8.0_111</code></li>\n<li>spark作业提交的时候，增加如下参数：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">--conf &quot;spark.yarn.appMasterEnv.JAVA_HOME=/usr/local/jdk1.8.0_111&quot; --conf &quot;spark.executorEnv.JAVA_HOME=/usr/local/jdk1.8.0_111&quot;</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>这样就能使用jdk1.8啦，亲测可用</p>\n","excerpt":"","more":"<p>我们hadoop集群使用jdk版本为1.7，由于往5.X的ES中写数据必须要使用jdk1.8，该怎么办呢？<br>首先，把hadoop集群升级到jdk1.8是肯定可以的，但是这样代价太大。<br>我们通过如下两步操作，就可以在不升级集群的基础上，在yarn上运行用jdk1.8编译的spark程序。</p>\n<ol>\n<li>yarn集群的每台nodeManager都需要安装jdk1.8，比如我们这边的安装路径是 <code>/usr/local/jdk1.8.0_111</code></li>\n<li>spark作业提交的时候，增加如下参数：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">--conf &quot;spark.yarn.appMasterEnv.JAVA_HOME=/usr/local/jdk1.8.0_111&quot; --conf &quot;spark.executorEnv.JAVA_HOME=/usr/local/jdk1.8.0_111&quot;</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>这样就能使用jdk1.8啦，亲测可用</p>\n"},{"title":"使用Ansable安装管理Spark客户端","toc":true,"date":"2017-10-31T12:52:03.000Z","_content":"\n随着生产的spark升级到2.2.0，我要为很多用户同时提供spark2.2.0以及spark1.6.2的客户端，这是一个比较繁琐耗时的操作，很烦，况且其它开发项目比较紧张，我决定学习使用Ansable来统一管理它们。\n\n## 关于Ansable\n略 配置管理的思想，基于推送的模式\n\n## 安装spark客户端的思路\n``` scala\nfor (eachNode <- sparkNodes: 所有客户端的机器) {\n\t1. 安装jdk1.8\n\t2. 从主控机传输已经打包好的spark客户端\n\t3. 解压到 /usr/local/spark2.2\n\t4. 配置环境变量\n\t5. (可选)配置history-server等其它客户端\n\t6. 运行自动化测试程序测试，并生成测试报告\n}\n```\n\n\ntele-spark-version-change 1.6|2.2\n通过该命令实现spark版本切换，核心就是 export相应的环境变量和PATH\n\n## 使用方式\n\n0. 首先，得打包编译更新好spark客户端，该配的东西配上，该改的jar包改掉，然后放到主控机的files目录下，修改ansible脚本的一些参数；\n1. ssh到 102.133 （master机器，确保该机器与每台机器的ssh免密打通），cd到 spark_client_deploy.yml 所在目录\n2. 确定想要修改的机器集群名称，修改 hosts文件，比如，要增加新机器组的安装，则增加如下配置：\n``` sh\n[yaxin2]\n10.257.101.128\n10.257.101.131\n10.257.101.132\n```\n3. 执行脚本即可：\n``` sh\nansible-playbook spark_client_deploy.yml \n```\n如果没有啥问题，就会发现如下输出：\n\n```\nPLAY [deploy spark client(for version 1.6.2 or 2.2.1)] ************************ \n\nTASK: [ping] ****************************************************************** \nok: [10.142.101.132]\nok: [10.142.101.131]\nok: [10.142.101.128]\nok: [10.142.101.129]\nok: [10.142.97.124]\nok: [10.142.101.130]\n\nTASK: [mkdir] ***************************************************************** \nok: [10.142.101.129]\nok: [10.142.101.131]\nok: [10.142.101.132]\nok: [10.142.101.128]\nok: [10.142.97.124]\nok: [10.142.101.130]\n\nTASK: [test java8 is exist] *************************************************** \nchanged: [10.142.101.129]\nchanged: [10.142.101.130]\nchanged: [10.142.101.132]\nchanged: [10.142.101.128]\nchanged: [10.142.101.131]\nchanged: [10.142.97.124]\n\nTASK: [copy java8] ************************************************************ \nskipping: [10.142.101.128]\nskipping: [10.142.97.124]\nskipping: [10.142.101.129]\nskipping: [10.142.101.130]\nskipping: [10.142.101.131]\nskipping: [10.142.101.132]\n\nTASK: [extract java] ********************************************************** \nskipping: [10.142.101.129]\nskipping: [10.142.97.124]\nskipping: [10.142.101.128]\nskipping: [10.142.101.132]\nskipping: [10.142.101.130]\nskipping: [10.142.101.131]\n\nTASK: [copy ctcc-spark] ******************************************************* \nok: [10.142.101.131]\nok: [10.142.101.128]\nok: [10.142.101.129]\nok: [10.142.101.132]\nok: [10.142.97.124]\nchanged: [10.142.101.130]\n\nTASK: [rm old spark2] ********************************************************* \nchanged: [10.142.101.130]\nok: [10.142.101.128]\nok: [10.142.101.131]\nok: [10.142.97.124]\nok: [10.142.101.132]\nok: [10.142.101.129]\n\nTASK: [extract ctcc-spark] **************************************************** \nchanged: [10.142.101.130]\nchanged: [10.142.101.131]\nchanged: [10.142.101.129]\nchanged: [10.142.101.132]\nchanged: [10.142.101.128]\nchanged: [10.142.97.124]\n\nPLAY RECAP ******************************************************************** \n10.142.101.128             : ok=6    changed=2    unreachable=0    failed=0   \n10.142.101.129             : ok=6    changed=2    unreachable=0    failed=0   \n10.142.101.130             : ok=6    changed=4    unreachable=0    failed=0   \n10.142.101.131             : ok=6    changed=2    unreachable=0    failed=0   \n10.142.101.132             : ok=6    changed=2    unreachable=0    failed=0   \n10.142.97.124              : ok=6    changed=2    unreachable=0    failed=0   \n```\n\n## 思考\n使用Ansable极大的简化了工作量，而且使得生产的组件管理变得统一，当然这次Ansable的使用很简单，以后有需求可以做很多事情。\n\n## 附 安装脚本\n\n``` sh spark_client_deploy.yml\n---\n# 自动在多台机器上安装spark客户端\n- name: deploy spark client(for version 1.6.2 or 2.2.0)\n  vars:\n    ansible_ssh_user: op\n    ansible_ssh_pass: ******\n    spark_auto_path:  /home/op/spark\n      spark_version: spark2.2.1\n    spark_tar_file: ctcc-spark-2.2.1.tar.gz\n    jdk_tar_file: jdk-8u111-linux-x64.tar.gz\n  hosts: yaxin3_2.2.1\n  gather_facts: false\n  sudo: True\n\n  tasks:\n    - name: ping\n      ping:\n\n    - name: mkdir\n      file: path={{spark_auto_path}} state=directory\n\n    - name: test java8 is exist\n      shell: '/usr/local/jdk1.8.0_111/bin/java -version'\n      register: test_java_result\n      ignore_errors: True\n\n    - name: copy java8\n      copy: backup=no src=files/{{jdk_tar_file}} force=no dest={{spark_auto_path}}/{{jdk_tar_file }}\n      when: test_java_result.rc != 0\n\n    - name: extract java\n#       shell: 'sudo tar -xzvf /home/op/spark/jdk-8u111-linux-x64.tar.gz -C /usr/local/' \n      shell: 'sudo tar -xzvf {{spark_auto_path}}/{{jdk_tar_file }} -C /usr/local/'\n      when: test_java_result.rc != 0\n\n    - name: copy ctcc-spark\n      copy: backup=yes src=files/{{spark_version}}/{{spark_tar_file}} dest={{spark_auto_path}}/{{spark_tar_file}} mode=0755\n\n    - name: rm old spark2\n      file: path=/usr/local/spark2.2 state=absent\n\n    - name: extract ctcc-spark\n      shell: 'sudo tar -xzvf {{spark_auto_path}}/{{spark_tar_file}} -C /usr/local/'\n\n    # - name: spark home\n    #   shell: sudo echo 'export SPARK_HOME=/usr/local/spark2.2'  >> /etc/profile\n```","source":"_posts/2017-10-31-使用Ansable安装管理Spark客户端.md","raw":"---\ntitle: 使用Ansable安装管理Spark客户端\ntoc: true\ndate: 2017-10-31 20:52:03\ntags: spark\ncategories: spark\n---\n\n随着生产的spark升级到2.2.0，我要为很多用户同时提供spark2.2.0以及spark1.6.2的客户端，这是一个比较繁琐耗时的操作，很烦，况且其它开发项目比较紧张，我决定学习使用Ansable来统一管理它们。\n\n## 关于Ansable\n略 配置管理的思想，基于推送的模式\n\n## 安装spark客户端的思路\n``` scala\nfor (eachNode <- sparkNodes: 所有客户端的机器) {\n\t1. 安装jdk1.8\n\t2. 从主控机传输已经打包好的spark客户端\n\t3. 解压到 /usr/local/spark2.2\n\t4. 配置环境变量\n\t5. (可选)配置history-server等其它客户端\n\t6. 运行自动化测试程序测试，并生成测试报告\n}\n```\n\n\ntele-spark-version-change 1.6|2.2\n通过该命令实现spark版本切换，核心就是 export相应的环境变量和PATH\n\n## 使用方式\n\n0. 首先，得打包编译更新好spark客户端，该配的东西配上，该改的jar包改掉，然后放到主控机的files目录下，修改ansible脚本的一些参数；\n1. ssh到 102.133 （master机器，确保该机器与每台机器的ssh免密打通），cd到 spark_client_deploy.yml 所在目录\n2. 确定想要修改的机器集群名称，修改 hosts文件，比如，要增加新机器组的安装，则增加如下配置：\n``` sh\n[yaxin2]\n10.257.101.128\n10.257.101.131\n10.257.101.132\n```\n3. 执行脚本即可：\n``` sh\nansible-playbook spark_client_deploy.yml \n```\n如果没有啥问题，就会发现如下输出：\n\n```\nPLAY [deploy spark client(for version 1.6.2 or 2.2.1)] ************************ \n\nTASK: [ping] ****************************************************************** \nok: [10.142.101.132]\nok: [10.142.101.131]\nok: [10.142.101.128]\nok: [10.142.101.129]\nok: [10.142.97.124]\nok: [10.142.101.130]\n\nTASK: [mkdir] ***************************************************************** \nok: [10.142.101.129]\nok: [10.142.101.131]\nok: [10.142.101.132]\nok: [10.142.101.128]\nok: [10.142.97.124]\nok: [10.142.101.130]\n\nTASK: [test java8 is exist] *************************************************** \nchanged: [10.142.101.129]\nchanged: [10.142.101.130]\nchanged: [10.142.101.132]\nchanged: [10.142.101.128]\nchanged: [10.142.101.131]\nchanged: [10.142.97.124]\n\nTASK: [copy java8] ************************************************************ \nskipping: [10.142.101.128]\nskipping: [10.142.97.124]\nskipping: [10.142.101.129]\nskipping: [10.142.101.130]\nskipping: [10.142.101.131]\nskipping: [10.142.101.132]\n\nTASK: [extract java] ********************************************************** \nskipping: [10.142.101.129]\nskipping: [10.142.97.124]\nskipping: [10.142.101.128]\nskipping: [10.142.101.132]\nskipping: [10.142.101.130]\nskipping: [10.142.101.131]\n\nTASK: [copy ctcc-spark] ******************************************************* \nok: [10.142.101.131]\nok: [10.142.101.128]\nok: [10.142.101.129]\nok: [10.142.101.132]\nok: [10.142.97.124]\nchanged: [10.142.101.130]\n\nTASK: [rm old spark2] ********************************************************* \nchanged: [10.142.101.130]\nok: [10.142.101.128]\nok: [10.142.101.131]\nok: [10.142.97.124]\nok: [10.142.101.132]\nok: [10.142.101.129]\n\nTASK: [extract ctcc-spark] **************************************************** \nchanged: [10.142.101.130]\nchanged: [10.142.101.131]\nchanged: [10.142.101.129]\nchanged: [10.142.101.132]\nchanged: [10.142.101.128]\nchanged: [10.142.97.124]\n\nPLAY RECAP ******************************************************************** \n10.142.101.128             : ok=6    changed=2    unreachable=0    failed=0   \n10.142.101.129             : ok=6    changed=2    unreachable=0    failed=0   \n10.142.101.130             : ok=6    changed=4    unreachable=0    failed=0   \n10.142.101.131             : ok=6    changed=2    unreachable=0    failed=0   \n10.142.101.132             : ok=6    changed=2    unreachable=0    failed=0   \n10.142.97.124              : ok=6    changed=2    unreachable=0    failed=0   \n```\n\n## 思考\n使用Ansable极大的简化了工作量，而且使得生产的组件管理变得统一，当然这次Ansable的使用很简单，以后有需求可以做很多事情。\n\n## 附 安装脚本\n\n``` sh spark_client_deploy.yml\n---\n# 自动在多台机器上安装spark客户端\n- name: deploy spark client(for version 1.6.2 or 2.2.0)\n  vars:\n    ansible_ssh_user: op\n    ansible_ssh_pass: ******\n    spark_auto_path:  /home/op/spark\n      spark_version: spark2.2.1\n    spark_tar_file: ctcc-spark-2.2.1.tar.gz\n    jdk_tar_file: jdk-8u111-linux-x64.tar.gz\n  hosts: yaxin3_2.2.1\n  gather_facts: false\n  sudo: True\n\n  tasks:\n    - name: ping\n      ping:\n\n    - name: mkdir\n      file: path={{spark_auto_path}} state=directory\n\n    - name: test java8 is exist\n      shell: '/usr/local/jdk1.8.0_111/bin/java -version'\n      register: test_java_result\n      ignore_errors: True\n\n    - name: copy java8\n      copy: backup=no src=files/{{jdk_tar_file}} force=no dest={{spark_auto_path}}/{{jdk_tar_file }}\n      when: test_java_result.rc != 0\n\n    - name: extract java\n#       shell: 'sudo tar -xzvf /home/op/spark/jdk-8u111-linux-x64.tar.gz -C /usr/local/' \n      shell: 'sudo tar -xzvf {{spark_auto_path}}/{{jdk_tar_file }} -C /usr/local/'\n      when: test_java_result.rc != 0\n\n    - name: copy ctcc-spark\n      copy: backup=yes src=files/{{spark_version}}/{{spark_tar_file}} dest={{spark_auto_path}}/{{spark_tar_file}} mode=0755\n\n    - name: rm old spark2\n      file: path=/usr/local/spark2.2 state=absent\n\n    - name: extract ctcc-spark\n      shell: 'sudo tar -xzvf {{spark_auto_path}}/{{spark_tar_file}} -C /usr/local/'\n\n    # - name: spark home\n    #   shell: sudo echo 'export SPARK_HOME=/usr/local/spark2.2'  >> /etc/profile\n```","slug":"使用Ansable安装管理Spark客户端","published":1,"updated":"2018-01-10T07:54:29.807Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfy10046psgueqkcoia2","content":"<p>随着生产的spark升级到2.2.0，我要为很多用户同时提供spark2.2.0以及spark1.6.2的客户端，这是一个比较繁琐耗时的操作，很烦，况且其它开发项目比较紧张，我决定学习使用Ansable来统一管理它们。</p>\n<h2 id=\"关于Ansable\"><a href=\"#关于Ansable\" class=\"headerlink\" title=\"关于Ansable\"></a>关于Ansable</h2><p>略 配置管理的思想，基于推送的模式</p>\n<h2 id=\"安装spark客户端的思路\"><a href=\"#安装spark客户端的思路\" class=\"headerlink\" title=\"安装spark客户端的思路\"></a>安装spark客户端的思路</h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> (eachNode &lt;- sparkNodes: 所有客户端的机器) &#123;</div><div class=\"line\">\t<span class=\"number\">1.</span> 安装jdk1<span class=\"number\">.8</span></div><div class=\"line\">\t<span class=\"number\">2.</span> 从主控机传输已经打包好的spark客户端</div><div class=\"line\">\t<span class=\"number\">3.</span> 解压到 /usr/local/spark2<span class=\"number\">.2</span></div><div class=\"line\">\t<span class=\"number\">4.</span> 配置环境变量</div><div class=\"line\">\t<span class=\"number\">5.</span> (可选)配置history-server等其它客户端</div><div class=\"line\">\t<span class=\"number\">6.</span> 运行自动化测试程序测试，并生成测试报告</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>tele-spark-version-change 1.6|2.2<br>通过该命令实现spark版本切换，核心就是 export相应的环境变量和PATH</p>\n<h2 id=\"使用方式\"><a href=\"#使用方式\" class=\"headerlink\" title=\"使用方式\"></a>使用方式</h2><ol>\n<li>首先，得打包编译更新好spark客户端，该配的东西配上，该改的jar包改掉，然后放到主控机的files目录下，修改ansible脚本的一些参数；</li>\n<li>ssh到 102.133 （master机器，确保该机器与每台机器的ssh免密打通），cd到 spark_client_deploy.yml 所在目录</li>\n<li><p>确定想要修改的机器集群名称，修改 hosts文件，比如，要增加新机器组的安装，则增加如下配置：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[yaxin2]</div><div class=\"line\">10.257.101.128</div><div class=\"line\">10.257.101.131</div><div class=\"line\">10.257.101.132</div></pre></td></tr></table></figure>\n</li>\n<li><p>执行脚本即可：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ansible-playbook spark_client_deploy.yml</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>如果没有啥问题，就会发现如下输出：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div></pre></td><td class=\"code\"><pre><div class=\"line\">PLAY [deploy spark client(for version 1.6.2 or 2.2.1)] ************************ </div><div class=\"line\"></div><div class=\"line\">TASK: [ping] ****************************************************************** </div><div class=\"line\">ok: [10.142.101.132]</div><div class=\"line\">ok: [10.142.101.131]</div><div class=\"line\">ok: [10.142.101.128]</div><div class=\"line\">ok: [10.142.101.129]</div><div class=\"line\">ok: [10.142.97.124]</div><div class=\"line\">ok: [10.142.101.130]</div><div class=\"line\"></div><div class=\"line\">TASK: [mkdir] ***************************************************************** </div><div class=\"line\">ok: [10.142.101.129]</div><div class=\"line\">ok: [10.142.101.131]</div><div class=\"line\">ok: [10.142.101.132]</div><div class=\"line\">ok: [10.142.101.128]</div><div class=\"line\">ok: [10.142.97.124]</div><div class=\"line\">ok: [10.142.101.130]</div><div class=\"line\"></div><div class=\"line\">TASK: [test java8 is exist] *************************************************** </div><div class=\"line\">changed: [10.142.101.129]</div><div class=\"line\">changed: [10.142.101.130]</div><div class=\"line\">changed: [10.142.101.132]</div><div class=\"line\">changed: [10.142.101.128]</div><div class=\"line\">changed: [10.142.101.131]</div><div class=\"line\">changed: [10.142.97.124]</div><div class=\"line\"></div><div class=\"line\">TASK: [copy java8] ************************************************************ </div><div class=\"line\">skipping: [10.142.101.128]</div><div class=\"line\">skipping: [10.142.97.124]</div><div class=\"line\">skipping: [10.142.101.129]</div><div class=\"line\">skipping: [10.142.101.130]</div><div class=\"line\">skipping: [10.142.101.131]</div><div class=\"line\">skipping: [10.142.101.132]</div><div class=\"line\"></div><div class=\"line\">TASK: [extract java] ********************************************************** </div><div class=\"line\">skipping: [10.142.101.129]</div><div class=\"line\">skipping: [10.142.97.124]</div><div class=\"line\">skipping: [10.142.101.128]</div><div class=\"line\">skipping: [10.142.101.132]</div><div class=\"line\">skipping: [10.142.101.130]</div><div class=\"line\">skipping: [10.142.101.131]</div><div class=\"line\"></div><div class=\"line\">TASK: [copy ctcc-spark] ******************************************************* </div><div class=\"line\">ok: [10.142.101.131]</div><div class=\"line\">ok: [10.142.101.128]</div><div class=\"line\">ok: [10.142.101.129]</div><div class=\"line\">ok: [10.142.101.132]</div><div class=\"line\">ok: [10.142.97.124]</div><div class=\"line\">changed: [10.142.101.130]</div><div class=\"line\"></div><div class=\"line\">TASK: [rm old spark2] ********************************************************* </div><div class=\"line\">changed: [10.142.101.130]</div><div class=\"line\">ok: [10.142.101.128]</div><div class=\"line\">ok: [10.142.101.131]</div><div class=\"line\">ok: [10.142.97.124]</div><div class=\"line\">ok: [10.142.101.132]</div><div class=\"line\">ok: [10.142.101.129]</div><div class=\"line\"></div><div class=\"line\">TASK: [extract ctcc-spark] **************************************************** </div><div class=\"line\">changed: [10.142.101.130]</div><div class=\"line\">changed: [10.142.101.131]</div><div class=\"line\">changed: [10.142.101.129]</div><div class=\"line\">changed: [10.142.101.132]</div><div class=\"line\">changed: [10.142.101.128]</div><div class=\"line\">changed: [10.142.97.124]</div><div class=\"line\"></div><div class=\"line\">PLAY RECAP ******************************************************************** </div><div class=\"line\">10.142.101.128             : ok=6    changed=2    unreachable=0    failed=0   </div><div class=\"line\">10.142.101.129             : ok=6    changed=2    unreachable=0    failed=0   </div><div class=\"line\">10.142.101.130             : ok=6    changed=4    unreachable=0    failed=0   </div><div class=\"line\">10.142.101.131             : ok=6    changed=2    unreachable=0    failed=0   </div><div class=\"line\">10.142.101.132             : ok=6    changed=2    unreachable=0    failed=0   </div><div class=\"line\">10.142.97.124              : ok=6    changed=2    unreachable=0    failed=0</div></pre></td></tr></table></figure>\n<h2 id=\"思考\"><a href=\"#思考\" class=\"headerlink\" title=\"思考\"></a>思考</h2><p>使用Ansable极大的简化了工作量，而且使得生产的组件管理变得统一，当然这次Ansable的使用很简单，以后有需求可以做很多事情。</p>\n<h2 id=\"附-安装脚本\"><a href=\"#附-安装脚本\" class=\"headerlink\" title=\"附 安装脚本\"></a>附 安装脚本</h2><figure class=\"highlight sh\"><figcaption><span>spark_client_deploy.yml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">---</div><div class=\"line\"><span class=\"comment\"># 自动在多台机器上安装spark客户端</span></div><div class=\"line\">- name: deploy spark client(<span class=\"keyword\">for</span> version 1.6.2 or 2.2.0)</div><div class=\"line\">  vars:</div><div class=\"line\">    ansible_ssh_user: op</div><div class=\"line\">    ansible_ssh_pass: ******</div><div class=\"line\">    spark_auto_path:  /home/op/spark</div><div class=\"line\">      spark_version: spark2.2.1</div><div class=\"line\">    spark_tar_file: ctcc-spark-2.2.1.tar.gz</div><div class=\"line\">    jdk_tar_file: jdk-8u111-linux-x64.tar.gz</div><div class=\"line\">  hosts: yaxin3_2.2.1</div><div class=\"line\">  gather_facts: <span class=\"literal\">false</span></div><div class=\"line\">  sudo: True</div><div class=\"line\"></div><div class=\"line\">  tasks:</div><div class=\"line\">    - name: ping</div><div class=\"line\">      ping:</div><div class=\"line\"></div><div class=\"line\">    - name: mkdir</div><div class=\"line\">      file: path=&#123;&#123;spark_auto_path&#125;&#125; state=directory</div><div class=\"line\"></div><div class=\"line\">    - name: <span class=\"built_in\">test</span> java8 is exist</div><div class=\"line\">      shell: <span class=\"string\">'/usr/local/jdk1.8.0_111/bin/java -version'</span></div><div class=\"line\">      register: test_java_result</div><div class=\"line\">      ignore_errors: True</div><div class=\"line\"></div><div class=\"line\">    - name: copy java8</div><div class=\"line\">      copy: backup=no src=files/&#123;&#123;jdk_tar_file&#125;&#125; force=no dest=&#123;&#123;spark_auto_path&#125;&#125;/&#123;&#123;jdk_tar_file &#125;&#125;</div><div class=\"line\">      when: test_java_result.rc != 0</div><div class=\"line\"></div><div class=\"line\">    - name: extract java</div><div class=\"line\"><span class=\"comment\">#       shell: 'sudo tar -xzvf /home/op/spark/jdk-8u111-linux-x64.tar.gz -C /usr/local/' </span></div><div class=\"line\">      shell: <span class=\"string\">'sudo tar -xzvf &#123;&#123;spark_auto_path&#125;&#125;/&#123;&#123;jdk_tar_file &#125;&#125; -C /usr/local/'</span></div><div class=\"line\">      when: test_java_result.rc != 0</div><div class=\"line\"></div><div class=\"line\">    - name: copy ctcc-spark</div><div class=\"line\">      copy: backup=yes src=files/&#123;&#123;spark_version&#125;&#125;/&#123;&#123;spark_tar_file&#125;&#125; dest=&#123;&#123;spark_auto_path&#125;&#125;/&#123;&#123;spark_tar_file&#125;&#125; mode=0755</div><div class=\"line\"></div><div class=\"line\">    - name: rm old spark2</div><div class=\"line\">      file: path=/usr/<span class=\"built_in\">local</span>/spark2.2 state=absent</div><div class=\"line\"></div><div class=\"line\">    - name: extract ctcc-spark</div><div class=\"line\">      shell: <span class=\"string\">'sudo tar -xzvf &#123;&#123;spark_auto_path&#125;&#125;/&#123;&#123;spark_tar_file&#125;&#125; -C /usr/local/'</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># - name: spark home</span></div><div class=\"line\">    <span class=\"comment\">#   shell: sudo echo 'export SPARK_HOME=/usr/local/spark2.2'  &gt;&gt; /etc/profile</span></div></pre></td></tr></table></figure>","excerpt":"","more":"<p>随着生产的spark升级到2.2.0，我要为很多用户同时提供spark2.2.0以及spark1.6.2的客户端，这是一个比较繁琐耗时的操作，很烦，况且其它开发项目比较紧张，我决定学习使用Ansable来统一管理它们。</p>\n<h2 id=\"关于Ansable\"><a href=\"#关于Ansable\" class=\"headerlink\" title=\"关于Ansable\"></a>关于Ansable</h2><p>略 配置管理的思想，基于推送的模式</p>\n<h2 id=\"安装spark客户端的思路\"><a href=\"#安装spark客户端的思路\" class=\"headerlink\" title=\"安装spark客户端的思路\"></a>安装spark客户端的思路</h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> (eachNode &lt;- sparkNodes: 所有客户端的机器) &#123;</div><div class=\"line\">\t<span class=\"number\">1.</span> 安装jdk1<span class=\"number\">.8</span></div><div class=\"line\">\t<span class=\"number\">2.</span> 从主控机传输已经打包好的spark客户端</div><div class=\"line\">\t<span class=\"number\">3.</span> 解压到 /usr/local/spark2<span class=\"number\">.2</span></div><div class=\"line\">\t<span class=\"number\">4.</span> 配置环境变量</div><div class=\"line\">\t<span class=\"number\">5.</span> (可选)配置history-server等其它客户端</div><div class=\"line\">\t<span class=\"number\">6.</span> 运行自动化测试程序测试，并生成测试报告</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>tele-spark-version-change 1.6|2.2<br>通过该命令实现spark版本切换，核心就是 export相应的环境变量和PATH</p>\n<h2 id=\"使用方式\"><a href=\"#使用方式\" class=\"headerlink\" title=\"使用方式\"></a>使用方式</h2><ol>\n<li>首先，得打包编译更新好spark客户端，该配的东西配上，该改的jar包改掉，然后放到主控机的files目录下，修改ansible脚本的一些参数；</li>\n<li>ssh到 102.133 （master机器，确保该机器与每台机器的ssh免密打通），cd到 spark_client_deploy.yml 所在目录</li>\n<li><p>确定想要修改的机器集群名称，修改 hosts文件，比如，要增加新机器组的安装，则增加如下配置：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[yaxin2]</div><div class=\"line\">10.257.101.128</div><div class=\"line\">10.257.101.131</div><div class=\"line\">10.257.101.132</div></pre></td></tr></table></figure>\n</li>\n<li><p>执行脚本即可：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ansible-playbook spark_client_deploy.yml</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>如果没有啥问题，就会发现如下输出：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div></pre></td><td class=\"code\"><pre><div class=\"line\">PLAY [deploy spark client(for version 1.6.2 or 2.2.1)] ************************ </div><div class=\"line\"></div><div class=\"line\">TASK: [ping] ****************************************************************** </div><div class=\"line\">ok: [10.142.101.132]</div><div class=\"line\">ok: [10.142.101.131]</div><div class=\"line\">ok: [10.142.101.128]</div><div class=\"line\">ok: [10.142.101.129]</div><div class=\"line\">ok: [10.142.97.124]</div><div class=\"line\">ok: [10.142.101.130]</div><div class=\"line\"></div><div class=\"line\">TASK: [mkdir] ***************************************************************** </div><div class=\"line\">ok: [10.142.101.129]</div><div class=\"line\">ok: [10.142.101.131]</div><div class=\"line\">ok: [10.142.101.132]</div><div class=\"line\">ok: [10.142.101.128]</div><div class=\"line\">ok: [10.142.97.124]</div><div class=\"line\">ok: [10.142.101.130]</div><div class=\"line\"></div><div class=\"line\">TASK: [test java8 is exist] *************************************************** </div><div class=\"line\">changed: [10.142.101.129]</div><div class=\"line\">changed: [10.142.101.130]</div><div class=\"line\">changed: [10.142.101.132]</div><div class=\"line\">changed: [10.142.101.128]</div><div class=\"line\">changed: [10.142.101.131]</div><div class=\"line\">changed: [10.142.97.124]</div><div class=\"line\"></div><div class=\"line\">TASK: [copy java8] ************************************************************ </div><div class=\"line\">skipping: [10.142.101.128]</div><div class=\"line\">skipping: [10.142.97.124]</div><div class=\"line\">skipping: [10.142.101.129]</div><div class=\"line\">skipping: [10.142.101.130]</div><div class=\"line\">skipping: [10.142.101.131]</div><div class=\"line\">skipping: [10.142.101.132]</div><div class=\"line\"></div><div class=\"line\">TASK: [extract java] ********************************************************** </div><div class=\"line\">skipping: [10.142.101.129]</div><div class=\"line\">skipping: [10.142.97.124]</div><div class=\"line\">skipping: [10.142.101.128]</div><div class=\"line\">skipping: [10.142.101.132]</div><div class=\"line\">skipping: [10.142.101.130]</div><div class=\"line\">skipping: [10.142.101.131]</div><div class=\"line\"></div><div class=\"line\">TASK: [copy ctcc-spark] ******************************************************* </div><div class=\"line\">ok: [10.142.101.131]</div><div class=\"line\">ok: [10.142.101.128]</div><div class=\"line\">ok: [10.142.101.129]</div><div class=\"line\">ok: [10.142.101.132]</div><div class=\"line\">ok: [10.142.97.124]</div><div class=\"line\">changed: [10.142.101.130]</div><div class=\"line\"></div><div class=\"line\">TASK: [rm old spark2] ********************************************************* </div><div class=\"line\">changed: [10.142.101.130]</div><div class=\"line\">ok: [10.142.101.128]</div><div class=\"line\">ok: [10.142.101.131]</div><div class=\"line\">ok: [10.142.97.124]</div><div class=\"line\">ok: [10.142.101.132]</div><div class=\"line\">ok: [10.142.101.129]</div><div class=\"line\"></div><div class=\"line\">TASK: [extract ctcc-spark] **************************************************** </div><div class=\"line\">changed: [10.142.101.130]</div><div class=\"line\">changed: [10.142.101.131]</div><div class=\"line\">changed: [10.142.101.129]</div><div class=\"line\">changed: [10.142.101.132]</div><div class=\"line\">changed: [10.142.101.128]</div><div class=\"line\">changed: [10.142.97.124]</div><div class=\"line\"></div><div class=\"line\">PLAY RECAP ******************************************************************** </div><div class=\"line\">10.142.101.128             : ok=6    changed=2    unreachable=0    failed=0   </div><div class=\"line\">10.142.101.129             : ok=6    changed=2    unreachable=0    failed=0   </div><div class=\"line\">10.142.101.130             : ok=6    changed=4    unreachable=0    failed=0   </div><div class=\"line\">10.142.101.131             : ok=6    changed=2    unreachable=0    failed=0   </div><div class=\"line\">10.142.101.132             : ok=6    changed=2    unreachable=0    failed=0   </div><div class=\"line\">10.142.97.124              : ok=6    changed=2    unreachable=0    failed=0</div></pre></td></tr></table></figure>\n<h2 id=\"思考\"><a href=\"#思考\" class=\"headerlink\" title=\"思考\"></a>思考</h2><p>使用Ansable极大的简化了工作量，而且使得生产的组件管理变得统一，当然这次Ansable的使用很简单，以后有需求可以做很多事情。</p>\n<h2 id=\"附-安装脚本\"><a href=\"#附-安装脚本\" class=\"headerlink\" title=\"附 安装脚本\"></a>附 安装脚本</h2><figure class=\"highlight sh\"><figcaption><span>spark_client_deploy.yml</span></figcaption><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">---</div><div class=\"line\"><span class=\"comment\"># 自动在多台机器上安装spark客户端</span></div><div class=\"line\">- name: deploy spark client(<span class=\"keyword\">for</span> version 1.6.2 or 2.2.0)</div><div class=\"line\">  vars:</div><div class=\"line\">    ansible_ssh_user: op</div><div class=\"line\">    ansible_ssh_pass: ******</div><div class=\"line\">    spark_auto_path:  /home/op/spark</div><div class=\"line\">      spark_version: spark2.2.1</div><div class=\"line\">    spark_tar_file: ctcc-spark-2.2.1.tar.gz</div><div class=\"line\">    jdk_tar_file: jdk-8u111-linux-x64.tar.gz</div><div class=\"line\">  hosts: yaxin3_2.2.1</div><div class=\"line\">  gather_facts: <span class=\"literal\">false</span></div><div class=\"line\">  sudo: True</div><div class=\"line\"></div><div class=\"line\">  tasks:</div><div class=\"line\">    - name: ping</div><div class=\"line\">      ping:</div><div class=\"line\"></div><div class=\"line\">    - name: mkdir</div><div class=\"line\">      file: path=&#123;&#123;spark_auto_path&#125;&#125; state=directory</div><div class=\"line\"></div><div class=\"line\">    - name: <span class=\"built_in\">test</span> java8 is exist</div><div class=\"line\">      shell: <span class=\"string\">'/usr/local/jdk1.8.0_111/bin/java -version'</span></div><div class=\"line\">      register: test_java_result</div><div class=\"line\">      ignore_errors: True</div><div class=\"line\"></div><div class=\"line\">    - name: copy java8</div><div class=\"line\">      copy: backup=no src=files/&#123;&#123;jdk_tar_file&#125;&#125; force=no dest=&#123;&#123;spark_auto_path&#125;&#125;/&#123;&#123;jdk_tar_file &#125;&#125;</div><div class=\"line\">      when: test_java_result.rc != 0</div><div class=\"line\"></div><div class=\"line\">    - name: extract java</div><div class=\"line\"><span class=\"comment\">#       shell: 'sudo tar -xzvf /home/op/spark/jdk-8u111-linux-x64.tar.gz -C /usr/local/' </span></div><div class=\"line\">      shell: <span class=\"string\">'sudo tar -xzvf &#123;&#123;spark_auto_path&#125;&#125;/&#123;&#123;jdk_tar_file &#125;&#125; -C /usr/local/'</span></div><div class=\"line\">      when: test_java_result.rc != 0</div><div class=\"line\"></div><div class=\"line\">    - name: copy ctcc-spark</div><div class=\"line\">      copy: backup=yes src=files/&#123;&#123;spark_version&#125;&#125;/&#123;&#123;spark_tar_file&#125;&#125; dest=&#123;&#123;spark_auto_path&#125;&#125;/&#123;&#123;spark_tar_file&#125;&#125; mode=0755</div><div class=\"line\"></div><div class=\"line\">    - name: rm old spark2</div><div class=\"line\">      file: path=/usr/<span class=\"built_in\">local</span>/spark2.2 state=absent</div><div class=\"line\"></div><div class=\"line\">    - name: extract ctcc-spark</div><div class=\"line\">      shell: <span class=\"string\">'sudo tar -xzvf &#123;&#123;spark_auto_path&#125;&#125;/&#123;&#123;spark_tar_file&#125;&#125; -C /usr/local/'</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># - name: spark home</span></div><div class=\"line\">    <span class=\"comment\">#   shell: sudo echo 'export SPARK_HOME=/usr/local/spark2.2'  &gt;&gt; /etc/profile</span></div></pre></td></tr></table></figure>"},{"title":"spark2.2新版本编译打包","toc":true,"date":"2018-01-08T11:25:39.000Z","_content":"\n之前已经多次打包过spark了，但是每次重新打包的时候都忘记一些细节，并且spark1.6与2.2打包也有不同，与官网推荐有一些出入，现在就spark2.2.1的打包，总结如下：\n\n由于直接下载预编译好的spark与我们的集群环境不够符合，因此需要下载源码后本地编译\n参考：\nhttp://spark.apache.org/docs/latest/building-spark.html\n\n### 前提\n安装好了**3.5版本以上的maven** 与java8+ ,spark 2.2.0已经取消了 Java 7的支持，Scala使用 2.11版本\n我的环境如下\n``` sh\n$ scala -version\nScala code runner version 2.11.11 -- Copyright 2002-2017, LAMP/EPFL\n\nAdam@Adam-PC MINGW64 /d/spark\n$ java -version\njava version \"1.8.0_102\"\nJava(TM) SE Runtime Environment (build 1.8.0_102-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode)\n\nAdam@Adam-PC MINGW64 /d/spark\n$ mvn -v\nApache Maven 3.5.0 (ff8f5e7444045639af65f6095c62210b5713f426; 2017-04-04T03:39:06+08:00)\n```\n\n#### 1. 下载源代码\nhttp://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.2.1/spark-2.2.1.tgz\n\n解压到 `D:\\spark\\spark-2.2.1`,然后进入这个目录\n#### 2. 修改pom.xml文件\n加入cloudera的代码仓库，其中可以在profile里面添加相应版本的文件，添加如下内容:\n``` xml\n<repository>  \n  <id>cloudera-repo</id>  \n  <name>Cloudera Repository</name>  \n  <url>https://repository.cloudera.com/artifactory/cloudera-repos</url>  \n  <releases>  \n    <enabled>true</enabled> \n  </releases>  \n  <snapshots>  \n    <enabled>false</enabled>  \n  </snapshots>  \n</repository>  \n\n\n<profile>  \n  <id>cdh5.4.7</id>  \n  <properties>  \n    <hadoop.version>2.6.0-cdh5.4.7</hadoop.version>  \n    <hbase.version>1.2.4-cdh5.4.7</hbase.version>  \n    <zookeeper.version>3.4.5-cdh5.4.7</zookeeper.version>  \n  </properties>  \n</profile>\n```\n\n#### 3. 使用make-distribution编译\n\n设置maven的Memory usage\n``` sh\nexport MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=512m\"\n```\n``` sh\n./dev/change-scala-version.sh 2.11\n```\n\n\n#### 4. 编译方法一：使用make-distribution编译\n```\n./dev/make-distribution.sh --name ctcc --tgz -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.4.7 -Dscala-2.11 -Phive -Phive-thriftserver\n```\n\n#### 5. 编译方法二：使用mvn编译\n```\n./build/mvn  -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.4.7 -Dscala-2.11 -Phive -Phive-thriftserver -DskipTests clean package\n\n// 然后将make-distribution.sh中@BUILD_COMMAND那部分注释掉\n\n./dev/make-distribution.sh --name hadoop-2.6.0-cdh5.4.7 --tgz -Phadoop-2.6  -Dhadoop.version=hadoop-2.6.0-cdh5.4.7 -Phive -Phive-thriftserver -Pyarn\n\n#### 6.编译完成\n编译成功后，就会发现多了一个 spark-2.2.1-bin-ctcc.tgz 的文件，解压后就能使用了，我们还要替换一些与hive相关的jar包，比如 hive-exec-1.2.1.spark2.jar 包，替换方法很简单，直接将自己打的包与 /jars 里的包替换一下就好了；\n\n#### 7. 客户端准备\n这一部分仅作为记录使用，每个公司使用情况不同，我们是提前准备好客户端，然后下发到每一台机器中，具体步骤如下：\n1. 替换掉有bug的jar包： hive-exec-1.2.1.spark2.jar等，这个是我们自己的hive的定制化；\n2. 在lib目录下增加一些需要的包，比如hbase相关的包；\n3. 在conf目录下配置统一的配置文件，比如 定制化的 hive-site.xml，hbase-site.xml 以及一些通用的配置，比如  spark-defaults.conf  spark-env.sh 等。小版本升级的情况下，可以将conf配置文件配置到其它位置，然后以软链接的形式链接过来。\n\n#### 一些报错的解决\n运行的过程中报错：\n``` sh\n[ERROR] Failed to execute goal on project spark-launcher_2.11: Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.2.0: Could not find artifact org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.4.7 in central (https://repo1.maven.org/maven2) -> [Help 1]\n```\n原因是官网是已经无法找到 hadoop-client:jar:2.6.0-cdh5.4.7 这个版本的jar包\n\n从 (https://repository.cloudera.com/content/repositories/releases/org/apache/hadoop/hadoop-client/2.6.0-cdh5.4.7/hadoop-client-2.6.0-cdh5.4.7.jar) 上下载，然后移动懂mvn 本地仓库中\nC:\\Users\\Adam\\.m2\\repository\\org\\apache\\hadoop\\hadoop-client\\2.6.0-cdh5.4.7\n\n如果第一次编译出错，在第二次重新编译的时候，会出现一个目录无法删除的情况，导致如下报错：\n``` sh\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:3.0.0:clean (default-clean) on project spark-tags_2.11: Failed to clean project: Failed to delete D:\\spark\\spark-2.2.0-ctcc\\spark-2.2.0\\common\\tags\\target\\spark-tags_2.11-2.2.0.jar -> [Help 1]\n```\n解决方案：跟zinc没有退出有关，在windows当前进程中找到一个Java进程，占用约800M的内存，结束这个进程\n\n再次编译，过段时间继续报错：\n``` sh\n[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile (scala-compile-first) on project spark-network-yarn_2.11: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile failed. CompileFailed -> [Help 1]\n```\n解决方案：\n删除 C:\\Users\\Adam\\.m2\\repository\\net\\alchim31\\maven\\scala-maven-plugin\\3.2.2 文件，然后重新编译 \n\n会在根目录直接生成 spark-1.6.2-bin-2.6.0-cdh5.4.7.tgz","source":"_posts/2018-01-08-spark2-2新版本编译打包.md","raw":"---\ntitle: spark2.2新版本编译打包\ntoc: true\ndate: 2018-01-08 19:25:39\ntags: spark\ncategories: spark\n---\n\n之前已经多次打包过spark了，但是每次重新打包的时候都忘记一些细节，并且spark1.6与2.2打包也有不同，与官网推荐有一些出入，现在就spark2.2.1的打包，总结如下：\n\n由于直接下载预编译好的spark与我们的集群环境不够符合，因此需要下载源码后本地编译\n参考：\nhttp://spark.apache.org/docs/latest/building-spark.html\n\n### 前提\n安装好了**3.5版本以上的maven** 与java8+ ,spark 2.2.0已经取消了 Java 7的支持，Scala使用 2.11版本\n我的环境如下\n``` sh\n$ scala -version\nScala code runner version 2.11.11 -- Copyright 2002-2017, LAMP/EPFL\n\nAdam@Adam-PC MINGW64 /d/spark\n$ java -version\njava version \"1.8.0_102\"\nJava(TM) SE Runtime Environment (build 1.8.0_102-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode)\n\nAdam@Adam-PC MINGW64 /d/spark\n$ mvn -v\nApache Maven 3.5.0 (ff8f5e7444045639af65f6095c62210b5713f426; 2017-04-04T03:39:06+08:00)\n```\n\n#### 1. 下载源代码\nhttp://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.2.1/spark-2.2.1.tgz\n\n解压到 `D:\\spark\\spark-2.2.1`,然后进入这个目录\n#### 2. 修改pom.xml文件\n加入cloudera的代码仓库，其中可以在profile里面添加相应版本的文件，添加如下内容:\n``` xml\n<repository>  \n  <id>cloudera-repo</id>  \n  <name>Cloudera Repository</name>  \n  <url>https://repository.cloudera.com/artifactory/cloudera-repos</url>  \n  <releases>  \n    <enabled>true</enabled> \n  </releases>  \n  <snapshots>  \n    <enabled>false</enabled>  \n  </snapshots>  \n</repository>  \n\n\n<profile>  \n  <id>cdh5.4.7</id>  \n  <properties>  \n    <hadoop.version>2.6.0-cdh5.4.7</hadoop.version>  \n    <hbase.version>1.2.4-cdh5.4.7</hbase.version>  \n    <zookeeper.version>3.4.5-cdh5.4.7</zookeeper.version>  \n  </properties>  \n</profile>\n```\n\n#### 3. 使用make-distribution编译\n\n设置maven的Memory usage\n``` sh\nexport MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=512m\"\n```\n``` sh\n./dev/change-scala-version.sh 2.11\n```\n\n\n#### 4. 编译方法一：使用make-distribution编译\n```\n./dev/make-distribution.sh --name ctcc --tgz -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.4.7 -Dscala-2.11 -Phive -Phive-thriftserver\n```\n\n#### 5. 编译方法二：使用mvn编译\n```\n./build/mvn  -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.4.7 -Dscala-2.11 -Phive -Phive-thriftserver -DskipTests clean package\n\n// 然后将make-distribution.sh中@BUILD_COMMAND那部分注释掉\n\n./dev/make-distribution.sh --name hadoop-2.6.0-cdh5.4.7 --tgz -Phadoop-2.6  -Dhadoop.version=hadoop-2.6.0-cdh5.4.7 -Phive -Phive-thriftserver -Pyarn\n\n#### 6.编译完成\n编译成功后，就会发现多了一个 spark-2.2.1-bin-ctcc.tgz 的文件，解压后就能使用了，我们还要替换一些与hive相关的jar包，比如 hive-exec-1.2.1.spark2.jar 包，替换方法很简单，直接将自己打的包与 /jars 里的包替换一下就好了；\n\n#### 7. 客户端准备\n这一部分仅作为记录使用，每个公司使用情况不同，我们是提前准备好客户端，然后下发到每一台机器中，具体步骤如下：\n1. 替换掉有bug的jar包： hive-exec-1.2.1.spark2.jar等，这个是我们自己的hive的定制化；\n2. 在lib目录下增加一些需要的包，比如hbase相关的包；\n3. 在conf目录下配置统一的配置文件，比如 定制化的 hive-site.xml，hbase-site.xml 以及一些通用的配置，比如  spark-defaults.conf  spark-env.sh 等。小版本升级的情况下，可以将conf配置文件配置到其它位置，然后以软链接的形式链接过来。\n\n#### 一些报错的解决\n运行的过程中报错：\n``` sh\n[ERROR] Failed to execute goal on project spark-launcher_2.11: Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.2.0: Could not find artifact org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.4.7 in central (https://repo1.maven.org/maven2) -> [Help 1]\n```\n原因是官网是已经无法找到 hadoop-client:jar:2.6.0-cdh5.4.7 这个版本的jar包\n\n从 (https://repository.cloudera.com/content/repositories/releases/org/apache/hadoop/hadoop-client/2.6.0-cdh5.4.7/hadoop-client-2.6.0-cdh5.4.7.jar) 上下载，然后移动懂mvn 本地仓库中\nC:\\Users\\Adam\\.m2\\repository\\org\\apache\\hadoop\\hadoop-client\\2.6.0-cdh5.4.7\n\n如果第一次编译出错，在第二次重新编译的时候，会出现一个目录无法删除的情况，导致如下报错：\n``` sh\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:3.0.0:clean (default-clean) on project spark-tags_2.11: Failed to clean project: Failed to delete D:\\spark\\spark-2.2.0-ctcc\\spark-2.2.0\\common\\tags\\target\\spark-tags_2.11-2.2.0.jar -> [Help 1]\n```\n解决方案：跟zinc没有退出有关，在windows当前进程中找到一个Java进程，占用约800M的内存，结束这个进程\n\n再次编译，过段时间继续报错：\n``` sh\n[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile (scala-compile-first) on project spark-network-yarn_2.11: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile failed. CompileFailed -> [Help 1]\n```\n解决方案：\n删除 C:\\Users\\Adam\\.m2\\repository\\net\\alchim31\\maven\\scala-maven-plugin\\3.2.2 文件，然后重新编译 \n\n会在根目录直接生成 spark-1.6.2-bin-2.6.0-cdh5.4.7.tgz","slug":"spark2-2新版本编译打包","published":1,"updated":"2018-01-09T08:25:39.799Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjldkwfy4004apsgubcq6f548","content":"<p>之前已经多次打包过spark了，但是每次重新打包的时候都忘记一些细节，并且spark1.6与2.2打包也有不同，与官网推荐有一些出入，现在就spark2.2.1的打包，总结如下：</p>\n<p>由于直接下载预编译好的spark与我们的集群环境不够符合，因此需要下载源码后本地编译<br>参考：<br><a href=\"http://spark.apache.org/docs/latest/building-spark.html\" target=\"_blank\" rel=\"external\">http://spark.apache.org/docs/latest/building-spark.html</a></p>\n<h3 id=\"前提\"><a href=\"#前提\" class=\"headerlink\" title=\"前提\"></a>前提</h3><p>安装好了<strong>3.5版本以上的maven</strong> 与java8+ ,spark 2.2.0已经取消了 Java 7的支持，Scala使用 2.11版本<br>我的环境如下<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ scala -version</div><div class=\"line\">Scala code runner version 2.11.11 -- Copyright 2002-2017, LAMP/EPFL</div><div class=\"line\"></div><div class=\"line\">Adam@Adam-PC MINGW64 /d/spark</div><div class=\"line\">$ java -version</div><div class=\"line\">java version <span class=\"string\">\"1.8.0_102\"</span></div><div class=\"line\">Java(TM) SE Runtime Environment (build 1.8.0_102-b14)</div><div class=\"line\">Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode)</div><div class=\"line\"></div><div class=\"line\">Adam@Adam-PC MINGW64 /d/spark</div><div class=\"line\">$ mvn -v</div><div class=\"line\">Apache Maven 3.5.0 (ff8f5e7444045639af65f6095c62210b5713f426; 2017-04-04T03:39:06+08:00)</div></pre></td></tr></table></figure></p>\n<h4 id=\"1-下载源代码\"><a href=\"#1-下载源代码\" class=\"headerlink\" title=\"1. 下载源代码\"></a>1. 下载源代码</h4><p><a href=\"http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.2.1/spark-2.2.1.tgz\" target=\"_blank\" rel=\"external\">http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.2.1/spark-2.2.1.tgz</a></p>\n<p>解压到 <code>D:\\spark\\spark-2.2.1</code>,然后进入这个目录</p>\n<h4 id=\"2-修改pom-xml文件\"><a href=\"#2-修改pom-xml文件\" class=\"headerlink\" title=\"2. 修改pom.xml文件\"></a>2. 修改pom.xml文件</h4><p>加入cloudera的代码仓库，其中可以在profile里面添加相应版本的文件，添加如下内容:<br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">repository</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">id</span>&gt;</span>cloudera-repo<span class=\"tag\">&lt;/<span class=\"name\">id</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>Cloudera Repository<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos<span class=\"tag\">&lt;/<span class=\"name\">url</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">releases</span>&gt;</span>  </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">enabled</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">enabled</span>&gt;</span> </div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">releases</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">snapshots</span>&gt;</span>  </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">enabled</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">enabled</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">snapshots</span>&gt;</span>  </div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">repository</span>&gt;</span>  </div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">profile</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">id</span>&gt;</span>cdh5.4.7<span class=\"tag\">&lt;/<span class=\"name\">id</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">properties</span>&gt;</span>  </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">hadoop.version</span>&gt;</span>2.6.0-cdh5.4.7<span class=\"tag\">&lt;/<span class=\"name\">hadoop.version</span>&gt;</span>  </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">hbase.version</span>&gt;</span>1.2.4-cdh5.4.7<span class=\"tag\">&lt;/<span class=\"name\">hbase.version</span>&gt;</span>  </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">zookeeper.version</span>&gt;</span>3.4.5-cdh5.4.7<span class=\"tag\">&lt;/<span class=\"name\">zookeeper.version</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">properties</span>&gt;</span>  </div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">profile</span>&gt;</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"3-使用make-distribution编译\"><a href=\"#3-使用make-distribution编译\" class=\"headerlink\" title=\"3. 使用make-distribution编译\"></a>3. 使用make-distribution编译</h4><p>设置maven的Memory usage<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">export</span> MAVEN_OPTS=<span class=\"string\">\"-Xmx2g -XX:ReservedCodeCacheSize=512m\"</span></div></pre></td></tr></table></figure></p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./dev/change-scala-version.sh 2.11</div></pre></td></tr></table></figure>\n<h4 id=\"4-编译方法一：使用make-distribution编译\"><a href=\"#4-编译方法一：使用make-distribution编译\" class=\"headerlink\" title=\"4. 编译方法一：使用make-distribution编译\"></a>4. 编译方法一：使用make-distribution编译</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./dev/make-distribution.sh --name ctcc --tgz -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.4.7 -Dscala-2.11 -Phive -Phive-thriftserver</div></pre></td></tr></table></figure>\n<h4 id=\"5-编译方法二：使用mvn编译\"><a href=\"#5-编译方法二：使用mvn编译\" class=\"headerlink\" title=\"5. 编译方法二：使用mvn编译\"></a>5. 编译方法二：使用mvn编译</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">./build/mvn  -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.4.7 -Dscala-2.11 -Phive -Phive-thriftserver -DskipTests clean package</div><div class=\"line\"></div><div class=\"line\">// 然后将make-distribution.sh中@BUILD_COMMAND那部分注释掉</div><div class=\"line\"></div><div class=\"line\">./dev/make-distribution.sh --name hadoop-2.6.0-cdh5.4.7 --tgz -Phadoop-2.6  -Dhadoop.version=hadoop-2.6.0-cdh5.4.7 -Phive -Phive-thriftserver -Pyarn</div><div class=\"line\"></div><div class=\"line\">#### 6.编译完成</div><div class=\"line\">编译成功后，就会发现多了一个 spark-2.2.1-bin-ctcc.tgz 的文件，解压后就能使用了，我们还要替换一些与hive相关的jar包，比如 hive-exec-1.2.1.spark2.jar 包，替换方法很简单，直接将自己打的包与 /jars 里的包替换一下就好了；</div><div class=\"line\"></div><div class=\"line\">#### 7. 客户端准备</div><div class=\"line\">这一部分仅作为记录使用，每个公司使用情况不同，我们是提前准备好客户端，然后下发到每一台机器中，具体步骤如下：</div><div class=\"line\">1. 替换掉有bug的jar包： hive-exec-1.2.1.spark2.jar等，这个是我们自己的hive的定制化；</div><div class=\"line\">2. 在lib目录下增加一些需要的包，比如hbase相关的包；</div><div class=\"line\">3. 在conf目录下配置统一的配置文件，比如 定制化的 hive-site.xml，hbase-site.xml 以及一些通用的配置，比如  spark-defaults.conf  spark-env.sh 等。小版本升级的情况下，可以将conf配置文件配置到其它位置，然后以软链接的形式链接过来。</div><div class=\"line\"></div><div class=\"line\">#### 一些报错的解决</div><div class=\"line\">运行的过程中报错：</div><div class=\"line\">``` sh</div><div class=\"line\">[ERROR] Failed to execute goal on project spark-launcher_2.11: Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.2.0: Could not find artifact org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.4.7 in central (https://repo1.maven.org/maven2) -&gt; [Help 1]</div></pre></td></tr></table></figure>\n<p>原因是官网是已经无法找到 hadoop-client:jar:2.6.0-cdh5.4.7 这个版本的jar包</p>\n<p>从 (<a href=\"https://repository.cloudera.com/content/repositories/releases/org/apache/hadoop/hadoop-client/2.6.0-cdh5.4.7/hadoop-client-2.6.0-cdh5.4.7.jar\" target=\"_blank\" rel=\"external\">https://repository.cloudera.com/content/repositories/releases/org/apache/hadoop/hadoop-client/2.6.0-cdh5.4.7/hadoop-client-2.6.0-cdh5.4.7.jar</a>) 上下载，然后移动懂mvn 本地仓库中<br>C:\\Users\\Adam.m2\\repository\\org\\apache\\hadoop\\hadoop-client\\2.6.0-cdh5.4.7</p>\n<p>如果第一次编译出错，在第二次重新编译的时候，会出现一个目录无法删除的情况，导致如下报错：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:3.0.0:clean (default-clean) on project spark-tags_2.11: Failed to clean project: Failed to delete D:\\spark\\spark-2.2.0-ctcc\\spark-2.2.0\\common\\tags\\target\\spark-tags_2.11-2.2.0.jar -&gt; [Help 1]</div></pre></td></tr></table></figure></p>\n<p>解决方案：跟zinc没有退出有关，在windows当前进程中找到一个Java进程，占用约800M的内存，结束这个进程</p>\n<p>再次编译，过段时间继续报错：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile (scala-compile-first) on project spark-network-yarn_2.11: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile failed. CompileFailed -&gt; [Help 1]</div></pre></td></tr></table></figure></p>\n<p>解决方案：<br>删除 C:\\Users\\Adam.m2\\repository\\net\\alchim31\\maven\\scala-maven-plugin\\3.2.2 文件，然后重新编译 </p>\n<p>会在根目录直接生成 spark-1.6.2-bin-2.6.0-cdh5.4.7.tgz</p>\n","excerpt":"","more":"<p>之前已经多次打包过spark了，但是每次重新打包的时候都忘记一些细节，并且spark1.6与2.2打包也有不同，与官网推荐有一些出入，现在就spark2.2.1的打包，总结如下：</p>\n<p>由于直接下载预编译好的spark与我们的集群环境不够符合，因此需要下载源码后本地编译<br>参考：<br><a href=\"http://spark.apache.org/docs/latest/building-spark.html\">http://spark.apache.org/docs/latest/building-spark.html</a></p>\n<h3 id=\"前提\"><a href=\"#前提\" class=\"headerlink\" title=\"前提\"></a>前提</h3><p>安装好了<strong>3.5版本以上的maven</strong> 与java8+ ,spark 2.2.0已经取消了 Java 7的支持，Scala使用 2.11版本<br>我的环境如下<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ scala -version</div><div class=\"line\">Scala code runner version 2.11.11 -- Copyright 2002-2017, LAMP/EPFL</div><div class=\"line\"></div><div class=\"line\">Adam@Adam-PC MINGW64 /d/spark</div><div class=\"line\">$ java -version</div><div class=\"line\">java version <span class=\"string\">\"1.8.0_102\"</span></div><div class=\"line\">Java(TM) SE Runtime Environment (build 1.8.0_102-b14)</div><div class=\"line\">Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode)</div><div class=\"line\"></div><div class=\"line\">Adam@Adam-PC MINGW64 /d/spark</div><div class=\"line\">$ mvn -v</div><div class=\"line\">Apache Maven 3.5.0 (ff8f5e7444045639af65f6095c62210b5713f426; 2017-04-04T03:39:06+08:00)</div></pre></td></tr></table></figure></p>\n<h4 id=\"1-下载源代码\"><a href=\"#1-下载源代码\" class=\"headerlink\" title=\"1. 下载源代码\"></a>1. 下载源代码</h4><p><a href=\"http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.2.1/spark-2.2.1.tgz\">http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.2.1/spark-2.2.1.tgz</a></p>\n<p>解压到 <code>D:\\spark\\spark-2.2.1</code>,然后进入这个目录</p>\n<h4 id=\"2-修改pom-xml文件\"><a href=\"#2-修改pom-xml文件\" class=\"headerlink\" title=\"2. 修改pom.xml文件\"></a>2. 修改pom.xml文件</h4><p>加入cloudera的代码仓库，其中可以在profile里面添加相应版本的文件，添加如下内容:<br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">repository</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">id</span>&gt;</span>cloudera-repo<span class=\"tag\">&lt;/<span class=\"name\">id</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>Cloudera Repository<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos<span class=\"tag\">&lt;/<span class=\"name\">url</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">releases</span>&gt;</span>  </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">enabled</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">enabled</span>&gt;</span> </div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">releases</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">snapshots</span>&gt;</span>  </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">enabled</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">enabled</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">snapshots</span>&gt;</span>  </div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">repository</span>&gt;</span>  </div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">profile</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">id</span>&gt;</span>cdh5.4.7<span class=\"tag\">&lt;/<span class=\"name\">id</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">properties</span>&gt;</span>  </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">hadoop.version</span>&gt;</span>2.6.0-cdh5.4.7<span class=\"tag\">&lt;/<span class=\"name\">hadoop.version</span>&gt;</span>  </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">hbase.version</span>&gt;</span>1.2.4-cdh5.4.7<span class=\"tag\">&lt;/<span class=\"name\">hbase.version</span>&gt;</span>  </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">zookeeper.version</span>&gt;</span>3.4.5-cdh5.4.7<span class=\"tag\">&lt;/<span class=\"name\">zookeeper.version</span>&gt;</span>  </div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">properties</span>&gt;</span>  </div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">profile</span>&gt;</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"3-使用make-distribution编译\"><a href=\"#3-使用make-distribution编译\" class=\"headerlink\" title=\"3. 使用make-distribution编译\"></a>3. 使用make-distribution编译</h4><p>设置maven的Memory usage<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">export</span> MAVEN_OPTS=<span class=\"string\">\"-Xmx2g -XX:ReservedCodeCacheSize=512m\"</span></div></pre></td></tr></table></figure></p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./dev/change-scala-version.sh 2.11</div></pre></td></tr></table></figure>\n<h4 id=\"4-编译方法一：使用make-distribution编译\"><a href=\"#4-编译方法一：使用make-distribution编译\" class=\"headerlink\" title=\"4. 编译方法一：使用make-distribution编译\"></a>4. 编译方法一：使用make-distribution编译</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./dev/make-distribution.sh --name ctcc --tgz -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.4.7 -Dscala-2.11 -Phive -Phive-thriftserver</div></pre></td></tr></table></figure>\n<h4 id=\"5-编译方法二：使用mvn编译\"><a href=\"#5-编译方法二：使用mvn编译\" class=\"headerlink\" title=\"5. 编译方法二：使用mvn编译\"></a>5. 编译方法二：使用mvn编译</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">./build/mvn  -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.4.7 -Dscala-2.11 -Phive -Phive-thriftserver -DskipTests clean package</div><div class=\"line\"></div><div class=\"line\">// 然后将make-distribution.sh中@BUILD_COMMAND那部分注释掉</div><div class=\"line\"></div><div class=\"line\">./dev/make-distribution.sh --name hadoop-2.6.0-cdh5.4.7 --tgz -Phadoop-2.6  -Dhadoop.version=hadoop-2.6.0-cdh5.4.7 -Phive -Phive-thriftserver -Pyarn</div><div class=\"line\"></div><div class=\"line\">#### 6.编译完成</div><div class=\"line\">编译成功后，就会发现多了一个 spark-2.2.1-bin-ctcc.tgz 的文件，解压后就能使用了，我们还要替换一些与hive相关的jar包，比如 hive-exec-1.2.1.spark2.jar 包，替换方法很简单，直接将自己打的包与 /jars 里的包替换一下就好了；</div><div class=\"line\"></div><div class=\"line\">#### 7. 客户端准备</div><div class=\"line\">这一部分仅作为记录使用，每个公司使用情况不同，我们是提前准备好客户端，然后下发到每一台机器中，具体步骤如下：</div><div class=\"line\">1. 替换掉有bug的jar包： hive-exec-1.2.1.spark2.jar等，这个是我们自己的hive的定制化；</div><div class=\"line\">2. 在lib目录下增加一些需要的包，比如hbase相关的包；</div><div class=\"line\">3. 在conf目录下配置统一的配置文件，比如 定制化的 hive-site.xml，hbase-site.xml 以及一些通用的配置，比如  spark-defaults.conf  spark-env.sh 等。小版本升级的情况下，可以将conf配置文件配置到其它位置，然后以软链接的形式链接过来。</div><div class=\"line\"></div><div class=\"line\">#### 一些报错的解决</div><div class=\"line\">运行的过程中报错：</div><div class=\"line\">``` sh</div><div class=\"line\">[ERROR] Failed to execute goal on project spark-launcher_2.11: Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.2.0: Could not find artifact org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.4.7 in central (https://repo1.maven.org/maven2) -&gt; [Help 1]</div></pre></td></tr></table></figure>\n<p>原因是官网是已经无法找到 hadoop-client:jar:2.6.0-cdh5.4.7 这个版本的jar包</p>\n<p>从 (<a href=\"https://repository.cloudera.com/content/repositories/releases/org/apache/hadoop/hadoop-client/2.6.0-cdh5.4.7/hadoop-client-2.6.0-cdh5.4.7.jar\">https://repository.cloudera.com/content/repositories/releases/org/apache/hadoop/hadoop-client/2.6.0-cdh5.4.7/hadoop-client-2.6.0-cdh5.4.7.jar</a>) 上下载，然后移动懂mvn 本地仓库中<br>C:\\Users\\Adam.m2\\repository\\org\\apache\\hadoop\\hadoop-client\\2.6.0-cdh5.4.7</p>\n<p>如果第一次编译出错，在第二次重新编译的时候，会出现一个目录无法删除的情况，导致如下报错：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:3.0.0:clean (default-clean) on project spark-tags_2.11: Failed to clean project: Failed to delete D:\\spark\\spark-2.2.0-ctcc\\spark-2.2.0\\common\\tags\\target\\spark-tags_2.11-2.2.0.jar -&gt; [Help 1]</div></pre></td></tr></table></figure></p>\n<p>解决方案：跟zinc没有退出有关，在windows当前进程中找到一个Java进程，占用约800M的内存，结束这个进程</p>\n<p>再次编译，过段时间继续报错：<br><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile (scala-compile-first) on project spark-network-yarn_2.11: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile failed. CompileFailed -&gt; [Help 1]</div></pre></td></tr></table></figure></p>\n<p>解决方案：<br>删除 C:\\Users\\Adam.m2\\repository\\net\\alchim31\\maven\\scala-maven-plugin\\3.2.2 文件，然后重新编译 </p>\n<p>会在根目录直接生成 spark-1.6.2-bin-2.6.0-cdh5.4.7.tgz</p>\n"}],"PostAsset":[{"_id":"source/_posts/2016-05-20-Scala学习资料总结/idea_scala.png","slug":"idea_scala.png","post":"cjldkwfsz0004psgu2lhev5rz","modified":1,"renderable":0},{"_id":"source/_drafts/2016-10-18-景区位置服务项目说明文档/1.png","post":"cjldkwfsv0002psgu3wsy26qj","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_drafts/位置服务说明文档/1.png","post":"cjldkwfsp0000psguigmu2nd9","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/2016-09-18-Spark在yarn中的资源申请与分配/spark-yarn-allocation.png","post":"cjldkwfv8000mpsguwun81s5x","slug":"spark-yarn-allocation.png","modified":1,"renderable":1},{"_id":"source/_posts/2016-10-18-Java中的“钩子”/hook.jpg","post":"cjldkwfvl000zpsgu4w8afrnz","slug":"hook.jpg","modified":1,"renderable":1},{"_id":"source/_posts/2017-01-03-Spark源码阅读之——StreamingContext详解/040.png","post":"cjldkwfwl0023psgu04jybaj6","slug":"040.png","modified":1,"renderable":1},{"_id":"source/_posts/2017-03-10-Apache-Eagle定义一个Application/eagle_1.png","post":"cjldkwfx3002ypsgu5whkp2or","slug":"eagle_1.png","modified":1,"renderable":1},{"_id":"source/_posts/2017-05-22-OpenAPI微服务接入规范/open-api1.png","post":"cjldkwfxe003ipsguo9a6olw3","slug":"open-api1.png","modified":1,"renderable":1},{"_id":"source/_posts/2016-06-13-使用scaladiagrams工具构建scala项目的UML图/spark_storage_part.png","post":"cjldkwftk0008psguwqj7yo49","slug":"spark_storage_part.png","modified":1,"renderable":1},{"_id":"source/_posts/2016-06-13-使用scaladiagrams工具构建scala项目的UML图/spark_storage_part2.png","post":"cjldkwftk0008psguwqj7yo49","slug":"spark_storage_part2.png","modified":1,"renderable":1},{"_id":"source/_posts/2018-08-17-菜鸟如何单排上王者/wangzhe_1.png","slug":"wangzhe_1.png","post":"cjldkwfxs003wpsgux6l2n88f","modified":1,"renderable":0},{"_id":"source/_posts/2018-08-17-菜鸟如何单排上王者/wangzhe_2.png","slug":"wangzhe_2.png","post":"cjldkwfxs003wpsgux6l2n88f","modified":1,"renderable":0},{"_id":"source/_posts/2016-09-07-Spark-Web与history测试/spark-yarn-web1.png","post":"cjldkwfv1000ipsgucy0ghz93","slug":"spark-yarn-web1.png","modified":1,"renderable":1},{"_id":"source/_posts/2016-09-07-Spark-Web与history测试/spark-yarn-web2.png","post":"cjldkwfv1000ipsgucy0ghz93","slug":"spark-yarn-web2.png","modified":1,"renderable":1},{"_id":"source/_posts/2016-09-07-Spark-Web与history测试/spark-yarn-web3.png","post":"cjldkwfv1000ipsgucy0ghz93","slug":"spark-yarn-web3.png","modified":1,"renderable":1},{"_id":"source/_posts/2017-06-28-位置服务开发上线总结/wzfw2_1.png","post":"cjldkwfxn003qpsgu1psmy1ta","slug":"wzfw2_1.png","modified":1,"renderable":1},{"_id":"source/_posts/2017-06-28-位置服务开发上线总结/wzfw2_2.png","post":"cjldkwfxn003qpsgu1psmy1ta","slug":"wzfw2_2.png","modified":1,"renderable":1},{"_id":"source/_posts/2017-06-28-位置服务开发上线总结/wzfw2_3.png","post":"cjldkwfxn003qpsgu1psmy1ta","slug":"wzfw2_3.png","modified":1,"renderable":1},{"_id":"source/_posts/2017-06-29-返回区域实时人数的思路与总结/wzfw2_1.png","post":"cjldkwfxq003tpsgugf2kbpa4","slug":"wzfw2_1.png","modified":1,"renderable":1},{"_id":"source/_posts/2017-06-29-返回区域实时人数的思路与总结/wzfw2_2.png","post":"cjldkwfxq003tpsgugf2kbpa4","slug":"wzfw2_2.png","modified":1,"renderable":1},{"_id":"source/_posts/2017-06-29-返回区域实时人数的思路与总结/wzfw2_3.png","post":"cjldkwfxq003tpsgugf2kbpa4","slug":"wzfw2_3.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"cjldkwfsv0002psgu3wsy26qj","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfue000dpsgusfwwfkjr"},{"post_id":"cjldkwfsz0004psgu2lhev5rz","category_id":"cjldkwftz000apsgu9pzyy6eu","_id":"cjldkwfv2000jpsgudc8dsvus"},{"post_id":"cjldkwftf0007psguuzhzj24i","category_id":"cjldkwftz000apsgu9pzyy6eu","_id":"cjldkwfva000opsgut4hg1r7a"},{"post_id":"cjldkwftk0008psguwqj7yo49","category_id":"cjldkwftz000apsgu9pzyy6eu","_id":"cjldkwfvg000tpsgusb2hfajy"},{"post_id":"cjldkwfvc000rpsgueg4ycqeu","category_id":"cjldkwftz000apsgu9pzyy6eu","_id":"cjldkwfvk000xpsgusfmrtk66"},{"post_id":"cjldkwftx0009psgur5e43tl9","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfvo0011psgu8gs7mw90"},{"post_id":"cjldkwfvf000spsgukjuactol","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfvs0015psgumgj5m7jp"},{"post_id":"cjldkwfu7000cpsguapukpi87","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfvv0019psgu2i5q8mk6"},{"post_id":"cjldkwfvp0013psgunn584gz5","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfw1001dpsgux4mrn92g"},{"post_id":"cjldkwfuf000epsguapd05z1h","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfw4001hpsgutr1tbw8g"},{"post_id":"cjldkwfvw001bpsguv3sgqym2","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfw7001lpsguakwnmyk1"},{"post_id":"cjldkwfur000hpsgu99774yfg","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfwa001ppsguze2e3gx8"},{"post_id":"cjldkwfw2001fpsguk3uhonyf","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfwd001tpsguex1x1lmp"},{"post_id":"cjldkwfw5001jpsgu5bpebr1j","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfwh001xpsgu7wyfrgz9"},{"post_id":"cjldkwfv1000ipsgucy0ghz93","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfwk0021psgur4m9nz66"},{"post_id":"cjldkwfw8001npsgum0jfd66h","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfwo0025psgukoa0qja8"},{"post_id":"cjldkwfwb001rpsgu552fo1vr","category_id":"cjldkwftz000apsgu9pzyy6eu","_id":"cjldkwfwq0028psgusxqpmma7"},{"post_id":"cjldkwfv8000mpsguwun81s5x","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfws002cpsgu6j3dlt3x"},{"post_id":"cjldkwfwf001vpsguh54wfh4h","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfwu002gpsgul28u42tq"},{"post_id":"cjldkwfv9000npsguzcp13fmk","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfwx002kpsguyoi7yp6b"},{"post_id":"cjldkwfwl0023psgu04jybaj6","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfwz002opsgubbxrdtqn"},{"post_id":"cjldkwfvh000vpsguobe3gpj7","category_id":"cjldkwfwn0024psgukclid3vp","_id":"cjldkwfx1002spsgux9xdtqjk"},{"post_id":"cjldkwfwt002fpsgu9umi85gb","category_id":"cjldkwftz000apsgu9pzyy6eu","_id":"cjldkwfx3002wpsgudh5z8ege"},{"post_id":"cjldkwfvl000zpsgu4w8afrnz","category_id":"cjldkwfws002dpsgubghi19zc","_id":"cjldkwfx50030psguza3blink"},{"post_id":"cjldkwfwz002qpsgu94ktu0x0","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfx70034psguaesmj0py"},{"post_id":"cjldkwfvt0017psgu94s2e79t","category_id":"cjldkwfws002dpsgubghi19zc","_id":"cjldkwfx80038psgu7lbuc4ky"},{"post_id":"cjldkwfx2002upsguokqnhuml","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfxa003cpsguq5i8m9ey"},{"post_id":"cjldkwfwh001zpsgut1btaa52","category_id":"cjldkwfx2002vpsguck8p3rdk","_id":"cjldkwfxe003gpsguizirde3m"},{"post_id":"cjldkwfx50032psgu9jskzjbj","category_id":"cjldkwfwn0024psgukclid3vp","_id":"cjldkwfxh003kpsgugtdiddym"},{"post_id":"cjldkwfx70036psguqwzpts96","category_id":"cjldkwfws002dpsgubghi19zc","_id":"cjldkwfxm003opsgui88chg32"},{"post_id":"cjldkwfwo0027psgu07m1p8co","category_id":"cjldkwfx2002vpsguck8p3rdk","_id":"cjldkwfxq003spsgupwlp253s"},{"post_id":"cjldkwfx9003apsgunfehs9ab","category_id":"cjldkwfws002dpsgubghi19zc","_id":"cjldkwfxs003vpsgu25qsadkx"},{"post_id":"cjldkwfxb003epsguyo29mh31","category_id":"cjldkwfws002dpsgubghi19zc","_id":"cjldkwfxv003ypsguhuddlx3j"},{"post_id":"cjldkwfwr002bpsgud51k3xe4","category_id":"cjldkwfx2002vpsguck8p3rdk","_id":"cjldkwfxy0041psgu7ivfq3sh"},{"post_id":"cjldkwfxi003mpsgucthh1dc2","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfy00045psgupx4tden6"},{"post_id":"cjldkwfwv002ipsgulmexvn0o","category_id":"cjldkwfx2002vpsguck8p3rdk","_id":"cjldkwfy30049psgux8tldvyk"},{"post_id":"cjldkwfxn003qpsgu1psmy1ta","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfy5004cpsgu3ao20npg"},{"post_id":"cjldkwfxq003tpsgugf2kbpa4","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfy6004fpsgu21p8k7ax"},{"post_id":"cjldkwfwx002mpsguiz6zivrg","category_id":"cjldkwfx2002vpsguck8p3rdk","_id":"cjldkwfy6004gpsgum8doa3m8"},{"post_id":"cjldkwfxv003zpsguuezusctq","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfy7004ipsgulixjljl5"},{"post_id":"cjldkwfx3002ypsgu5whkp2or","category_id":"cjldkwfxu003xpsguiu18xzrk","_id":"cjldkwfy8004kpsguxwlqi85y"},{"post_id":"cjldkwfxy0042psgump2ter1o","category_id":"cjldkwft70005psguwctee2f2","_id":"cjldkwfy9004npsgu8io270gg"},{"post_id":"cjldkwfy10046psgueqkcoia2","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfya004ppsgukng9bovw"},{"post_id":"cjldkwfxe003ipsguo9a6olw3","category_id":"cjldkwfy00044psgunqa7mfuh","_id":"cjldkwfyb004spsgu3cfa45g3"},{"post_id":"cjldkwfy4004apsgubcq6f548","category_id":"cjldkwfvb000ppsgu564yv1op","_id":"cjldkwfyc004tpsgusl91y8hu"}],"PostTag":[{"post_id":"cjldkwfsv0002psgu3wsy26qj","tag_id":"cjldkwftd0006psgup8vekaxq","_id":"cjldkwfvk000ypsguyi0pon8r"},{"post_id":"cjldkwfsv0002psgu3wsy26qj","tag_id":"cjldkwfu0000bpsgua4gydtas","_id":"cjldkwfvp0012psguh1yo2hyh"},{"post_id":"cjldkwfsv0002psgu3wsy26qj","tag_id":"cjldkwfui000gpsgujy3sqwja","_id":"cjldkwfvs0016psgurrd70abl"},{"post_id":"cjldkwfsv0002psgu3wsy26qj","tag_id":"cjldkwfv5000lpsguh0t3wgbd","_id":"cjldkwfvv001apsgusgamgwcf"},{"post_id":"cjldkwfsv0002psgu3wsy26qj","tag_id":"cjldkwfvb000qpsgu2fm3aau4","_id":"cjldkwfw2001epsgu7b1t0k3s"},{"post_id":"cjldkwfvh000vpsguobe3gpj7","tag_id":"cjldkwfu0000bpsgua4gydtas","_id":"cjldkwfw4001ipsguw0u3bdnx"},{"post_id":"cjldkwfsz0004psgu2lhev5rz","tag_id":"cjldkwfvj000wpsguu1lithme","_id":"cjldkwfw8001mpsgu7y2vhaun"},{"post_id":"cjldkwfvp0013psgunn584gz5","tag_id":"cjldkwftd0006psgup8vekaxq","_id":"cjldkwfwb001qpsgumczx469h"},{"post_id":"cjldkwfvp0013psgunn584gz5","tag_id":"cjldkwfu0000bpsgua4gydtas","_id":"cjldkwfwe001upsgu13i6sr21"},{"post_id":"cjldkwfvp0013psgunn584gz5","tag_id":"cjldkwfui000gpsgujy3sqwja","_id":"cjldkwfwh001ypsgurqnwh8m1"},{"post_id":"cjldkwfvp0013psgunn584gz5","tag_id":"cjldkwfv5000lpsguh0t3wgbd","_id":"cjldkwfwl0022psguvcbf58du"},{"post_id":"cjldkwfvp0013psgunn584gz5","tag_id":"cjldkwfvb000qpsgu2fm3aau4","_id":"cjldkwfwo0026psguyxhuewjo"},{"post_id":"cjldkwftf0007psguuzhzj24i","tag_id":"cjldkwfvj000wpsguu1lithme","_id":"cjldkwfwq002apsgu93mjdowy"},{"post_id":"cjldkwftk0008psguwqj7yo49","tag_id":"cjldkwfvj000wpsguu1lithme","_id":"cjldkwfws002epsguy1sizehr"},{"post_id":"cjldkwftx0009psgur5e43tl9","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfwv002hpsgug5qj9mw1"},{"post_id":"cjldkwfwl0023psgu04jybaj6","tag_id":"cjldkwftd0006psgup8vekaxq","_id":"cjldkwfwx002lpsgujigbeqcf"},{"post_id":"cjldkwfwt002fpsgu9umi85gb","tag_id":"cjldkwfvj000wpsguu1lithme","_id":"cjldkwfwz002ppsgupu5ukr7u"},{"post_id":"cjldkwfu7000cpsguapukpi87","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfx1002tpsgukp9txqym"},{"post_id":"cjldkwfu7000cpsguapukpi87","tag_id":"cjldkwfwk0020psgu26afuq52","_id":"cjldkwfx3002xpsguajp05mmn"},{"post_id":"cjldkwfu7000cpsguapukpi87","tag_id":"cjldkwfwq0029psgusxqgneip","_id":"cjldkwfx50031psgu8y2rbw9d"},{"post_id":"cjldkwfuf000epsguapd05z1h","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfx70035psgupyz7nd1l"},{"post_id":"cjldkwfuf000epsguapd05z1h","tag_id":"cjldkwfwq0029psgusxqgneip","_id":"cjldkwfx90039psguxjh6bbod"},{"post_id":"cjldkwfx50032psgu9jskzjbj","tag_id":"cjldkwfu0000bpsgua4gydtas","_id":"cjldkwfxa003dpsgu88rcqjcz"},{"post_id":"cjldkwfur000hpsgu99774yfg","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfxe003hpsgujh32kbuu"},{"post_id":"cjldkwfur000hpsgu99774yfg","tag_id":"cjldkwfwq0029psgusxqgneip","_id":"cjldkwfxh003lpsgumy23nqk0"},{"post_id":"cjldkwfv1000ipsgucy0ghz93","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfxm003ppsguplqt6wju"},{"post_id":"cjldkwfv8000mpsguwun81s5x","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfxz0043psguvucui7o9"},{"post_id":"cjldkwfv8000mpsguwun81s5x","tag_id":"cjldkwfxr003upsguij8f1eyo","_id":"cjldkwfy20047psgu35rxpjbd"},{"post_id":"cjldkwfy10046psgueqkcoia2","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfy5004bpsgu38dli317"},{"post_id":"cjldkwfy4004apsgubcq6f548","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfy6004epsgujsgppiij"},{"post_id":"cjldkwfv9000npsguzcp13fmk","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfy8004jpsgu5nfr5wqs"},{"post_id":"cjldkwfv9000npsguzcp13fmk","tag_id":"cjldkwfy30048psgubkjufu19","_id":"cjldkwfy8004lpsgub6o35hok"},{"post_id":"cjldkwfv9000npsguzcp13fmk","tag_id":"cjldkwfwq0029psgusxqgneip","_id":"cjldkwfya004opsgufutf5xbp"},{"post_id":"cjldkwfvc000rpsgueg4ycqeu","tag_id":"cjldkwfvj000wpsguu1lithme","_id":"cjldkwfyb004qpsguox1vlm6i"},{"post_id":"cjldkwfvf000spsgukjuactol","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfyd004vpsguyusptu6b"},{"post_id":"cjldkwfvf000spsgukjuactol","tag_id":"cjldkwfyb004rpsguepf4rv6h","_id":"cjldkwfyd004wpsgutedwe7t2"},{"post_id":"cjldkwfvl000zpsgu4w8afrnz","tag_id":"cjldkwfyc004upsguc8ftu7ja","_id":"cjldkwfye004ypsguzi8jq5er"},{"post_id":"cjldkwfvt0017psgu94s2e79t","tag_id":"cjldkwfyc004upsguc8ftu7ja","_id":"cjldkwfye0050psgub62a545m"},{"post_id":"cjldkwfvw001bpsguv3sgqym2","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfyf0053psgu3nlug63k"},{"post_id":"cjldkwfvw001bpsguv3sgqym2","tag_id":"cjldkwfvj000wpsguu1lithme","_id":"cjldkwfyg0054psgug3axxs3u"},{"post_id":"cjldkwfvw001bpsguv3sgqym2","tag_id":"cjldkwfyf0051psgu2oa38pwt","_id":"cjldkwfyh0056psgu4hajfla9"},{"post_id":"cjldkwfw2001fpsguk3uhonyf","tag_id":"cjldkwftd0006psgup8vekaxq","_id":"cjldkwfyh0057psguuxinc0ep"},{"post_id":"cjldkwfw2001fpsguk3uhonyf","tag_id":"cjldkwfvb000qpsgu2fm3aau4","_id":"cjldkwfyi0059psgu2rx9lreq"},{"post_id":"cjldkwfw2001fpsguk3uhonyf","tag_id":"cjldkwfyf0052psgu5dpymv8t","_id":"cjldkwfyi005apsguctc93t7z"},{"post_id":"cjldkwfw5001jpsgu5bpebr1j","tag_id":"cjldkwftd0006psgup8vekaxq","_id":"cjldkwfyj005cpsguww41afbv"},{"post_id":"cjldkwfw5001jpsgu5bpebr1j","tag_id":"cjldkwfyg0055psguggtms860","_id":"cjldkwfyj005dpsguxtolgghk"},{"post_id":"cjldkwfw5001jpsgu5bpebr1j","tag_id":"cjldkwfyf0052psgu5dpymv8t","_id":"cjldkwfyj005fpsguwaxqjswf"},{"post_id":"cjldkwfw8001npsgum0jfd66h","tag_id":"cjldkwftd0006psgup8vekaxq","_id":"cjldkwfyk005hpsgu2qabj1mk"},{"post_id":"cjldkwfw8001npsgum0jfd66h","tag_id":"cjldkwfwq0029psgusxqgneip","_id":"cjldkwfyk005ipsgufzorgw3b"},{"post_id":"cjldkwfw8001npsgum0jfd66h","tag_id":"cjldkwfyf0052psgu5dpymv8t","_id":"cjldkwfyl005kpsgu129zm4t7"},{"post_id":"cjldkwfwb001rpsgu552fo1vr","tag_id":"cjldkwfyk005gpsguxwg10331","_id":"cjldkwfyl005lpsguahu5b0t3"},{"post_id":"cjldkwfwb001rpsgu552fo1vr","tag_id":"cjldkwfvj000wpsguu1lithme","_id":"cjldkwfym005npsgum4r9i41l"},{"post_id":"cjldkwfwf001vpsguh54wfh4h","tag_id":"cjldkwfyf0052psgu5dpymv8t","_id":"cjldkwfym005opsguv5d9twn4"},{"post_id":"cjldkwfwh001zpsgut1btaa52","tag_id":"cjldkwfyl005mpsgu6gcsqga6","_id":"cjldkwfyn005qpsgu34r2uiwk"},{"post_id":"cjldkwfwo0027psgu07m1p8co","tag_id":"cjldkwfyl005mpsgu6gcsqga6","_id":"cjldkwfyn005spsgu9dbwlbce"},{"post_id":"cjldkwfwr002bpsgud51k3xe4","tag_id":"cjldkwfyl005mpsgu6gcsqga6","_id":"cjldkwfyo005upsgu1c290s2v"},{"post_id":"cjldkwfwx002mpsguiz6zivrg","tag_id":"cjldkwfyl005mpsgu6gcsqga6","_id":"cjldkwfyp005wpsgurcpr93hs"},{"post_id":"cjldkwfwz002qpsgu94ktu0x0","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfyp005ypsguyjd99hd1"},{"post_id":"cjldkwfwz002qpsgu94ktu0x0","tag_id":"cjldkwfyb004rpsguepf4rv6h","_id":"cjldkwfyq005zpsguiqraeazc"},{"post_id":"cjldkwfx2002upsguokqnhuml","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfyq0061psgu9c4yv83r"},{"post_id":"cjldkwfx2002upsguokqnhuml","tag_id":"cjldkwfxr003upsguij8f1eyo","_id":"cjldkwfyr0062psgu7ye0njsf"},{"post_id":"cjldkwfx3002ypsgu5whkp2or","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfyr0064psgurnl848q7"},{"post_id":"cjldkwfx3002ypsgu5whkp2or","tag_id":"cjldkwfyq0060psguclew3302","_id":"cjldkwfyr0065psgu6ao6oepe"},{"post_id":"cjldkwfx70036psguqwzpts96","tag_id":"cjldkwfyc004upsguc8ftu7ja","_id":"cjldkwfys0067psguaeic9hxi"},{"post_id":"cjldkwfx9003apsgunfehs9ab","tag_id":"cjldkwfyc004upsguc8ftu7ja","_id":"cjldkwfyu006apsguwailqbzq"},{"post_id":"cjldkwfx9003apsgunfehs9ab","tag_id":"cjldkwfys0068psgugccdjpc3","_id":"cjldkwfyu006bpsgumvpn5k8o"},{"post_id":"cjldkwfxb003epsguyo29mh31","tag_id":"cjldkwfyc004upsguc8ftu7ja","_id":"cjldkwfyv006dpsgulbwpub8i"},{"post_id":"cjldkwfxe003ipsguo9a6olw3","tag_id":"cjldkwfyc004upsguc8ftu7ja","_id":"cjldkwfyx006ipsgu24v4w9ud"},{"post_id":"cjldkwfxe003ipsguo9a6olw3","tag_id":"cjldkwfys0068psgugccdjpc3","_id":"cjldkwfyx006jpsgu1g76s8e9"},{"post_id":"cjldkwfxe003ipsguo9a6olw3","tag_id":"cjldkwfyv006fpsgua4jgz6cy","_id":"cjldkwfyy006lpsgup66cfu62"},{"post_id":"cjldkwfxe003ipsguo9a6olw3","tag_id":"cjldkwfyw006gpsguiwa6vjdc","_id":"cjldkwfyy006mpsgube75jycx"},{"post_id":"cjldkwfxi003mpsgucthh1dc2","tag_id":"cjldkwfyf0052psgu5dpymv8t","_id":"cjldkwfyz006opsgu7u9ep54u"},{"post_id":"cjldkwfxn003qpsgu1psmy1ta","tag_id":"cjldkwfyf0052psgu5dpymv8t","_id":"cjldkwfz0006ppsguoxpe5f9v"},{"post_id":"cjldkwfxq003tpsgugf2kbpa4","tag_id":"cjldkwfyf0052psgu5dpymv8t","_id":"cjldkwfz0006rpsgughtkqb37"},{"post_id":"cjldkwfxq003tpsgugf2kbpa4","tag_id":"cjldkwfvb000qpsgu2fm3aau4","_id":"cjldkwfz0006spsguimayfoq8"},{"post_id":"cjldkwfxs003wpsgux6l2n88f","tag_id":"cjldkwfz0006qpsguffsz9shn","_id":"cjldkwfz1006upsgueeb648oz"},{"post_id":"cjldkwfxv003zpsguuezusctq","tag_id":"cjldkwfw7001kpsguhpbe2k20","_id":"cjldkwfz2006wpsgup99gt7q8"},{"post_id":"cjldkwfxv003zpsguuezusctq","tag_id":"cjldkwfz1006tpsgu7taii2rp","_id":"cjldkwfz2006xpsguud6ybhht"},{"post_id":"cjldkwfxy0042psgump2ter1o","tag_id":"cjldkwfyf0052psgu5dpymv8t","_id":"cjldkwfz3006ypsgu1j0etll4"}],"Tag":[{"name":"spark streaming","_id":"cjldkwftd0006psgup8vekaxq"},{"name":"flume","_id":"cjldkwfu0000bpsgua4gydtas"},{"name":"kafka","_id":"cjldkwfui000gpsgujy3sqwja"},{"name":"大数据开发","_id":"cjldkwfv5000lpsguh0t3wgbd"},{"name":"redis","_id":"cjldkwfvb000qpsgu2fm3aau4"},{"name":"scala","_id":"cjldkwfvj000wpsguu1lithme"},{"name":"spark","_id":"cjldkwfw7001kpsguhpbe2k20"},{"name":"hue","_id":"cjldkwfwk0020psgu26afuq52"},{"name":"kerberos","_id":"cjldkwfwq0029psgusxqgneip"},{"name":"yarn","_id":"cjldkwfxr003upsguij8f1eyo"},{"name":"hbase","_id":"cjldkwfy30048psgubkjufu19"},{"name":"livy","_id":"cjldkwfyb004rpsguepf4rv6h"},{"name":"java","_id":"cjldkwfyc004upsguc8ftu7ja"},{"name":"持续更新","_id":"cjldkwfyf0051psgu2oa38pwt"},{"name":"spark开发","_id":"cjldkwfyf0052psgu5dpymv8t"},{"name":"mysql","_id":"cjldkwfyg0055psguggtms860"},{"name":"算法","_id":"cjldkwfyk005gpsguxwg10331"},{"name":"散文","_id":"cjldkwfyl005mpsgu6gcsqga6"},{"name":"eagle","_id":"cjldkwfyq0060psguclew3302"},{"name":"架构","_id":"cjldkwfys0068psgugccdjpc3"},{"name":"spring cloud","_id":"cjldkwfyv006fpsgua4jgz6cy"},{"name":"微服务","_id":"cjldkwfyw006gpsguiwa6vjdc"},{"name":"game","_id":"cjldkwfz0006qpsguffsz9shn"},{"name":"es","_id":"cjldkwfz1006tpsgu7taii2rp"}]}}
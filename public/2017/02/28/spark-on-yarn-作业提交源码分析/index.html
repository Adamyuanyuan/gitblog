<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>spark on yarn 作业提交源码分析 | Adam Home</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">spark on yarn 作业提交源码分析</h1><a id="logo" href="/.">Adam Home</a><p class="description">快意回首，拂心莫停</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于我</i></a><a href="/timeline/"><i class="fa fa-line-chart"> 历史</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">spark on yarn 作业提交源码分析</h1><div class="post-meta">Feb 28, 2017<span> | </span><span class="category"><a href="/categories/spark/">spark</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#从spark-sumbit开始"><span class="toc-number">1.</span> <span class="toc-text">从spark-sumbit开始</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#launcher-Main"><span class="toc-number">2.</span> <span class="toc-text">launcher.Main</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SparkSubmit类"><span class="toc-number">3.</span> <span class="toc-text">SparkSubmit类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#引用"><span class="toc-number">4.</span> <span class="toc-text">引用</span></a></li></ol></div></div><div class="post-content"><p>最近因为上线hdfs的federation功能，测试spark程序的时候遇到了问题，在分析此问题的过程中对spark on yarn提交作业的过程记录一下，以SparkPi为例，从spark-submit开始，通过debug日志分析详细的过程：<br>整个过程中主要涉及：spark源码（1.6.2）hadoop源码（2.6.0-cdh5.4.7） Kerberos相关</p>
<h4 id="从spark-sumbit开始"><a href="#从spark-sumbit开始" class="headerlink" title="从spark-sumbit开始"></a>从spark-sumbit开始</h4><p>提交命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">spark-submit --master yarn-client --class SparkPi ./<span class="built_in">test</span>Jars/my.jar</div></pre></td></tr></table></figure></p>
<p>进入<code>./bin/spark-submit</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">exec</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-class org.apache.spark.deploy.SparkSubmit <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure></p>
<p>以上脚本调用了spark-class脚本，并增加了参数 org.apache.spark.deploy.SparkSubmit。<br>进入 <code>./bin/spark-class</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">// 首先执行load-spark-env.sh载入 ./conf/spark-env.sh，保证只载入一次，如果之前手动载入过一次的话，就不会再覆盖载入</div><div class="line">. <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/load-spark-env.sh</div><div class="line">// 寻找spark安装包，是这个样子：`spark-assembly.*hadoop.*\.jar$`，而且只能有一个</div><div class="line">...</div><div class="line">// 最后会通过java调用spark的 `org.apache.spark.launcher.Main`作为spark应用程序的主入口，首先循环读取ARG参数，加入到CMD中：</div><div class="line">CMD=()</div><div class="line"><span class="keyword">while</span> IFS= <span class="built_in">read</span> <span class="_">-d</span> <span class="string">''</span> -r ARG; <span class="keyword">do</span></div><div class="line">  CMD+=(<span class="string">"<span class="variable">$ARG</span>"</span>)</div><div class="line"><span class="keyword">done</span> &lt; &lt;(<span class="string">"<span class="variable">$RUNNER</span>"</span> -cp <span class="string">"<span class="variable">$LAUNCH_CLASSPATH</span>"</span> org.apache.spark.launcher.Main <span class="string">"<span class="variable">$@</span>"</span>)</div><div class="line"><span class="built_in">exec</span> <span class="string">"<span class="variable">$&#123;CMD[@]&#125;</span>"</span></div></pre></td></tr></table></figure>
<p>翻译过来就是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/bin/java -cp /usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar org.apache.spark.launcher.Main org.apache.spark.deploy.SparkSubmit --master yarn-client --class SparkPi ./testJars/my.jar</div></pre></td></tr></table></figure></p>
<p>将这个命令执行的结果返回给 CMD参数，然后执行</p>
<h4 id="launcher-Main"><a href="#launcher-Main" class="headerlink" title="launcher.Main"></a>launcher.Main</h4><p>这个类的目的是同时适应unix和windows操作系统</p>
<figure class="highlight java"><figcaption><span>org.apache.spark.launcher.Main.java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (className.equals(<span class="string">"org.apache.spark.deploy.SparkSubmit"</span>)) &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">// 创建一个命令解析器，这里会优先将提交的命令中的 --master之类的参数解析，然后保存到 SparkSubmitCommandBuilder 中</span></div><div class="line">        builder = <span class="keyword">new</span> SparkSubmitCommandBuilder(args);</div><div class="line">      &#125; <span class="keyword">catch</span> (IllegalArgumentException e) &#123;</div><div class="line">        ...</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      builder = <span class="keyword">new</span> SparkClassCommandBuilder(className, args);</div><div class="line">    &#125;</div><div class="line">    Map&lt;String, String&gt; env = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</div><div class="line">    <span class="comment">// 使用解析器来解析参数，并且得到环境变量的HashMap值</span></div><div class="line">    List&lt;String&gt; cmd = builder.buildCommand(env);</div><div class="line">    ...</div><div class="line">    <span class="keyword">if</span> (isWindows()) &#123;</div><div class="line">      System.out.println(prepareWindowsCommand(cmd, env));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// In bash, use NULL as the arg separator since it cannot be used in an argument.</span></div><div class="line">      <span class="comment">// 根据输入的参数，准备bash的命令，然后打印出来，传给 spark-class脚本中的$CMD 变量，然后执行，</span></div><div class="line">      List&lt;String&gt; bashCmd = prepareBashCommand(cmd, env);</div><div class="line">      <span class="keyword">for</span> (String c : bashCmd) &#123;</div><div class="line">        System.out.print(c);</div><div class="line">        System.out.print(<span class="string">'\0'</span>);</div><div class="line">      &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>再来看看这个cmd到底是怎么build的：<br><figure class="highlight java"><figcaption><span>org.apache.spark.launcher.SparkSubmitCommandBuilder.java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 这段代码进入 buildSparkSubmitCommand</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">buildCommand</span><span class="params">(Map&lt;String, String&gt; env)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">  <span class="keyword">if</span> (PYSPARK_SHELL_RESOURCE.equals(appResource) &amp;&amp; !printInfo) &#123;</div><div class="line">    <span class="keyword">return</span> buildPySparkShellCommand(env);</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (SPARKR_SHELL_RESOURCE.equals(appResource) &amp;&amp; !printInfo) &#123;</div><div class="line">    <span class="keyword">return</span> buildSparkRCommand(env);</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">return</span> buildSparkSubmitCommand(env);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p> 看看buildSparkSubmitCommand函数<br> <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"> <span class="function"><span class="keyword">private</span> List&lt;String&gt; <span class="title">buildSparkSubmitCommand</span><span class="params">(Map&lt;String, String&gt; env)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">  <span class="comment">// Load the properties file and check whether spark-submit will be running the app's driver</span></div><div class="line">  <span class="comment">// or just launching a cluster app. When running the driver, the JVM's argument will be</span></div><div class="line">  <span class="comment">// modified to cover the driver's configuration.</span></div><div class="line">  <span class="comment">// 先获取有效的配置，如果用户不自己制定的话，默认情况下会去 $SPARK_HOME/conf/spark-defaults.conf 中拿</span></div><div class="line">  Map&lt;String, String&gt; config = getEffectiveConfig();</div><div class="line">  <span class="keyword">boolean</span> isClientMode = isClientMode(config);</div><div class="line">  String extraClassPath = isClientMode ? config.get(SparkLauncher.DRIVER_EXTRA_CLASSPATH) : <span class="keyword">null</span>;</div><div class="line"></div><div class="line">  List&lt;String&gt; cmd = buildJavaCommand(extraClassPath);</div><div class="line">  <span class="comment">// Take Thrift Server as daemon</span></div><div class="line">  <span class="keyword">if</span> (isThriftServer(mainClass)) &#123;</div><div class="line">    addOptionString(cmd, System.getenv(<span class="string">"SPARK_DAEMON_JAVA_OPTS"</span>));</div><div class="line">  &#125;</div><div class="line">  addOptionString(cmd, System.getenv(<span class="string">"SPARK_SUBMIT_OPTS"</span>));</div><div class="line">  addOptionString(cmd, System.getenv(<span class="string">"SPARK_JAVA_OPTS"</span>));</div><div class="line"></div><div class="line">  <span class="comment">// 接下来的代码的意思是，很多没有手动指定的参数，会根据配置文件中拿，如果配置文件中没有，会给个默认值</span></div><div class="line">  <span class="keyword">if</span> (isClientMode) &#123;</div><div class="line">    <span class="comment">// Figuring out where the memory value come from is a little tricky due to precedence.</span></div><div class="line">    <span class="comment">// Precedence is observed in the following order:</span></div><div class="line">    <span class="comment">// - explicit configuration (setConf()), which also covers --driver-memory cli argument.</span></div><div class="line">    <span class="comment">// - properties file.</span></div><div class="line">    <span class="comment">// - SPARK_DRIVER_MEMORY env variable</span></div><div class="line">    <span class="comment">// - SPARK_MEM env variable</span></div><div class="line">    <span class="comment">// - default value (1g)</span></div><div class="line">    <span class="comment">// Take Thrift Server as daemon</span></div><div class="line">    String tsMemory =</div><div class="line">      isThriftServer(mainClass) ? System.getenv(<span class="string">"SPARK_DAEMON_MEMORY"</span>) : <span class="keyword">null</span>;</div><div class="line">    String memory = firstNonEmpty(tsMemory, config.get(SparkLauncher.DRIVER_MEMORY),</div><div class="line">      System.getenv(<span class="string">"SPARK_DRIVER_MEMORY"</span>), System.getenv(<span class="string">"SPARK_MEM"</span>), DEFAULT_MEM);</div><div class="line">    cmd.add(<span class="string">"-Xms"</span> + memory);</div><div class="line">    cmd.add(<span class="string">"-Xmx"</span> + memory);</div><div class="line">    addOptionString(cmd, config.get(SparkLauncher.DRIVER_EXTRA_JAVA_OPTIONS));</div><div class="line">    mergeEnvPathList(env, getLibPathEnvName(),</div><div class="line">      config.get(SparkLauncher.DRIVER_EXTRA_LIBRARY_PATH));</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  addPermGenSizeOpt(cmd);</div><div class="line">  cmd.add(<span class="string">"org.apache.spark.deploy.SparkSubmit"</span>);</div><div class="line">  cmd.addAll(buildSparkSubmitArgs());</div><div class="line">  <span class="keyword">return</span> cmd;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h4 id="SparkSubmit类"><a href="#SparkSubmit类" class="headerlink" title="SparkSubmit类"></a>SparkSubmit类</h4><p>通过上述方法生成了命令到 $CMD 参数中后，就通过 <code>exec &quot;${CMD[@]}&quot;</code> 命令执行之前生成的命令，也就是 <code>org.apache.spark.deploy.SparkSubmit</code> 类。大概是这样子：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">// 在Unix中，分隔符为<span class="string">'\0'</span>，以下是大概写法</div><div class="line">/bin/java -Xms1g -XX:MaxPermSize=256m -cp /usr/lib/spark/lib/spark-assembly-1.6.2-hadoop2.6.0-cdh5.4.7.jar org.apache.spark.deploy.SparkSubmit --master yarn-client --class SparkPi ./<span class="built_in">test</span>Jars/my.jar</div></pre></td></tr></table></figure></p>
<p>进入main函数：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> appArgs = <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args)</div><div class="line">    <span class="keyword">if</span> (appArgs.verbose) &#123;</div><div class="line">      <span class="comment">// scalastyle:off println</span></div><div class="line">      printStream.println(appArgs)</div><div class="line">      <span class="comment">// scalastyle:on println</span></div><div class="line">    &#125;</div><div class="line">    appArgs.action <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs)</div><div class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</div><div class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>咱们调用的是 submit(appArgs)：如下代码，主要分为两步，第一步，准备提交的参数四元组，第二步，<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submit</span></span>(args: <span class="type">SparkSubmitArguments</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">// 这里的代码很长，主要目的是准备提交应用的环境，针对 yarn standalone Mesos等各类环境进行针对性处理，并对输入的args进行一些校验和修改，返回一个 4-tuple：</span></div><div class="line">    <span class="comment">// 1. 子程序的参数列表； 2. 子程序的classpath列表 3. 系统环境变量HashMap 4. mainClass</span></div><div class="line">    <span class="keyword">val</span> (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)</div><div class="line"></div><div class="line">    <span class="comment">// 如果有代理晕乎的话，需要创建一个代理用户，然后验证，后运行runMain</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doRunMain</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">      <span class="keyword">if</span> (args.proxyUser != <span class="literal">null</span>) &#123;</div><div class="line">        <span class="keyword">val</span> proxyUser = <span class="type">UserGroupInformation</span>.createProxyUser(args.proxyUser,</div><div class="line">          <span class="type">UserGroupInformation</span>.getCurrentUser())</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          proxyUser.doAs(<span class="keyword">new</span> <span class="type">PrivilegedExceptionAction</span>[<span class="type">Unit</span>]() &#123;</div><div class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">              runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</div><div class="line">            &#125;</div><div class="line">          &#125;)</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">            <span class="comment">// Hadoop's AuthorizationException suppresses the exception's stack trace, which</span></div><div class="line">            <span class="comment">// makes the message printed to the output by the JVM not very helpful. Instead,</span></div><div class="line">            <span class="comment">// detect exceptions with empty stack traces here, and treat them differently.</span></div><div class="line">            <span class="keyword">if</span> (e.getStackTrace().length == <span class="number">0</span>) &#123;</div><div class="line">              <span class="comment">// scalastyle:off println</span></div><div class="line">              printStream.println(<span class="string">s"ERROR: <span class="subst">$&#123;e.getClass().getName()&#125;</span>: <span class="subst">$&#123;e.getMessage()&#125;</span>"</span>)</div><div class="line">              <span class="comment">// scalastyle:on println</span></div><div class="line">              exitFn(<span class="number">1</span>)</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">              <span class="keyword">throw</span> e</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">     <span class="comment">// In standalone cluster mode, there are two submission gateways:</span></div><div class="line">     <span class="comment">//   (1) The traditional RPC gateway using o.a.s.deploy.Client as a wrapper</span></div><div class="line">     <span class="comment">//   (2) The new REST-based gateway introduced in Spark 1.3</span></div><div class="line">     <span class="comment">// The latter is the default behavior as of Spark 1.3, but Spark submit will fail over</span></div><div class="line">     <span class="comment">// to use the legacy gateway if the master endpoint turns out to be not a REST server.</span></div><div class="line">。。。</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>接下来执行的是runMain方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//复用反射加载childMainClass</span></div><div class="line"><span class="comment">//调用反射机制加载main方法</span></div><div class="line"><span class="comment">//执行main方法,进入 SparkPi 的main方法，执行spark应用程序</span></div></pre></td></tr></table></figure>
<p>至此，正式完成spark应用程序的提交。</p>
<h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p><a href="http://www.cnblogs.com/xing901022/p/6426408.html" target="_blank" rel="external">http://www.cnblogs.com/xing901022/p/6426408.html</a><br><a href="http://blog.csdn.net/lovehuangjiaju/article/details/49123975" target="_blank" rel="external">http://blog.csdn.net/lovehuangjiaju/article/details/49123975</a></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://flume.cn/2017/02/28/spark-on-yarn-作业提交源码分析/" data-id="cjgd2byfg002qeggu2qmh9596" class="article-share-link">分享</a><div class="tags"><a href="/tags/spark/">spark</a><a href="/tags/yarn/">yarn</a></div><div class="post-nav"><a href="/2017/03/10/Apache-Eagle定义一个Application/" class="pre">Apache-Eagle定义一个Application</a><a href="/2017/02/24/Livy-server的搭建与简单测试/" class="next">Livy-server的搭建与简单测试</a></div><div id="uyan_frame"></div><script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2139407"></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/eagle/">eagle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/flume/">flume</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark开发/">spark开发</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/spring-cloud/">spring cloud</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/单车岁月/">单车岁月</a><span class="category-list-count">5</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/大数据开发/" style="font-size: 15px;">大数据开发</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/hue/" style="font-size: 15px;">hue</a> <a href="/tags/kerberos/" style="font-size: 15px;">kerberos</a> <a href="/tags/livy/" style="font-size: 15px;">livy</a> <a href="/tags/hbase/" style="font-size: 15px;">hbase</a> <a href="/tags/yarn/" style="font-size: 15px;">yarn</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/flume/" style="font-size: 15px;">flume</a> <a href="/tags/spark-streaming/" style="font-size: 15px;">spark streaming</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/redis/" style="font-size: 15px;">redis</a> <a href="/tags/持续更新/" style="font-size: 15px;">持续更新</a> <a href="/tags/spark开发/" style="font-size: 15px;">spark开发</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/散文/" style="font-size: 15px;">散文</a> <a href="/tags/eagle/" style="font-size: 15px;">eagle</a> <a href="/tags/架构/" style="font-size: 15px;">架构</a> <a href="/tags/spring-cloud/" style="font-size: 15px;">spring cloud</a> <a href="/tags/微服务/" style="font-size: 15px;">微服务</a> <a href="/tags/es/" style="font-size: 15px;">es</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/01/08/spark2-2新版本编译打包/">spark2.2新版本编译打包</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/31/使用Ansable安装管理Spark客户端/">使用Ansable安装管理Spark客户端</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/17/spark往ES中写入数据的方法/">spark往ES中写入数据的方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/17/spark指定java版本向yarn提交程序/">spark在yarn中运行jdk8</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/29/返回区域实时人数的思路与总结/">返回区域实时人数的思路与总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/28/位置服务开发上线总结/">位置服务开发上线总结————实时数据推送</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/22/spark奇技淫巧总结之flatMap/">spark奇技淫巧总结之强大的flatMap</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/22/OpenAPI微服务接入规范/">OpenAPI微服务接入规范</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/29/OpenApi之我浅薄见解/">OpenApi之我浅薄见解</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/24/阿里巴巴Java开发手册学习笔记2/">阿里巴巴Java开发手册学习笔记（下）</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://issues.apache.org/jira/secure/Dashboard.jspa" title="有问题上JIRA" target="_blank">有问题上JIRA</a><ul></ul><a href="https://github.com/lw-lin/CoolplaySpark" title="酷玩 Spark" target="_blank">酷玩 Spark</a><ul></ul><a href="http://lqding.blog.51cto.com" title="叮咚的51博客" target="_blank">叮咚的51博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Adam Home.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>